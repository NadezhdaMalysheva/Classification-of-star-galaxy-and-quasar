{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Эксперименты с гиперпараметрами",
      "provenance": [],
      "collapsed_sections": [
        "m0kPZJ5yOVbo",
        "N-hlXdD5xbL8",
        "pB1sVwYkEq9Z"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadezhdaMalysheva/projects/blob/main/Experiments_with_hyper%20parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXFJs19Ks2y",
        "outputId": "46925184-0487-498f-904f-485ce6466fb1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0kPZJ5yOVbo"
      },
      "source": [
        "# Подгружаем необходимые библиотеки "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtFLkOBsdnJR",
        "outputId": "799d3814-4c53-4118-ce43-7472662dd45a"
      },
      "source": [
        "pip install pytorch-tabnet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.41.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLtYHFqwmod"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B2lA0dgTwmib",
        "outputId": "ab2ea8ad-7c9b-4dae-a8ee-2f2abdfd125c"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.offline as py\n",
        "color = sns.color_palette()\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.tools as tls\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVxxcXZGwmdb"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "from hyperopt import hp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
        "\n",
        "from time import time\n",
        "import joblib"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuBGRaaiDoJ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hlXdD5xbL8"
      },
      "source": [
        "#Загружаем данные, параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16jPxCCCBTwd"
      },
      "source": [
        "target = 'class'\n",
        "\n",
        "add_columns = ['subClass', 'objID', 'z', 'zErr', 'ra', 'dec']\n",
        "\n",
        "photo_columns = ['psfMag_u',\t'psfMag_g',\t'psfMag_r',\t'psfMag_i',\t'psfMag_z',\n",
        "                 'cModelMag_u',\t'cModelMag_g',\t'cModelMag_r',\t'cModelMag_i',\t'cModelMag_z']\n",
        "\n",
        "feature_columns = (\n",
        "    photo_columns + add_columns + [target])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIwVGqf2C1KA"
      },
      "source": [
        "#df = joblib.load('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/df_agg.pkl')#pd.read_csv('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/df_agg.csv')\n",
        "#agr_feature = [x for x in df.columns if x not in feature_columns]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yer4R5LMyP0m"
      },
      "source": [
        "def data_preparation(X, y, c=10000, test_size = 0.8):\n",
        "\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(X[y==1], y[y==1], test_size=test_size, random_state = 43)\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(X[y==2], y[y==2], test_size=test_size, random_state = 43)\n",
        "    X3_train, X3_test, y3_train, y3_test = train_test_split(X[y==3], y[y==3], test_size=test_size, random_state = 43)\n",
        "    \n",
        "    count = c\n",
        "    count1 = c\n",
        "\n",
        "    X_train, X_test = np.concatenate((X1_train[:count], X2_train[:count], X3_train[:count])), np.concatenate((X1_test[:count1], X2_test[:count1], X3_test[:count1]))\n",
        "    y_train, y_test = np.concatenate((y1_train[:count], y2_train[:count], y3_train[:count])), np.concatenate((y1_test[:count1], y2_test[:count1], y3_test[:count1]))\n",
        "    \n",
        "\n",
        "    return [X_train, X_test, y_train, y_test, X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test, X3_train, X3_test, y3_train, y3_test]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NopTyppkzaA"
      },
      "source": [
        "def scor(y_test, y_pred):\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "lgb_reg_params = {\n",
        "    'learning_rate':    hp.uniform('learning_rate', 0.001, 0.1),\n",
        "    'min_child_samples':hp.randint('min_child_samples', 50)+1,\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.9),\n",
        "    'num_leaves' :      hp.randint('num_leaves', 100)+10,\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 0.001, 0.99),\n",
        "    'n_estimators':     hp.randint('n_estimators', 1500)+100\n",
        "}\n",
        "lgb_fit_params = {\n",
        "    'early_stopping_rounds': 20,\n",
        "    'verbose': False\n",
        "}\n",
        "lgb_para = dict()\n",
        "lgb_para['reg_params'] = lgb_reg_params\n",
        "lgb_para['fit_params'] = lgb_fit_params\n",
        "lgb_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "\n",
        "rf_reg_params = {\n",
        "    'min_samples_leaf': hp.randint('min_samples_leaf', 20)+1,\n",
        "    'min_samples_split':hp.uniform('min_samples_split', 0.001, 0.1),\n",
        "    #'max_features':     hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
        "    #'learning_rate':    hp.uniform('learning_rate', 0.001, 0.1),\n",
        "    'n_estimators':     hp.randint('n_estimators', 800)+100\n",
        "}\n",
        "rf_fit_params = {\n",
        "}\n",
        "rf_para = dict()\n",
        "rf_para['reg_params'] = rf_reg_params\n",
        "rf_para['fit_params'] = rf_fit_params\n",
        "rf_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "tabnet_reg_params = {\n",
        "    'n_d' :              64,\n",
        "    'n_a' :              64,\n",
        "    'n_steps' :          hp.randint('n_steps', 10-3)+3,\n",
        "    'gamma' :            hp.uniform('gamma', 1.0, 3.0),\n",
        "    'lambda_sparse' :    hp.uniform('lambda_sparse', 0.0, 0.01),\n",
        "    'momentum' :         0.3, \n",
        "    'clip_value' :       2.,\n",
        "    'optimizer_params' : dict(lr=2e-2),\n",
        "    'scheduler_params' : {\"step_size\":50, \"gamma\":0.9},\n",
        "    'scheduler_fn' :     torch.optim.lr_scheduler.StepLR,\n",
        "    'mask_type' :       'entmax'\n",
        "}\n",
        "\n",
        "tabnet_fit_params = {\n",
        "    'max_epochs' : 100, \n",
        "    'patience' : 15,\n",
        "    'batch_size' : 512,\n",
        "    'virtual_batch_size' : 128,\n",
        "    'num_workers' : 0,\n",
        "    'weights' : 1,\n",
        "    'drop_last' : False,\n",
        "    #'from_unsupervised' : unsupervised_model\n",
        "}\n",
        "tabnet_para = dict()\n",
        "tabnet_para['reg_params'] = tabnet_reg_params\n",
        "tabnet_para['fit_params'] = tabnet_fit_params\n",
        "tabnet_para['score'] = lambda y, pred: -accuracy_score(y, pred)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Krr4en2uzv-"
      },
      "source": [
        "target = 'class'\n",
        "\n",
        "add_columns = ['subClass', 'objID', 'z', 'zErr', 'ra', 'dec']\n",
        "\n",
        "photo_columns = ['psfMag_u',\t'psfMag_g',\t'psfMag_r',\t'psfMag_i',\t'psfMag_z',\n",
        "                 'cModelMag_u',\t'cModelMag_g',\t'cModelMag_r',\t'cModelMag_i',\t'cModelMag_z']\n",
        "\n",
        "feature_columns = (\n",
        "    photo_columns + add_columns + [target])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeLm9XEC7z88"
      },
      "source": [
        "X, y  = joblib.load('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/X_y.pkl')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Senx9ogAuulc"
      },
      "source": [
        "# Общая функция"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eonNui1w1Cfx"
      },
      "source": [
        "#dftn_time = pd.DataFrame(columns = ['Rows', 'train', 'test', 'Nd',\t'Na',\t'B',\t'BV',\t'mB',\t'λsparse',\t'Nsteps',\t'γ',\t'learning rate',\t'decay rate',\t'decay iterations', 'shared', \t'decision',\t'mask_type',\t'accuracy_tn', 'time_learn_tn',\t'time_tn',\t'accuracy_gb', 'time_learn_gb',\t'time_gb'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JY_V9ybJ9_o"
      },
      "source": [
        "dftn_time = pd.read_csv('/content/drive/MyDrive/Научная работа/Data/hyper/time.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "2rGmf-Eg3_ht",
        "outputId": "acea6f8d-be54-4b7b-962d-3c59d8282e34"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885889</td>\n",
              "      <td>240.117316</td>\n",
              "      <td>77.182918</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>1.362470</td>\n",
              "      <td>2.113432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.922444</td>\n",
              "      <td>15321.927262</td>\n",
              "      <td>13.301398</td>\n",
              "      <td>0.922553</td>\n",
              "      <td>1216.291787</td>\n",
              "      <td>163.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.923124</td>\n",
              "      <td>12098.683749</td>\n",
              "      <td>13.741256</td>\n",
              "      <td>0.922124</td>\n",
              "      <td>937.402770</td>\n",
              "      <td>120.162277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.923039</td>\n",
              "      <td>15052.750955</td>\n",
              "      <td>20.004159</td>\n",
              "      <td>0.922625</td>\n",
              "      <td>1470.582161</td>\n",
              "      <td>183.236929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rows    train     test  ...  accuracy_gb  time_learn_gb     time_gb\n",
              "0      9000     9000     9000  ...     0.883778       3.483706   16.722444\n",
              "1      9000     9000     9000  ...     0.883778       3.464101   17.349063\n",
              "2      9000     9000     9000  ...     0.883778       3.411332   16.978554\n",
              "3      9000     9000     9000  ...     0.883778       3.509229   17.331297\n",
              "4      9000     9000     9000  ...     0.883778       3.475400   17.451218\n",
              "5     30000    30000    30000  ...     0.891767       8.276289   27.093480\n",
              "6     30000    30000    30000  ...     0.891767       8.210687   27.343578\n",
              "7     30000    30000    30000  ...     0.891767       8.244918   28.102216\n",
              "8     30000    30000    30000  ...     0.891767       8.093423   27.197022\n",
              "9     30000    30000    30000  ...     0.891767       7.857594   27.867677\n",
              "10   300000   300000   300000  ...     0.901553     140.282187   95.826268\n",
              "11   300000   300000   300000  ...     0.901490     122.025770   81.670524\n",
              "12   300000   300000   300000  ...     0.901667     135.688677   95.713423\n",
              "13     9000     9000     9000  ...     0.883778       1.362470    2.113432\n",
              "14  3000000  1912767  1912769  ...     0.922553    1216.291787  163.861900\n",
              "15  3000000  1912767  1912769  ...     0.922124     937.402770  120.162277\n",
              "16  3000000  1912767  1912769  ...     0.922625    1470.582161  183.236929\n",
              "\n",
              "[17 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB1sVwYkEq9Z"
      },
      "source": [
        "##Старые"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqcSoieSu6bG"
      },
      "source": [
        "with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'w') as f:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvnRCetO2baB"
      },
      "source": [
        "def bootstrap_accuracy(model, X_test, y_test):\n",
        "  from sklearn.utils import resample\n",
        "  from matplotlib import pyplot\n",
        "  values = np.concatenate((X_test, y_test.reshape((len(y_test), 1))), axis=1)\n",
        "  n_iterations = 100\n",
        "  n_size = int(len(y_test) * 0.50)\n",
        "  stats = list()\n",
        "  for i in range(n_iterations):\n",
        "    test = resample(values, n_samples=n_size)\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    score = accuracy_score(test[:,-1], predictions)\n",
        "    stats.append(score)\n",
        "  #pyplot.hist(stats, range=(0.872, 0.9))\n",
        "  #pyplot.show()\n",
        "  alpha = 0.97\n",
        "  p = ((1.0-alpha)/2.0) * 100\n",
        "  lower = max(0.0, np.percentile(stats, p))\n",
        "  p = (alpha+((1.0-alpha)/2.0)) * 100\n",
        "  upper = min(1.0, np.percentile(stats, p))\n",
        "  main = np.mean(stats)\n",
        "  #print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
        "  return main, np.max([main-lower, upper-main])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ysnlzZFAZ8T"
      },
      "source": [
        "def bootstrap(X_test, y_test, X_valid, y_valid):\n",
        "  from sklearn.utils import resample\n",
        "  from matplotlib import pyplot\n",
        "  values = np.concatenate((X_test, y_test.reshape((len(y_test), 1))), axis=1)\n",
        "  n_iterations = 100\n",
        "  n_size = int(len(y_test) * 0.50)\n",
        "  stats = list()\n",
        "  for i in range(n_iterations):\n",
        "    train_ind = np.random.randint(0, len(values), n_size)\n",
        "    test_ind = np.setdiff1d(range(len(values)), train_ind)\n",
        "    train = values[train_ind]\n",
        "    test = values[test_ind]\n",
        "    # fit model\n",
        "    model = lgb.LGBMClassifier()\n",
        "    model.fit(train[:,:-1], train[:,-1], eval_set=[(train[:,:-1], train[:,-1]), (X_valid, y_valid)], **lgb_fit_params)\n",
        "    # evaluate model\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    test = resample(values, n_samples=n_size)\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    score = accuracy_score(test[:,-1], predictions)\n",
        "    stats.append(score)\n",
        "  alpha = 0.97\n",
        "  p = ((1.0-alpha)/2.0) * 100\n",
        "  lower = max(0.0, np.percentile(stats, p))\n",
        "  p = (alpha+((1.0-alpha)/2.0)) * 100\n",
        "  upper = min(1.0, np.percentile(stats, p))\n",
        "  main = np.mean(stats)\n",
        "  #print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
        "  return main, np.max([main-lower, upper-main])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0pUcz3TxaRJ"
      },
      "source": [
        "def feature_acc(model, str_m, Rows):\n",
        "  print(str_m)\n",
        "\n",
        "  feature_imp=pd.DataFrame((zip(model.feature_importances_, photo_columns+agr_feature)), columns=['Model','Feature'])\n",
        "  t = 3\n",
        "  feature = feature_imp.sort_values(by='Model', ascending=False).iloc[:t]['Feature'].values\n",
        "  X = df[feature].values\n",
        "\n",
        "  data_split = data_preparation(X, y, Rows, 0.8)\n",
        "  count = Rows//3\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_valid   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  y_valid   = np.concatenate((y1_test[count : 2*count], y2_test[count : 2*count], y3_test[count : 2*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "  acc, err = bootstrap(X_test_norm, y_test, X_valid_norm, y_valid)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc_feature '+str_m+': '+str(acc)+'+-'+str(err)+', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUaTn8q9wItC"
      },
      "source": [
        "def ones(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type):\n",
        "  \n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('number: '+str(number_exp)+', ')\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=50\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 1000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #Accuracy\n",
        "  acc, err = bootstrap_accuracy(tn, X_test, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc TN: '+str(acc)+'+-'+str(err)+', ')\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(tn, 'TN', 9000)\n",
        "  feature_acc(gb, 'GB', 9000)\n",
        "\n",
        "  #save model\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.txt')\n",
        "  tn.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNijQemux8TZ"
      },
      "source": [
        "def memory(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type):\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.5)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  #X_valid      = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  #y_valid      = np.concatenate((y1_test[count : 2*count], y2_test[count : 2*count], y3_test[count : 2*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  #X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=50\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 1000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_test_norm, y_test)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #save model\n",
        "\n",
        "  joblib.dump(gb, '/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.pkl')\n",
        "  joblib.dump(tn, '/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp)+'.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoqLuZDE2wh"
      },
      "source": [
        "##Новая"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei75tuaUSggZ"
      },
      "source": [
        "X_main_test = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/X_main_test.pkl')\n",
        "y_main_test = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/y_main_test.pkl')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApXy6-E1kyv"
      },
      "source": [
        "def time_model(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type):\n",
        "\n",
        "  global dftn_time\n",
        "  global X_main_test \n",
        "  global y_main_test\n",
        "\n",
        "  data = {'Rows':Rows, 'Nd':Nd,\t'Na':Na,\t'B':B,\t'BV':BV,\t'mB':mB,\t'λsparse':λsparse,\t'Nsteps':Nsteps,\t'γ':γ,\t'learning rate':learning_rate,\t'decay rate':decay_rate,\t'decay iterations':decay_iterations,\t'shared':shared, \t'decision':decision,\t'mask_type':mask_type}\n",
        "  'accuracy_tn', 'time_learn_tn',\t'time_tn',\t'accuracy_gb', 'time_learn_tn',\t'time_gb'\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.5)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  data['train'] = len(y_train)\n",
        "  data['test'] = len(y_test)\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_valid = X_test\n",
        "  y_valid = y_test\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_main_test_norm = robust.transform(X_main_test)\n",
        "  X_valid_norm = X_test_norm\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=50\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  data['time_learn_tn'] = t\n",
        "\n",
        "  t = time()\n",
        "  pred = tn.predict(X_main_test)\n",
        "  t = time() - t\n",
        "  data['time_tn'] = t\n",
        "  print('Confusion Matrix: \\n', confusion_matrix(y_main_test, pred))\n",
        "  acc = accuracy_score(y_test, tn.predict(X_test))\n",
        "  data['accuracy_tn'] = acc\n",
        "  print('Testing Score: ', acc)\n",
        "\n",
        "  gb = lgb.LGBMClassifier( **{'n_estimators': 1000})\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  data['time_learn_gb'] = t\n",
        "\n",
        "  t = time()\n",
        "  pred = gb.predict(X_main_test_norm)\n",
        "  t = time() - t\n",
        "  data['time_gb'] = t\n",
        "  print('Confusion Matrix: \\n', confusion_matrix(y_main_test, pred))\n",
        "  acc = accuracy_score(y_test, gb.predict(X_test_norm))\n",
        "  data['accuracy_gb'] = acc\n",
        "  print('Testing Score: ', acc)\n",
        "  #save model\n",
        "  print(data)\n",
        "  dftn_time = dftn_time.append(data, ignore_index=True)\n",
        "  dftn_time.to_csv('/content/drive/MyDrive/Научная работа/Data/hyper/time.csv', index=False)\n",
        "\n",
        "  joblib.dump(gb, '/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.pkl')\n",
        "  joblib.dump(tn, '/content/drive/MyDrive/Научная работа/Data/hyper/53/tn'+str(number_exp)+'.pkl')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGuS7NKi9x6T"
      },
      "source": [
        "#Запускаем тесты по времени (в данном эксперименте не так важна точность, она будет проверяться дальше)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VemFf9oi99hV"
      },
      "source": [
        "##9000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29n-76Co_nSM",
        "outputId": "54444325-2023-4ffa-c133-78d4208914c1"
      },
      "source": [
        "time_model(number_exp=1.1, \n",
        "     Rows=9000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.99334 | train_logloss: 9.61676 | train_accuracy: 0.248   | valid_logloss: 9.46599 | valid_accuracy: 0.24978 |  0:00:01s\n",
            "epoch 1  | loss: 0.68227 | train_logloss: 8.78314 | train_accuracy: 0.48922 | valid_logloss: 8.69884 | valid_accuracy: 0.50467 |  0:00:03s\n",
            "epoch 2  | loss: 0.56358 | train_logloss: 15.50828| train_accuracy: 0.26867 | valid_logloss: 15.43422| valid_accuracy: 0.272   |  0:00:04s\n",
            "epoch 3  | loss: 0.49315 | train_logloss: 8.84363 | train_accuracy: 0.43444 | valid_logloss: 9.04509 | valid_accuracy: 0.42233 |  0:00:06s\n",
            "epoch 4  | loss: 0.44271 | train_logloss: 8.63715 | train_accuracy: 0.36622 | valid_logloss: 8.71142 | valid_accuracy: 0.37589 |  0:00:07s\n",
            "epoch 5  | loss: 0.42997 | train_logloss: 22.82182| train_accuracy: 0.09344 | valid_logloss: 22.8533 | valid_accuracy: 0.09333 |  0:00:09s\n",
            "epoch 6  | loss: 0.406   | train_logloss: 8.37036 | train_accuracy: 0.17756 | valid_logloss: 8.27681 | valid_accuracy: 0.181   |  0:00:10s\n",
            "epoch 7  | loss: 0.39034 | train_logloss: 2.66046 | train_accuracy: 0.35189 | valid_logloss: 2.70199 | valid_accuracy: 0.34422 |  0:00:12s\n",
            "epoch 8  | loss: 0.38586 | train_logloss: 4.00333 | train_accuracy: 0.23489 | valid_logloss: 4.00134 | valid_accuracy: 0.23467 |  0:00:13s\n",
            "epoch 9  | loss: 0.37417 | train_logloss: 1.81245 | train_accuracy: 0.28822 | valid_logloss: 1.78189 | valid_accuracy: 0.28822 |  0:00:15s\n",
            "epoch 10 | loss: 0.36736 | train_logloss: 1.81863 | train_accuracy: 0.37544 | valid_logloss: 1.8108  | valid_accuracy: 0.37067 |  0:00:16s\n",
            "epoch 11 | loss: 0.36832 | train_logloss: 2.03747 | train_accuracy: 0.15622 | valid_logloss: 2.01705 | valid_accuracy: 0.15789 |  0:00:17s\n",
            "epoch 12 | loss: 0.3602  | train_logloss: 2.04725 | train_accuracy: 0.26233 | valid_logloss: 2.07709 | valid_accuracy: 0.26389 |  0:00:19s\n",
            "epoch 13 | loss: 0.35502 | train_logloss: 2.99412 | train_accuracy: 0.28233 | valid_logloss: 2.93564 | valid_accuracy: 0.27467 |  0:00:20s\n",
            "epoch 14 | loss: 0.34515 | train_logloss: 7.00027 | train_accuracy: 0.15089 | valid_logloss: 6.91118 | valid_accuracy: 0.14922 |  0:00:22s\n",
            "epoch 15 | loss: 0.34708 | train_logloss: 6.42711 | train_accuracy: 0.28067 | valid_logloss: 6.38347 | valid_accuracy: 0.27656 |  0:00:23s\n",
            "epoch 16 | loss: 0.33471 | train_logloss: 8.75824 | train_accuracy: 0.27033 | valid_logloss: 8.78729 | valid_accuracy: 0.27    |  0:00:24s\n",
            "epoch 17 | loss: 0.32745 | train_logloss: 5.44567 | train_accuracy: 0.31222 | valid_logloss: 5.49419 | valid_accuracy: 0.30778 |  0:00:26s\n",
            "epoch 18 | loss: 0.32417 | train_logloss: 6.92329 | train_accuracy: 0.25489 | valid_logloss: 6.96067 | valid_accuracy: 0.24944 |  0:00:27s\n",
            "epoch 19 | loss: 0.32094 | train_logloss: 5.64353 | train_accuracy: 0.30322 | valid_logloss: 5.73919 | valid_accuracy: 0.29356 |  0:00:29s\n",
            "epoch 20 | loss: 0.31464 | train_logloss: 4.79926 | train_accuracy: 0.34489 | valid_logloss: 4.85517 | valid_accuracy: 0.34589 |  0:00:30s\n",
            "epoch 21 | loss: 0.31952 | train_logloss: 4.20455 | train_accuracy: 0.30278 | valid_logloss: 4.22172 | valid_accuracy: 0.29633 |  0:00:32s\n",
            "epoch 22 | loss: 0.31328 | train_logloss: 4.63174 | train_accuracy: 0.25889 | valid_logloss: 4.66433 | valid_accuracy: 0.254   |  0:00:33s\n",
            "epoch 23 | loss: 0.31146 | train_logloss: 4.15204 | train_accuracy: 0.26578 | valid_logloss: 4.14145 | valid_accuracy: 0.257   |  0:00:35s\n",
            "epoch 24 | loss: 0.30913 | train_logloss: 5.07505 | train_accuracy: 0.231   | valid_logloss: 5.07001 | valid_accuracy: 0.23056 |  0:00:36s\n",
            "epoch 25 | loss: 0.3091  | train_logloss: 4.57009 | train_accuracy: 0.17511 | valid_logloss: 4.56756 | valid_accuracy: 0.17244 |  0:00:37s\n",
            "epoch 26 | loss: 0.30503 | train_logloss: 3.32927 | train_accuracy: 0.22967 | valid_logloss: 3.33473 | valid_accuracy: 0.22167 |  0:00:39s\n",
            "epoch 27 | loss: 0.30343 | train_logloss: 2.2625  | train_accuracy: 0.30889 | valid_logloss: 2.23921 | valid_accuracy: 0.305   |  0:00:40s\n",
            "epoch 28 | loss: 0.30385 | train_logloss: 2.53676 | train_accuracy: 0.36644 | valid_logloss: 2.51397 | valid_accuracy: 0.36889 |  0:00:42s\n",
            "epoch 29 | loss: 0.30351 | train_logloss: 3.46642 | train_accuracy: 0.354   | valid_logloss: 3.45283 | valid_accuracy: 0.35689 |  0:00:43s\n",
            "epoch 30 | loss: 0.30486 | train_logloss: 2.84356 | train_accuracy: 0.38967 | valid_logloss: 2.83498 | valid_accuracy: 0.39267 |  0:00:45s\n",
            "epoch 31 | loss: 0.31138 | train_logloss: 2.12903 | train_accuracy: 0.42833 | valid_logloss: 2.10592 | valid_accuracy: 0.433   |  0:00:46s\n",
            "epoch 32 | loss: 0.3094  | train_logloss: 1.33507 | train_accuracy: 0.53    | valid_logloss: 1.33201 | valid_accuracy: 0.52811 |  0:00:48s\n",
            "epoch 33 | loss: 0.30697 | train_logloss: 1.599   | train_accuracy: 0.51456 | valid_logloss: 1.58878 | valid_accuracy: 0.51467 |  0:00:49s\n",
            "epoch 34 | loss: 0.30069 | train_logloss: 2.41833 | train_accuracy: 0.46056 | valid_logloss: 2.40926 | valid_accuracy: 0.46089 |  0:00:51s\n",
            "epoch 35 | loss: 0.3033  | train_logloss: 2.16347 | train_accuracy: 0.46322 | valid_logloss: 2.16784 | valid_accuracy: 0.46311 |  0:00:52s\n",
            "epoch 36 | loss: 0.29482 | train_logloss: 1.57279 | train_accuracy: 0.46444 | valid_logloss: 1.59003 | valid_accuracy: 0.463   |  0:00:54s\n",
            "epoch 37 | loss: 0.29483 | train_logloss: 1.21912 | train_accuracy: 0.53733 | valid_logloss: 1.21521 | valid_accuracy: 0.54189 |  0:00:55s\n",
            "epoch 38 | loss: 0.29888 | train_logloss: 1.17338 | train_accuracy: 0.59778 | valid_logloss: 1.14764 | valid_accuracy: 0.60511 |  0:00:57s\n",
            "epoch 39 | loss: 0.30071 | train_logloss: 1.2835  | train_accuracy: 0.59278 | valid_logloss: 1.24615 | valid_accuracy: 0.60056 |  0:00:58s\n",
            "epoch 40 | loss: 0.3011  | train_logloss: 1.03834 | train_accuracy: 0.642   | valid_logloss: 1.03712 | valid_accuracy: 0.65222 |  0:01:00s\n",
            "epoch 41 | loss: 0.30136 | train_logloss: 1.11719 | train_accuracy: 0.60944 | valid_logloss: 1.12021 | valid_accuracy: 0.61389 |  0:01:01s\n",
            "epoch 42 | loss: 0.29814 | train_logloss: 1.20926 | train_accuracy: 0.60844 | valid_logloss: 1.24134 | valid_accuracy: 0.60122 |  0:01:02s\n",
            "epoch 43 | loss: 0.30006 | train_logloss: 1.00751 | train_accuracy: 0.61556 | valid_logloss: 1.03074 | valid_accuracy: 0.60767 |  0:01:04s\n",
            "epoch 44 | loss: 0.30025 | train_logloss: 0.9033  | train_accuracy: 0.62711 | valid_logloss: 0.93073 | valid_accuracy: 0.62033 |  0:01:06s\n",
            "epoch 45 | loss: 0.29425 | train_logloss: 0.79578 | train_accuracy: 0.67844 | valid_logloss: 0.80889 | valid_accuracy: 0.67778 |  0:01:07s\n",
            "epoch 46 | loss: 0.29303 | train_logloss: 0.86827 | train_accuracy: 0.67244 | valid_logloss: 0.89807 | valid_accuracy: 0.66956 |  0:01:09s\n",
            "epoch 47 | loss: 0.28852 | train_logloss: 0.9151  | train_accuracy: 0.69589 | valid_logloss: 0.93211 | valid_accuracy: 0.69189 |  0:01:10s\n",
            "epoch 48 | loss: 0.28708 | train_logloss: 0.75309 | train_accuracy: 0.72278 | valid_logloss: 0.77149 | valid_accuracy: 0.71711 |  0:01:11s\n",
            "epoch 49 | loss: 0.28249 | train_logloss: 0.83609 | train_accuracy: 0.70322 | valid_logloss: 0.85188 | valid_accuracy: 0.69944 |  0:01:13s\n",
            "epoch 50 | loss: 0.28408 | train_logloss: 0.72057 | train_accuracy: 0.73922 | valid_logloss: 0.73621 | valid_accuracy: 0.73533 |  0:01:14s\n",
            "epoch 51 | loss: 0.2794  | train_logloss: 0.70946 | train_accuracy: 0.73167 | valid_logloss: 0.73456 | valid_accuracy: 0.72478 |  0:01:16s\n",
            "epoch 52 | loss: 0.28072 | train_logloss: 0.68271 | train_accuracy: 0.76278 | valid_logloss: 0.70726 | valid_accuracy: 0.75544 |  0:01:17s\n",
            "epoch 53 | loss: 0.28017 | train_logloss: 0.66661 | train_accuracy: 0.76111 | valid_logloss: 0.6934  | valid_accuracy: 0.75533 |  0:01:19s\n",
            "epoch 54 | loss: 0.27631 | train_logloss: 0.70291 | train_accuracy: 0.75356 | valid_logloss: 0.74052 | valid_accuracy: 0.74889 |  0:01:20s\n",
            "epoch 55 | loss: 0.28073 | train_logloss: 0.50472 | train_accuracy: 0.812   | valid_logloss: 0.53335 | valid_accuracy: 0.81189 |  0:01:22s\n",
            "epoch 56 | loss: 0.28108 | train_logloss: 0.48548 | train_accuracy: 0.81044 | valid_logloss: 0.51358 | valid_accuracy: 0.81478 |  0:01:23s\n",
            "epoch 57 | loss: 0.27591 | train_logloss: 0.51659 | train_accuracy: 0.801   | valid_logloss: 0.54458 | valid_accuracy: 0.79644 |  0:01:25s\n",
            "epoch 58 | loss: 0.27597 | train_logloss: 0.43666 | train_accuracy: 0.82478 | valid_logloss: 0.47304 | valid_accuracy: 0.81833 |  0:01:26s\n",
            "epoch 59 | loss: 0.27317 | train_logloss: 0.47521 | train_accuracy: 0.80789 | valid_logloss: 0.51605 | valid_accuracy: 0.80278 |  0:01:28s\n",
            "epoch 60 | loss: 0.27129 | train_logloss: 0.48318 | train_accuracy: 0.81367 | valid_logloss: 0.5266  | valid_accuracy: 0.80611 |  0:01:29s\n",
            "epoch 61 | loss: 0.26754 | train_logloss: 0.39234 | train_accuracy: 0.84333 | valid_logloss: 0.43657 | valid_accuracy: 0.83544 |  0:01:31s\n",
            "epoch 62 | loss: 0.27282 | train_logloss: 0.41618 | train_accuracy: 0.83778 | valid_logloss: 0.4614  | valid_accuracy: 0.83411 |  0:01:32s\n",
            "epoch 63 | loss: 0.28545 | train_logloss: 0.3522  | train_accuracy: 0.86444 | valid_logloss: 0.40061 | valid_accuracy: 0.85833 |  0:01:33s\n",
            "epoch 64 | loss: 0.28782 | train_logloss: 0.34134 | train_accuracy: 0.86889 | valid_logloss: 0.38577 | valid_accuracy: 0.86344 |  0:01:35s\n",
            "epoch 65 | loss: 0.28602 | train_logloss: 0.34475 | train_accuracy: 0.866   | valid_logloss: 0.40027 | valid_accuracy: 0.85133 |  0:01:36s\n",
            "epoch 66 | loss: 0.27856 | train_logloss: 0.33926 | train_accuracy: 0.87189 | valid_logloss: 0.38581 | valid_accuracy: 0.86244 |  0:01:38s\n",
            "epoch 67 | loss: 0.27942 | train_logloss: 0.32937 | train_accuracy: 0.87556 | valid_logloss: 0.37907 | valid_accuracy: 0.86778 |  0:01:39s\n",
            "epoch 68 | loss: 0.27158 | train_logloss: 0.31826 | train_accuracy: 0.87789 | valid_logloss: 0.37078 | valid_accuracy: 0.86778 |  0:01:41s\n",
            "epoch 69 | loss: 0.26718 | train_logloss: 0.31199 | train_accuracy: 0.88156 | valid_logloss: 0.3688  | valid_accuracy: 0.86967 |  0:01:42s\n",
            "epoch 70 | loss: 0.26777 | train_logloss: 0.3192  | train_accuracy: 0.87689 | valid_logloss: 0.37399 | valid_accuracy: 0.87078 |  0:01:44s\n",
            "epoch 71 | loss: 0.26831 | train_logloss: 0.31638 | train_accuracy: 0.87833 | valid_logloss: 0.37235 | valid_accuracy: 0.86767 |  0:01:45s\n",
            "epoch 72 | loss: 0.27478 | train_logloss: 0.32977 | train_accuracy: 0.87111 | valid_logloss: 0.38489 | valid_accuracy: 0.86178 |  0:01:47s\n",
            "epoch 73 | loss: 0.26986 | train_logloss: 0.3096  | train_accuracy: 0.87689 | valid_logloss: 0.37093 | valid_accuracy: 0.86656 |  0:01:48s\n",
            "epoch 74 | loss: 0.27214 | train_logloss: 0.30823 | train_accuracy: 0.88    | valid_logloss: 0.37148 | valid_accuracy: 0.87011 |  0:01:49s\n",
            "epoch 75 | loss: 0.26597 | train_logloss: 0.30404 | train_accuracy: 0.88578 | valid_logloss: 0.35968 | valid_accuracy: 0.87267 |  0:01:51s\n",
            "epoch 76 | loss: 0.26265 | train_logloss: 0.29507 | train_accuracy: 0.88811 | valid_logloss: 0.36133 | valid_accuracy: 0.87444 |  0:01:52s\n",
            "epoch 77 | loss: 0.26262 | train_logloss: 0.2989  | train_accuracy: 0.88722 | valid_logloss: 0.36705 | valid_accuracy: 0.87544 |  0:01:54s\n",
            "epoch 78 | loss: 0.26755 | train_logloss: 0.29389 | train_accuracy: 0.88311 | valid_logloss: 0.36136 | valid_accuracy: 0.87433 |  0:01:55s\n",
            "epoch 79 | loss: 0.26861 | train_logloss: 0.28902 | train_accuracy: 0.884   | valid_logloss: 0.36112 | valid_accuracy: 0.87122 |  0:01:57s\n",
            "epoch 80 | loss: 0.26595 | train_logloss: 0.30344 | train_accuracy: 0.882   | valid_logloss: 0.36348 | valid_accuracy: 0.87133 |  0:01:58s\n",
            "epoch 81 | loss: 0.28431 | train_logloss: 0.31835 | train_accuracy: 0.87533 | valid_logloss: 0.37395 | valid_accuracy: 0.86511 |  0:02:00s\n",
            "epoch 82 | loss: 0.27468 | train_logloss: 0.29227 | train_accuracy: 0.88489 | valid_logloss: 0.36099 | valid_accuracy: 0.87222 |  0:02:01s\n",
            "epoch 83 | loss: 0.27331 | train_logloss: 0.28668 | train_accuracy: 0.88844 | valid_logloss: 0.3547  | valid_accuracy: 0.876   |  0:02:02s\n",
            "epoch 84 | loss: 0.26876 | train_logloss: 0.27105 | train_accuracy: 0.89522 | valid_logloss: 0.34497 | valid_accuracy: 0.87911 |  0:02:04s\n",
            "epoch 85 | loss: 0.25959 | train_logloss: 0.27625 | train_accuracy: 0.89244 | valid_logloss: 0.34777 | valid_accuracy: 0.88011 |  0:02:05s\n",
            "epoch 86 | loss: 0.26305 | train_logloss: 0.27042 | train_accuracy: 0.89511 | valid_logloss: 0.35021 | valid_accuracy: 0.87911 |  0:02:07s\n",
            "epoch 87 | loss: 0.26122 | train_logloss: 0.2648  | train_accuracy: 0.89522 | valid_logloss: 0.35398 | valid_accuracy: 0.87889 |  0:02:08s\n",
            "epoch 88 | loss: 0.25987 | train_logloss: 0.27309 | train_accuracy: 0.89322 | valid_logloss: 0.35222 | valid_accuracy: 0.87922 |  0:02:10s\n",
            "epoch 89 | loss: 0.2673  | train_logloss: 0.27292 | train_accuracy: 0.89322 | valid_logloss: 0.34521 | valid_accuracy: 0.87944 |  0:02:11s\n",
            "epoch 90 | loss: 0.26663 | train_logloss: 0.27653 | train_accuracy: 0.89478 | valid_logloss: 0.34291 | valid_accuracy: 0.87867 |  0:02:13s\n",
            "epoch 91 | loss: 0.27414 | train_logloss: 0.2783  | train_accuracy: 0.89144 | valid_logloss: 0.34213 | valid_accuracy: 0.88022 |  0:02:14s\n",
            "epoch 92 | loss: 0.27016 | train_logloss: 0.26224 | train_accuracy: 0.89756 | valid_logloss: 0.33713 | valid_accuracy: 0.88456 |  0:02:16s\n",
            "epoch 93 | loss: 0.26699 | train_logloss: 0.27162 | train_accuracy: 0.89589 | valid_logloss: 0.34417 | valid_accuracy: 0.88    |  0:02:17s\n",
            "epoch 94 | loss: 0.26515 | train_logloss: 0.27214 | train_accuracy: 0.89367 | valid_logloss: 0.35011 | valid_accuracy: 0.87956 |  0:02:19s\n",
            "epoch 95 | loss: 0.26393 | train_logloss: 0.25865 | train_accuracy: 0.89767 | valid_logloss: 0.34194 | valid_accuracy: 0.88411 |  0:02:20s\n",
            "epoch 96 | loss: 0.26355 | train_logloss: 0.25866 | train_accuracy: 0.896   | valid_logloss: 0.34757 | valid_accuracy: 0.88089 |  0:02:22s\n",
            "epoch 97 | loss: 0.25843 | train_logloss: 0.25242 | train_accuracy: 0.89867 | valid_logloss: 0.33491 | valid_accuracy: 0.88433 |  0:02:23s\n",
            "epoch 98 | loss: 0.25831 | train_logloss: 0.25459 | train_accuracy: 0.899   | valid_logloss: 0.33509 | valid_accuracy: 0.87911 |  0:02:24s\n",
            "epoch 99 | loss: 0.25457 | train_logloss: 0.24543 | train_accuracy: 0.90533 | valid_logloss: 0.33936 | valid_accuracy: 0.88122 |  0:02:26s\n",
            "epoch 100| loss: 0.25512 | train_logloss: 0.25955 | train_accuracy: 0.89889 | valid_logloss: 0.3435  | valid_accuracy: 0.88033 |  0:02:27s\n",
            "epoch 101| loss: 0.25468 | train_logloss: 0.25047 | train_accuracy: 0.901   | valid_logloss: 0.34131 | valid_accuracy: 0.88222 |  0:02:29s\n",
            "epoch 102| loss: 0.25138 | train_logloss: 0.24382 | train_accuracy: 0.90456 | valid_logloss: 0.34031 | valid_accuracy: 0.88211 |  0:02:30s\n",
            "epoch 103| loss: 0.25678 | train_logloss: 0.2484  | train_accuracy: 0.90189 | valid_logloss: 0.34545 | valid_accuracy: 0.87967 |  0:02:32s\n",
            "epoch 104| loss: 0.25824 | train_logloss: 0.24807 | train_accuracy: 0.90244 | valid_logloss: 0.34615 | valid_accuracy: 0.88378 |  0:02:34s\n",
            "epoch 105| loss: 0.25684 | train_logloss: 0.2523  | train_accuracy: 0.9     | valid_logloss: 0.36018 | valid_accuracy: 0.87522 |  0:02:35s\n",
            "epoch 106| loss: 0.25117 | train_logloss: 0.24888 | train_accuracy: 0.90278 | valid_logloss: 0.34204 | valid_accuracy: 0.88178 |  0:02:37s\n",
            "epoch 107| loss: 0.25659 | train_logloss: 0.25411 | train_accuracy: 0.90411 | valid_logloss: 0.35247 | valid_accuracy: 0.88044 |  0:02:38s\n",
            "epoch 108| loss: 0.25863 | train_logloss: 0.24412 | train_accuracy: 0.904   | valid_logloss: 0.3434  | valid_accuracy: 0.88311 |  0:02:39s\n",
            "epoch 109| loss: 0.25055 | train_logloss: 0.24047 | train_accuracy: 0.90567 | valid_logloss: 0.33989 | valid_accuracy: 0.88578 |  0:02:41s\n",
            "epoch 110| loss: 0.25173 | train_logloss: 0.24409 | train_accuracy: 0.90211 | valid_logloss: 0.34413 | valid_accuracy: 0.88311 |  0:02:42s\n",
            "epoch 111| loss: 0.24929 | train_logloss: 0.23596 | train_accuracy: 0.90667 | valid_logloss: 0.34896 | valid_accuracy: 0.88411 |  0:02:44s\n",
            "epoch 112| loss: 0.24371 | train_logloss: 0.2336  | train_accuracy: 0.90611 | valid_logloss: 0.33964 | valid_accuracy: 0.88589 |  0:02:45s\n",
            "epoch 113| loss: 0.24795 | train_logloss: 0.24898 | train_accuracy: 0.90189 | valid_logloss: 0.35284 | valid_accuracy: 0.87967 |  0:02:47s\n",
            "epoch 114| loss: 0.24863 | train_logloss: 0.23686 | train_accuracy: 0.906   | valid_logloss: 0.35228 | valid_accuracy: 0.88222 |  0:02:48s\n",
            "epoch 115| loss: 0.24146 | train_logloss: 0.24027 | train_accuracy: 0.90611 | valid_logloss: 0.35827 | valid_accuracy: 0.88244 |  0:02:50s\n",
            "epoch 116| loss: 0.24312 | train_logloss: 0.23385 | train_accuracy: 0.91033 | valid_logloss: 0.34275 | valid_accuracy: 0.88244 |  0:02:51s\n",
            "epoch 117| loss: 0.24462 | train_logloss: 0.236   | train_accuracy: 0.90711 | valid_logloss: 0.36062 | valid_accuracy: 0.87967 |  0:02:53s\n",
            "epoch 118| loss: 0.24257 | train_logloss: 0.23082 | train_accuracy: 0.91022 | valid_logloss: 0.35594 | valid_accuracy: 0.88389 |  0:02:54s\n",
            "epoch 119| loss: 0.23837 | train_logloss: 0.23231 | train_accuracy: 0.90811 | valid_logloss: 0.3572  | valid_accuracy: 0.88011 |  0:02:56s\n",
            "epoch 120| loss: 0.23988 | train_logloss: 0.23214 | train_accuracy: 0.907   | valid_logloss: 0.36186 | valid_accuracy: 0.88389 |  0:02:57s\n",
            "epoch 121| loss: 0.24566 | train_logloss: 0.23639 | train_accuracy: 0.90733 | valid_logloss: 0.35707 | valid_accuracy: 0.88167 |  0:02:59s\n",
            "epoch 122| loss: 0.24804 | train_logloss: 0.24456 | train_accuracy: 0.90689 | valid_logloss: 0.3592  | valid_accuracy: 0.87944 |  0:03:00s\n",
            "epoch 123| loss: 0.25345 | train_logloss: 0.23874 | train_accuracy: 0.904   | valid_logloss: 0.35117 | valid_accuracy: 0.88022 |  0:03:01s\n",
            "epoch 124| loss: 0.24632 | train_logloss: 0.23342 | train_accuracy: 0.90867 | valid_logloss: 0.34816 | valid_accuracy: 0.88211 |  0:03:03s\n",
            "epoch 125| loss: 0.24009 | train_logloss: 0.22561 | train_accuracy: 0.91133 | valid_logloss: 0.35446 | valid_accuracy: 0.88267 |  0:03:05s\n",
            "epoch 126| loss: 0.24062 | train_logloss: 0.23259 | train_accuracy: 0.90722 | valid_logloss: 0.35494 | valid_accuracy: 0.88289 |  0:03:06s\n",
            "epoch 127| loss: 0.24391 | train_logloss: 0.23005 | train_accuracy: 0.90922 | valid_logloss: 0.3593  | valid_accuracy: 0.88067 |  0:03:08s\n",
            "epoch 128| loss: 0.24047 | train_logloss: 0.22952 | train_accuracy: 0.91044 | valid_logloss: 0.35651 | valid_accuracy: 0.87856 |  0:03:09s\n",
            "epoch 129| loss: 0.24919 | train_logloss: 0.23743 | train_accuracy: 0.90411 | valid_logloss: 0.35459 | valid_accuracy: 0.88278 |  0:03:11s\n",
            "epoch 130| loss: 0.24426 | train_logloss: 0.23095 | train_accuracy: 0.90744 | valid_logloss: 0.35142 | valid_accuracy: 0.88322 |  0:03:12s\n",
            "epoch 131| loss: 0.24269 | train_logloss: 0.24328 | train_accuracy: 0.90311 | valid_logloss: 0.36322 | valid_accuracy: 0.87922 |  0:03:14s\n",
            "epoch 132| loss: 0.25084 | train_logloss: 0.23669 | train_accuracy: 0.90478 | valid_logloss: 0.3671  | valid_accuracy: 0.87978 |  0:03:15s\n",
            "epoch 133| loss: 0.24231 | train_logloss: 0.22907 | train_accuracy: 0.90878 | valid_logloss: 0.35543 | valid_accuracy: 0.88089 |  0:03:17s\n",
            "epoch 134| loss: 0.24206 | train_logloss: 0.23209 | train_accuracy: 0.90822 | valid_logloss: 0.34952 | valid_accuracy: 0.88411 |  0:03:18s\n",
            "epoch 135| loss: 0.23738 | train_logloss: 0.22768 | train_accuracy: 0.91233 | valid_logloss: 0.35515 | valid_accuracy: 0.88067 |  0:03:20s\n",
            "epoch 136| loss: 0.2406  | train_logloss: 0.23347 | train_accuracy: 0.90733 | valid_logloss: 0.36526 | valid_accuracy: 0.88333 |  0:03:21s\n",
            "epoch 137| loss: 0.23628 | train_logloss: 0.22391 | train_accuracy: 0.91278 | valid_logloss: 0.37374 | valid_accuracy: 0.88    |  0:03:22s\n",
            "epoch 138| loss: 0.23576 | train_logloss: 0.22661 | train_accuracy: 0.91033 | valid_logloss: 0.36771 | valid_accuracy: 0.88189 |  0:03:24s\n",
            "epoch 139| loss: 0.23577 | train_logloss: 0.22094 | train_accuracy: 0.912   | valid_logloss: 0.37238 | valid_accuracy: 0.87956 |  0:03:25s\n",
            "epoch 140| loss: 0.23333 | train_logloss: 0.2241  | train_accuracy: 0.91122 | valid_logloss: 0.36685 | valid_accuracy: 0.88344 |  0:03:27s\n",
            "epoch 141| loss: 0.23325 | train_logloss: 0.22822 | train_accuracy: 0.90822 | valid_logloss: 0.35196 | valid_accuracy: 0.88533 |  0:03:28s\n",
            "epoch 142| loss: 0.23871 | train_logloss: 0.22133 | train_accuracy: 0.91456 | valid_logloss: 0.35535 | valid_accuracy: 0.88244 |  0:03:30s\n",
            "epoch 143| loss: 0.23897 | train_logloss: 0.22703 | train_accuracy: 0.90922 | valid_logloss: 0.3697  | valid_accuracy: 0.87844 |  0:03:31s\n",
            "epoch 144| loss: 0.23516 | train_logloss: 0.22022 | train_accuracy: 0.91111 | valid_logloss: 0.36697 | valid_accuracy: 0.88    |  0:03:33s\n",
            "epoch 145| loss: 0.23245 | train_logloss: 0.22542 | train_accuracy: 0.91    | valid_logloss: 0.35749 | valid_accuracy: 0.88422 |  0:03:34s\n",
            "epoch 146| loss: 0.23151 | train_logloss: 0.2153  | train_accuracy: 0.914   | valid_logloss: 0.36772 | valid_accuracy: 0.87922 |  0:03:36s\n",
            "epoch 147| loss: 0.23109 | train_logloss: 0.22686 | train_accuracy: 0.90744 | valid_logloss: 0.36292 | valid_accuracy: 0.88578 |  0:03:37s\n",
            "epoch 148| loss: 0.23989 | train_logloss: 0.23383 | train_accuracy: 0.90833 | valid_logloss: 0.37328 | valid_accuracy: 0.87889 |  0:03:39s\n",
            "epoch 149| loss: 0.24802 | train_logloss: 0.23219 | train_accuracy: 0.90856 | valid_logloss: 0.3783  | valid_accuracy: 0.87567 |  0:03:40s\n",
            "epoch 150| loss: 0.24271 | train_logloss: 0.22837 | train_accuracy: 0.91044 | valid_logloss: 0.36558 | valid_accuracy: 0.88289 |  0:03:41s\n",
            "epoch 151| loss: 0.24377 | train_logloss: 0.23343 | train_accuracy: 0.90889 | valid_logloss: 0.35767 | valid_accuracy: 0.88189 |  0:03:43s\n",
            "epoch 152| loss: 0.2475  | train_logloss: 0.23418 | train_accuracy: 0.90711 | valid_logloss: 0.35806 | valid_accuracy: 0.88144 |  0:03:44s\n",
            "epoch 153| loss: 0.23774 | train_logloss: 0.22643 | train_accuracy: 0.90889 | valid_logloss: 0.35146 | valid_accuracy: 0.87889 |  0:03:46s\n",
            "epoch 154| loss: 0.2387  | train_logloss: 0.22333 | train_accuracy: 0.91178 | valid_logloss: 0.37073 | valid_accuracy: 0.87811 |  0:03:47s\n",
            "epoch 155| loss: 0.23368 | train_logloss: 0.2184  | train_accuracy: 0.91078 | valid_logloss: 0.36524 | valid_accuracy: 0.87933 |  0:03:49s\n",
            "epoch 156| loss: 0.23045 | train_logloss: 0.24821 | train_accuracy: 0.901   | valid_logloss: 0.39089 | valid_accuracy: 0.86867 |  0:03:50s\n",
            "epoch 157| loss: 0.25713 | train_logloss: 0.25722 | train_accuracy: 0.90022 | valid_logloss: 0.38302 | valid_accuracy: 0.87522 |  0:03:52s\n",
            "epoch 158| loss: 0.25976 | train_logloss: 0.24362 | train_accuracy: 0.907   | valid_logloss: 0.37803 | valid_accuracy: 0.88122 |  0:03:53s\n",
            "epoch 159| loss: 0.24802 | train_logloss: 0.22935 | train_accuracy: 0.90767 | valid_logloss: 0.37047 | valid_accuracy: 0.879   |  0:03:55s\n",
            "epoch 160| loss: 0.23745 | train_logloss: 0.22447 | train_accuracy: 0.908   | valid_logloss: 0.37858 | valid_accuracy: 0.88111 |  0:03:56s\n",
            "epoch 161| loss: 0.24095 | train_logloss: 0.22403 | train_accuracy: 0.914   | valid_logloss: 0.37753 | valid_accuracy: 0.87933 |  0:03:58s\n",
            "epoch 162| loss: 0.2334  | train_logloss: 0.22752 | train_accuracy: 0.91033 | valid_logloss: 0.37967 | valid_accuracy: 0.87889 |  0:03:59s\n",
            "\n",
            "Early stopping occurred at epoch 162 with best_epoch = 112 and best_valid_accuracy = 0.88589\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[411984  14193  54005]\n",
            " [ 20779 943981  35240]\n",
            " [ 34451  30090 368046]]\n",
            "Testing Score:  0.8858888888888888\n",
            "Confusion Matrix: \n",
            " [[415499  13908  50775]\n",
            " [ 14615 951836  33549]\n",
            " [ 39000  28591 364996]]\n",
            "Testing Score:  0.8837777777777778\n",
            "{'Rows': 9000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 240.1173164844513, 'time_tn': 77.18291807174683, 'accuracy_tn': 0.8858888888888888, 'time_learn_gb': 1.3624699115753174, 'time_gb': 2.1134321689605713, 'accuracy_gb': 0.8837777777777778}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "DRI66ThhH2iA",
        "outputId": "83565d44-7ccf-447a-8e9a-5a235f5d8a41"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885889</td>\n",
              "      <td>240.117316</td>\n",
              "      <td>77.182918</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>1.362470</td>\n",
              "      <td>2.113432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ...  accuracy_gb  time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...     0.883778       3.483706  16.722444\n",
              "1     9000    9000    9000  ...     0.883778       3.464101  17.349063\n",
              "2     9000    9000    9000  ...     0.883778       3.411332  16.978554\n",
              "3     9000    9000    9000  ...     0.883778       3.509229  17.331297\n",
              "4     9000    9000    9000  ...     0.883778       3.475400  17.451218\n",
              "5    30000   30000   30000  ...     0.891767       8.276289  27.093480\n",
              "6    30000   30000   30000  ...     0.891767       8.210687  27.343578\n",
              "7    30000   30000   30000  ...     0.891767       8.244918  28.102216\n",
              "8    30000   30000   30000  ...     0.891767       8.093423  27.197022\n",
              "9    30000   30000   30000  ...     0.891767       7.857594  27.867677\n",
              "10  300000  300000  300000  ...     0.901553     140.282187  95.826268\n",
              "11  300000  300000  300000  ...     0.901490     122.025770  81.670524\n",
              "12  300000  300000  300000  ...     0.901667     135.688677  95.713423\n",
              "13    9000    9000    9000  ...     0.883778       1.362470   2.113432\n",
              "\n",
              "[14 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9QOZGb_nrT",
        "outputId": "66a78042-16da-464d-a184-4f63b28639eb"
      },
      "source": [
        "time_model(number_exp=2, \n",
        "     Rows=9000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.82143 | train_logloss: 6.32878 | train_accuracy: 0.28522 | valid_logloss: 6.27567 | valid_accuracy: 0.28133 |  0:00:01s\n",
            "epoch 1  | loss: 0.47692 | train_logloss: 3.73853 | train_accuracy: 0.46722 | valid_logloss: 3.60361 | valid_accuracy: 0.47044 |  0:00:02s\n",
            "epoch 2  | loss: 0.43349 | train_logloss: 2.93796 | train_accuracy: 0.38289 | valid_logloss: 2.93365 | valid_accuracy: 0.38278 |  0:00:04s\n",
            "epoch 3  | loss: 0.40709 | train_logloss: 8.79966 | train_accuracy: 0.33711 | valid_logloss: 8.77917 | valid_accuracy: 0.32978 |  0:00:05s\n",
            "epoch 4  | loss: 0.3973  | train_logloss: 1.78235 | train_accuracy: 0.59833 | valid_logloss: 1.7947  | valid_accuracy: 0.60733 |  0:00:07s\n",
            "epoch 5  | loss: 0.37775 | train_logloss: 1.26544 | train_accuracy: 0.40322 | valid_logloss: 1.28292 | valid_accuracy: 0.39467 |  0:00:08s\n",
            "epoch 6  | loss: 0.37014 | train_logloss: 1.67077 | train_accuracy: 0.42756 | valid_logloss: 1.68211 | valid_accuracy: 0.42844 |  0:00:09s\n",
            "epoch 7  | loss: 0.35694 | train_logloss: 1.73414 | train_accuracy: 0.56944 | valid_logloss: 1.72914 | valid_accuracy: 0.57378 |  0:00:11s\n",
            "epoch 8  | loss: 0.35207 | train_logloss: 1.39086 | train_accuracy: 0.66322 | valid_logloss: 1.40286 | valid_accuracy: 0.66711 |  0:00:12s\n",
            "epoch 9  | loss: 0.34746 | train_logloss: 0.97224 | train_accuracy: 0.70522 | valid_logloss: 0.96727 | valid_accuracy: 0.71156 |  0:00:13s\n",
            "epoch 10 | loss: 0.35199 | train_logloss: 1.22463 | train_accuracy: 0.72756 | valid_logloss: 1.19841 | valid_accuracy: 0.73811 |  0:00:15s\n",
            "epoch 11 | loss: 0.34605 | train_logloss: 1.02321 | train_accuracy: 0.77356 | valid_logloss: 1.01358 | valid_accuracy: 0.77856 |  0:00:16s\n",
            "epoch 12 | loss: 0.33799 | train_logloss: 0.70711 | train_accuracy: 0.77589 | valid_logloss: 0.70144 | valid_accuracy: 0.78078 |  0:00:17s\n",
            "epoch 13 | loss: 0.34846 | train_logloss: 0.565   | train_accuracy: 0.79467 | valid_logloss: 0.57012 | valid_accuracy: 0.79344 |  0:00:19s\n",
            "epoch 14 | loss: 0.3404  | train_logloss: 0.50617 | train_accuracy: 0.81433 | valid_logloss: 0.51177 | valid_accuracy: 0.81433 |  0:00:20s\n",
            "epoch 15 | loss: 0.33693 | train_logloss: 0.48132 | train_accuracy: 0.82078 | valid_logloss: 0.49153 | valid_accuracy: 0.82033 |  0:00:22s\n",
            "epoch 16 | loss: 0.34047 | train_logloss: 0.47209 | train_accuracy: 0.83589 | valid_logloss: 0.48182 | valid_accuracy: 0.83244 |  0:00:23s\n",
            "epoch 17 | loss: 0.34129 | train_logloss: 0.46514 | train_accuracy: 0.81856 | valid_logloss: 0.48315 | valid_accuracy: 0.81733 |  0:00:24s\n",
            "epoch 18 | loss: 0.33453 | train_logloss: 0.39144 | train_accuracy: 0.84756 | valid_logloss: 0.40521 | valid_accuracy: 0.84489 |  0:00:26s\n",
            "epoch 19 | loss: 0.33676 | train_logloss: 0.37151 | train_accuracy: 0.85667 | valid_logloss: 0.39745 | valid_accuracy: 0.85056 |  0:00:27s\n",
            "epoch 20 | loss: 0.33303 | train_logloss: 0.33907 | train_accuracy: 0.87444 | valid_logloss: 0.35813 | valid_accuracy: 0.86633 |  0:00:28s\n",
            "epoch 21 | loss: 0.33628 | train_logloss: 0.38054 | train_accuracy: 0.85111 | valid_logloss: 0.39123 | valid_accuracy: 0.85056 |  0:00:30s\n",
            "epoch 22 | loss: 0.33091 | train_logloss: 0.34184 | train_accuracy: 0.86989 | valid_logloss: 0.36459 | valid_accuracy: 0.86611 |  0:00:31s\n",
            "epoch 23 | loss: 0.32639 | train_logloss: 0.31392 | train_accuracy: 0.87922 | valid_logloss: 0.34657 | valid_accuracy: 0.874   |  0:00:33s\n",
            "epoch 24 | loss: 0.32169 | train_logloss: 0.31054 | train_accuracy: 0.879   | valid_logloss: 0.34829 | valid_accuracy: 0.87311 |  0:00:34s\n",
            "epoch 25 | loss: 0.32185 | train_logloss: 0.31533 | train_accuracy: 0.87944 | valid_logloss: 0.34995 | valid_accuracy: 0.87344 |  0:00:35s\n",
            "epoch 26 | loss: 0.32723 | train_logloss: 0.30764 | train_accuracy: 0.88233 | valid_logloss: 0.34258 | valid_accuracy: 0.87678 |  0:00:37s\n",
            "epoch 27 | loss: 0.32377 | train_logloss: 0.30315 | train_accuracy: 0.88644 | valid_logloss: 0.33971 | valid_accuracy: 0.87622 |  0:00:38s\n",
            "epoch 28 | loss: 0.31632 | train_logloss: 0.30746 | train_accuracy: 0.88044 | valid_logloss: 0.34708 | valid_accuracy: 0.87522 |  0:00:39s\n",
            "epoch 29 | loss: 0.30732 | train_logloss: 0.29582 | train_accuracy: 0.88811 | valid_logloss: 0.33736 | valid_accuracy: 0.87444 |  0:00:41s\n",
            "epoch 30 | loss: 0.31406 | train_logloss: 0.30763 | train_accuracy: 0.87922 | valid_logloss: 0.34672 | valid_accuracy: 0.87233 |  0:00:42s\n",
            "epoch 31 | loss: 0.31083 | train_logloss: 0.29733 | train_accuracy: 0.88556 | valid_logloss: 0.34201 | valid_accuracy: 0.87811 |  0:00:43s\n",
            "epoch 32 | loss: 0.30402 | train_logloss: 0.29941 | train_accuracy: 0.88056 | valid_logloss: 0.34572 | valid_accuracy: 0.87267 |  0:00:45s\n",
            "epoch 33 | loss: 0.30366 | train_logloss: 0.28773 | train_accuracy: 0.89056 | valid_logloss: 0.33703 | valid_accuracy: 0.87789 |  0:00:46s\n",
            "epoch 34 | loss: 0.30034 | train_logloss: 0.28897 | train_accuracy: 0.88822 | valid_logloss: 0.34433 | valid_accuracy: 0.87833 |  0:00:47s\n",
            "epoch 35 | loss: 0.30443 | train_logloss: 0.30172 | train_accuracy: 0.88289 | valid_logloss: 0.35069 | valid_accuracy: 0.87189 |  0:00:49s\n",
            "epoch 36 | loss: 0.30163 | train_logloss: 0.28381 | train_accuracy: 0.89122 | valid_logloss: 0.33171 | valid_accuracy: 0.87978 |  0:00:50s\n",
            "epoch 37 | loss: 0.3022  | train_logloss: 0.28315 | train_accuracy: 0.89156 | valid_logloss: 0.33659 | valid_accuracy: 0.87711 |  0:00:52s\n",
            "epoch 38 | loss: 0.2949  | train_logloss: 0.28665 | train_accuracy: 0.89144 | valid_logloss: 0.335   | valid_accuracy: 0.87867 |  0:00:53s\n",
            "epoch 39 | loss: 0.29212 | train_logloss: 0.27657 | train_accuracy: 0.89378 | valid_logloss: 0.33494 | valid_accuracy: 0.882   |  0:00:55s\n",
            "epoch 40 | loss: 0.2955  | train_logloss: 0.27338 | train_accuracy: 0.89322 | valid_logloss: 0.33412 | valid_accuracy: 0.88089 |  0:00:56s\n",
            "epoch 41 | loss: 0.28861 | train_logloss: 0.26898 | train_accuracy: 0.89744 | valid_logloss: 0.33048 | valid_accuracy: 0.88378 |  0:00:57s\n",
            "epoch 42 | loss: 0.29515 | train_logloss: 0.27435 | train_accuracy: 0.89567 | valid_logloss: 0.3335  | valid_accuracy: 0.883   |  0:00:59s\n",
            "epoch 43 | loss: 0.29628 | train_logloss: 0.28663 | train_accuracy: 0.88789 | valid_logloss: 0.34411 | valid_accuracy: 0.87489 |  0:01:00s\n",
            "epoch 44 | loss: 0.3001  | train_logloss: 0.28897 | train_accuracy: 0.89056 | valid_logloss: 0.34437 | valid_accuracy: 0.88111 |  0:01:01s\n",
            "epoch 45 | loss: 0.29784 | train_logloss: 0.27702 | train_accuracy: 0.89289 | valid_logloss: 0.33696 | valid_accuracy: 0.87844 |  0:01:03s\n",
            "epoch 46 | loss: 0.29321 | train_logloss: 0.27482 | train_accuracy: 0.897   | valid_logloss: 0.3358  | valid_accuracy: 0.88156 |  0:01:04s\n",
            "epoch 47 | loss: 0.29507 | train_logloss: 0.28286 | train_accuracy: 0.891   | valid_logloss: 0.33801 | valid_accuracy: 0.87489 |  0:01:06s\n",
            "epoch 48 | loss: 0.29495 | train_logloss: 0.28435 | train_accuracy: 0.89144 | valid_logloss: 0.33852 | valid_accuracy: 0.87744 |  0:01:07s\n",
            "epoch 49 | loss: 0.28702 | train_logloss: 0.26828 | train_accuracy: 0.89922 | valid_logloss: 0.33377 | valid_accuracy: 0.88422 |  0:01:08s\n",
            "epoch 50 | loss: 0.28539 | train_logloss: 0.26724 | train_accuracy: 0.89756 | valid_logloss: 0.3343  | valid_accuracy: 0.87956 |  0:01:10s\n",
            "epoch 51 | loss: 0.27905 | train_logloss: 0.2763  | train_accuracy: 0.88989 | valid_logloss: 0.3434  | valid_accuracy: 0.875   |  0:01:11s\n",
            "epoch 52 | loss: 0.28499 | train_logloss: 0.27799 | train_accuracy: 0.89378 | valid_logloss: 0.33591 | valid_accuracy: 0.87844 |  0:01:12s\n",
            "epoch 53 | loss: 0.2818  | train_logloss: 0.26306 | train_accuracy: 0.899   | valid_logloss: 0.32864 | valid_accuracy: 0.88489 |  0:01:14s\n",
            "epoch 54 | loss: 0.28722 | train_logloss: 0.26511 | train_accuracy: 0.89833 | valid_logloss: 0.33849 | valid_accuracy: 0.87933 |  0:01:15s\n",
            "epoch 55 | loss: 0.28661 | train_logloss: 0.27471 | train_accuracy: 0.89356 | valid_logloss: 0.3398  | valid_accuracy: 0.87822 |  0:01:16s\n",
            "epoch 56 | loss: 0.2867  | train_logloss: 0.27802 | train_accuracy: 0.89589 | valid_logloss: 0.34434 | valid_accuracy: 0.87833 |  0:01:18s\n",
            "epoch 57 | loss: 0.29516 | train_logloss: 0.26884 | train_accuracy: 0.89756 | valid_logloss: 0.3339  | valid_accuracy: 0.88267 |  0:01:19s\n",
            "epoch 58 | loss: 0.28972 | train_logloss: 0.26517 | train_accuracy: 0.89922 | valid_logloss: 0.33689 | valid_accuracy: 0.88067 |  0:01:21s\n",
            "epoch 59 | loss: 0.28057 | train_logloss: 0.26518 | train_accuracy: 0.89978 | valid_logloss: 0.34335 | valid_accuracy: 0.87811 |  0:01:22s\n",
            "epoch 60 | loss: 0.28816 | train_logloss: 0.26921 | train_accuracy: 0.90022 | valid_logloss: 0.33627 | valid_accuracy: 0.87778 |  0:01:23s\n",
            "epoch 61 | loss: 0.28397 | train_logloss: 0.26063 | train_accuracy: 0.9     | valid_logloss: 0.33287 | valid_accuracy: 0.882   |  0:01:25s\n",
            "epoch 62 | loss: 0.28083 | train_logloss: 0.26106 | train_accuracy: 0.902   | valid_logloss: 0.33416 | valid_accuracy: 0.88478 |  0:01:26s\n",
            "epoch 63 | loss: 0.27695 | train_logloss: 0.25549 | train_accuracy: 0.90433 | valid_logloss: 0.33599 | valid_accuracy: 0.88189 |  0:01:27s\n",
            "epoch 64 | loss: 0.27885 | train_logloss: 0.2609  | train_accuracy: 0.89944 | valid_logloss: 0.33774 | valid_accuracy: 0.88189 |  0:01:29s\n",
            "epoch 65 | loss: 0.28376 | train_logloss: 0.28686 | train_accuracy: 0.89    | valid_logloss: 0.35709 | valid_accuracy: 0.876   |  0:01:30s\n",
            "epoch 66 | loss: 0.28661 | train_logloss: 0.25901 | train_accuracy: 0.90167 | valid_logloss: 0.3285  | valid_accuracy: 0.88433 |  0:01:31s\n",
            "epoch 67 | loss: 0.27809 | train_logloss: 0.25754 | train_accuracy: 0.89967 | valid_logloss: 0.33401 | valid_accuracy: 0.88344 |  0:01:33s\n",
            "epoch 68 | loss: 0.28234 | train_logloss: 0.2738  | train_accuracy: 0.89489 | valid_logloss: 0.34903 | valid_accuracy: 0.87933 |  0:01:34s\n",
            "epoch 69 | loss: 0.2753  | train_logloss: 0.26319 | train_accuracy: 0.89811 | valid_logloss: 0.33331 | valid_accuracy: 0.88144 |  0:01:36s\n",
            "epoch 70 | loss: 0.27521 | train_logloss: 0.25678 | train_accuracy: 0.89867 | valid_logloss: 0.33811 | valid_accuracy: 0.88033 |  0:01:37s\n",
            "epoch 71 | loss: 0.26879 | train_logloss: 0.25536 | train_accuracy: 0.90478 | valid_logloss: 0.34105 | valid_accuracy: 0.88156 |  0:01:38s\n",
            "epoch 72 | loss: 0.26902 | train_logloss: 0.25947 | train_accuracy: 0.89944 | valid_logloss: 0.35156 | valid_accuracy: 0.87789 |  0:01:40s\n",
            "epoch 73 | loss: 0.28621 | train_logloss: 0.26611 | train_accuracy: 0.89911 | valid_logloss: 0.35273 | valid_accuracy: 0.87789 |  0:01:41s\n",
            "epoch 74 | loss: 0.27597 | train_logloss: 0.26506 | train_accuracy: 0.89878 | valid_logloss: 0.34924 | valid_accuracy: 0.87556 |  0:01:42s\n",
            "epoch 75 | loss: 0.27251 | train_logloss: 0.25482 | train_accuracy: 0.90111 | valid_logloss: 0.34348 | valid_accuracy: 0.87989 |  0:01:44s\n",
            "epoch 76 | loss: 0.26869 | train_logloss: 0.25635 | train_accuracy: 0.90244 | valid_logloss: 0.33149 | valid_accuracy: 0.88356 |  0:01:45s\n",
            "epoch 77 | loss: 0.27306 | train_logloss: 0.26532 | train_accuracy: 0.89833 | valid_logloss: 0.3418  | valid_accuracy: 0.87922 |  0:01:46s\n",
            "epoch 78 | loss: 0.27499 | train_logloss: 0.25701 | train_accuracy: 0.90344 | valid_logloss: 0.3338  | valid_accuracy: 0.88311 |  0:01:48s\n",
            "epoch 79 | loss: 0.27808 | train_logloss: 0.26262 | train_accuracy: 0.90011 | valid_logloss: 0.33495 | valid_accuracy: 0.88033 |  0:01:49s\n",
            "epoch 80 | loss: 0.27995 | train_logloss: 0.27507 | train_accuracy: 0.89422 | valid_logloss: 0.33507 | valid_accuracy: 0.88211 |  0:01:50s\n",
            "epoch 81 | loss: 0.2846  | train_logloss: 0.27064 | train_accuracy: 0.89822 | valid_logloss: 0.33128 | valid_accuracy: 0.88367 |  0:01:52s\n",
            "epoch 82 | loss: 0.28245 | train_logloss: 0.26833 | train_accuracy: 0.899   | valid_logloss: 0.33293 | valid_accuracy: 0.88267 |  0:01:53s\n",
            "epoch 83 | loss: 0.2765  | train_logloss: 0.26225 | train_accuracy: 0.89967 | valid_logloss: 0.32871 | valid_accuracy: 0.88356 |  0:01:55s\n",
            "epoch 84 | loss: 0.27201 | train_logloss: 0.26686 | train_accuracy: 0.89911 | valid_logloss: 0.33375 | valid_accuracy: 0.88722 |  0:01:56s\n",
            "epoch 85 | loss: 0.28199 | train_logloss: 0.2771  | train_accuracy: 0.89411 | valid_logloss: 0.35213 | valid_accuracy: 0.87511 |  0:01:57s\n",
            "epoch 86 | loss: 0.29049 | train_logloss: 0.27756 | train_accuracy: 0.89367 | valid_logloss: 0.33505 | valid_accuracy: 0.881   |  0:01:59s\n",
            "epoch 87 | loss: 0.29234 | train_logloss: 0.26987 | train_accuracy: 0.897   | valid_logloss: 0.33651 | valid_accuracy: 0.88178 |  0:02:00s\n",
            "epoch 88 | loss: 0.27971 | train_logloss: 0.25671 | train_accuracy: 0.90133 | valid_logloss: 0.32832 | valid_accuracy: 0.88356 |  0:02:01s\n",
            "epoch 89 | loss: 0.26804 | train_logloss: 0.24926 | train_accuracy: 0.90589 | valid_logloss: 0.32693 | valid_accuracy: 0.88378 |  0:02:03s\n",
            "epoch 90 | loss: 0.26268 | train_logloss: 0.25457 | train_accuracy: 0.90444 | valid_logloss: 0.32869 | valid_accuracy: 0.88333 |  0:02:04s\n",
            "epoch 91 | loss: 0.27167 | train_logloss: 0.25394 | train_accuracy: 0.899   | valid_logloss: 0.33326 | valid_accuracy: 0.88122 |  0:02:05s\n",
            "epoch 92 | loss: 0.26134 | train_logloss: 0.24608 | train_accuracy: 0.90411 | valid_logloss: 0.33444 | valid_accuracy: 0.88311 |  0:02:07s\n",
            "epoch 93 | loss: 0.26215 | train_logloss: 0.2407  | train_accuracy: 0.90778 | valid_logloss: 0.33046 | valid_accuracy: 0.88256 |  0:02:08s\n",
            "epoch 94 | loss: 0.25939 | train_logloss: 0.24652 | train_accuracy: 0.905   | valid_logloss: 0.34148 | valid_accuracy: 0.88089 |  0:02:10s\n",
            "epoch 95 | loss: 0.26713 | train_logloss: 0.24309 | train_accuracy: 0.90744 | valid_logloss: 0.33943 | valid_accuracy: 0.882   |  0:02:11s\n",
            "epoch 96 | loss: 0.26672 | train_logloss: 0.24817 | train_accuracy: 0.90611 | valid_logloss: 0.32792 | valid_accuracy: 0.88122 |  0:02:12s\n",
            "epoch 97 | loss: 0.26744 | train_logloss: 0.27428 | train_accuracy: 0.893   | valid_logloss: 0.36347 | valid_accuracy: 0.87189 |  0:02:14s\n",
            "epoch 98 | loss: 0.28479 | train_logloss: 0.2483  | train_accuracy: 0.90544 | valid_logloss: 0.33009 | valid_accuracy: 0.88611 |  0:02:15s\n",
            "epoch 99 | loss: 0.27213 | train_logloss: 0.2495  | train_accuracy: 0.904   | valid_logloss: 0.34114 | valid_accuracy: 0.88233 |  0:02:16s\n",
            "epoch 100| loss: 0.26448 | train_logloss: 0.24255 | train_accuracy: 0.90344 | valid_logloss: 0.33969 | valid_accuracy: 0.88478 |  0:02:18s\n",
            "epoch 101| loss: 0.27249 | train_logloss: 0.26388 | train_accuracy: 0.90133 | valid_logloss: 0.33605 | valid_accuracy: 0.883   |  0:02:19s\n",
            "epoch 102| loss: 0.2784  | train_logloss: 0.26083 | train_accuracy: 0.89922 | valid_logloss: 0.32997 | valid_accuracy: 0.88322 |  0:02:21s\n",
            "epoch 103| loss: 0.27291 | train_logloss: 0.24836 | train_accuracy: 0.90322 | valid_logloss: 0.32184 | valid_accuracy: 0.88444 |  0:02:22s\n",
            "epoch 104| loss: 0.26523 | train_logloss: 0.26015 | train_accuracy: 0.89722 | valid_logloss: 0.34057 | valid_accuracy: 0.87733 |  0:02:23s\n",
            "epoch 105| loss: 0.26112 | train_logloss: 0.24933 | train_accuracy: 0.90744 | valid_logloss: 0.34781 | valid_accuracy: 0.88078 |  0:02:25s\n",
            "epoch 106| loss: 0.26007 | train_logloss: 0.23847 | train_accuracy: 0.908   | valid_logloss: 0.33129 | valid_accuracy: 0.88522 |  0:02:26s\n",
            "epoch 107| loss: 0.25396 | train_logloss: 0.23168 | train_accuracy: 0.91144 | valid_logloss: 0.33259 | valid_accuracy: 0.88511 |  0:02:27s\n",
            "epoch 108| loss: 0.25113 | train_logloss: 0.23298 | train_accuracy: 0.90767 | valid_logloss: 0.33882 | valid_accuracy: 0.88267 |  0:02:29s\n",
            "epoch 109| loss: 0.25009 | train_logloss: 0.24438 | train_accuracy: 0.90267 | valid_logloss: 0.34868 | valid_accuracy: 0.883   |  0:02:30s\n",
            "epoch 110| loss: 0.2544  | train_logloss: 0.24054 | train_accuracy: 0.90633 | valid_logloss: 0.3491  | valid_accuracy: 0.88056 |  0:02:32s\n",
            "epoch 111| loss: 0.25874 | train_logloss: 0.24541 | train_accuracy: 0.90544 | valid_logloss: 0.34029 | valid_accuracy: 0.88089 |  0:02:33s\n",
            "epoch 112| loss: 0.25227 | train_logloss: 0.22729 | train_accuracy: 0.91133 | valid_logloss: 0.32612 | valid_accuracy: 0.88889 |  0:02:34s\n",
            "epoch 113| loss: 0.25488 | train_logloss: 0.22773 | train_accuracy: 0.911   | valid_logloss: 0.33468 | valid_accuracy: 0.88433 |  0:02:36s\n",
            "epoch 114| loss: 0.24864 | train_logloss: 0.22838 | train_accuracy: 0.91211 | valid_logloss: 0.34173 | valid_accuracy: 0.88467 |  0:02:37s\n",
            "epoch 115| loss: 0.24727 | train_logloss: 0.23562 | train_accuracy: 0.90844 | valid_logloss: 0.34671 | valid_accuracy: 0.87922 |  0:02:38s\n",
            "epoch 116| loss: 0.24728 | train_logloss: 0.23504 | train_accuracy: 0.90978 | valid_logloss: 0.35512 | valid_accuracy: 0.88089 |  0:02:40s\n",
            "epoch 117| loss: 0.25217 | train_logloss: 0.23565 | train_accuracy: 0.90911 | valid_logloss: 0.35018 | valid_accuracy: 0.88367 |  0:02:41s\n",
            "epoch 118| loss: 0.25581 | train_logloss: 0.24158 | train_accuracy: 0.90644 | valid_logloss: 0.34312 | valid_accuracy: 0.88133 |  0:02:43s\n",
            "epoch 119| loss: 0.25307 | train_logloss: 0.23597 | train_accuracy: 0.91056 | valid_logloss: 0.34912 | valid_accuracy: 0.87844 |  0:02:44s\n",
            "epoch 120| loss: 0.25356 | train_logloss: 0.23479 | train_accuracy: 0.90867 | valid_logloss: 0.35891 | valid_accuracy: 0.87922 |  0:02:45s\n",
            "epoch 121| loss: 0.24941 | train_logloss: 0.23387 | train_accuracy: 0.90756 | valid_logloss: 0.33763 | valid_accuracy: 0.88233 |  0:02:47s\n",
            "epoch 122| loss: 0.25011 | train_logloss: 0.2279  | train_accuracy: 0.91    | valid_logloss: 0.34897 | valid_accuracy: 0.88189 |  0:02:48s\n",
            "epoch 123| loss: 0.24007 | train_logloss: 0.22386 | train_accuracy: 0.91178 | valid_logloss: 0.35496 | valid_accuracy: 0.87833 |  0:02:49s\n",
            "epoch 124| loss: 0.24387 | train_logloss: 0.23711 | train_accuracy: 0.90733 | valid_logloss: 0.36191 | valid_accuracy: 0.87944 |  0:02:51s\n",
            "epoch 125| loss: 0.24984 | train_logloss: 0.24469 | train_accuracy: 0.90411 | valid_logloss: 0.36494 | valid_accuracy: 0.87689 |  0:02:52s\n",
            "epoch 126| loss: 0.25941 | train_logloss: 0.24859 | train_accuracy: 0.90533 | valid_logloss: 0.35569 | valid_accuracy: 0.88189 |  0:02:53s\n",
            "epoch 127| loss: 0.25544 | train_logloss: 0.24711 | train_accuracy: 0.90411 | valid_logloss: 0.35824 | valid_accuracy: 0.87822 |  0:02:55s\n",
            "epoch 128| loss: 0.25761 | train_logloss: 0.25134 | train_accuracy: 0.90444 | valid_logloss: 0.35788 | valid_accuracy: 0.88189 |  0:02:56s\n",
            "epoch 129| loss: 0.26073 | train_logloss: 0.25705 | train_accuracy: 0.89833 | valid_logloss: 0.36353 | valid_accuracy: 0.87267 |  0:02:57s\n",
            "epoch 130| loss: 0.26349 | train_logloss: 0.23914 | train_accuracy: 0.90844 | valid_logloss: 0.3365  | valid_accuracy: 0.88278 |  0:02:59s\n",
            "epoch 131| loss: 0.25656 | train_logloss: 0.23127 | train_accuracy: 0.90989 | valid_logloss: 0.33462 | valid_accuracy: 0.88678 |  0:03:00s\n",
            "epoch 132| loss: 0.25662 | train_logloss: 0.23268 | train_accuracy: 0.907   | valid_logloss: 0.34055 | valid_accuracy: 0.88478 |  0:03:02s\n",
            "epoch 133| loss: 0.25419 | train_logloss: 0.22592 | train_accuracy: 0.91011 | valid_logloss: 0.33045 | valid_accuracy: 0.88478 |  0:03:03s\n",
            "epoch 134| loss: 0.23767 | train_logloss: 0.21953 | train_accuracy: 0.91244 | valid_logloss: 0.33923 | valid_accuracy: 0.88411 |  0:03:04s\n",
            "epoch 135| loss: 0.24584 | train_logloss: 0.2284  | train_accuracy: 0.91178 | valid_logloss: 0.36242 | valid_accuracy: 0.87833 |  0:03:06s\n",
            "epoch 136| loss: 0.24051 | train_logloss: 0.21427 | train_accuracy: 0.91711 | valid_logloss: 0.33593 | valid_accuracy: 0.88322 |  0:03:07s\n",
            "epoch 137| loss: 0.23904 | train_logloss: 0.22731 | train_accuracy: 0.91533 | valid_logloss: 0.35409 | valid_accuracy: 0.88056 |  0:03:08s\n",
            "epoch 138| loss: 0.24005 | train_logloss: 0.22423 | train_accuracy: 0.91167 | valid_logloss: 0.36256 | valid_accuracy: 0.87733 |  0:03:10s\n",
            "epoch 139| loss: 0.24431 | train_logloss: 0.22701 | train_accuracy: 0.911   | valid_logloss: 0.34815 | valid_accuracy: 0.88311 |  0:03:11s\n",
            "epoch 140| loss: 0.24621 | train_logloss: 0.23176 | train_accuracy: 0.90589 | valid_logloss: 0.35135 | valid_accuracy: 0.88011 |  0:03:12s\n",
            "epoch 141| loss: 0.24427 | train_logloss: 0.21843 | train_accuracy: 0.91644 | valid_logloss: 0.35133 | valid_accuracy: 0.88344 |  0:03:14s\n",
            "epoch 142| loss: 0.23843 | train_logloss: 0.2116  | train_accuracy: 0.91811 | valid_logloss: 0.36614 | valid_accuracy: 0.881   |  0:03:15s\n",
            "epoch 143| loss: 0.23549 | train_logloss: 0.21831 | train_accuracy: 0.91611 | valid_logloss: 0.34915 | valid_accuracy: 0.88222 |  0:03:16s\n",
            "epoch 144| loss: 0.23277 | train_logloss: 0.20941 | train_accuracy: 0.91922 | valid_logloss: 0.34182 | valid_accuracy: 0.88222 |  0:03:18s\n",
            "epoch 145| loss: 0.23017 | train_logloss: 0.21989 | train_accuracy: 0.91011 | valid_logloss: 0.36956 | valid_accuracy: 0.87956 |  0:03:19s\n",
            "epoch 146| loss: 0.23952 | train_logloss: 0.23496 | train_accuracy: 0.90744 | valid_logloss: 0.37983 | valid_accuracy: 0.877   |  0:03:21s\n",
            "epoch 147| loss: 0.23158 | train_logloss: 0.21472 | train_accuracy: 0.91167 | valid_logloss: 0.36984 | valid_accuracy: 0.88089 |  0:03:22s\n",
            "epoch 148| loss: 0.23299 | train_logloss: 0.21885 | train_accuracy: 0.91411 | valid_logloss: 0.35332 | valid_accuracy: 0.88289 |  0:03:23s\n",
            "epoch 149| loss: 0.22905 | train_logloss: 0.209   | train_accuracy: 0.91844 | valid_logloss: 0.36507 | valid_accuracy: 0.88144 |  0:03:25s\n",
            "epoch 150| loss: 0.22655 | train_logloss: 0.22022 | train_accuracy: 0.91333 | valid_logloss: 0.38912 | valid_accuracy: 0.87489 |  0:03:26s\n",
            "epoch 151| loss: 0.24198 | train_logloss: 0.2145  | train_accuracy: 0.91533 | valid_logloss: 0.35074 | valid_accuracy: 0.88478 |  0:03:27s\n",
            "epoch 152| loss: 0.23438 | train_logloss: 0.21168 | train_accuracy: 0.91733 | valid_logloss: 0.3565  | valid_accuracy: 0.88156 |  0:03:29s\n",
            "epoch 153| loss: 0.23046 | train_logloss: 0.21953 | train_accuracy: 0.91589 | valid_logloss: 0.36466 | valid_accuracy: 0.87911 |  0:03:30s\n",
            "epoch 154| loss: 0.2511  | train_logloss: 0.23556 | train_accuracy: 0.91311 | valid_logloss: 0.34224 | valid_accuracy: 0.887   |  0:03:31s\n",
            "epoch 155| loss: 0.24638 | train_logloss: 0.22702 | train_accuracy: 0.91222 | valid_logloss: 0.34863 | valid_accuracy: 0.883   |  0:03:33s\n",
            "epoch 156| loss: 0.23805 | train_logloss: 0.22071 | train_accuracy: 0.91133 | valid_logloss: 0.34541 | valid_accuracy: 0.88044 |  0:03:34s\n",
            "epoch 157| loss: 0.23833 | train_logloss: 0.2101  | train_accuracy: 0.91589 | valid_logloss: 0.36034 | valid_accuracy: 0.87811 |  0:03:35s\n",
            "epoch 158| loss: 0.23214 | train_logloss: 0.21859 | train_accuracy: 0.91678 | valid_logloss: 0.35475 | valid_accuracy: 0.88    |  0:03:37s\n",
            "epoch 159| loss: 0.24294 | train_logloss: 0.21679 | train_accuracy: 0.91767 | valid_logloss: 0.35718 | valid_accuracy: 0.88156 |  0:03:38s\n",
            "epoch 160| loss: 0.23846 | train_logloss: 0.216   | train_accuracy: 0.91578 | valid_logloss: 0.37057 | valid_accuracy: 0.87689 |  0:03:39s\n",
            "epoch 161| loss: 0.23738 | train_logloss: 0.21546 | train_accuracy: 0.91778 | valid_logloss: 0.36151 | valid_accuracy: 0.87967 |  0:03:41s\n",
            "epoch 162| loss: 0.23525 | train_logloss: 0.21511 | train_accuracy: 0.91767 | valid_logloss: 0.35944 | valid_accuracy: 0.88111 |  0:03:42s\n",
            "\n",
            "Early stopping occurred at epoch 162 with best_epoch = 112 and best_valid_accuracy = 0.88889\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[414706  15116  50360]\n",
            " [ 16890 944726  38384]\n",
            " [ 36699  28178 367710]]\n",
            "Testing Score:  0.8888888888888888\n",
            "Confusion Matrix: \n",
            " [[415499  13908  50775]\n",
            " [ 14615 951836  33549]\n",
            " [ 39000  28591 364996]]\n",
            "Testing Score:  0.8837777777777778\n",
            "{'Rows': 9000, 'Nd': 16, 'Na': 16, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 222.94587564468384, 'time_tn': 61.207308292388916, 'accuracy_tn': 0.8888888888888888, 'time_learn_gb': 3.4641013145446777, 'time_gb': 17.3490629196167, 'accuracy_gb': 0.8837777777777778}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "uatx344UH3la",
        "outputId": "7e7912e1-26db-4738-ef2c-700c2532ad46"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1  9000  9000  9000  16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xJNKwSI-ldE",
        "outputId": "a446aef0-ee5e-49d6-ea17-43253d85f00f"
      },
      "source": [
        "time_model(number_exp=3, \n",
        "          Rows=9000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=512,\tBV=128,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.08905 | train_logloss: 18.61257| train_accuracy: 0.369   | valid_logloss: 18.53504| valid_accuracy: 0.36978 |  0:00:02s\n",
            "epoch 1  | loss: 0.71233 | train_logloss: 20.00418| train_accuracy: 0.29611 | valid_logloss: 20.22298| valid_accuracy: 0.289   |  0:00:04s\n",
            "epoch 2  | loss: 0.52017 | train_logloss: 10.20336| train_accuracy: 0.40122 | valid_logloss: 10.19558| valid_accuracy: 0.39778 |  0:00:06s\n",
            "epoch 3  | loss: 0.4706  | train_logloss: 10.50671| train_accuracy: 0.29522 | valid_logloss: 10.48509| valid_accuracy: 0.29267 |  0:00:08s\n",
            "epoch 4  | loss: 0.42457 | train_logloss: 2.085   | train_accuracy: 0.38533 | valid_logloss: 2.11969 | valid_accuracy: 0.37678 |  0:00:10s\n",
            "epoch 5  | loss: 0.39834 | train_logloss: 2.38837 | train_accuracy: 0.28467 | valid_logloss: 2.38355 | valid_accuracy: 0.28244 |  0:00:12s\n",
            "epoch 6  | loss: 0.3888  | train_logloss: 4.24199 | train_accuracy: 0.29911 | valid_logloss: 4.28773 | valid_accuracy: 0.29133 |  0:00:14s\n",
            "epoch 7  | loss: 0.38059 | train_logloss: 2.95219 | train_accuracy: 0.347   | valid_logloss: 2.95418 | valid_accuracy: 0.34811 |  0:00:15s\n",
            "epoch 8  | loss: 0.36768 | train_logloss: 1.45857 | train_accuracy: 0.44122 | valid_logloss: 1.45035 | valid_accuracy: 0.44356 |  0:00:18s\n",
            "epoch 9  | loss: 0.36737 | train_logloss: 1.43981 | train_accuracy: 0.53411 | valid_logloss: 1.4542  | valid_accuracy: 0.52989 |  0:00:20s\n",
            "epoch 10 | loss: 0.3663  | train_logloss: 1.37139 | train_accuracy: 0.497   | valid_logloss: 1.39561 | valid_accuracy: 0.49622 |  0:00:22s\n",
            "epoch 11 | loss: 0.36303 | train_logloss: 1.08492 | train_accuracy: 0.58067 | valid_logloss: 1.10638 | valid_accuracy: 0.57178 |  0:00:24s\n",
            "epoch 12 | loss: 0.35583 | train_logloss: 0.60102 | train_accuracy: 0.79767 | valid_logloss: 0.6162  | valid_accuracy: 0.79733 |  0:00:26s\n",
            "epoch 13 | loss: 0.35617 | train_logloss: 0.54907 | train_accuracy: 0.80011 | valid_logloss: 0.5556  | valid_accuracy: 0.79811 |  0:00:28s\n",
            "epoch 14 | loss: 0.34912 | train_logloss: 0.59501 | train_accuracy: 0.771   | valid_logloss: 0.60679 | valid_accuracy: 0.77033 |  0:00:30s\n",
            "epoch 15 | loss: 0.36241 | train_logloss: 0.45068 | train_accuracy: 0.83833 | valid_logloss: 0.45218 | valid_accuracy: 0.83489 |  0:00:32s\n",
            "epoch 16 | loss: 0.36959 | train_logloss: 0.43316 | train_accuracy: 0.839   | valid_logloss: 0.44405 | valid_accuracy: 0.835   |  0:00:34s\n",
            "epoch 17 | loss: 0.35438 | train_logloss: 0.38334 | train_accuracy: 0.85144 | valid_logloss: 0.40481 | valid_accuracy: 0.84944 |  0:00:36s\n",
            "epoch 18 | loss: 0.34559 | train_logloss: 0.42747 | train_accuracy: 0.842   | valid_logloss: 0.44012 | valid_accuracy: 0.83856 |  0:00:38s\n",
            "epoch 19 | loss: 0.3521  | train_logloss: 0.39211 | train_accuracy: 0.85578 | valid_logloss: 0.40385 | valid_accuracy: 0.85233 |  0:00:40s\n",
            "epoch 20 | loss: 0.34856 | train_logloss: 0.35133 | train_accuracy: 0.86533 | valid_logloss: 0.37387 | valid_accuracy: 0.86322 |  0:00:42s\n",
            "epoch 21 | loss: 0.3461  | train_logloss: 0.35357 | train_accuracy: 0.863   | valid_logloss: 0.37159 | valid_accuracy: 0.86033 |  0:00:44s\n",
            "epoch 22 | loss: 0.35847 | train_logloss: 0.34674 | train_accuracy: 0.87256 | valid_logloss: 0.36909 | valid_accuracy: 0.86478 |  0:00:46s\n",
            "epoch 23 | loss: 0.34945 | train_logloss: 0.36237 | train_accuracy: 0.85844 | valid_logloss: 0.39018 | valid_accuracy: 0.85711 |  0:00:47s\n",
            "epoch 24 | loss: 0.35336 | train_logloss: 0.35645 | train_accuracy: 0.866   | valid_logloss: 0.37487 | valid_accuracy: 0.86656 |  0:00:49s\n",
            "epoch 25 | loss: 0.36204 | train_logloss: 0.37145 | train_accuracy: 0.85344 | valid_logloss: 0.38213 | valid_accuracy: 0.85089 |  0:00:51s\n",
            "epoch 26 | loss: 0.37358 | train_logloss: 0.36123 | train_accuracy: 0.86411 | valid_logloss: 0.36649 | valid_accuracy: 0.86167 |  0:00:53s\n",
            "epoch 27 | loss: 0.35862 | train_logloss: 0.36672 | train_accuracy: 0.863   | valid_logloss: 0.38148 | valid_accuracy: 0.85789 |  0:00:55s\n",
            "epoch 28 | loss: 0.37497 | train_logloss: 0.34327 | train_accuracy: 0.87122 | valid_logloss: 0.35806 | valid_accuracy: 0.86811 |  0:00:57s\n",
            "epoch 29 | loss: 0.36238 | train_logloss: 0.35254 | train_accuracy: 0.86878 | valid_logloss: 0.37684 | valid_accuracy: 0.86311 |  0:00:59s\n",
            "epoch 30 | loss: 0.36317 | train_logloss: 0.35191 | train_accuracy: 0.863   | valid_logloss: 0.37563 | valid_accuracy: 0.86156 |  0:01:01s\n",
            "epoch 31 | loss: 0.34991 | train_logloss: 0.33385 | train_accuracy: 0.87544 | valid_logloss: 0.36003 | valid_accuracy: 0.86978 |  0:01:03s\n",
            "epoch 32 | loss: 0.34702 | train_logloss: 0.33802 | train_accuracy: 0.87044 | valid_logloss: 0.35754 | valid_accuracy: 0.86811 |  0:01:05s\n",
            "epoch 33 | loss: 0.34746 | train_logloss: 0.33415 | train_accuracy: 0.87256 | valid_logloss: 0.35696 | valid_accuracy: 0.86911 |  0:01:07s\n",
            "epoch 34 | loss: 0.3397  | train_logloss: 0.32594 | train_accuracy: 0.87322 | valid_logloss: 0.35269 | valid_accuracy: 0.87111 |  0:01:09s\n",
            "epoch 35 | loss: 0.33719 | train_logloss: 0.33018 | train_accuracy: 0.87633 | valid_logloss: 0.35749 | valid_accuracy: 0.86856 |  0:01:11s\n",
            "epoch 36 | loss: 0.33452 | train_logloss: 0.32542 | train_accuracy: 0.87522 | valid_logloss: 0.34932 | valid_accuracy: 0.87344 |  0:01:13s\n",
            "epoch 37 | loss: 0.33503 | train_logloss: 0.32645 | train_accuracy: 0.87844 | valid_logloss: 0.35444 | valid_accuracy: 0.87256 |  0:01:15s\n",
            "epoch 38 | loss: 0.3351  | train_logloss: 0.32495 | train_accuracy: 0.878   | valid_logloss: 0.35052 | valid_accuracy: 0.875   |  0:01:17s\n",
            "epoch 39 | loss: 0.33443 | train_logloss: 0.31249 | train_accuracy: 0.88222 | valid_logloss: 0.34101 | valid_accuracy: 0.87744 |  0:01:19s\n",
            "epoch 40 | loss: 0.3276  | train_logloss: 0.31393 | train_accuracy: 0.88267 | valid_logloss: 0.3435  | valid_accuracy: 0.87689 |  0:01:21s\n",
            "epoch 41 | loss: 0.32798 | train_logloss: 0.3127  | train_accuracy: 0.88367 | valid_logloss: 0.34565 | valid_accuracy: 0.87644 |  0:01:23s\n",
            "epoch 42 | loss: 0.32032 | train_logloss: 0.32309 | train_accuracy: 0.87744 | valid_logloss: 0.35702 | valid_accuracy: 0.87233 |  0:01:25s\n",
            "epoch 43 | loss: 0.31528 | train_logloss: 0.29802 | train_accuracy: 0.88867 | valid_logloss: 0.33789 | valid_accuracy: 0.87833 |  0:01:27s\n",
            "epoch 44 | loss: 0.33017 | train_logloss: 0.33075 | train_accuracy: 0.87322 | valid_logloss: 0.36454 | valid_accuracy: 0.87022 |  0:01:29s\n",
            "epoch 45 | loss: 0.32901 | train_logloss: 0.30859 | train_accuracy: 0.88378 | valid_logloss: 0.34582 | valid_accuracy: 0.87644 |  0:01:31s\n",
            "epoch 46 | loss: 0.32442 | train_logloss: 0.3193  | train_accuracy: 0.87744 | valid_logloss: 0.35627 | valid_accuracy: 0.873   |  0:01:33s\n",
            "epoch 47 | loss: 0.34399 | train_logloss: 0.31747 | train_accuracy: 0.88133 | valid_logloss: 0.34807 | valid_accuracy: 0.87589 |  0:01:35s\n",
            "epoch 48 | loss: 0.34194 | train_logloss: 0.32684 | train_accuracy: 0.87644 | valid_logloss: 0.35426 | valid_accuracy: 0.86822 |  0:01:37s\n",
            "epoch 49 | loss: 0.33067 | train_logloss: 0.31563 | train_accuracy: 0.88222 | valid_logloss: 0.34543 | valid_accuracy: 0.87456 |  0:01:39s\n",
            "epoch 50 | loss: 0.33912 | train_logloss: 0.31763 | train_accuracy: 0.88044 | valid_logloss: 0.35086 | valid_accuracy: 0.87167 |  0:01:41s\n",
            "epoch 51 | loss: 0.33397 | train_logloss: 0.31678 | train_accuracy: 0.88267 | valid_logloss: 0.34785 | valid_accuracy: 0.876   |  0:01:43s\n",
            "epoch 52 | loss: 0.32236 | train_logloss: 0.31032 | train_accuracy: 0.88633 | valid_logloss: 0.34731 | valid_accuracy: 0.87489 |  0:01:45s\n",
            "epoch 53 | loss: 0.32532 | train_logloss: 0.3112  | train_accuracy: 0.88322 | valid_logloss: 0.34935 | valid_accuracy: 0.876   |  0:01:47s\n",
            "epoch 54 | loss: 0.31929 | train_logloss: 0.30435 | train_accuracy: 0.88533 | valid_logloss: 0.34149 | valid_accuracy: 0.87611 |  0:01:49s\n",
            "epoch 55 | loss: 0.31107 | train_logloss: 0.29805 | train_accuracy: 0.88867 | valid_logloss: 0.34281 | valid_accuracy: 0.88022 |  0:01:51s\n",
            "epoch 56 | loss: 0.31182 | train_logloss: 0.29507 | train_accuracy: 0.88878 | valid_logloss: 0.33724 | valid_accuracy: 0.87978 |  0:01:53s\n",
            "epoch 57 | loss: 0.30652 | train_logloss: 0.29304 | train_accuracy: 0.88989 | valid_logloss: 0.33627 | valid_accuracy: 0.88011 |  0:01:55s\n",
            "epoch 58 | loss: 0.30325 | train_logloss: 0.28896 | train_accuracy: 0.88911 | valid_logloss: 0.34749 | valid_accuracy: 0.87722 |  0:01:57s\n",
            "epoch 59 | loss: 0.30666 | train_logloss: 0.29438 | train_accuracy: 0.88744 | valid_logloss: 0.33718 | valid_accuracy: 0.88156 |  0:01:59s\n",
            "epoch 60 | loss: 0.30849 | train_logloss: 0.29341 | train_accuracy: 0.889   | valid_logloss: 0.33966 | valid_accuracy: 0.87767 |  0:02:01s\n",
            "epoch 61 | loss: 0.31494 | train_logloss: 0.31063 | train_accuracy: 0.88467 | valid_logloss: 0.35408 | valid_accuracy: 0.87589 |  0:02:03s\n",
            "epoch 62 | loss: 0.31538 | train_logloss: 0.30646 | train_accuracy: 0.88067 | valid_logloss: 0.35747 | valid_accuracy: 0.87367 |  0:02:05s\n",
            "epoch 63 | loss: 0.31254 | train_logloss: 0.29532 | train_accuracy: 0.88833 | valid_logloss: 0.3489  | valid_accuracy: 0.87711 |  0:02:07s\n",
            "epoch 64 | loss: 0.30577 | train_logloss: 0.29807 | train_accuracy: 0.89078 | valid_logloss: 0.34739 | valid_accuracy: 0.87933 |  0:02:09s\n",
            "epoch 65 | loss: 0.31436 | train_logloss: 0.29945 | train_accuracy: 0.88633 | valid_logloss: 0.34516 | valid_accuracy: 0.87656 |  0:02:11s\n",
            "epoch 66 | loss: 0.31046 | train_logloss: 0.29931 | train_accuracy: 0.88333 | valid_logloss: 0.34483 | valid_accuracy: 0.87622 |  0:02:13s\n",
            "epoch 67 | loss: 0.31051 | train_logloss: 0.31387 | train_accuracy: 0.88611 | valid_logloss: 0.36012 | valid_accuracy: 0.87667 |  0:02:15s\n",
            "epoch 68 | loss: 0.31177 | train_logloss: 0.29214 | train_accuracy: 0.88667 | valid_logloss: 0.35076 | valid_accuracy: 0.87733 |  0:02:17s\n",
            "epoch 69 | loss: 0.30384 | train_logloss: 0.28765 | train_accuracy: 0.88844 | valid_logloss: 0.33808 | valid_accuracy: 0.882   |  0:02:19s\n",
            "epoch 70 | loss: 0.30068 | train_logloss: 0.28109 | train_accuracy: 0.89556 | valid_logloss: 0.33904 | valid_accuracy: 0.883   |  0:02:21s\n",
            "epoch 71 | loss: 0.28861 | train_logloss: 0.27705 | train_accuracy: 0.89467 | valid_logloss: 0.3436  | valid_accuracy: 0.88189 |  0:02:23s\n",
            "epoch 72 | loss: 0.2913  | train_logloss: 0.2739  | train_accuracy: 0.89533 | valid_logloss: 0.34406 | valid_accuracy: 0.88244 |  0:02:25s\n",
            "epoch 73 | loss: 0.28936 | train_logloss: 0.27712 | train_accuracy: 0.89211 | valid_logloss: 0.33666 | valid_accuracy: 0.87978 |  0:02:27s\n",
            "epoch 74 | loss: 0.287   | train_logloss: 0.26985 | train_accuracy: 0.89556 | valid_logloss: 0.33485 | valid_accuracy: 0.88411 |  0:02:29s\n",
            "epoch 75 | loss: 0.28923 | train_logloss: 0.27925 | train_accuracy: 0.89367 | valid_logloss: 0.34236 | valid_accuracy: 0.88022 |  0:02:31s\n",
            "epoch 76 | loss: 0.29199 | train_logloss: 0.28139 | train_accuracy: 0.89011 | valid_logloss: 0.35469 | valid_accuracy: 0.87744 |  0:02:33s\n",
            "epoch 77 | loss: 0.28591 | train_logloss: 0.2658  | train_accuracy: 0.89744 | valid_logloss: 0.34711 | valid_accuracy: 0.88222 |  0:02:35s\n",
            "epoch 78 | loss: 0.28439 | train_logloss: 0.28216 | train_accuracy: 0.89189 | valid_logloss: 0.34847 | valid_accuracy: 0.88244 |  0:02:37s\n",
            "epoch 79 | loss: 0.28571 | train_logloss: 0.26967 | train_accuracy: 0.89822 | valid_logloss: 0.33175 | valid_accuracy: 0.88444 |  0:02:39s\n",
            "epoch 80 | loss: 0.28446 | train_logloss: 0.2796  | train_accuracy: 0.89478 | valid_logloss: 0.3514  | valid_accuracy: 0.877   |  0:02:41s\n",
            "epoch 81 | loss: 0.2877  | train_logloss: 0.27085 | train_accuracy: 0.89667 | valid_logloss: 0.34059 | valid_accuracy: 0.87922 |  0:02:43s\n",
            "epoch 82 | loss: 0.28687 | train_logloss: 0.27221 | train_accuracy: 0.89667 | valid_logloss: 0.35283 | valid_accuracy: 0.87711 |  0:02:45s\n",
            "epoch 83 | loss: 0.28196 | train_logloss: 0.2637  | train_accuracy: 0.89744 | valid_logloss: 0.34338 | valid_accuracy: 0.87867 |  0:02:47s\n",
            "epoch 84 | loss: 0.27379 | train_logloss: 0.26336 | train_accuracy: 0.90133 | valid_logloss: 0.34357 | valid_accuracy: 0.88044 |  0:02:49s\n",
            "epoch 85 | loss: 0.27933 | train_logloss: 0.27143 | train_accuracy: 0.89511 | valid_logloss: 0.35831 | valid_accuracy: 0.87511 |  0:02:51s\n",
            "epoch 86 | loss: 0.28216 | train_logloss: 0.26628 | train_accuracy: 0.89767 | valid_logloss: 0.3426  | valid_accuracy: 0.88133 |  0:02:53s\n",
            "epoch 87 | loss: 0.27805 | train_logloss: 0.27666 | train_accuracy: 0.89133 | valid_logloss: 0.3567  | valid_accuracy: 0.87567 |  0:02:55s\n",
            "epoch 88 | loss: 0.28739 | train_logloss: 0.28374 | train_accuracy: 0.89111 | valid_logloss: 0.36935 | valid_accuracy: 0.87644 |  0:02:57s\n",
            "epoch 89 | loss: 0.28573 | train_logloss: 0.28334 | train_accuracy: 0.89678 | valid_logloss: 0.35195 | valid_accuracy: 0.87911 |  0:02:59s\n",
            "epoch 90 | loss: 0.3051  | train_logloss: 0.28295 | train_accuracy: 0.89611 | valid_logloss: 0.34532 | valid_accuracy: 0.87978 |  0:03:01s\n",
            "epoch 91 | loss: 0.29335 | train_logloss: 0.27964 | train_accuracy: 0.89367 | valid_logloss: 0.34412 | valid_accuracy: 0.88033 |  0:03:03s\n",
            "epoch 92 | loss: 0.28965 | train_logloss: 0.26676 | train_accuracy: 0.89956 | valid_logloss: 0.33713 | valid_accuracy: 0.88256 |  0:03:05s\n",
            "epoch 93 | loss: 0.28039 | train_logloss: 0.26974 | train_accuracy: 0.89778 | valid_logloss: 0.34131 | valid_accuracy: 0.88233 |  0:03:07s\n",
            "epoch 94 | loss: 0.277   | train_logloss: 0.27246 | train_accuracy: 0.89056 | valid_logloss: 0.34698 | valid_accuracy: 0.88    |  0:03:08s\n",
            "epoch 95 | loss: 0.28087 | train_logloss: 0.26299 | train_accuracy: 0.89778 | valid_logloss: 0.35119 | valid_accuracy: 0.88367 |  0:03:10s\n",
            "epoch 96 | loss: 0.27764 | train_logloss: 0.26932 | train_accuracy: 0.89667 | valid_logloss: 0.34941 | valid_accuracy: 0.88    |  0:03:12s\n",
            "epoch 97 | loss: 0.27653 | train_logloss: 0.26025 | train_accuracy: 0.90178 | valid_logloss: 0.33518 | valid_accuracy: 0.88233 |  0:03:14s\n",
            "epoch 98 | loss: 0.2729  | train_logloss: 0.26268 | train_accuracy: 0.89789 | valid_logloss: 0.33975 | valid_accuracy: 0.87767 |  0:03:16s\n",
            "epoch 99 | loss: 0.26912 | train_logloss: 0.26363 | train_accuracy: 0.89889 | valid_logloss: 0.35084 | valid_accuracy: 0.88011 |  0:03:18s\n",
            "epoch 100| loss: 0.27583 | train_logloss: 0.29648 | train_accuracy: 0.88611 | valid_logloss: 0.38165 | valid_accuracy: 0.87011 |  0:03:20s\n",
            "epoch 101| loss: 0.27497 | train_logloss: 0.26547 | train_accuracy: 0.898   | valid_logloss: 0.35022 | valid_accuracy: 0.88011 |  0:03:22s\n",
            "epoch 102| loss: 0.27828 | train_logloss: 0.2617  | train_accuracy: 0.89911 | valid_logloss: 0.34669 | valid_accuracy: 0.87967 |  0:03:24s\n",
            "epoch 103| loss: 0.26929 | train_logloss: 0.25366 | train_accuracy: 0.90267 | valid_logloss: 0.34595 | valid_accuracy: 0.884   |  0:03:26s\n",
            "epoch 104| loss: 0.26358 | train_logloss: 0.25457 | train_accuracy: 0.9     | valid_logloss: 0.35783 | valid_accuracy: 0.88178 |  0:03:28s\n",
            "epoch 105| loss: 0.26066 | train_logloss: 0.24294 | train_accuracy: 0.90678 | valid_logloss: 0.34235 | valid_accuracy: 0.88311 |  0:03:30s\n",
            "epoch 106| loss: 0.26403 | train_logloss: 0.25414 | train_accuracy: 0.90444 | valid_logloss: 0.34472 | valid_accuracy: 0.88289 |  0:03:32s\n",
            "epoch 107| loss: 0.26025 | train_logloss: 0.24579 | train_accuracy: 0.90678 | valid_logloss: 0.34677 | valid_accuracy: 0.88278 |  0:03:34s\n",
            "epoch 108| loss: 0.26196 | train_logloss: 0.24167 | train_accuracy: 0.90822 | valid_logloss: 0.33993 | valid_accuracy: 0.88478 |  0:03:36s\n",
            "epoch 109| loss: 0.25775 | train_logloss: 0.23815 | train_accuracy: 0.90689 | valid_logloss: 0.35183 | valid_accuracy: 0.88167 |  0:03:38s\n",
            "epoch 110| loss: 0.2584  | train_logloss: 0.24438 | train_accuracy: 0.90733 | valid_logloss: 0.35908 | valid_accuracy: 0.87922 |  0:03:40s\n",
            "epoch 111| loss: 0.26242 | train_logloss: 0.24125 | train_accuracy: 0.90778 | valid_logloss: 0.35037 | valid_accuracy: 0.88267 |  0:03:42s\n",
            "epoch 112| loss: 0.25777 | train_logloss: 0.24354 | train_accuracy: 0.90622 | valid_logloss: 0.34539 | valid_accuracy: 0.88044 |  0:03:44s\n",
            "epoch 113| loss: 0.25908 | train_logloss: 0.2378  | train_accuracy: 0.90711 | valid_logloss: 0.35045 | valid_accuracy: 0.88244 |  0:03:46s\n",
            "epoch 114| loss: 0.25696 | train_logloss: 0.25719 | train_accuracy: 0.90167 | valid_logloss: 0.3611  | valid_accuracy: 0.87733 |  0:03:48s\n",
            "epoch 115| loss: 0.26253 | train_logloss: 0.23006 | train_accuracy: 0.91222 | valid_logloss: 0.34643 | valid_accuracy: 0.88267 |  0:03:50s\n",
            "epoch 116| loss: 0.25505 | train_logloss: 0.24581 | train_accuracy: 0.90322 | valid_logloss: 0.35255 | valid_accuracy: 0.88033 |  0:03:52s\n",
            "epoch 117| loss: 0.25561 | train_logloss: 0.24535 | train_accuracy: 0.90767 | valid_logloss: 0.34969 | valid_accuracy: 0.88122 |  0:03:54s\n",
            "epoch 118| loss: 0.2634  | train_logloss: 0.24568 | train_accuracy: 0.90333 | valid_logloss: 0.35517 | valid_accuracy: 0.87878 |  0:03:56s\n",
            "epoch 119| loss: 0.2696  | train_logloss: 0.26637 | train_accuracy: 0.89933 | valid_logloss: 0.36289 | valid_accuracy: 0.87389 |  0:03:58s\n",
            "epoch 120| loss: 0.26707 | train_logloss: 0.2411  | train_accuracy: 0.90889 | valid_logloss: 0.34593 | valid_accuracy: 0.883   |  0:04:00s\n",
            "epoch 121| loss: 0.25464 | train_logloss: 0.23856 | train_accuracy: 0.909   | valid_logloss: 0.33993 | valid_accuracy: 0.88456 |  0:04:02s\n",
            "epoch 122| loss: 0.25401 | train_logloss: 0.24333 | train_accuracy: 0.90822 | valid_logloss: 0.35041 | valid_accuracy: 0.88067 |  0:04:04s\n",
            "epoch 123| loss: 0.25435 | train_logloss: 0.24961 | train_accuracy: 0.90244 | valid_logloss: 0.36711 | valid_accuracy: 0.87467 |  0:04:06s\n",
            "epoch 124| loss: 0.24971 | train_logloss: 0.23556 | train_accuracy: 0.90544 | valid_logloss: 0.35725 | valid_accuracy: 0.881   |  0:04:08s\n",
            "epoch 125| loss: 0.25016 | train_logloss: 0.22659 | train_accuracy: 0.91344 | valid_logloss: 0.36239 | valid_accuracy: 0.88067 |  0:04:10s\n",
            "epoch 126| loss: 0.24331 | train_logloss: 0.24174 | train_accuracy: 0.907   | valid_logloss: 0.36309 | valid_accuracy: 0.88144 |  0:04:12s\n",
            "epoch 127| loss: 0.24328 | train_logloss: 0.22496 | train_accuracy: 0.91422 | valid_logloss: 0.36839 | valid_accuracy: 0.87822 |  0:04:14s\n",
            "epoch 128| loss: 0.24339 | train_logloss: 0.22602 | train_accuracy: 0.91456 | valid_logloss: 0.36287 | valid_accuracy: 0.88122 |  0:04:16s\n",
            "epoch 129| loss: 0.24238 | train_logloss: 0.22362 | train_accuracy: 0.91533 | valid_logloss: 0.35532 | valid_accuracy: 0.88133 |  0:04:18s\n",
            "epoch 130| loss: 0.2419  | train_logloss: 0.22614 | train_accuracy: 0.91222 | valid_logloss: 0.36395 | valid_accuracy: 0.88244 |  0:04:20s\n",
            "epoch 131| loss: 0.23927 | train_logloss: 0.22466 | train_accuracy: 0.916   | valid_logloss: 0.35755 | valid_accuracy: 0.88322 |  0:04:22s\n",
            "epoch 132| loss: 0.23954 | train_logloss: 0.21884 | train_accuracy: 0.91122 | valid_logloss: 0.362   | valid_accuracy: 0.88567 |  0:04:24s\n",
            "epoch 133| loss: 0.23963 | train_logloss: 0.22504 | train_accuracy: 0.912   | valid_logloss: 0.37198 | valid_accuracy: 0.87978 |  0:04:26s\n",
            "epoch 134| loss: 0.25093 | train_logloss: 0.25392 | train_accuracy: 0.90333 | valid_logloss: 0.3764  | valid_accuracy: 0.87167 |  0:04:28s\n",
            "epoch 135| loss: 0.27182 | train_logloss: 0.25964 | train_accuracy: 0.90367 | valid_logloss: 0.38447 | valid_accuracy: 0.87778 |  0:04:30s\n",
            "epoch 136| loss: 0.29384 | train_logloss: 0.28096 | train_accuracy: 0.893   | valid_logloss: 0.36971 | valid_accuracy: 0.87322 |  0:04:32s\n",
            "epoch 137| loss: 0.28363 | train_logloss: 0.2538  | train_accuracy: 0.90233 | valid_logloss: 0.34762 | valid_accuracy: 0.87944 |  0:04:34s\n",
            "epoch 138| loss: 0.26875 | train_logloss: 0.2412  | train_accuracy: 0.90956 | valid_logloss: 0.34397 | valid_accuracy: 0.87944 |  0:04:35s\n",
            "epoch 139| loss: 0.25897 | train_logloss: 0.22996 | train_accuracy: 0.912   | valid_logloss: 0.34684 | valid_accuracy: 0.88467 |  0:04:37s\n",
            "epoch 140| loss: 0.24791 | train_logloss: 0.22809 | train_accuracy: 0.91167 | valid_logloss: 0.35549 | valid_accuracy: 0.88122 |  0:04:39s\n",
            "epoch 141| loss: 0.2567  | train_logloss: 0.23087 | train_accuracy: 0.91278 | valid_logloss: 0.35381 | valid_accuracy: 0.88044 |  0:04:41s\n",
            "epoch 142| loss: 0.24904 | train_logloss: 0.22862 | train_accuracy: 0.91322 | valid_logloss: 0.34763 | valid_accuracy: 0.88156 |  0:04:43s\n",
            "epoch 143| loss: 0.25057 | train_logloss: 0.26485 | train_accuracy: 0.90033 | valid_logloss: 0.36105 | valid_accuracy: 0.87989 |  0:04:45s\n",
            "epoch 144| loss: 0.27856 | train_logloss: 0.25223 | train_accuracy: 0.90033 | valid_logloss: 0.34348 | valid_accuracy: 0.88222 |  0:04:47s\n",
            "epoch 145| loss: 0.2587  | train_logloss: 0.25019 | train_accuracy: 0.907   | valid_logloss: 0.34849 | valid_accuracy: 0.87978 |  0:04:49s\n",
            "epoch 146| loss: 0.2557  | train_logloss: 0.23311 | train_accuracy: 0.90867 | valid_logloss: 0.34412 | valid_accuracy: 0.88222 |  0:04:51s\n",
            "epoch 147| loss: 0.24851 | train_logloss: 0.22719 | train_accuracy: 0.91422 | valid_logloss: 0.36394 | valid_accuracy: 0.88356 |  0:04:53s\n",
            "epoch 148| loss: 0.24981 | train_logloss: 0.22479 | train_accuracy: 0.91433 | valid_logloss: 0.35875 | valid_accuracy: 0.87844 |  0:04:55s\n",
            "epoch 149| loss: 0.23961 | train_logloss: 0.22332 | train_accuracy: 0.91511 | valid_logloss: 0.35802 | valid_accuracy: 0.87611 |  0:04:57s\n",
            "epoch 150| loss: 0.24933 | train_logloss: 0.25419 | train_accuracy: 0.89967 | valid_logloss: 0.36185 | valid_accuracy: 0.87311 |  0:04:59s\n",
            "epoch 151| loss: 0.25234 | train_logloss: 0.26095 | train_accuracy: 0.89744 | valid_logloss: 0.37777 | valid_accuracy: 0.87378 |  0:05:01s\n",
            "epoch 152| loss: 0.25432 | train_logloss: 0.24472 | train_accuracy: 0.90778 | valid_logloss: 0.37956 | valid_accuracy: 0.87956 |  0:05:03s\n",
            "epoch 153| loss: 0.25383 | train_logloss: 0.22902 | train_accuracy: 0.91522 | valid_logloss: 0.36161 | valid_accuracy: 0.88078 |  0:05:05s\n",
            "epoch 154| loss: 0.24187 | train_logloss: 0.21513 | train_accuracy: 0.91867 | valid_logloss: 0.35865 | valid_accuracy: 0.88222 |  0:05:07s\n",
            "epoch 155| loss: 0.23397 | train_logloss: 0.21924 | train_accuracy: 0.91689 | valid_logloss: 0.36648 | valid_accuracy: 0.879   |  0:05:09s\n",
            "epoch 156| loss: 0.23255 | train_logloss: 0.21321 | train_accuracy: 0.91778 | valid_logloss: 0.37281 | valid_accuracy: 0.87978 |  0:05:11s\n",
            "epoch 157| loss: 0.24447 | train_logloss: 0.2381  | train_accuracy: 0.91278 | valid_logloss: 0.36934 | valid_accuracy: 0.88267 |  0:05:13s\n",
            "epoch 158| loss: 0.2461  | train_logloss: 0.23606 | train_accuracy: 0.90589 | valid_logloss: 0.37897 | valid_accuracy: 0.87822 |  0:05:15s\n",
            "epoch 159| loss: 0.25089 | train_logloss: 0.24728 | train_accuracy: 0.90922 | valid_logloss: 0.38344 | valid_accuracy: 0.879   |  0:05:17s\n",
            "epoch 160| loss: 0.25606 | train_logloss: 0.23302 | train_accuracy: 0.91156 | valid_logloss: 0.37018 | valid_accuracy: 0.87889 |  0:05:19s\n",
            "epoch 161| loss: 0.24289 | train_logloss: 0.22649 | train_accuracy: 0.91556 | valid_logloss: 0.36821 | valid_accuracy: 0.87356 |  0:05:21s\n",
            "epoch 162| loss: 0.23745 | train_logloss: 0.21967 | train_accuracy: 0.916   | valid_logloss: 0.37478 | valid_accuracy: 0.881   |  0:05:23s\n",
            "epoch 163| loss: 0.23903 | train_logloss: 0.22983 | train_accuracy: 0.91356 | valid_logloss: 0.37682 | valid_accuracy: 0.87544 |  0:05:25s\n",
            "epoch 164| loss: 0.24388 | train_logloss: 0.2174  | train_accuracy: 0.91789 | valid_logloss: 0.3672  | valid_accuracy: 0.88156 |  0:05:27s\n",
            "epoch 165| loss: 0.23858 | train_logloss: 0.22932 | train_accuracy: 0.91356 | valid_logloss: 0.3733  | valid_accuracy: 0.88122 |  0:05:29s\n",
            "epoch 166| loss: 0.23492 | train_logloss: 0.2205  | train_accuracy: 0.91256 | valid_logloss: 0.38061 | valid_accuracy: 0.87922 |  0:05:31s\n",
            "epoch 167| loss: 0.23279 | train_logloss: 0.21342 | train_accuracy: 0.91956 | valid_logloss: 0.37547 | valid_accuracy: 0.88011 |  0:05:33s\n",
            "epoch 168| loss: 0.22407 | train_logloss: 0.20618 | train_accuracy: 0.92367 | valid_logloss: 0.3632  | valid_accuracy: 0.877   |  0:05:35s\n",
            "epoch 169| loss: 0.23327 | train_logloss: 0.22032 | train_accuracy: 0.91889 | valid_logloss: 0.37926 | valid_accuracy: 0.87978 |  0:05:37s\n",
            "epoch 170| loss: 0.22983 | train_logloss: 0.20031 | train_accuracy: 0.92556 | valid_logloss: 0.37765 | valid_accuracy: 0.88011 |  0:05:39s\n",
            "epoch 171| loss: 0.2285  | train_logloss: 0.20502 | train_accuracy: 0.92356 | valid_logloss: 0.38235 | valid_accuracy: 0.87744 |  0:05:41s\n",
            "epoch 172| loss: 0.22811 | train_logloss: 0.22201 | train_accuracy: 0.922   | valid_logloss: 0.37566 | valid_accuracy: 0.87978 |  0:05:43s\n",
            "epoch 173| loss: 0.23082 | train_logloss: 0.23957 | train_accuracy: 0.91444 | valid_logloss: 0.39986 | valid_accuracy: 0.87644 |  0:05:45s\n",
            "epoch 174| loss: 0.24308 | train_logloss: 0.26579 | train_accuracy: 0.90567 | valid_logloss: 0.43157 | valid_accuracy: 0.866   |  0:05:47s\n",
            "epoch 175| loss: 0.25899 | train_logloss: 0.2302  | train_accuracy: 0.91367 | valid_logloss: 0.3559  | valid_accuracy: 0.87944 |  0:05:49s\n",
            "epoch 176| loss: 0.24481 | train_logloss: 0.21408 | train_accuracy: 0.918   | valid_logloss: 0.35481 | valid_accuracy: 0.88333 |  0:05:51s\n",
            "epoch 177| loss: 0.23287 | train_logloss: 0.20723 | train_accuracy: 0.92067 | valid_logloss: 0.36848 | valid_accuracy: 0.878   |  0:05:53s\n",
            "epoch 178| loss: 0.21805 | train_logloss: 0.21501 | train_accuracy: 0.91444 | valid_logloss: 0.39331 | valid_accuracy: 0.87056 |  0:05:55s\n",
            "epoch 179| loss: 0.22263 | train_logloss: 0.20925 | train_accuracy: 0.91967 | valid_logloss: 0.39361 | valid_accuracy: 0.87811 |  0:05:57s\n",
            "epoch 180| loss: 0.22415 | train_logloss: 0.19699 | train_accuracy: 0.92456 | valid_logloss: 0.37615 | valid_accuracy: 0.87833 |  0:05:59s\n",
            "epoch 181| loss: 0.22447 | train_logloss: 0.2087  | train_accuracy: 0.921   | valid_logloss: 0.3911  | valid_accuracy: 0.876   |  0:06:01s\n",
            "epoch 182| loss: 0.22038 | train_logloss: 0.20039 | train_accuracy: 0.92256 | valid_logloss: 0.38075 | valid_accuracy: 0.87967 |  0:06:03s\n",
            "\n",
            "Early stopping occurred at epoch 182 with best_epoch = 132 and best_valid_accuracy = 0.88567\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[398815  16146  65221]\n",
            " [ 13449 944188  42363]\n",
            " [ 26090  28414 378083]]\n",
            "Testing Score:  0.8856666666666667\n",
            "Confusion Matrix: \n",
            " [[415499  13908  50775]\n",
            " [ 14615 951836  33549]\n",
            " [ 39000  28591 364996]]\n",
            "Testing Score:  0.8837777777777778\n",
            "{'Rows': 9000, 'Nd': 64, 'Na': 64, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 364.02540159225464, 'time_tn': 90.06645321846008, 'accuracy_tn': 0.8856666666666667, 'time_learn_gb': 3.411332130432129, 'time_gb': 16.978554487228394, 'accuracy_gb': 0.8837777777777778}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "1caD4daqH5qn",
        "outputId": "6488090d-5725-4fa7-eef6-1427a9e90453"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1  9000  9000  9000  16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2  9000  9000  9000  64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "\n",
              "[3 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT6LTX2j_R1T",
        "outputId": "2d3bce60-e806-402f-8a3c-fc449c0d958c"
      },
      "source": [
        "time_model(number_exp=4, \n",
        "     Rows=9000, \n",
        "     Nd=32,\tNa=32,\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=3, decision=3, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.79962 | train_logloss: 10.63308| train_accuracy: 0.40778 | valid_logloss: 11.09464| valid_accuracy: 0.39244 |  0:00:01s\n",
            "epoch 1  | loss: 0.47858 | train_logloss: 5.28805 | train_accuracy: 0.21689 | valid_logloss: 5.14806 | valid_accuracy: 0.22322 |  0:00:03s\n",
            "epoch 2  | loss: 0.41724 | train_logloss: 2.92017 | train_accuracy: 0.32989 | valid_logloss: 2.87568 | valid_accuracy: 0.33378 |  0:00:05s\n",
            "epoch 3  | loss: 0.39143 | train_logloss: 3.39911 | train_accuracy: 0.42067 | valid_logloss: 3.40527 | valid_accuracy: 0.41289 |  0:00:07s\n",
            "epoch 4  | loss: 0.39031 | train_logloss: 3.75939 | train_accuracy: 0.36456 | valid_logloss: 3.75492 | valid_accuracy: 0.36511 |  0:00:08s\n",
            "epoch 5  | loss: 0.36455 | train_logloss: 7.18896 | train_accuracy: 0.33311 | valid_logloss: 7.22629 | valid_accuracy: 0.33367 |  0:00:10s\n",
            "epoch 6  | loss: 0.34964 | train_logloss: 4.84895 | train_accuracy: 0.34778 | valid_logloss: 4.85696 | valid_accuracy: 0.34989 |  0:00:12s\n",
            "epoch 7  | loss: 0.35184 | train_logloss: 1.69289 | train_accuracy: 0.43544 | valid_logloss: 1.66305 | valid_accuracy: 0.44133 |  0:00:14s\n",
            "epoch 8  | loss: 0.34419 | train_logloss: 1.46925 | train_accuracy: 0.41856 | valid_logloss: 1.43092 | valid_accuracy: 0.42844 |  0:00:15s\n",
            "epoch 9  | loss: 0.3557  | train_logloss: 0.9943  | train_accuracy: 0.53889 | valid_logloss: 1.00286 | valid_accuracy: 0.53478 |  0:00:17s\n",
            "epoch 10 | loss: 0.33745 | train_logloss: 1.11996 | train_accuracy: 0.53722 | valid_logloss: 1.12678 | valid_accuracy: 0.53467 |  0:00:19s\n",
            "epoch 11 | loss: 0.33817 | train_logloss: 0.94693 | train_accuracy: 0.63167 | valid_logloss: 0.9574  | valid_accuracy: 0.62489 |  0:00:21s\n",
            "epoch 12 | loss: 0.33199 | train_logloss: 0.60007 | train_accuracy: 0.77489 | valid_logloss: 0.61264 | valid_accuracy: 0.76767 |  0:00:23s\n",
            "epoch 13 | loss: 0.33016 | train_logloss: 0.64672 | train_accuracy: 0.74867 | valid_logloss: 0.65667 | valid_accuracy: 0.74478 |  0:00:24s\n",
            "epoch 14 | loss: 0.33733 | train_logloss: 0.49695 | train_accuracy: 0.80367 | valid_logloss: 0.51396 | valid_accuracy: 0.79778 |  0:00:26s\n",
            "epoch 15 | loss: 0.33013 | train_logloss: 0.45643 | train_accuracy: 0.83233 | valid_logloss: 0.46929 | valid_accuracy: 0.828   |  0:00:28s\n",
            "epoch 16 | loss: 0.33106 | train_logloss: 0.4104  | train_accuracy: 0.85122 | valid_logloss: 0.42802 | valid_accuracy: 0.845   |  0:00:30s\n",
            "epoch 17 | loss: 0.33062 | train_logloss: 0.42484 | train_accuracy: 0.84711 | valid_logloss: 0.43802 | valid_accuracy: 0.84356 |  0:00:31s\n",
            "epoch 18 | loss: 0.3294  | train_logloss: 0.3936  | train_accuracy: 0.84478 | valid_logloss: 0.40909 | valid_accuracy: 0.84089 |  0:00:33s\n",
            "epoch 19 | loss: 0.3232  | train_logloss: 0.33361 | train_accuracy: 0.87756 | valid_logloss: 0.35989 | valid_accuracy: 0.87267 |  0:00:35s\n",
            "epoch 20 | loss: 0.31723 | train_logloss: 0.312   | train_accuracy: 0.881   | valid_logloss: 0.34681 | valid_accuracy: 0.87467 |  0:00:37s\n",
            "epoch 21 | loss: 0.31696 | train_logloss: 0.32656 | train_accuracy: 0.87511 | valid_logloss: 0.36895 | valid_accuracy: 0.86744 |  0:00:38s\n",
            "epoch 22 | loss: 0.31939 | train_logloss: 0.32367 | train_accuracy: 0.87644 | valid_logloss: 0.36072 | valid_accuracy: 0.87067 |  0:00:40s\n",
            "epoch 23 | loss: 0.31904 | train_logloss: 0.31215 | train_accuracy: 0.88456 | valid_logloss: 0.3397  | valid_accuracy: 0.87789 |  0:00:42s\n",
            "epoch 24 | loss: 0.31995 | train_logloss: 0.31277 | train_accuracy: 0.88033 | valid_logloss: 0.34914 | valid_accuracy: 0.87433 |  0:00:44s\n",
            "epoch 25 | loss: 0.32056 | train_logloss: 0.31336 | train_accuracy: 0.88133 | valid_logloss: 0.34408 | valid_accuracy: 0.87544 |  0:00:45s\n",
            "epoch 26 | loss: 0.33668 | train_logloss: 0.33939 | train_accuracy: 0.87567 | valid_logloss: 0.37077 | valid_accuracy: 0.86822 |  0:00:47s\n",
            "epoch 27 | loss: 0.31729 | train_logloss: 0.30067 | train_accuracy: 0.88644 | valid_logloss: 0.34054 | valid_accuracy: 0.87811 |  0:00:49s\n",
            "epoch 28 | loss: 0.31097 | train_logloss: 0.30261 | train_accuracy: 0.88667 | valid_logloss: 0.34328 | valid_accuracy: 0.87922 |  0:00:51s\n",
            "epoch 29 | loss: 0.30889 | train_logloss: 0.29781 | train_accuracy: 0.88367 | valid_logloss: 0.34572 | valid_accuracy: 0.87589 |  0:00:52s\n",
            "epoch 30 | loss: 0.30796 | train_logloss: 0.28819 | train_accuracy: 0.889   | valid_logloss: 0.33728 | valid_accuracy: 0.87878 |  0:00:54s\n",
            "epoch 31 | loss: 0.29793 | train_logloss: 0.28501 | train_accuracy: 0.89089 | valid_logloss: 0.33748 | valid_accuracy: 0.87989 |  0:00:56s\n",
            "epoch 32 | loss: 0.29326 | train_logloss: 0.28371 | train_accuracy: 0.89089 | valid_logloss: 0.33116 | valid_accuracy: 0.88178 |  0:00:58s\n",
            "epoch 33 | loss: 0.29764 | train_logloss: 0.28097 | train_accuracy: 0.89222 | valid_logloss: 0.33006 | valid_accuracy: 0.87967 |  0:00:59s\n",
            "epoch 34 | loss: 0.29791 | train_logloss: 0.27466 | train_accuracy: 0.89478 | valid_logloss: 0.33678 | valid_accuracy: 0.87944 |  0:01:01s\n",
            "epoch 35 | loss: 0.3065  | train_logloss: 0.31145 | train_accuracy: 0.88489 | valid_logloss: 0.35938 | valid_accuracy: 0.87189 |  0:01:03s\n",
            "epoch 36 | loss: 0.30729 | train_logloss: 0.28798 | train_accuracy: 0.89189 | valid_logloss: 0.34133 | valid_accuracy: 0.88111 |  0:01:05s\n",
            "epoch 37 | loss: 0.30736 | train_logloss: 0.283   | train_accuracy: 0.88978 | valid_logloss: 0.33165 | valid_accuracy: 0.87833 |  0:01:06s\n",
            "epoch 38 | loss: 0.29759 | train_logloss: 0.29181 | train_accuracy: 0.88867 | valid_logloss: 0.33934 | valid_accuracy: 0.88089 |  0:01:08s\n",
            "epoch 39 | loss: 0.3007  | train_logloss: 0.29546 | train_accuracy: 0.88744 | valid_logloss: 0.34068 | valid_accuracy: 0.87711 |  0:01:10s\n",
            "epoch 40 | loss: 0.30433 | train_logloss: 0.28451 | train_accuracy: 0.89278 | valid_logloss: 0.33661 | valid_accuracy: 0.88456 |  0:01:12s\n",
            "epoch 41 | loss: 0.29514 | train_logloss: 0.28259 | train_accuracy: 0.88789 | valid_logloss: 0.33121 | valid_accuracy: 0.87756 |  0:01:14s\n",
            "epoch 42 | loss: 0.29508 | train_logloss: 0.27514 | train_accuracy: 0.89389 | valid_logloss: 0.32857 | valid_accuracy: 0.87844 |  0:01:15s\n",
            "epoch 43 | loss: 0.29111 | train_logloss: 0.28349 | train_accuracy: 0.88811 | valid_logloss: 0.33681 | valid_accuracy: 0.87433 |  0:01:17s\n",
            "epoch 44 | loss: 0.30151 | train_logloss: 0.28483 | train_accuracy: 0.89289 | valid_logloss: 0.33622 | valid_accuracy: 0.87656 |  0:01:19s\n",
            "epoch 45 | loss: 0.29476 | train_logloss: 0.27693 | train_accuracy: 0.89289 | valid_logloss: 0.33499 | valid_accuracy: 0.87789 |  0:01:21s\n",
            "epoch 46 | loss: 0.29177 | train_logloss: 0.2778  | train_accuracy: 0.892   | valid_logloss: 0.34124 | valid_accuracy: 0.881   |  0:01:22s\n",
            "epoch 47 | loss: 0.28924 | train_logloss: 0.27302 | train_accuracy: 0.89378 | valid_logloss: 0.34475 | valid_accuracy: 0.87889 |  0:01:24s\n",
            "epoch 48 | loss: 0.29583 | train_logloss: 0.27156 | train_accuracy: 0.897   | valid_logloss: 0.33879 | valid_accuracy: 0.88078 |  0:01:26s\n",
            "epoch 49 | loss: 0.28604 | train_logloss: 0.27658 | train_accuracy: 0.89089 | valid_logloss: 0.35357 | valid_accuracy: 0.87811 |  0:01:28s\n",
            "epoch 50 | loss: 0.28293 | train_logloss: 0.26363 | train_accuracy: 0.89933 | valid_logloss: 0.33604 | valid_accuracy: 0.88633 |  0:01:29s\n",
            "epoch 51 | loss: 0.28439 | train_logloss: 0.2642  | train_accuracy: 0.89733 | valid_logloss: 0.34071 | valid_accuracy: 0.88167 |  0:01:31s\n",
            "epoch 52 | loss: 0.28229 | train_logloss: 0.25869 | train_accuracy: 0.89833 | valid_logloss: 0.33337 | valid_accuracy: 0.88611 |  0:01:33s\n",
            "epoch 53 | loss: 0.27867 | train_logloss: 0.26519 | train_accuracy: 0.89678 | valid_logloss: 0.34169 | valid_accuracy: 0.87933 |  0:01:35s\n",
            "epoch 54 | loss: 0.27605 | train_logloss: 0.25062 | train_accuracy: 0.90189 | valid_logloss: 0.33734 | valid_accuracy: 0.88533 |  0:01:36s\n",
            "epoch 55 | loss: 0.27201 | train_logloss: 0.26022 | train_accuracy: 0.89656 | valid_logloss: 0.34753 | valid_accuracy: 0.87922 |  0:01:38s\n",
            "epoch 56 | loss: 0.27373 | train_logloss: 0.25665 | train_accuracy: 0.89867 | valid_logloss: 0.34305 | valid_accuracy: 0.88144 |  0:01:40s\n",
            "epoch 57 | loss: 0.27181 | train_logloss: 0.25616 | train_accuracy: 0.90067 | valid_logloss: 0.35591 | valid_accuracy: 0.87833 |  0:01:42s\n",
            "epoch 58 | loss: 0.274   | train_logloss: 0.254   | train_accuracy: 0.902   | valid_logloss: 0.35201 | valid_accuracy: 0.88022 |  0:01:43s\n",
            "epoch 59 | loss: 0.28085 | train_logloss: 0.25615 | train_accuracy: 0.90033 | valid_logloss: 0.34305 | valid_accuracy: 0.88111 |  0:01:45s\n",
            "epoch 60 | loss: 0.281   | train_logloss: 0.26207 | train_accuracy: 0.90211 | valid_logloss: 0.3402  | valid_accuracy: 0.88256 |  0:01:47s\n",
            "epoch 61 | loss: 0.30947 | train_logloss: 0.32004 | train_accuracy: 0.87956 | valid_logloss: 0.37303 | valid_accuracy: 0.87311 |  0:01:49s\n",
            "epoch 62 | loss: 0.32515 | train_logloss: 0.33499 | train_accuracy: 0.87089 | valid_logloss: 0.37619 | valid_accuracy: 0.86433 |  0:01:50s\n",
            "epoch 63 | loss: 0.32635 | train_logloss: 0.31088 | train_accuracy: 0.88278 | valid_logloss: 0.34559 | valid_accuracy: 0.88111 |  0:01:52s\n",
            "epoch 64 | loss: 0.31474 | train_logloss: 0.29783 | train_accuracy: 0.88322 | valid_logloss: 0.3455  | valid_accuracy: 0.87478 |  0:01:54s\n",
            "epoch 65 | loss: 0.30375 | train_logloss: 0.28208 | train_accuracy: 0.89378 | valid_logloss: 0.33756 | valid_accuracy: 0.88011 |  0:01:56s\n",
            "epoch 66 | loss: 0.29401 | train_logloss: 0.27    | train_accuracy: 0.89789 | valid_logloss: 0.33492 | valid_accuracy: 0.88378 |  0:01:57s\n",
            "epoch 67 | loss: 0.28163 | train_logloss: 0.26399 | train_accuracy: 0.89922 | valid_logloss: 0.33226 | valid_accuracy: 0.88422 |  0:01:59s\n",
            "epoch 68 | loss: 0.28384 | train_logloss: 0.27655 | train_accuracy: 0.89733 | valid_logloss: 0.34099 | valid_accuracy: 0.882   |  0:02:01s\n",
            "epoch 69 | loss: 0.28079 | train_logloss: 0.2643  | train_accuracy: 0.89867 | valid_logloss: 0.33992 | valid_accuracy: 0.87956 |  0:02:03s\n",
            "epoch 70 | loss: 0.28274 | train_logloss: 0.26086 | train_accuracy: 0.901   | valid_logloss: 0.33898 | valid_accuracy: 0.88356 |  0:02:04s\n",
            "epoch 71 | loss: 0.27632 | train_logloss: 0.26669 | train_accuracy: 0.89689 | valid_logloss: 0.34246 | valid_accuracy: 0.88278 |  0:02:06s\n",
            "epoch 72 | loss: 0.28027 | train_logloss: 0.26398 | train_accuracy: 0.89733 | valid_logloss: 0.34508 | valid_accuracy: 0.88189 |  0:02:08s\n",
            "epoch 73 | loss: 0.28466 | train_logloss: 0.26622 | train_accuracy: 0.90078 | valid_logloss: 0.35062 | valid_accuracy: 0.88167 |  0:02:10s\n",
            "epoch 74 | loss: 0.27742 | train_logloss: 0.25361 | train_accuracy: 0.90222 | valid_logloss: 0.33376 | valid_accuracy: 0.88533 |  0:02:11s\n",
            "epoch 75 | loss: 0.27328 | train_logloss: 0.25464 | train_accuracy: 0.90356 | valid_logloss: 0.34274 | valid_accuracy: 0.87867 |  0:02:13s\n",
            "epoch 76 | loss: 0.26753 | train_logloss: 0.25344 | train_accuracy: 0.90578 | valid_logloss: 0.33894 | valid_accuracy: 0.88411 |  0:02:15s\n",
            "epoch 77 | loss: 0.27264 | train_logloss: 0.25501 | train_accuracy: 0.90144 | valid_logloss: 0.344   | valid_accuracy: 0.88167 |  0:02:17s\n",
            "epoch 78 | loss: 0.26946 | train_logloss: 0.25054 | train_accuracy: 0.90411 | valid_logloss: 0.34344 | valid_accuracy: 0.88322 |  0:02:18s\n",
            "epoch 79 | loss: 0.27362 | train_logloss: 0.26751 | train_accuracy: 0.89733 | valid_logloss: 0.34296 | valid_accuracy: 0.882   |  0:02:20s\n",
            "epoch 80 | loss: 0.28404 | train_logloss: 0.25931 | train_accuracy: 0.90322 | valid_logloss: 0.33932 | valid_accuracy: 0.88322 |  0:02:22s\n",
            "epoch 81 | loss: 0.27703 | train_logloss: 0.26823 | train_accuracy: 0.89867 | valid_logloss: 0.34691 | valid_accuracy: 0.88211 |  0:02:24s\n",
            "epoch 82 | loss: 0.27453 | train_logloss: 0.28268 | train_accuracy: 0.89067 | valid_logloss: 0.37144 | valid_accuracy: 0.87211 |  0:02:25s\n",
            "epoch 83 | loss: 0.28011 | train_logloss: 0.26245 | train_accuracy: 0.89733 | valid_logloss: 0.34827 | valid_accuracy: 0.87744 |  0:02:27s\n",
            "epoch 84 | loss: 0.27749 | train_logloss: 0.26599 | train_accuracy: 0.90022 | valid_logloss: 0.35905 | valid_accuracy: 0.88222 |  0:02:29s\n",
            "epoch 85 | loss: 0.28028 | train_logloss: 0.28252 | train_accuracy: 0.89378 | valid_logloss: 0.3613  | valid_accuracy: 0.875   |  0:02:31s\n",
            "epoch 86 | loss: 0.2833  | train_logloss: 0.26214 | train_accuracy: 0.89956 | valid_logloss: 0.33613 | valid_accuracy: 0.88456 |  0:02:32s\n",
            "epoch 87 | loss: 0.27848 | train_logloss: 0.2555  | train_accuracy: 0.90289 | valid_logloss: 0.33912 | valid_accuracy: 0.88089 |  0:02:34s\n",
            "epoch 88 | loss: 0.27219 | train_logloss: 0.25445 | train_accuracy: 0.90267 | valid_logloss: 0.34062 | valid_accuracy: 0.88222 |  0:02:36s\n",
            "epoch 89 | loss: 0.267   | train_logloss: 0.25263 | train_accuracy: 0.90222 | valid_logloss: 0.34149 | valid_accuracy: 0.88111 |  0:02:38s\n",
            "epoch 90 | loss: 0.26541 | train_logloss: 0.25275 | train_accuracy: 0.90222 | valid_logloss: 0.34549 | valid_accuracy: 0.88233 |  0:02:39s\n",
            "epoch 91 | loss: 0.26938 | train_logloss: 0.2455  | train_accuracy: 0.90611 | valid_logloss: 0.32399 | valid_accuracy: 0.88733 |  0:02:41s\n",
            "epoch 92 | loss: 0.26586 | train_logloss: 0.25023 | train_accuracy: 0.90344 | valid_logloss: 0.34018 | valid_accuracy: 0.87967 |  0:02:43s\n",
            "epoch 93 | loss: 0.2601  | train_logloss: 0.23736 | train_accuracy: 0.905   | valid_logloss: 0.33148 | valid_accuracy: 0.88156 |  0:02:45s\n",
            "epoch 94 | loss: 0.25592 | train_logloss: 0.23959 | train_accuracy: 0.90756 | valid_logloss: 0.34806 | valid_accuracy: 0.88089 |  0:02:46s\n",
            "epoch 95 | loss: 0.2566  | train_logloss: 0.23802 | train_accuracy: 0.90656 | valid_logloss: 0.35852 | valid_accuracy: 0.878   |  0:02:48s\n",
            "epoch 96 | loss: 0.26089 | train_logloss: 0.24164 | train_accuracy: 0.90667 | valid_logloss: 0.34441 | valid_accuracy: 0.87767 |  0:02:50s\n",
            "epoch 97 | loss: 0.25886 | train_logloss: 0.25241 | train_accuracy: 0.90356 | valid_logloss: 0.37246 | valid_accuracy: 0.87444 |  0:02:52s\n",
            "epoch 98 | loss: 0.258   | train_logloss: 0.23352 | train_accuracy: 0.91033 | valid_logloss: 0.34084 | valid_accuracy: 0.883   |  0:02:53s\n",
            "epoch 99 | loss: 0.25037 | train_logloss: 0.24306 | train_accuracy: 0.90544 | valid_logloss: 0.35025 | valid_accuracy: 0.88011 |  0:02:55s\n",
            "epoch 100| loss: 0.25694 | train_logloss: 0.23307 | train_accuracy: 0.90656 | valid_logloss: 0.34136 | valid_accuracy: 0.88533 |  0:02:57s\n",
            "epoch 101| loss: 0.24973 | train_logloss: 0.22839 | train_accuracy: 0.91356 | valid_logloss: 0.34992 | valid_accuracy: 0.88111 |  0:02:59s\n",
            "epoch 102| loss: 0.25316 | train_logloss: 0.24125 | train_accuracy: 0.90567 | valid_logloss: 0.35254 | valid_accuracy: 0.879   |  0:03:00s\n",
            "epoch 103| loss: 0.25062 | train_logloss: 0.226   | train_accuracy: 0.91122 | valid_logloss: 0.3492  | valid_accuracy: 0.88156 |  0:03:02s\n",
            "epoch 104| loss: 0.24998 | train_logloss: 0.24056 | train_accuracy: 0.91144 | valid_logloss: 0.3461  | valid_accuracy: 0.88133 |  0:03:04s\n",
            "epoch 105| loss: 0.25318 | train_logloss: 0.22818 | train_accuracy: 0.91167 | valid_logloss: 0.34153 | valid_accuracy: 0.88389 |  0:03:06s\n",
            "epoch 106| loss: 0.25929 | train_logloss: 0.23838 | train_accuracy: 0.90533 | valid_logloss: 0.33821 | valid_accuracy: 0.88467 |  0:03:07s\n",
            "epoch 107| loss: 0.26098 | train_logloss: 0.24668 | train_accuracy: 0.90156 | valid_logloss: 0.35605 | valid_accuracy: 0.88133 |  0:03:09s\n",
            "epoch 108| loss: 0.26291 | train_logloss: 0.23511 | train_accuracy: 0.91211 | valid_logloss: 0.34902 | valid_accuracy: 0.87878 |  0:03:11s\n",
            "epoch 109| loss: 0.26076 | train_logloss: 0.23681 | train_accuracy: 0.90844 | valid_logloss: 0.34187 | valid_accuracy: 0.88133 |  0:03:13s\n",
            "epoch 110| loss: 0.25795 | train_logloss: 0.25668 | train_accuracy: 0.89722 | valid_logloss: 0.35571 | valid_accuracy: 0.87111 |  0:03:14s\n",
            "epoch 111| loss: 0.25691 | train_logloss: 0.23703 | train_accuracy: 0.90756 | valid_logloss: 0.3561  | valid_accuracy: 0.88289 |  0:03:16s\n",
            "epoch 112| loss: 0.2476  | train_logloss: 0.22891 | train_accuracy: 0.91089 | valid_logloss: 0.34966 | valid_accuracy: 0.878   |  0:03:18s\n",
            "epoch 113| loss: 0.25575 | train_logloss: 0.25378 | train_accuracy: 0.89756 | valid_logloss: 0.37121 | valid_accuracy: 0.87033 |  0:03:20s\n",
            "epoch 114| loss: 0.24676 | train_logloss: 0.22703 | train_accuracy: 0.91167 | valid_logloss: 0.35252 | valid_accuracy: 0.88178 |  0:03:21s\n",
            "epoch 115| loss: 0.25046 | train_logloss: 0.23758 | train_accuracy: 0.903   | valid_logloss: 0.35188 | valid_accuracy: 0.87633 |  0:03:23s\n",
            "epoch 116| loss: 0.2458  | train_logloss: 0.22523 | train_accuracy: 0.90989 | valid_logloss: 0.35774 | valid_accuracy: 0.881   |  0:03:25s\n",
            "epoch 117| loss: 0.25358 | train_logloss: 0.23804 | train_accuracy: 0.90644 | valid_logloss: 0.36938 | valid_accuracy: 0.87722 |  0:03:27s\n",
            "epoch 118| loss: 0.25087 | train_logloss: 0.2304  | train_accuracy: 0.91044 | valid_logloss: 0.34587 | valid_accuracy: 0.88278 |  0:03:28s\n",
            "epoch 119| loss: 0.24915 | train_logloss: 0.23948 | train_accuracy: 0.90844 | valid_logloss: 0.37067 | valid_accuracy: 0.87967 |  0:03:30s\n",
            "epoch 120| loss: 0.25564 | train_logloss: 0.2438  | train_accuracy: 0.90389 | valid_logloss: 0.35854 | valid_accuracy: 0.87678 |  0:03:32s\n",
            "epoch 121| loss: 0.26587 | train_logloss: 0.24259 | train_accuracy: 0.90522 | valid_logloss: 0.35387 | valid_accuracy: 0.87622 |  0:03:34s\n",
            "epoch 122| loss: 0.25524 | train_logloss: 0.24724 | train_accuracy: 0.90778 | valid_logloss: 0.37389 | valid_accuracy: 0.874   |  0:03:35s\n",
            "epoch 123| loss: 0.25208 | train_logloss: 0.24676 | train_accuracy: 0.90056 | valid_logloss: 0.37004 | valid_accuracy: 0.86511 |  0:03:37s\n",
            "epoch 124| loss: 0.25461 | train_logloss: 0.23822 | train_accuracy: 0.90867 | valid_logloss: 0.35196 | valid_accuracy: 0.88011 |  0:03:39s\n",
            "epoch 125| loss: 0.24202 | train_logloss: 0.22061 | train_accuracy: 0.91456 | valid_logloss: 0.34442 | valid_accuracy: 0.88256 |  0:03:41s\n",
            "epoch 126| loss: 0.25172 | train_logloss: 0.22753 | train_accuracy: 0.91244 | valid_logloss: 0.35426 | valid_accuracy: 0.87611 |  0:03:42s\n",
            "epoch 127| loss: 0.25114 | train_logloss: 0.2321  | train_accuracy: 0.90922 | valid_logloss: 0.35968 | valid_accuracy: 0.87989 |  0:03:44s\n",
            "epoch 128| loss: 0.25112 | train_logloss: 0.22857 | train_accuracy: 0.91256 | valid_logloss: 0.34832 | valid_accuracy: 0.87789 |  0:03:46s\n",
            "epoch 129| loss: 0.2467  | train_logloss: 0.23019 | train_accuracy: 0.90967 | valid_logloss: 0.36773 | valid_accuracy: 0.87556 |  0:03:48s\n",
            "epoch 130| loss: 0.24525 | train_logloss: 0.22298 | train_accuracy: 0.91656 | valid_logloss: 0.35654 | valid_accuracy: 0.87478 |  0:03:49s\n",
            "epoch 131| loss: 0.24014 | train_logloss: 0.22935 | train_accuracy: 0.91056 | valid_logloss: 0.37477 | valid_accuracy: 0.87689 |  0:03:51s\n",
            "epoch 132| loss: 0.2388  | train_logloss: 0.2171  | train_accuracy: 0.91389 | valid_logloss: 0.34811 | valid_accuracy: 0.88156 |  0:03:53s\n",
            "epoch 133| loss: 0.24282 | train_logloss: 0.23309 | train_accuracy: 0.91044 | valid_logloss: 0.37262 | valid_accuracy: 0.88111 |  0:03:55s\n",
            "epoch 134| loss: 0.23724 | train_logloss: 0.21388 | train_accuracy: 0.91722 | valid_logloss: 0.35925 | valid_accuracy: 0.87811 |  0:03:56s\n",
            "epoch 135| loss: 0.23918 | train_logloss: 0.21991 | train_accuracy: 0.91267 | valid_logloss: 0.36789 | valid_accuracy: 0.87833 |  0:03:58s\n",
            "epoch 136| loss: 0.23495 | train_logloss: 0.22524 | train_accuracy: 0.91389 | valid_logloss: 0.37324 | valid_accuracy: 0.87633 |  0:04:00s\n",
            "epoch 137| loss: 0.23992 | train_logloss: 0.21678 | train_accuracy: 0.91789 | valid_logloss: 0.36906 | valid_accuracy: 0.87911 |  0:04:02s\n",
            "epoch 138| loss: 0.23623 | train_logloss: 0.22156 | train_accuracy: 0.91356 | valid_logloss: 0.37034 | valid_accuracy: 0.87911 |  0:04:03s\n",
            "epoch 139| loss: 0.23703 | train_logloss: 0.21798 | train_accuracy: 0.91389 | valid_logloss: 0.35179 | valid_accuracy: 0.88067 |  0:04:05s\n",
            "epoch 140| loss: 0.2386  | train_logloss: 0.23525 | train_accuracy: 0.90711 | valid_logloss: 0.37484 | valid_accuracy: 0.87356 |  0:04:07s\n",
            "epoch 141| loss: 0.24722 | train_logloss: 0.22893 | train_accuracy: 0.91367 | valid_logloss: 0.37114 | valid_accuracy: 0.88033 |  0:04:09s\n",
            "\n",
            "Early stopping occurred at epoch 141 with best_epoch = 91 and best_valid_accuracy = 0.88733\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[418339  15360  46483]\n",
            " [ 17311 952394  30295]\n",
            " [ 39131  31701 361755]]\n",
            "Testing Score:  0.8873333333333333\n",
            "Confusion Matrix: \n",
            " [[415499  13908  50775]\n",
            " [ 14615 951836  33549]\n",
            " [ 39000  28591 364996]]\n",
            "Testing Score:  0.8837777777777778\n",
            "{'Rows': 9000, 'Nd': 32, 'Na': 32, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 3, 'decision': 3, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 249.5335774421692, 'time_tn': 77.37146854400635, 'accuracy_tn': 0.8873333333333333, 'time_learn_gb': 3.5092289447784424, 'time_gb': 17.331297397613525, 'accuracy_gb': 0.8837777777777778}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "DeOxiMxyH6_A",
        "outputId": "ba032dae-baf7-4225-fcb1-26a6227cd8eb"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1  9000  9000  9000  16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2  9000  9000  9000  64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3  9000  9000  9000  32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "\n",
              "[4 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSD7AS2k_a7N",
        "outputId": "1fca8212-3b4f-40ec-da22-3185a826fc04"
      },
      "source": [
        "time_model(number_exp=5, \n",
        "     Rows=9000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.5721  | train_logloss: 15.15309| train_accuracy: 0.48111 | valid_logloss: 15.12557| valid_accuracy: 0.48756 |  0:00:02s\n",
            "epoch 1  | loss: 0.74351 | train_logloss: 6.50309 | train_accuracy: 0.447   | valid_logloss: 6.47254 | valid_accuracy: 0.44122 |  0:00:04s\n",
            "epoch 2  | loss: 0.67446 | train_logloss: 6.73639 | train_accuracy: 0.304   | valid_logloss: 6.9005  | valid_accuracy: 0.29444 |  0:00:06s\n",
            "epoch 3  | loss: 0.50327 | train_logloss: 7.71363 | train_accuracy: 0.33056 | valid_logloss: 7.75076 | valid_accuracy: 0.33011 |  0:00:08s\n",
            "epoch 4  | loss: 0.43796 | train_logloss: 3.02725 | train_accuracy: 0.29711 | valid_logloss: 3.03408 | valid_accuracy: 0.29356 |  0:00:10s\n",
            "epoch 5  | loss: 0.42911 | train_logloss: 2.1664  | train_accuracy: 0.25989 | valid_logloss: 2.17608 | valid_accuracy: 0.25667 |  0:00:12s\n",
            "epoch 6  | loss: 0.41588 | train_logloss: 3.1766  | train_accuracy: 0.35911 | valid_logloss: 3.1663  | valid_accuracy: 0.35589 |  0:00:14s\n",
            "epoch 7  | loss: 0.41402 | train_logloss: 1.40319 | train_accuracy: 0.47778 | valid_logloss: 1.40288 | valid_accuracy: 0.47567 |  0:00:16s\n",
            "epoch 8  | loss: 0.41501 | train_logloss: 1.93363 | train_accuracy: 0.38578 | valid_logloss: 1.93522 | valid_accuracy: 0.38622 |  0:00:18s\n",
            "epoch 9  | loss: 0.39635 | train_logloss: 2.34322 | train_accuracy: 0.46889 | valid_logloss: 2.35898 | valid_accuracy: 0.46756 |  0:00:19s\n",
            "epoch 10 | loss: 0.38701 | train_logloss: 0.8455  | train_accuracy: 0.71211 | valid_logloss: 0.82993 | valid_accuracy: 0.71422 |  0:00:22s\n",
            "epoch 11 | loss: 0.37786 | train_logloss: 0.86544 | train_accuracy: 0.62078 | valid_logloss: 0.85776 | valid_accuracy: 0.62011 |  0:00:24s\n",
            "epoch 12 | loss: 0.37462 | train_logloss: 0.60455 | train_accuracy: 0.77778 | valid_logloss: 0.60037 | valid_accuracy: 0.78167 |  0:00:26s\n",
            "epoch 13 | loss: 0.35936 | train_logloss: 0.6769  | train_accuracy: 0.76322 | valid_logloss: 0.67699 | valid_accuracy: 0.76544 |  0:00:28s\n",
            "epoch 14 | loss: 0.36535 | train_logloss: 0.60849 | train_accuracy: 0.76478 | valid_logloss: 0.61817 | valid_accuracy: 0.75811 |  0:00:30s\n",
            "epoch 15 | loss: 0.35714 | train_logloss: 0.53297 | train_accuracy: 0.78667 | valid_logloss: 0.54108 | valid_accuracy: 0.78856 |  0:00:32s\n",
            "epoch 16 | loss: 0.35452 | train_logloss: 0.44913 | train_accuracy: 0.83067 | valid_logloss: 0.46334 | valid_accuracy: 0.825   |  0:00:34s\n",
            "epoch 17 | loss: 0.3483  | train_logloss: 0.55649 | train_accuracy: 0.75822 | valid_logloss: 0.56903 | valid_accuracy: 0.75533 |  0:00:36s\n",
            "epoch 18 | loss: 0.36352 | train_logloss: 0.39462 | train_accuracy: 0.83978 | valid_logloss: 0.41151 | valid_accuracy: 0.835   |  0:00:38s\n",
            "epoch 19 | loss: 0.35498 | train_logloss: 0.3754  | train_accuracy: 0.85844 | valid_logloss: 0.39398 | valid_accuracy: 0.86    |  0:00:40s\n",
            "epoch 20 | loss: 0.35378 | train_logloss: 0.36386 | train_accuracy: 0.86133 | valid_logloss: 0.3855  | valid_accuracy: 0.85944 |  0:00:42s\n",
            "epoch 21 | loss: 0.35328 | train_logloss: 0.36637 | train_accuracy: 0.85822 | valid_logloss: 0.3792  | valid_accuracy: 0.85689 |  0:00:44s\n",
            "epoch 22 | loss: 0.3491  | train_logloss: 0.33849 | train_accuracy: 0.87022 | valid_logloss: 0.36288 | valid_accuracy: 0.86722 |  0:00:46s\n",
            "epoch 23 | loss: 0.33546 | train_logloss: 0.34452 | train_accuracy: 0.87067 | valid_logloss: 0.36091 | valid_accuracy: 0.86811 |  0:00:48s\n",
            "epoch 24 | loss: 0.33816 | train_logloss: 0.32982 | train_accuracy: 0.87522 | valid_logloss: 0.36105 | valid_accuracy: 0.87111 |  0:00:50s\n",
            "epoch 25 | loss: 0.33014 | train_logloss: 0.31643 | train_accuracy: 0.87867 | valid_logloss: 0.34774 | valid_accuracy: 0.87589 |  0:00:52s\n",
            "epoch 26 | loss: 0.31899 | train_logloss: 0.31902 | train_accuracy: 0.87711 | valid_logloss: 0.35006 | valid_accuracy: 0.874   |  0:00:54s\n",
            "epoch 27 | loss: 0.31697 | train_logloss: 0.30916 | train_accuracy: 0.879   | valid_logloss: 0.35045 | valid_accuracy: 0.87411 |  0:00:56s\n",
            "epoch 28 | loss: 0.32698 | train_logloss: 0.31679 | train_accuracy: 0.87744 | valid_logloss: 0.36176 | valid_accuracy: 0.87278 |  0:00:58s\n",
            "epoch 29 | loss: 0.32234 | train_logloss: 0.30292 | train_accuracy: 0.88156 | valid_logloss: 0.34047 | valid_accuracy: 0.87544 |  0:01:00s\n",
            "epoch 30 | loss: 0.3269  | train_logloss: 0.32019 | train_accuracy: 0.87933 | valid_logloss: 0.36174 | valid_accuracy: 0.87222 |  0:01:02s\n",
            "epoch 31 | loss: 0.33041 | train_logloss: 0.31561 | train_accuracy: 0.87544 | valid_logloss: 0.35179 | valid_accuracy: 0.86922 |  0:01:04s\n",
            "epoch 32 | loss: 0.3242  | train_logloss: 0.31558 | train_accuracy: 0.87989 | valid_logloss: 0.35262 | valid_accuracy: 0.87133 |  0:01:06s\n",
            "epoch 33 | loss: 0.32667 | train_logloss: 0.30728 | train_accuracy: 0.88156 | valid_logloss: 0.34443 | valid_accuracy: 0.87367 |  0:01:08s\n",
            "epoch 34 | loss: 0.32569 | train_logloss: 0.30748 | train_accuracy: 0.87922 | valid_logloss: 0.34865 | valid_accuracy: 0.86833 |  0:01:09s\n",
            "epoch 35 | loss: 0.31715 | train_logloss: 0.30639 | train_accuracy: 0.88222 | valid_logloss: 0.34973 | valid_accuracy: 0.87656 |  0:01:11s\n",
            "epoch 36 | loss: 0.32874 | train_logloss: 0.30189 | train_accuracy: 0.88322 | valid_logloss: 0.34038 | valid_accuracy: 0.87333 |  0:01:13s\n",
            "epoch 37 | loss: 0.30544 | train_logloss: 0.29612 | train_accuracy: 0.88411 | valid_logloss: 0.35131 | valid_accuracy: 0.87189 |  0:01:15s\n",
            "epoch 38 | loss: 0.30797 | train_logloss: 0.30031 | train_accuracy: 0.88089 | valid_logloss: 0.34667 | valid_accuracy: 0.87044 |  0:01:17s\n",
            "epoch 39 | loss: 0.31827 | train_logloss: 0.29257 | train_accuracy: 0.88733 | valid_logloss: 0.33902 | valid_accuracy: 0.87833 |  0:01:19s\n",
            "epoch 40 | loss: 0.30903 | train_logloss: 0.29767 | train_accuracy: 0.88756 | valid_logloss: 0.35535 | valid_accuracy: 0.87456 |  0:01:21s\n",
            "epoch 41 | loss: 0.31154 | train_logloss: 0.2956  | train_accuracy: 0.88256 | valid_logloss: 0.34675 | valid_accuracy: 0.87356 |  0:01:23s\n",
            "epoch 42 | loss: 0.31178 | train_logloss: 0.2937  | train_accuracy: 0.88822 | valid_logloss: 0.34177 | valid_accuracy: 0.87833 |  0:01:25s\n",
            "epoch 43 | loss: 0.32196 | train_logloss: 0.32062 | train_accuracy: 0.87756 | valid_logloss: 0.36815 | valid_accuracy: 0.86878 |  0:01:27s\n",
            "epoch 44 | loss: 0.33091 | train_logloss: 0.34139 | train_accuracy: 0.87322 | valid_logloss: 0.37441 | valid_accuracy: 0.86778 |  0:01:29s\n",
            "epoch 45 | loss: 0.3463  | train_logloss: 0.33966 | train_accuracy: 0.87278 | valid_logloss: 0.372   | valid_accuracy: 0.86478 |  0:01:31s\n",
            "epoch 46 | loss: 0.36168 | train_logloss: 0.34222 | train_accuracy: 0.87289 | valid_logloss: 0.37261 | valid_accuracy: 0.86644 |  0:01:33s\n",
            "epoch 47 | loss: 0.3414  | train_logloss: 0.32496 | train_accuracy: 0.87944 | valid_logloss: 0.35168 | valid_accuracy: 0.87622 |  0:01:35s\n",
            "epoch 48 | loss: 0.33432 | train_logloss: 0.31298 | train_accuracy: 0.87744 | valid_logloss: 0.34947 | valid_accuracy: 0.87167 |  0:01:37s\n",
            "epoch 49 | loss: 0.32672 | train_logloss: 0.30969 | train_accuracy: 0.88422 | valid_logloss: 0.3466  | valid_accuracy: 0.875   |  0:01:39s\n",
            "epoch 50 | loss: 0.31717 | train_logloss: 0.31063 | train_accuracy: 0.87811 | valid_logloss: 0.34401 | valid_accuracy: 0.87433 |  0:01:41s\n",
            "epoch 51 | loss: 0.31179 | train_logloss: 0.30011 | train_accuracy: 0.885   | valid_logloss: 0.33841 | valid_accuracy: 0.87833 |  0:01:43s\n",
            "epoch 52 | loss: 0.31378 | train_logloss: 0.2958  | train_accuracy: 0.88711 | valid_logloss: 0.33709 | valid_accuracy: 0.87656 |  0:01:45s\n",
            "epoch 53 | loss: 0.31686 | train_logloss: 0.34118 | train_accuracy: 0.87422 | valid_logloss: 0.36544 | valid_accuracy: 0.86744 |  0:01:47s\n",
            "epoch 54 | loss: 0.33517 | train_logloss: 0.31549 | train_accuracy: 0.876   | valid_logloss: 0.34474 | valid_accuracy: 0.87033 |  0:01:49s\n",
            "epoch 55 | loss: 0.3342  | train_logloss: 0.31899 | train_accuracy: 0.87978 | valid_logloss: 0.35224 | valid_accuracy: 0.87    |  0:01:51s\n",
            "epoch 56 | loss: 0.33255 | train_logloss: 0.3095  | train_accuracy: 0.88078 | valid_logloss: 0.34096 | valid_accuracy: 0.875   |  0:01:53s\n",
            "epoch 57 | loss: 0.32612 | train_logloss: 0.31476 | train_accuracy: 0.87856 | valid_logloss: 0.34691 | valid_accuracy: 0.87467 |  0:01:55s\n",
            "epoch 58 | loss: 0.32876 | train_logloss: 0.32796 | train_accuracy: 0.87667 | valid_logloss: 0.36729 | valid_accuracy: 0.86456 |  0:01:57s\n",
            "epoch 59 | loss: 0.32875 | train_logloss: 0.30517 | train_accuracy: 0.88556 | valid_logloss: 0.33781 | valid_accuracy: 0.875   |  0:01:59s\n",
            "epoch 60 | loss: 0.31934 | train_logloss: 0.29684 | train_accuracy: 0.885   | valid_logloss: 0.33121 | valid_accuracy: 0.87978 |  0:02:01s\n",
            "epoch 61 | loss: 0.31647 | train_logloss: 0.30002 | train_accuracy: 0.88544 | valid_logloss: 0.3324  | valid_accuracy: 0.87756 |  0:02:03s\n",
            "epoch 62 | loss: 0.31549 | train_logloss: 0.3     | train_accuracy: 0.88778 | valid_logloss: 0.33386 | valid_accuracy: 0.87978 |  0:02:05s\n",
            "epoch 63 | loss: 0.32129 | train_logloss: 0.31446 | train_accuracy: 0.88133 | valid_logloss: 0.34879 | valid_accuracy: 0.874   |  0:02:07s\n",
            "epoch 64 | loss: 0.33822 | train_logloss: 0.32276 | train_accuracy: 0.87389 | valid_logloss: 0.34944 | valid_accuracy: 0.86767 |  0:02:09s\n",
            "epoch 65 | loss: 0.33249 | train_logloss: 0.3303  | train_accuracy: 0.87767 | valid_logloss: 0.3551  | valid_accuracy: 0.86922 |  0:02:11s\n",
            "epoch 66 | loss: 0.33886 | train_logloss: 0.3151  | train_accuracy: 0.87678 | valid_logloss: 0.35148 | valid_accuracy: 0.86733 |  0:02:13s\n",
            "epoch 67 | loss: 0.31952 | train_logloss: 0.30227 | train_accuracy: 0.88089 | valid_logloss: 0.33479 | valid_accuracy: 0.87333 |  0:02:15s\n",
            "epoch 68 | loss: 0.32259 | train_logloss: 0.29926 | train_accuracy: 0.88478 | valid_logloss: 0.33044 | valid_accuracy: 0.87533 |  0:02:17s\n",
            "epoch 69 | loss: 0.315   | train_logloss: 0.29773 | train_accuracy: 0.88522 | valid_logloss: 0.33701 | valid_accuracy: 0.87767 |  0:02:19s\n",
            "epoch 70 | loss: 0.30905 | train_logloss: 0.30103 | train_accuracy: 0.88989 | valid_logloss: 0.33614 | valid_accuracy: 0.87967 |  0:02:21s\n",
            "epoch 71 | loss: 0.31793 | train_logloss: 0.32177 | train_accuracy: 0.87933 | valid_logloss: 0.35348 | valid_accuracy: 0.87078 |  0:02:23s\n",
            "epoch 72 | loss: 0.31657 | train_logloss: 0.32042 | train_accuracy: 0.87867 | valid_logloss: 0.35012 | valid_accuracy: 0.87511 |  0:02:25s\n",
            "epoch 73 | loss: 0.31625 | train_logloss: 0.29655 | train_accuracy: 0.889   | valid_logloss: 0.3275  | valid_accuracy: 0.87811 |  0:02:27s\n",
            "epoch 74 | loss: 0.30864 | train_logloss: 0.31165 | train_accuracy: 0.87833 | valid_logloss: 0.3413  | valid_accuracy: 0.87144 |  0:02:29s\n",
            "epoch 75 | loss: 0.31588 | train_logloss: 0.32179 | train_accuracy: 0.87367 | valid_logloss: 0.35507 | valid_accuracy: 0.86956 |  0:02:31s\n",
            "epoch 76 | loss: 0.31299 | train_logloss: 0.29519 | train_accuracy: 0.87878 | valid_logloss: 0.3372  | valid_accuracy: 0.87678 |  0:02:33s\n",
            "epoch 77 | loss: 0.31719 | train_logloss: 0.30159 | train_accuracy: 0.88244 | valid_logloss: 0.34134 | valid_accuracy: 0.87533 |  0:02:35s\n",
            "epoch 78 | loss: 0.30675 | train_logloss: 0.29597 | train_accuracy: 0.88344 | valid_logloss: 0.34195 | valid_accuracy: 0.87533 |  0:02:37s\n",
            "epoch 79 | loss: 0.30726 | train_logloss: 0.28535 | train_accuracy: 0.88667 | valid_logloss: 0.32772 | valid_accuracy: 0.87978 |  0:02:39s\n",
            "epoch 80 | loss: 0.30072 | train_logloss: 0.285   | train_accuracy: 0.89133 | valid_logloss: 0.3315  | valid_accuracy: 0.88189 |  0:02:41s\n",
            "epoch 81 | loss: 0.2943  | train_logloss: 0.29294 | train_accuracy: 0.89011 | valid_logloss: 0.33166 | valid_accuracy: 0.88156 |  0:02:43s\n",
            "epoch 82 | loss: 0.30107 | train_logloss: 0.2783  | train_accuracy: 0.893   | valid_logloss: 0.32141 | valid_accuracy: 0.88367 |  0:02:45s\n",
            "epoch 83 | loss: 0.29475 | train_logloss: 0.27624 | train_accuracy: 0.89433 | valid_logloss: 0.33044 | valid_accuracy: 0.88544 |  0:02:47s\n",
            "epoch 84 | loss: 0.29441 | train_logloss: 0.2802  | train_accuracy: 0.88978 | valid_logloss: 0.32645 | valid_accuracy: 0.882   |  0:02:49s\n",
            "epoch 85 | loss: 0.29562 | train_logloss: 0.27266 | train_accuracy: 0.89356 | valid_logloss: 0.32532 | valid_accuracy: 0.88456 |  0:02:51s\n",
            "epoch 86 | loss: 0.28772 | train_logloss: 0.2687  | train_accuracy: 0.89567 | valid_logloss: 0.32951 | valid_accuracy: 0.88456 |  0:02:53s\n",
            "epoch 87 | loss: 0.28256 | train_logloss: 0.26752 | train_accuracy: 0.89711 | valid_logloss: 0.32287 | valid_accuracy: 0.88567 |  0:02:55s\n",
            "epoch 88 | loss: 0.28289 | train_logloss: 0.26812 | train_accuracy: 0.89911 | valid_logloss: 0.33223 | valid_accuracy: 0.88233 |  0:02:57s\n",
            "epoch 89 | loss: 0.2783  | train_logloss: 0.26394 | train_accuracy: 0.89922 | valid_logloss: 0.32917 | valid_accuracy: 0.88667 |  0:02:59s\n",
            "epoch 90 | loss: 0.27806 | train_logloss: 0.26143 | train_accuracy: 0.898   | valid_logloss: 0.3275  | valid_accuracy: 0.88578 |  0:03:01s\n",
            "epoch 91 | loss: 0.27579 | train_logloss: 0.26074 | train_accuracy: 0.89733 | valid_logloss: 0.32761 | valid_accuracy: 0.88389 |  0:03:03s\n",
            "epoch 92 | loss: 0.28037 | train_logloss: 0.26166 | train_accuracy: 0.89989 | valid_logloss: 0.32974 | valid_accuracy: 0.88289 |  0:03:05s\n",
            "epoch 93 | loss: 0.27851 | train_logloss: 0.26144 | train_accuracy: 0.89833 | valid_logloss: 0.33337 | valid_accuracy: 0.88411 |  0:03:07s\n",
            "epoch 94 | loss: 0.28003 | train_logloss: 0.27319 | train_accuracy: 0.89378 | valid_logloss: 0.34049 | valid_accuracy: 0.883   |  0:03:09s\n",
            "epoch 95 | loss: 0.28098 | train_logloss: 0.26596 | train_accuracy: 0.89544 | valid_logloss: 0.3267  | valid_accuracy: 0.88156 |  0:03:11s\n",
            "epoch 96 | loss: 0.27916 | train_logloss: 0.2586  | train_accuracy: 0.90244 | valid_logloss: 0.33215 | valid_accuracy: 0.88589 |  0:03:13s\n",
            "epoch 97 | loss: 0.27555 | train_logloss: 0.2564  | train_accuracy: 0.90133 | valid_logloss: 0.32726 | valid_accuracy: 0.88378 |  0:03:15s\n",
            "epoch 98 | loss: 0.27211 | train_logloss: 0.25814 | train_accuracy: 0.90044 | valid_logloss: 0.32531 | valid_accuracy: 0.88644 |  0:03:17s\n",
            "epoch 99 | loss: 0.2768  | train_logloss: 0.25613 | train_accuracy: 0.90111 | valid_logloss: 0.32914 | valid_accuracy: 0.88556 |  0:03:19s\n",
            "epoch 100| loss: 0.27175 | train_logloss: 0.25418 | train_accuracy: 0.90078 | valid_logloss: 0.33449 | valid_accuracy: 0.88422 |  0:03:21s\n",
            "epoch 101| loss: 0.27138 | train_logloss: 0.2522  | train_accuracy: 0.90011 | valid_logloss: 0.34108 | valid_accuracy: 0.88233 |  0:03:23s\n",
            "epoch 102| loss: 0.26651 | train_logloss: 0.25032 | train_accuracy: 0.903   | valid_logloss: 0.33183 | valid_accuracy: 0.88433 |  0:03:25s\n",
            "epoch 103| loss: 0.27338 | train_logloss: 0.25755 | train_accuracy: 0.902   | valid_logloss: 0.33326 | valid_accuracy: 0.88811 |  0:03:27s\n",
            "epoch 104| loss: 0.27313 | train_logloss: 0.24848 | train_accuracy: 0.90489 | valid_logloss: 0.33364 | valid_accuracy: 0.88456 |  0:03:29s\n",
            "epoch 105| loss: 0.27969 | train_logloss: 0.30034 | train_accuracy: 0.87967 | valid_logloss: 0.36743 | valid_accuracy: 0.86778 |  0:03:31s\n",
            "epoch 106| loss: 0.29602 | train_logloss: 0.27543 | train_accuracy: 0.89556 | valid_logloss: 0.34195 | valid_accuracy: 0.88222 |  0:03:33s\n",
            "epoch 107| loss: 0.2879  | train_logloss: 0.2773  | train_accuracy: 0.88956 | valid_logloss: 0.35091 | valid_accuracy: 0.87367 |  0:03:35s\n",
            "epoch 108| loss: 0.2763  | train_logloss: 0.2661  | train_accuracy: 0.89667 | valid_logloss: 0.34835 | valid_accuracy: 0.87967 |  0:03:37s\n",
            "epoch 109| loss: 0.28349 | train_logloss: 0.26487 | train_accuracy: 0.899   | valid_logloss: 0.3401  | valid_accuracy: 0.88422 |  0:03:39s\n",
            "epoch 110| loss: 0.2745  | train_logloss: 0.25152 | train_accuracy: 0.90244 | valid_logloss: 0.33396 | valid_accuracy: 0.88411 |  0:03:41s\n",
            "epoch 111| loss: 0.27089 | train_logloss: 0.25431 | train_accuracy: 0.90122 | valid_logloss: 0.33552 | valid_accuracy: 0.88544 |  0:03:43s\n",
            "epoch 112| loss: 0.27267 | train_logloss: 0.2715  | train_accuracy: 0.895   | valid_logloss: 0.35381 | valid_accuracy: 0.88067 |  0:03:45s\n",
            "epoch 113| loss: 0.29554 | train_logloss: 0.28286 | train_accuracy: 0.89322 | valid_logloss: 0.34727 | valid_accuracy: 0.88011 |  0:03:47s\n",
            "epoch 114| loss: 0.28731 | train_logloss: 0.26204 | train_accuracy: 0.89967 | valid_logloss: 0.32716 | valid_accuracy: 0.88511 |  0:03:49s\n",
            "epoch 115| loss: 0.27415 | train_logloss: 0.26112 | train_accuracy: 0.89956 | valid_logloss: 0.33883 | valid_accuracy: 0.88167 |  0:03:51s\n",
            "epoch 116| loss: 0.27194 | train_logloss: 0.25443 | train_accuracy: 0.90322 | valid_logloss: 0.33031 | valid_accuracy: 0.88511 |  0:03:53s\n",
            "epoch 117| loss: 0.264   | train_logloss: 0.24541 | train_accuracy: 0.90278 | valid_logloss: 0.32742 | valid_accuracy: 0.88489 |  0:03:55s\n",
            "epoch 118| loss: 0.26513 | train_logloss: 0.24535 | train_accuracy: 0.90389 | valid_logloss: 0.33805 | valid_accuracy: 0.88522 |  0:03:57s\n",
            "epoch 119| loss: 0.26221 | train_logloss: 0.25322 | train_accuracy: 0.89867 | valid_logloss: 0.33645 | valid_accuracy: 0.88122 |  0:03:59s\n",
            "epoch 120| loss: 0.27108 | train_logloss: 0.24688 | train_accuracy: 0.90444 | valid_logloss: 0.32937 | valid_accuracy: 0.886   |  0:04:01s\n",
            "epoch 121| loss: 0.26926 | train_logloss: 0.25266 | train_accuracy: 0.90456 | valid_logloss: 0.34801 | valid_accuracy: 0.88167 |  0:04:03s\n",
            "epoch 122| loss: 0.27704 | train_logloss: 0.25794 | train_accuracy: 0.89767 | valid_logloss: 0.34891 | valid_accuracy: 0.88144 |  0:04:05s\n",
            "epoch 123| loss: 0.27101 | train_logloss: 0.25013 | train_accuracy: 0.90178 | valid_logloss: 0.33587 | valid_accuracy: 0.88244 |  0:04:07s\n",
            "epoch 124| loss: 0.26176 | train_logloss: 0.24796 | train_accuracy: 0.90244 | valid_logloss: 0.34784 | valid_accuracy: 0.886   |  0:04:09s\n",
            "epoch 125| loss: 0.26377 | train_logloss: 0.26109 | train_accuracy: 0.89467 | valid_logloss: 0.36132 | valid_accuracy: 0.874   |  0:04:11s\n",
            "epoch 126| loss: 0.26791 | train_logloss: 0.24144 | train_accuracy: 0.90744 | valid_logloss: 0.3521  | valid_accuracy: 0.88311 |  0:04:13s\n",
            "epoch 127| loss: 0.26553 | train_logloss: 0.27738 | train_accuracy: 0.89511 | valid_logloss: 0.35789 | valid_accuracy: 0.879   |  0:04:15s\n",
            "epoch 128| loss: 0.28496 | train_logloss: 0.25701 | train_accuracy: 0.90167 | valid_logloss: 0.34788 | valid_accuracy: 0.88267 |  0:04:17s\n",
            "epoch 129| loss: 0.27218 | train_logloss: 0.24902 | train_accuracy: 0.90289 | valid_logloss: 0.33859 | valid_accuracy: 0.88389 |  0:04:19s\n",
            "epoch 130| loss: 0.26383 | train_logloss: 0.24423 | train_accuracy: 0.905   | valid_logloss: 0.33863 | valid_accuracy: 0.88656 |  0:04:21s\n",
            "epoch 131| loss: 0.25954 | train_logloss: 0.25013 | train_accuracy: 0.89978 | valid_logloss: 0.36023 | valid_accuracy: 0.88111 |  0:04:23s\n",
            "epoch 132| loss: 0.25259 | train_logloss: 0.24477 | train_accuracy: 0.90389 | valid_logloss: 0.35037 | valid_accuracy: 0.88289 |  0:04:25s\n",
            "epoch 133| loss: 0.25684 | train_logloss: 0.25077 | train_accuracy: 0.90433 | valid_logloss: 0.36668 | valid_accuracy: 0.882   |  0:04:27s\n",
            "epoch 134| loss: 0.25775 | train_logloss: 0.23816 | train_accuracy: 0.90856 | valid_logloss: 0.34918 | valid_accuracy: 0.88267 |  0:04:29s\n",
            "epoch 135| loss: 0.25805 | train_logloss: 0.24078 | train_accuracy: 0.90111 | valid_logloss: 0.3471  | valid_accuracy: 0.87944 |  0:04:31s\n",
            "epoch 136| loss: 0.26154 | train_logloss: 0.23203 | train_accuracy: 0.90722 | valid_logloss: 0.34871 | valid_accuracy: 0.88633 |  0:04:33s\n",
            "epoch 137| loss: 0.26178 | train_logloss: 0.24875 | train_accuracy: 0.90422 | valid_logloss: 0.34962 | valid_accuracy: 0.88089 |  0:04:35s\n",
            "epoch 138| loss: 0.27154 | train_logloss: 0.24931 | train_accuracy: 0.90489 | valid_logloss: 0.34333 | valid_accuracy: 0.883   |  0:04:37s\n",
            "epoch 139| loss: 0.25574 | train_logloss: 0.2318  | train_accuracy: 0.90522 | valid_logloss: 0.34797 | valid_accuracy: 0.88478 |  0:04:39s\n",
            "epoch 140| loss: 0.25203 | train_logloss: 0.23757 | train_accuracy: 0.90789 | valid_logloss: 0.359   | valid_accuracy: 0.88033 |  0:04:41s\n",
            "epoch 141| loss: 0.24363 | train_logloss: 0.22634 | train_accuracy: 0.91289 | valid_logloss: 0.35317 | valid_accuracy: 0.88222 |  0:04:43s\n",
            "epoch 142| loss: 0.24157 | train_logloss: 0.23263 | train_accuracy: 0.90567 | valid_logloss: 0.36713 | valid_accuracy: 0.87878 |  0:04:45s\n",
            "epoch 143| loss: 0.24599 | train_logloss: 0.24965 | train_accuracy: 0.89678 | valid_logloss: 0.35717 | valid_accuracy: 0.87211 |  0:04:47s\n",
            "epoch 144| loss: 0.24538 | train_logloss: 0.23912 | train_accuracy: 0.90744 | valid_logloss: 0.36651 | valid_accuracy: 0.87844 |  0:04:49s\n",
            "epoch 145| loss: 0.24217 | train_logloss: 0.22058 | train_accuracy: 0.91378 | valid_logloss: 0.35716 | valid_accuracy: 0.88444 |  0:04:51s\n",
            "epoch 146| loss: 0.24792 | train_logloss: 0.23257 | train_accuracy: 0.90544 | valid_logloss: 0.37876 | valid_accuracy: 0.87678 |  0:04:53s\n",
            "epoch 147| loss: 0.24325 | train_logloss: 0.22098 | train_accuracy: 0.91278 | valid_logloss: 0.35321 | valid_accuracy: 0.88211 |  0:04:55s\n",
            "epoch 148| loss: 0.23621 | train_logloss: 0.22306 | train_accuracy: 0.91233 | valid_logloss: 0.36362 | valid_accuracy: 0.88422 |  0:04:57s\n",
            "epoch 149| loss: 0.24203 | train_logloss: 0.21567 | train_accuracy: 0.91578 | valid_logloss: 0.35484 | valid_accuracy: 0.88422 |  0:04:59s\n",
            "epoch 150| loss: 0.2365  | train_logloss: 0.21285 | train_accuracy: 0.91411 | valid_logloss: 0.34553 | valid_accuracy: 0.88556 |  0:05:01s\n",
            "epoch 151| loss: 0.24588 | train_logloss: 0.2359  | train_accuracy: 0.90667 | valid_logloss: 0.37209 | valid_accuracy: 0.87856 |  0:05:02s\n",
            "epoch 152| loss: 0.27236 | train_logloss: 0.24709 | train_accuracy: 0.90289 | valid_logloss: 0.33688 | valid_accuracy: 0.88311 |  0:05:04s\n",
            "epoch 153| loss: 0.25195 | train_logloss: 0.2334  | train_accuracy: 0.90944 | valid_logloss: 0.34105 | valid_accuracy: 0.88578 |  0:05:07s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 103 and best_valid_accuracy = 0.88811\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[412104  18121  49957]\n",
            " [ 13147 956378  30475]\n",
            " [ 36914  31105 364568]]\n",
            "Testing Score:  0.8881111111111111\n",
            "Confusion Matrix: \n",
            " [[415499  13908  50775]\n",
            " [ 14615 951836  33549]\n",
            " [ 39000  28591 364996]]\n",
            "Testing Score:  0.8837777777777778\n",
            "{'Rows': 9000, 'Nd': 128, 'Na': 128, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 307.5609984397888, 'time_tn': 90.36438369750977, 'accuracy_tn': 0.8881111111111111, 'time_learn_gb': 3.4753997325897217, 'time_gb': 17.451217889785767, 'accuracy_gb': 0.8837777777777778}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "aCsVElMBCf9_",
        "outputId": "185bf260-8943-4e7e-b3ee-071ef981f2cb"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1  9000  9000  9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2  9000  9000  9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3  9000  9000  9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4  9000  9000  9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ql0Rid9-mW5"
      },
      "source": [
        "## 30000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JE4AgcBLAU",
        "outputId": "bf67d0ad-476c-430a-d264-ffa522221535"
      },
      "source": [
        "time_model(number_exp=6, \n",
        "     Rows=30000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.70132 | train_logloss: 15.17457| train_accuracy: 0.35347 | valid_logloss: 15.3099 | valid_accuracy: 0.3517  |  0:00:01s\n",
            "epoch 1  | loss: 0.42177 | train_logloss: 7.30538 | train_accuracy: 0.18657 | valid_logloss: 7.29039 | valid_accuracy: 0.19063 |  0:00:02s\n",
            "epoch 2  | loss: 0.37971 | train_logloss: 4.83377 | train_accuracy: 0.3335  | valid_logloss: 4.8499  | valid_accuracy: 0.33347 |  0:00:04s\n",
            "epoch 3  | loss: 0.35056 | train_logloss: 2.52646 | train_accuracy: 0.2967  | valid_logloss: 2.52153 | valid_accuracy: 0.30157 |  0:00:05s\n",
            "epoch 4  | loss: 0.33972 | train_logloss: 1.27628 | train_accuracy: 0.45773 | valid_logloss: 1.27142 | valid_accuracy: 0.4559  |  0:00:06s\n",
            "epoch 5  | loss: 0.33557 | train_logloss: 1.44308 | train_accuracy: 0.6143  | valid_logloss: 1.45259 | valid_accuracy: 0.60827 |  0:00:08s\n",
            "epoch 6  | loss: 0.32793 | train_logloss: 1.46689 | train_accuracy: 0.5593  | valid_logloss: 1.4742  | valid_accuracy: 0.55867 |  0:00:09s\n",
            "epoch 7  | loss: 0.33662 | train_logloss: 1.49556 | train_accuracy: 0.3201  | valid_logloss: 1.49805 | valid_accuracy: 0.32237 |  0:00:10s\n",
            "epoch 8  | loss: 0.32374 | train_logloss: 1.41797 | train_accuracy: 0.37417 | valid_logloss: 1.42039 | valid_accuracy: 0.37747 |  0:00:12s\n",
            "epoch 9  | loss: 0.31677 | train_logloss: 1.04765 | train_accuracy: 0.50863 | valid_logloss: 1.05514 | valid_accuracy: 0.50913 |  0:00:13s\n",
            "epoch 10 | loss: 0.31096 | train_logloss: 1.2415  | train_accuracy: 0.4018  | valid_logloss: 1.25322 | valid_accuracy: 0.4015  |  0:00:15s\n",
            "epoch 11 | loss: 0.31153 | train_logloss: 1.03381 | train_accuracy: 0.54573 | valid_logloss: 1.04708 | valid_accuracy: 0.54097 |  0:00:16s\n",
            "epoch 12 | loss: 0.30714 | train_logloss: 0.96827 | train_accuracy: 0.53087 | valid_logloss: 0.97739 | valid_accuracy: 0.5264  |  0:00:17s\n",
            "epoch 13 | loss: 0.30526 | train_logloss: 0.87147 | train_accuracy: 0.5858  | valid_logloss: 0.88153 | valid_accuracy: 0.58047 |  0:00:19s\n",
            "epoch 14 | loss: 0.30532 | train_logloss: 0.73397 | train_accuracy: 0.65863 | valid_logloss: 0.74547 | valid_accuracy: 0.65177 |  0:00:20s\n",
            "epoch 15 | loss: 0.30452 | train_logloss: 0.59388 | train_accuracy: 0.7532  | valid_logloss: 0.59971 | valid_accuracy: 0.7501  |  0:00:21s\n",
            "epoch 16 | loss: 0.30145 | train_logloss: 0.55793 | train_accuracy: 0.76583 | valid_logloss: 0.56216 | valid_accuracy: 0.76363 |  0:00:23s\n",
            "epoch 17 | loss: 0.29802 | train_logloss: 0.567   | train_accuracy: 0.75663 | valid_logloss: 0.57547 | valid_accuracy: 0.75397 |  0:00:24s\n",
            "epoch 18 | loss: 0.29506 | train_logloss: 0.47887 | train_accuracy: 0.81407 | valid_logloss: 0.48752 | valid_accuracy: 0.80963 |  0:00:25s\n",
            "epoch 19 | loss: 0.29066 | train_logloss: 0.38363 | train_accuracy: 0.85333 | valid_logloss: 0.38908 | valid_accuracy: 0.852   |  0:00:27s\n",
            "epoch 20 | loss: 0.29126 | train_logloss: 0.37036 | train_accuracy: 0.86067 | valid_logloss: 0.37966 | valid_accuracy: 0.85667 |  0:00:28s\n",
            "epoch 21 | loss: 0.28954 | train_logloss: 0.34549 | train_accuracy: 0.8713  | valid_logloss: 0.35676 | valid_accuracy: 0.86753 |  0:00:30s\n",
            "epoch 22 | loss: 0.29181 | train_logloss: 0.33671 | train_accuracy: 0.8756  | valid_logloss: 0.34757 | valid_accuracy: 0.87293 |  0:00:31s\n",
            "epoch 23 | loss: 0.28884 | train_logloss: 0.30733 | train_accuracy: 0.8852  | valid_logloss: 0.32264 | valid_accuracy: 0.8797  |  0:00:33s\n",
            "epoch 24 | loss: 0.28916 | train_logloss: 0.31529 | train_accuracy: 0.8839  | valid_logloss: 0.33042 | valid_accuracy: 0.8789  |  0:00:34s\n",
            "epoch 25 | loss: 0.28982 | train_logloss: 0.3034  | train_accuracy: 0.88353 | valid_logloss: 0.3192  | valid_accuracy: 0.8795  |  0:00:35s\n",
            "epoch 26 | loss: 0.28464 | train_logloss: 0.30867 | train_accuracy: 0.88237 | valid_logloss: 0.33004 | valid_accuracy: 0.87703 |  0:00:37s\n",
            "epoch 27 | loss: 0.28889 | train_logloss: 0.28827 | train_accuracy: 0.89097 | valid_logloss: 0.30728 | valid_accuracy: 0.8839  |  0:00:38s\n",
            "epoch 28 | loss: 0.29116 | train_logloss: 0.29129 | train_accuracy: 0.89067 | valid_logloss: 0.30917 | valid_accuracy: 0.8846  |  0:00:39s\n",
            "epoch 29 | loss: 0.29258 | train_logloss: 0.29632 | train_accuracy: 0.88753 | valid_logloss: 0.31458 | valid_accuracy: 0.88293 |  0:00:41s\n",
            "epoch 30 | loss: 0.29133 | train_logloss: 0.28217 | train_accuracy: 0.89397 | valid_logloss: 0.30062 | valid_accuracy: 0.8858  |  0:00:42s\n",
            "epoch 31 | loss: 0.29006 | train_logloss: 0.2885  | train_accuracy: 0.892   | valid_logloss: 0.30601 | valid_accuracy: 0.88393 |  0:00:43s\n",
            "epoch 32 | loss: 0.28755 | train_logloss: 0.2833  | train_accuracy: 0.89497 | valid_logloss: 0.303   | valid_accuracy: 0.8877  |  0:00:45s\n",
            "epoch 33 | loss: 0.28319 | train_logloss: 0.27632 | train_accuracy: 0.89497 | valid_logloss: 0.29829 | valid_accuracy: 0.8883  |  0:00:46s\n",
            "epoch 34 | loss: 0.28278 | train_logloss: 0.27524 | train_accuracy: 0.8946  | valid_logloss: 0.29805 | valid_accuracy: 0.8864  |  0:00:48s\n",
            "epoch 35 | loss: 0.28287 | train_logloss: 0.27853 | train_accuracy: 0.8954  | valid_logloss: 0.29981 | valid_accuracy: 0.88613 |  0:00:49s\n",
            "epoch 36 | loss: 0.28624 | train_logloss: 0.28075 | train_accuracy: 0.89563 | valid_logloss: 0.30194 | valid_accuracy: 0.8875  |  0:00:50s\n",
            "epoch 37 | loss: 0.2831  | train_logloss: 0.27418 | train_accuracy: 0.89503 | valid_logloss: 0.2959  | valid_accuracy: 0.88893 |  0:00:52s\n",
            "epoch 38 | loss: 0.27861 | train_logloss: 0.27366 | train_accuracy: 0.89643 | valid_logloss: 0.29507 | valid_accuracy: 0.8884  |  0:00:53s\n",
            "epoch 39 | loss: 0.2819  | train_logloss: 0.2772  | train_accuracy: 0.89423 | valid_logloss: 0.29946 | valid_accuracy: 0.88683 |  0:00:54s\n",
            "epoch 40 | loss: 0.28165 | train_logloss: 0.27198 | train_accuracy: 0.89757 | valid_logloss: 0.29313 | valid_accuracy: 0.8878  |  0:00:56s\n",
            "epoch 41 | loss: 0.27918 | train_logloss: 0.27195 | train_accuracy: 0.89717 | valid_logloss: 0.29405 | valid_accuracy: 0.88893 |  0:00:57s\n",
            "epoch 42 | loss: 0.28006 | train_logloss: 0.27904 | train_accuracy: 0.89533 | valid_logloss: 0.30021 | valid_accuracy: 0.88523 |  0:00:58s\n",
            "epoch 43 | loss: 0.27862 | train_logloss: 0.26936 | train_accuracy: 0.89667 | valid_logloss: 0.29208 | valid_accuracy: 0.88937 |  0:01:00s\n",
            "epoch 44 | loss: 0.28234 | train_logloss: 0.27776 | train_accuracy: 0.89563 | valid_logloss: 0.30277 | valid_accuracy: 0.8867  |  0:01:01s\n",
            "epoch 45 | loss: 0.27792 | train_logloss: 0.26911 | train_accuracy: 0.8984  | valid_logloss: 0.29267 | valid_accuracy: 0.89003 |  0:01:03s\n",
            "epoch 46 | loss: 0.27932 | train_logloss: 0.26738 | train_accuracy: 0.89743 | valid_logloss: 0.29306 | valid_accuracy: 0.88963 |  0:01:04s\n",
            "epoch 47 | loss: 0.27467 | train_logloss: 0.26744 | train_accuracy: 0.89843 | valid_logloss: 0.29436 | valid_accuracy: 0.88947 |  0:01:05s\n",
            "epoch 48 | loss: 0.27879 | train_logloss: 0.27385 | train_accuracy: 0.89727 | valid_logloss: 0.29909 | valid_accuracy: 0.88827 |  0:01:07s\n",
            "epoch 49 | loss: 0.28015 | train_logloss: 0.27204 | train_accuracy: 0.89647 | valid_logloss: 0.29849 | valid_accuracy: 0.88703 |  0:01:08s\n",
            "epoch 50 | loss: 0.27576 | train_logloss: 0.27278 | train_accuracy: 0.8952  | valid_logloss: 0.29853 | valid_accuracy: 0.88783 |  0:01:09s\n",
            "epoch 51 | loss: 0.28253 | train_logloss: 0.27331 | train_accuracy: 0.8975  | valid_logloss: 0.29764 | valid_accuracy: 0.88773 |  0:01:11s\n",
            "epoch 52 | loss: 0.28048 | train_logloss: 0.2768  | train_accuracy: 0.8967  | valid_logloss: 0.30615 | valid_accuracy: 0.8863  |  0:01:12s\n",
            "epoch 53 | loss: 0.28124 | train_logloss: 0.26941 | train_accuracy: 0.89787 | valid_logloss: 0.29533 | valid_accuracy: 0.88937 |  0:01:14s\n",
            "epoch 54 | loss: 0.27451 | train_logloss: 0.26445 | train_accuracy: 0.9005  | valid_logloss: 0.28903 | valid_accuracy: 0.8907  |  0:01:15s\n",
            "epoch 55 | loss: 0.27413 | train_logloss: 0.26652 | train_accuracy: 0.89903 | valid_logloss: 0.29287 | valid_accuracy: 0.8902  |  0:01:16s\n",
            "epoch 56 | loss: 0.27388 | train_logloss: 0.27101 | train_accuracy: 0.89763 | valid_logloss: 0.29769 | valid_accuracy: 0.88923 |  0:01:18s\n",
            "epoch 57 | loss: 0.28082 | train_logloss: 0.26955 | train_accuracy: 0.8974  | valid_logloss: 0.29359 | valid_accuracy: 0.8895  |  0:01:19s\n",
            "epoch 58 | loss: 0.27853 | train_logloss: 0.28173 | train_accuracy: 0.89317 | valid_logloss: 0.30716 | valid_accuracy: 0.88537 |  0:01:20s\n",
            "epoch 59 | loss: 0.28067 | train_logloss: 0.26982 | train_accuracy: 0.8979  | valid_logloss: 0.29576 | valid_accuracy: 0.88903 |  0:01:22s\n",
            "epoch 60 | loss: 0.27841 | train_logloss: 0.2704  | train_accuracy: 0.89863 | valid_logloss: 0.29909 | valid_accuracy: 0.89003 |  0:01:23s\n",
            "epoch 61 | loss: 0.27513 | train_logloss: 0.26616 | train_accuracy: 0.89953 | valid_logloss: 0.29486 | valid_accuracy: 0.88937 |  0:01:25s\n",
            "epoch 62 | loss: 0.27258 | train_logloss: 0.26216 | train_accuracy: 0.90027 | valid_logloss: 0.28982 | valid_accuracy: 0.89063 |  0:01:26s\n",
            "epoch 63 | loss: 0.27178 | train_logloss: 0.26765 | train_accuracy: 0.89767 | valid_logloss: 0.29779 | valid_accuracy: 0.8883  |  0:01:27s\n",
            "epoch 64 | loss: 0.27223 | train_logloss: 0.26288 | train_accuracy: 0.90033 | valid_logloss: 0.29512 | valid_accuracy: 0.88963 |  0:01:29s\n",
            "epoch 65 | loss: 0.2717  | train_logloss: 0.26217 | train_accuracy: 0.89913 | valid_logloss: 0.29304 | valid_accuracy: 0.8888  |  0:01:30s\n",
            "epoch 66 | loss: 0.27263 | train_logloss: 0.26192 | train_accuracy: 0.89967 | valid_logloss: 0.2965  | valid_accuracy: 0.89003 |  0:01:31s\n",
            "epoch 67 | loss: 0.26986 | train_logloss: 0.26438 | train_accuracy: 0.89913 | valid_logloss: 0.29923 | valid_accuracy: 0.8891  |  0:01:33s\n",
            "epoch 68 | loss: 0.26838 | train_logloss: 0.26081 | train_accuracy: 0.89987 | valid_logloss: 0.29379 | valid_accuracy: 0.88913 |  0:01:34s\n",
            "epoch 69 | loss: 0.26602 | train_logloss: 0.26696 | train_accuracy: 0.9003  | valid_logloss: 0.29889 | valid_accuracy: 0.88773 |  0:01:35s\n",
            "epoch 70 | loss: 0.26606 | train_logloss: 0.2588  | train_accuracy: 0.90177 | valid_logloss: 0.29294 | valid_accuracy: 0.89067 |  0:01:37s\n",
            "epoch 71 | loss: 0.26979 | train_logloss: 0.26122 | train_accuracy: 0.9018  | valid_logloss: 0.29634 | valid_accuracy: 0.88983 |  0:01:38s\n",
            "epoch 72 | loss: 0.27041 | train_logloss: 0.26176 | train_accuracy: 0.89803 | valid_logloss: 0.30067 | valid_accuracy: 0.88833 |  0:01:39s\n",
            "epoch 73 | loss: 0.27028 | train_logloss: 0.2716  | train_accuracy: 0.8963  | valid_logloss: 0.29943 | valid_accuracy: 0.8882  |  0:01:41s\n",
            "epoch 74 | loss: 0.27493 | train_logloss: 0.26697 | train_accuracy: 0.89753 | valid_logloss: 0.30287 | valid_accuracy: 0.8866  |  0:01:42s\n",
            "epoch 75 | loss: 0.26986 | train_logloss: 0.26064 | train_accuracy: 0.9016  | valid_logloss: 0.29579 | valid_accuracy: 0.8901  |  0:01:44s\n",
            "epoch 76 | loss: 0.26444 | train_logloss: 0.25881 | train_accuracy: 0.90203 | valid_logloss: 0.29691 | valid_accuracy: 0.8902  |  0:01:45s\n",
            "epoch 77 | loss: 0.26822 | train_logloss: 0.26482 | train_accuracy: 0.89953 | valid_logloss: 0.29755 | valid_accuracy: 0.8893  |  0:01:46s\n",
            "epoch 78 | loss: 0.27264 | train_logloss: 0.26318 | train_accuracy: 0.89933 | valid_logloss: 0.29835 | valid_accuracy: 0.88943 |  0:01:48s\n",
            "epoch 79 | loss: 0.26669 | train_logloss: 0.25495 | train_accuracy: 0.90267 | valid_logloss: 0.29106 | valid_accuracy: 0.89123 |  0:01:49s\n",
            "epoch 80 | loss: 0.2659  | train_logloss: 0.25512 | train_accuracy: 0.9021  | valid_logloss: 0.29342 | valid_accuracy: 0.89157 |  0:01:50s\n",
            "epoch 81 | loss: 0.26552 | train_logloss: 0.25725 | train_accuracy: 0.90163 | valid_logloss: 0.2954  | valid_accuracy: 0.892   |  0:01:52s\n",
            "epoch 82 | loss: 0.26563 | train_logloss: 0.2571  | train_accuracy: 0.90123 | valid_logloss: 0.294   | valid_accuracy: 0.89167 |  0:01:53s\n",
            "epoch 83 | loss: 0.26445 | train_logloss: 0.25791 | train_accuracy: 0.9006  | valid_logloss: 0.29592 | valid_accuracy: 0.891   |  0:01:54s\n",
            "epoch 84 | loss: 0.26404 | train_logloss: 0.25688 | train_accuracy: 0.90173 | valid_logloss: 0.29834 | valid_accuracy: 0.88987 |  0:01:56s\n",
            "epoch 85 | loss: 0.26394 | train_logloss: 0.25942 | train_accuracy: 0.90193 | valid_logloss: 0.29877 | valid_accuracy: 0.88963 |  0:01:57s\n",
            "epoch 86 | loss: 0.26278 | train_logloss: 0.25549 | train_accuracy: 0.90197 | valid_logloss: 0.29813 | valid_accuracy: 0.89027 |  0:01:58s\n",
            "epoch 87 | loss: 0.25958 | train_logloss: 0.26002 | train_accuracy: 0.89987 | valid_logloss: 0.29773 | valid_accuracy: 0.89147 |  0:02:00s\n",
            "epoch 88 | loss: 0.26416 | train_logloss: 0.25569 | train_accuracy: 0.90137 | valid_logloss: 0.29319 | valid_accuracy: 0.8903  |  0:02:01s\n",
            "epoch 89 | loss: 0.26394 | train_logloss: 0.25693 | train_accuracy: 0.90237 | valid_logloss: 0.29341 | valid_accuracy: 0.89097 |  0:02:03s\n",
            "epoch 90 | loss: 0.26137 | train_logloss: 0.25108 | train_accuracy: 0.90433 | valid_logloss: 0.29466 | valid_accuracy: 0.89207 |  0:02:04s\n",
            "epoch 91 | loss: 0.26006 | train_logloss: 0.24994 | train_accuracy: 0.90357 | valid_logloss: 0.29146 | valid_accuracy: 0.8924  |  0:02:05s\n",
            "epoch 92 | loss: 0.2585  | train_logloss: 0.24849 | train_accuracy: 0.90537 | valid_logloss: 0.29136 | valid_accuracy: 0.89203 |  0:02:07s\n",
            "epoch 93 | loss: 0.26049 | train_logloss: 0.25393 | train_accuracy: 0.90197 | valid_logloss: 0.29649 | valid_accuracy: 0.89113 |  0:02:08s\n",
            "epoch 94 | loss: 0.25953 | train_logloss: 0.24768 | train_accuracy: 0.90583 | valid_logloss: 0.28972 | valid_accuracy: 0.8928  |  0:02:09s\n",
            "epoch 95 | loss: 0.25756 | train_logloss: 0.25008 | train_accuracy: 0.90193 | valid_logloss: 0.29596 | valid_accuracy: 0.88987 |  0:02:11s\n",
            "epoch 96 | loss: 0.25798 | train_logloss: 0.25193 | train_accuracy: 0.90403 | valid_logloss: 0.29438 | valid_accuracy: 0.8931  |  0:02:12s\n",
            "epoch 97 | loss: 0.25804 | train_logloss: 0.25524 | train_accuracy: 0.90313 | valid_logloss: 0.2986  | valid_accuracy: 0.89003 |  0:02:14s\n",
            "epoch 98 | loss: 0.26007 | train_logloss: 0.25681 | train_accuracy: 0.9008  | valid_logloss: 0.29981 | valid_accuracy: 0.8913  |  0:02:15s\n",
            "epoch 99 | loss: 0.26038 | train_logloss: 0.25583 | train_accuracy: 0.90193 | valid_logloss: 0.29935 | valid_accuracy: 0.89173 |  0:02:16s\n",
            "epoch 100| loss: 0.26157 | train_logloss: 0.25367 | train_accuracy: 0.90313 | valid_logloss: 0.29337 | valid_accuracy: 0.8915  |  0:02:18s\n",
            "epoch 101| loss: 0.25968 | train_logloss: 0.25475 | train_accuracy: 0.9016  | valid_logloss: 0.29706 | valid_accuracy: 0.88873 |  0:02:19s\n",
            "epoch 102| loss: 0.26051 | train_logloss: 0.25569 | train_accuracy: 0.90147 | valid_logloss: 0.29758 | valid_accuracy: 0.89057 |  0:02:21s\n",
            "epoch 103| loss: 0.26188 | train_logloss: 0.25044 | train_accuracy: 0.90427 | valid_logloss: 0.29418 | valid_accuracy: 0.8918  |  0:02:22s\n",
            "epoch 104| loss: 0.25887 | train_logloss: 0.2483  | train_accuracy: 0.90383 | valid_logloss: 0.29446 | valid_accuracy: 0.89    |  0:02:23s\n",
            "epoch 105| loss: 0.25505 | train_logloss: 0.25105 | train_accuracy: 0.9037  | valid_logloss: 0.29508 | valid_accuracy: 0.88993 |  0:02:25s\n",
            "epoch 106| loss: 0.25742 | train_logloss: 0.2512  | train_accuracy: 0.9037  | valid_logloss: 0.29846 | valid_accuracy: 0.89067 |  0:02:26s\n",
            "epoch 107| loss: 0.25405 | train_logloss: 0.24333 | train_accuracy: 0.90573 | valid_logloss: 0.29695 | valid_accuracy: 0.89277 |  0:02:27s\n",
            "epoch 108| loss: 0.25193 | train_logloss: 0.24423 | train_accuracy: 0.90473 | valid_logloss: 0.29505 | valid_accuracy: 0.89227 |  0:02:29s\n",
            "epoch 109| loss: 0.25323 | train_logloss: 0.25024 | train_accuracy: 0.9038  | valid_logloss: 0.30226 | valid_accuracy: 0.89037 |  0:02:30s\n",
            "epoch 110| loss: 0.25758 | train_logloss: 0.25172 | train_accuracy: 0.9045  | valid_logloss: 0.29649 | valid_accuracy: 0.8917  |  0:02:31s\n",
            "epoch 111| loss: 0.2574  | train_logloss: 0.24922 | train_accuracy: 0.9037  | valid_logloss: 0.29408 | valid_accuracy: 0.89053 |  0:02:33s\n",
            "epoch 112| loss: 0.25431 | train_logloss: 0.24648 | train_accuracy: 0.9053  | valid_logloss: 0.29749 | valid_accuracy: 0.8911  |  0:02:34s\n",
            "epoch 113| loss: 0.25618 | train_logloss: 0.24948 | train_accuracy: 0.90557 | valid_logloss: 0.29493 | valid_accuracy: 0.892   |  0:02:36s\n",
            "epoch 114| loss: 0.25781 | train_logloss: 0.246   | train_accuracy: 0.90533 | valid_logloss: 0.29488 | valid_accuracy: 0.891   |  0:02:37s\n",
            "epoch 115| loss: 0.25534 | train_logloss: 0.25418 | train_accuracy: 0.903   | valid_logloss: 0.29971 | valid_accuracy: 0.88887 |  0:02:38s\n",
            "epoch 116| loss: 0.25357 | train_logloss: 0.24591 | train_accuracy: 0.9058  | valid_logloss: 0.29593 | valid_accuracy: 0.89327 |  0:02:40s\n",
            "epoch 117| loss: 0.2553  | train_logloss: 0.24566 | train_accuracy: 0.90537 | valid_logloss: 0.29558 | valid_accuracy: 0.89063 |  0:02:41s\n",
            "epoch 118| loss: 0.25399 | train_logloss: 0.24898 | train_accuracy: 0.90443 | valid_logloss: 0.29418 | valid_accuracy: 0.88967 |  0:02:42s\n",
            "epoch 119| loss: 0.25033 | train_logloss: 0.24452 | train_accuracy: 0.9055  | valid_logloss: 0.29709 | valid_accuracy: 0.89197 |  0:02:44s\n",
            "epoch 120| loss: 0.25118 | train_logloss: 0.24421 | train_accuracy: 0.9046  | valid_logloss: 0.29644 | valid_accuracy: 0.89197 |  0:02:45s\n",
            "epoch 121| loss: 0.2532  | train_logloss: 0.24299 | train_accuracy: 0.90663 | valid_logloss: 0.29361 | valid_accuracy: 0.89293 |  0:02:46s\n",
            "epoch 122| loss: 0.25156 | train_logloss: 0.24597 | train_accuracy: 0.90407 | valid_logloss: 0.29887 | valid_accuracy: 0.8899  |  0:02:48s\n",
            "epoch 123| loss: 0.25057 | train_logloss: 0.2431  | train_accuracy: 0.90607 | valid_logloss: 0.29358 | valid_accuracy: 0.89187 |  0:02:49s\n",
            "epoch 124| loss: 0.25458 | train_logloss: 0.2478  | train_accuracy: 0.9045  | valid_logloss: 0.29568 | valid_accuracy: 0.89153 |  0:02:51s\n",
            "epoch 125| loss: 0.25914 | train_logloss: 0.25092 | train_accuracy: 0.9043  | valid_logloss: 0.2962  | valid_accuracy: 0.891   |  0:02:52s\n",
            "epoch 126| loss: 0.25582 | train_logloss: 0.24573 | train_accuracy: 0.90597 | valid_logloss: 0.29864 | valid_accuracy: 0.89163 |  0:02:54s\n",
            "epoch 127| loss: 0.25502 | train_logloss: 0.24425 | train_accuracy: 0.90727 | valid_logloss: 0.29797 | valid_accuracy: 0.89343 |  0:02:55s\n",
            "epoch 128| loss: 0.25234 | train_logloss: 0.24785 | train_accuracy: 0.90463 | valid_logloss: 0.29997 | valid_accuracy: 0.89163 |  0:02:56s\n",
            "epoch 129| loss: 0.25015 | train_logloss: 0.23902 | train_accuracy: 0.90797 | valid_logloss: 0.29782 | valid_accuracy: 0.89163 |  0:02:58s\n",
            "epoch 130| loss: 0.24821 | train_logloss: 0.24117 | train_accuracy: 0.90613 | valid_logloss: 0.29676 | valid_accuracy: 0.89193 |  0:02:59s\n",
            "epoch 131| loss: 0.24946 | train_logloss: 0.24136 | train_accuracy: 0.907   | valid_logloss: 0.29779 | valid_accuracy: 0.89407 |  0:03:01s\n",
            "epoch 132| loss: 0.25195 | train_logloss: 0.24802 | train_accuracy: 0.90403 | valid_logloss: 0.29984 | valid_accuracy: 0.89007 |  0:03:02s\n",
            "epoch 133| loss: 0.25004 | train_logloss: 0.24803 | train_accuracy: 0.90493 | valid_logloss: 0.30735 | valid_accuracy: 0.88997 |  0:03:03s\n",
            "epoch 134| loss: 0.24928 | train_logloss: 0.2424  | train_accuracy: 0.90463 | valid_logloss: 0.30076 | valid_accuracy: 0.89183 |  0:03:05s\n",
            "epoch 135| loss: 0.25067 | train_logloss: 0.24287 | train_accuracy: 0.90707 | valid_logloss: 0.29764 | valid_accuracy: 0.89143 |  0:03:06s\n",
            "epoch 136| loss: 0.2488  | train_logloss: 0.23869 | train_accuracy: 0.9076  | valid_logloss: 0.29666 | valid_accuracy: 0.89117 |  0:03:07s\n",
            "epoch 137| loss: 0.24941 | train_logloss: 0.23976 | train_accuracy: 0.90737 | valid_logloss: 0.30051 | valid_accuracy: 0.89047 |  0:03:09s\n",
            "epoch 138| loss: 0.24836 | train_logloss: 0.23772 | train_accuracy: 0.90827 | valid_logloss: 0.29636 | valid_accuracy: 0.89207 |  0:03:10s\n",
            "epoch 139| loss: 0.24955 | train_logloss: 0.24106 | train_accuracy: 0.9071  | valid_logloss: 0.29822 | valid_accuracy: 0.8921  |  0:03:12s\n",
            "epoch 140| loss: 0.25224 | train_logloss: 0.24238 | train_accuracy: 0.9051  | valid_logloss: 0.29914 | valid_accuracy: 0.8896  |  0:03:13s\n",
            "epoch 141| loss: 0.25192 | train_logloss: 0.23909 | train_accuracy: 0.90843 | valid_logloss: 0.29473 | valid_accuracy: 0.89317 |  0:03:14s\n",
            "epoch 142| loss: 0.24973 | train_logloss: 0.24211 | train_accuracy: 0.90787 | valid_logloss: 0.29615 | valid_accuracy: 0.89203 |  0:03:16s\n",
            "epoch 143| loss: 0.2467  | train_logloss: 0.24266 | train_accuracy: 0.90683 | valid_logloss: 0.30512 | valid_accuracy: 0.88927 |  0:03:17s\n",
            "epoch 144| loss: 0.24663 | train_logloss: 0.23511 | train_accuracy: 0.91033 | valid_logloss: 0.2972  | valid_accuracy: 0.8917  |  0:03:18s\n",
            "epoch 145| loss: 0.24819 | train_logloss: 0.23879 | train_accuracy: 0.90807 | valid_logloss: 0.30467 | valid_accuracy: 0.88983 |  0:03:20s\n",
            "epoch 146| loss: 0.24675 | train_logloss: 0.23763 | train_accuracy: 0.9083  | valid_logloss: 0.30564 | valid_accuracy: 0.8901  |  0:03:21s\n",
            "epoch 147| loss: 0.24618 | train_logloss: 0.24018 | train_accuracy: 0.9068  | valid_logloss: 0.30694 | valid_accuracy: 0.88903 |  0:03:22s\n",
            "epoch 148| loss: 0.24552 | train_logloss: 0.23481 | train_accuracy: 0.90803 | valid_logloss: 0.30183 | valid_accuracy: 0.89273 |  0:03:24s\n",
            "epoch 149| loss: 0.2449  | train_logloss: 0.23454 | train_accuracy: 0.90963 | valid_logloss: 0.3056  | valid_accuracy: 0.8906  |  0:03:25s\n",
            "epoch 150| loss: 0.24556 | train_logloss: 0.24271 | train_accuracy: 0.905   | valid_logloss: 0.30681 | valid_accuracy: 0.88777 |  0:03:27s\n",
            "epoch 151| loss: 0.24525 | train_logloss: 0.23526 | train_accuracy: 0.9092  | valid_logloss: 0.30722 | valid_accuracy: 0.89193 |  0:03:28s\n",
            "epoch 152| loss: 0.24589 | train_logloss: 0.2346  | train_accuracy: 0.9093  | valid_logloss: 0.30118 | valid_accuracy: 0.8921  |  0:03:29s\n",
            "epoch 153| loss: 0.24457 | train_logloss: 0.23769 | train_accuracy: 0.9093  | valid_logloss: 0.30827 | valid_accuracy: 0.8909  |  0:03:31s\n",
            "epoch 154| loss: 0.24573 | train_logloss: 0.23621 | train_accuracy: 0.9082  | valid_logloss: 0.30669 | valid_accuracy: 0.8909  |  0:03:32s\n",
            "epoch 155| loss: 0.24522 | train_logloss: 0.23536 | train_accuracy: 0.9087  | valid_logloss: 0.30308 | valid_accuracy: 0.89077 |  0:03:33s\n",
            "epoch 156| loss: 0.2448  | train_logloss: 0.23179 | train_accuracy: 0.91173 | valid_logloss: 0.30188 | valid_accuracy: 0.8912  |  0:03:35s\n",
            "epoch 157| loss: 0.24791 | train_logloss: 0.237   | train_accuracy: 0.9098  | valid_logloss: 0.30321 | valid_accuracy: 0.892   |  0:03:36s\n",
            "epoch 158| loss: 0.24611 | train_logloss: 0.23509 | train_accuracy: 0.91067 | valid_logloss: 0.3027  | valid_accuracy: 0.888   |  0:03:37s\n",
            "epoch 159| loss: 0.24398 | train_logloss: 0.23675 | train_accuracy: 0.90933 | valid_logloss: 0.30311 | valid_accuracy: 0.89123 |  0:03:39s\n",
            "epoch 160| loss: 0.24411 | train_logloss: 0.23964 | train_accuracy: 0.9085  | valid_logloss: 0.30445 | valid_accuracy: 0.889   |  0:03:40s\n",
            "epoch 161| loss: 0.24517 | train_logloss: 0.23346 | train_accuracy: 0.90973 | valid_logloss: 0.30324 | valid_accuracy: 0.89063 |  0:03:42s\n",
            "epoch 162| loss: 0.24584 | train_logloss: 0.23554 | train_accuracy: 0.90953 | valid_logloss: 0.30349 | valid_accuracy: 0.8919  |  0:03:43s\n",
            "epoch 163| loss: 0.24474 | train_logloss: 0.23187 | train_accuracy: 0.9109  | valid_logloss: 0.29961 | valid_accuracy: 0.89217 |  0:03:44s\n",
            "epoch 164| loss: 0.2454  | train_logloss: 0.24392 | train_accuracy: 0.90613 | valid_logloss: 0.3078  | valid_accuracy: 0.88843 |  0:03:46s\n",
            "epoch 165| loss: 0.25261 | train_logloss: 0.23982 | train_accuracy: 0.90863 | valid_logloss: 0.29741 | valid_accuracy: 0.88977 |  0:03:47s\n",
            "epoch 166| loss: 0.2486  | train_logloss: 0.24562 | train_accuracy: 0.90513 | valid_logloss: 0.30787 | valid_accuracy: 0.88707 |  0:03:48s\n",
            "epoch 167| loss: 0.24743 | train_logloss: 0.235   | train_accuracy: 0.90937 | valid_logloss: 0.29921 | valid_accuracy: 0.89147 |  0:03:50s\n",
            "epoch 168| loss: 0.24517 | train_logloss: 0.2399  | train_accuracy: 0.908   | valid_logloss: 0.30603 | valid_accuracy: 0.89073 |  0:03:51s\n",
            "epoch 169| loss: 0.25011 | train_logloss: 0.23846 | train_accuracy: 0.90693 | valid_logloss: 0.30477 | valid_accuracy: 0.8895  |  0:03:53s\n",
            "epoch 170| loss: 0.24719 | train_logloss: 0.23886 | train_accuracy: 0.90683 | valid_logloss: 0.30137 | valid_accuracy: 0.8901  |  0:03:54s\n",
            "epoch 171| loss: 0.24892 | train_logloss: 0.2458  | train_accuracy: 0.9058  | valid_logloss: 0.30707 | valid_accuracy: 0.8884  |  0:03:55s\n",
            "epoch 172| loss: 0.24904 | train_logloss: 0.23999 | train_accuracy: 0.9088  | valid_logloss: 0.29952 | valid_accuracy: 0.89033 |  0:03:57s\n",
            "epoch 173| loss: 0.26151 | train_logloss: 0.25933 | train_accuracy: 0.89953 | valid_logloss: 0.31096 | valid_accuracy: 0.88893 |  0:03:58s\n",
            "epoch 174| loss: 0.25518 | train_logloss: 0.24266 | train_accuracy: 0.90653 | valid_logloss: 0.30605 | valid_accuracy: 0.8904  |  0:03:59s\n",
            "epoch 175| loss: 0.25073 | train_logloss: 0.23931 | train_accuracy: 0.9082  | valid_logloss: 0.3006  | valid_accuracy: 0.8922  |  0:04:01s\n",
            "epoch 176| loss: 0.2489  | train_logloss: 0.235   | train_accuracy: 0.90977 | valid_logloss: 0.30012 | valid_accuracy: 0.89187 |  0:04:02s\n",
            "epoch 177| loss: 0.24509 | train_logloss: 0.23342 | train_accuracy: 0.90927 | valid_logloss: 0.29829 | valid_accuracy: 0.8914  |  0:04:04s\n",
            "epoch 178| loss: 0.24433 | train_logloss: 0.24129 | train_accuracy: 0.9075  | valid_logloss: 0.30676 | valid_accuracy: 0.88907 |  0:04:05s\n",
            "epoch 179| loss: 0.25226 | train_logloss: 0.24394 | train_accuracy: 0.9044  | valid_logloss: 0.30848 | valid_accuracy: 0.8892  |  0:04:06s\n",
            "epoch 180| loss: 0.25052 | train_logloss: 0.23413 | train_accuracy: 0.90957 | valid_logloss: 0.29967 | valid_accuracy: 0.89123 |  0:04:08s\n",
            "epoch 181| loss: 0.24708 | train_logloss: 0.23455 | train_accuracy: 0.9085  | valid_logloss: 0.30393 | valid_accuracy: 0.89153 |  0:04:09s\n",
            "\n",
            "Early stopping occurred at epoch 181 with best_epoch = 131 and best_valid_accuracy = 0.89407\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[422940  11729  45513]\n",
            " [ 18365 946095  35540]\n",
            " [ 36503  27251 368833]]\n",
            "Testing Score:  0.8940666666666667\n",
            "Confusion Matrix: \n",
            " [[419412  12587  48183]\n",
            " [ 13732 954252  32016]\n",
            " [ 35048  28128 369411]]\n",
            "Testing Score:  0.8917666666666667\n",
            "{'Rows': 30000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 249.8592677116394, 'time_tn': 20.273621559143066, 'accuracy_tn': 0.8940666666666667, 'time_learn_gb': 8.276288509368896, 'time_gb': 27.093479871749878, 'accuracy_gb': 0.8917666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLhfko9RWbI6",
        "outputId": "699f9288-a9fe-4099-aa6a-3c1b278434c1"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1   9000   9000   9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2   9000   9000   9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3   9000   9000   9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4   9000   9000   9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "5  30000  30000  30000    8  ...  20.273622    0.891767      8.276289  27.093480\n",
              "\n",
              "[6 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djIcrrHxBLAc",
        "outputId": "2147d956-44ff-4726-fafb-10a4710960eb"
      },
      "source": [
        "time_model(number_exp=7, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.76376 | train_logloss: 8.69401 | train_accuracy: 0.34033 | valid_logloss: 8.49261 | valid_accuracy: 0.34137 |  0:00:01s\n",
            "epoch 1  | loss: 0.4436  | train_logloss: 7.67326 | train_accuracy: 0.38297 | valid_logloss: 7.70181 | valid_accuracy: 0.38297 |  0:00:03s\n",
            "epoch 2  | loss: 0.38025 | train_logloss: 5.99814 | train_accuracy: 0.37207 | valid_logloss: 5.98659 | valid_accuracy: 0.37387 |  0:00:04s\n",
            "epoch 3  | loss: 0.36162 | train_logloss: 2.36577 | train_accuracy: 0.2892  | valid_logloss: 2.37139 | valid_accuracy: 0.29147 |  0:00:06s\n",
            "epoch 4  | loss: 0.34708 | train_logloss: 1.52415 | train_accuracy: 0.3922  | valid_logloss: 1.53449 | valid_accuracy: 0.3895  |  0:00:07s\n",
            "epoch 5  | loss: 0.33675 | train_logloss: 1.93036 | train_accuracy: 0.42287 | valid_logloss: 1.92694 | valid_accuracy: 0.4237  |  0:00:09s\n",
            "epoch 6  | loss: 0.33209 | train_logloss: 1.3011  | train_accuracy: 0.43963 | valid_logloss: 1.31314 | valid_accuracy: 0.43527 |  0:00:10s\n",
            "epoch 7  | loss: 0.32907 | train_logloss: 1.56332 | train_accuracy: 0.4073  | valid_logloss: 1.5635  | valid_accuracy: 0.4058  |  0:00:12s\n",
            "epoch 8  | loss: 0.32087 | train_logloss: 2.35124 | train_accuracy: 0.3823  | valid_logloss: 2.36351 | valid_accuracy: 0.38453 |  0:00:13s\n",
            "epoch 9  | loss: 0.31951 | train_logloss: 1.48821 | train_accuracy: 0.35173 | valid_logloss: 1.50713 | valid_accuracy: 0.34723 |  0:00:15s\n",
            "epoch 10 | loss: 0.31569 | train_logloss: 1.21356 | train_accuracy: 0.41027 | valid_logloss: 1.22059 | valid_accuracy: 0.40517 |  0:00:17s\n",
            "epoch 11 | loss: 0.3206  | train_logloss: 1.25033 | train_accuracy: 0.44353 | valid_logloss: 1.2642  | valid_accuracy: 0.43823 |  0:00:18s\n",
            "epoch 12 | loss: 0.31589 | train_logloss: 0.93388 | train_accuracy: 0.57433 | valid_logloss: 0.9398  | valid_accuracy: 0.5719  |  0:00:20s\n",
            "epoch 13 | loss: 0.31786 | train_logloss: 0.74417 | train_accuracy: 0.68097 | valid_logloss: 0.75052 | valid_accuracy: 0.6737  |  0:00:21s\n",
            "epoch 14 | loss: 0.3172  | train_logloss: 0.8015  | train_accuracy: 0.6454  | valid_logloss: 0.80569 | valid_accuracy: 0.63993 |  0:00:23s\n",
            "epoch 15 | loss: 0.31292 | train_logloss: 0.66661 | train_accuracy: 0.70307 | valid_logloss: 0.67349 | valid_accuracy: 0.69663 |  0:00:24s\n",
            "epoch 16 | loss: 0.3129  | train_logloss: 0.4879  | train_accuracy: 0.81817 | valid_logloss: 0.49196 | valid_accuracy: 0.81337 |  0:00:26s\n",
            "epoch 17 | loss: 0.3101  | train_logloss: 0.51045 | train_accuracy: 0.8128  | valid_logloss: 0.51573 | valid_accuracy: 0.81073 |  0:00:27s\n",
            "epoch 18 | loss: 0.30752 | train_logloss: 0.43988 | train_accuracy: 0.83713 | valid_logloss: 0.44866 | valid_accuracy: 0.83117 |  0:00:29s\n",
            "epoch 19 | loss: 0.31013 | train_logloss: 0.38287 | train_accuracy: 0.85047 | valid_logloss: 0.39281 | valid_accuracy: 0.84817 |  0:00:30s\n",
            "epoch 20 | loss: 0.29983 | train_logloss: 0.37077 | train_accuracy: 0.8569  | valid_logloss: 0.38324 | valid_accuracy: 0.85567 |  0:00:32s\n",
            "epoch 21 | loss: 0.30056 | train_logloss: 0.34934 | train_accuracy: 0.86893 | valid_logloss: 0.36264 | valid_accuracy: 0.86413 |  0:00:34s\n",
            "epoch 22 | loss: 0.29498 | train_logloss: 0.33543 | train_accuracy: 0.87323 | valid_logloss: 0.35068 | valid_accuracy: 0.8716  |  0:00:35s\n",
            "epoch 23 | loss: 0.2922  | train_logloss: 0.31163 | train_accuracy: 0.88063 | valid_logloss: 0.32972 | valid_accuracy: 0.87777 |  0:00:37s\n",
            "epoch 24 | loss: 0.28797 | train_logloss: 0.30243 | train_accuracy: 0.88463 | valid_logloss: 0.32133 | valid_accuracy: 0.87883 |  0:00:38s\n",
            "epoch 25 | loss: 0.28567 | train_logloss: 0.31847 | train_accuracy: 0.87883 | valid_logloss: 0.3449  | valid_accuracy: 0.87277 |  0:00:40s\n",
            "epoch 26 | loss: 0.28835 | train_logloss: 0.31631 | train_accuracy: 0.8775  | valid_logloss: 0.33873 | valid_accuracy: 0.87313 |  0:00:41s\n",
            "epoch 27 | loss: 0.29371 | train_logloss: 0.29346 | train_accuracy: 0.88747 | valid_logloss: 0.3164  | valid_accuracy: 0.8818  |  0:00:43s\n",
            "epoch 28 | loss: 0.29571 | train_logloss: 0.31193 | train_accuracy: 0.87957 | valid_logloss: 0.33292 | valid_accuracy: 0.87603 |  0:00:44s\n",
            "epoch 29 | loss: 0.293   | train_logloss: 0.29099 | train_accuracy: 0.889   | valid_logloss: 0.31179 | valid_accuracy: 0.88457 |  0:00:46s\n",
            "epoch 30 | loss: 0.2897  | train_logloss: 0.28648 | train_accuracy: 0.8917  | valid_logloss: 0.30648 | valid_accuracy: 0.88627 |  0:00:47s\n",
            "epoch 31 | loss: 0.28839 | train_logloss: 0.28518 | train_accuracy: 0.8923  | valid_logloss: 0.30827 | valid_accuracy: 0.88437 |  0:00:49s\n",
            "epoch 32 | loss: 0.28727 | train_logloss: 0.28103 | train_accuracy: 0.8928  | valid_logloss: 0.30284 | valid_accuracy: 0.88863 |  0:00:51s\n",
            "epoch 33 | loss: 0.2828  | train_logloss: 0.27834 | train_accuracy: 0.89333 | valid_logloss: 0.30425 | valid_accuracy: 0.88643 |  0:00:52s\n",
            "epoch 34 | loss: 0.28518 | train_logloss: 0.28302 | train_accuracy: 0.89097 | valid_logloss: 0.30813 | valid_accuracy: 0.8847  |  0:00:54s\n",
            "epoch 35 | loss: 0.28302 | train_logloss: 0.27945 | train_accuracy: 0.89297 | valid_logloss: 0.30641 | valid_accuracy: 0.88517 |  0:00:55s\n",
            "epoch 36 | loss: 0.28051 | train_logloss: 0.2785  | train_accuracy: 0.8938  | valid_logloss: 0.30358 | valid_accuracy: 0.88677 |  0:00:57s\n",
            "epoch 37 | loss: 0.2843  | train_logloss: 0.28643 | train_accuracy: 0.89357 | valid_logloss: 0.31302 | valid_accuracy: 0.88507 |  0:00:59s\n",
            "epoch 38 | loss: 0.28468 | train_logloss: 0.27377 | train_accuracy: 0.89483 | valid_logloss: 0.30307 | valid_accuracy: 0.88587 |  0:01:00s\n",
            "epoch 39 | loss: 0.27951 | train_logloss: 0.27383 | train_accuracy: 0.8954  | valid_logloss: 0.30384 | valid_accuracy: 0.88527 |  0:01:02s\n",
            "epoch 40 | loss: 0.27536 | train_logloss: 0.2663  | train_accuracy: 0.89693 | valid_logloss: 0.29691 | valid_accuracy: 0.889   |  0:01:03s\n",
            "epoch 41 | loss: 0.27764 | train_logloss: 0.26839 | train_accuracy: 0.8979  | valid_logloss: 0.30271 | valid_accuracy: 0.8892  |  0:01:05s\n",
            "epoch 42 | loss: 0.27793 | train_logloss: 0.26525 | train_accuracy: 0.8977  | valid_logloss: 0.29802 | valid_accuracy: 0.88943 |  0:01:06s\n",
            "epoch 43 | loss: 0.27342 | train_logloss: 0.26892 | train_accuracy: 0.89827 | valid_logloss: 0.30329 | valid_accuracy: 0.88817 |  0:01:08s\n",
            "epoch 44 | loss: 0.27399 | train_logloss: 0.26565 | train_accuracy: 0.89983 | valid_logloss: 0.29683 | valid_accuracy: 0.89017 |  0:01:09s\n",
            "epoch 45 | loss: 0.27341 | train_logloss: 0.26585 | train_accuracy: 0.89767 | valid_logloss: 0.30104 | valid_accuracy: 0.88777 |  0:01:11s\n",
            "epoch 46 | loss: 0.27182 | train_logloss: 0.26984 | train_accuracy: 0.8963  | valid_logloss: 0.30573 | valid_accuracy: 0.88593 |  0:01:12s\n",
            "epoch 47 | loss: 0.27082 | train_logloss: 0.26073 | train_accuracy: 0.89953 | valid_logloss: 0.29867 | valid_accuracy: 0.88987 |  0:01:14s\n",
            "epoch 48 | loss: 0.26916 | train_logloss: 0.26303 | train_accuracy: 0.8995  | valid_logloss: 0.30046 | valid_accuracy: 0.8878  |  0:01:15s\n",
            "epoch 49 | loss: 0.26759 | train_logloss: 0.26149 | train_accuracy: 0.8999  | valid_logloss: 0.30424 | valid_accuracy: 0.8867  |  0:01:17s\n",
            "epoch 50 | loss: 0.271   | train_logloss: 0.25603 | train_accuracy: 0.9007  | valid_logloss: 0.29831 | valid_accuracy: 0.88997 |  0:01:18s\n",
            "epoch 51 | loss: 0.26835 | train_logloss: 0.26459 | train_accuracy: 0.89673 | valid_logloss: 0.30418 | valid_accuracy: 0.88663 |  0:01:20s\n",
            "epoch 52 | loss: 0.26831 | train_logloss: 0.25873 | train_accuracy: 0.8985  | valid_logloss: 0.30136 | valid_accuracy: 0.88877 |  0:01:21s\n",
            "epoch 53 | loss: 0.26628 | train_logloss: 0.25943 | train_accuracy: 0.90007 | valid_logloss: 0.30395 | valid_accuracy: 0.8871  |  0:01:23s\n",
            "epoch 54 | loss: 0.26565 | train_logloss: 0.25882 | train_accuracy: 0.89823 | valid_logloss: 0.30953 | valid_accuracy: 0.88693 |  0:01:25s\n",
            "epoch 55 | loss: 0.2649  | train_logloss: 0.26087 | train_accuracy: 0.90037 | valid_logloss: 0.3051  | valid_accuracy: 0.88767 |  0:01:26s\n",
            "epoch 56 | loss: 0.27208 | train_logloss: 0.26619 | train_accuracy: 0.89763 | valid_logloss: 0.30443 | valid_accuracy: 0.88763 |  0:01:28s\n",
            "epoch 57 | loss: 0.27201 | train_logloss: 0.26544 | train_accuracy: 0.8971  | valid_logloss: 0.30412 | valid_accuracy: 0.8879  |  0:01:29s\n",
            "epoch 58 | loss: 0.27304 | train_logloss: 0.26001 | train_accuracy: 0.8995  | valid_logloss: 0.30039 | valid_accuracy: 0.8893  |  0:01:31s\n",
            "epoch 59 | loss: 0.27163 | train_logloss: 0.26299 | train_accuracy: 0.89923 | valid_logloss: 0.30127 | valid_accuracy: 0.8878  |  0:01:32s\n",
            "epoch 60 | loss: 0.26952 | train_logloss: 0.25993 | train_accuracy: 0.89843 | valid_logloss: 0.2979  | valid_accuracy: 0.8883  |  0:01:34s\n",
            "epoch 61 | loss: 0.26871 | train_logloss: 0.25667 | train_accuracy: 0.89987 | valid_logloss: 0.30584 | valid_accuracy: 0.88793 |  0:01:35s\n",
            "epoch 62 | loss: 0.26861 | train_logloss: 0.25737 | train_accuracy: 0.90043 | valid_logloss: 0.30329 | valid_accuracy: 0.88883 |  0:01:37s\n",
            "epoch 63 | loss: 0.26635 | train_logloss: 0.25558 | train_accuracy: 0.90137 | valid_logloss: 0.30384 | valid_accuracy: 0.8893  |  0:01:38s\n",
            "epoch 64 | loss: 0.26559 | train_logloss: 0.25628 | train_accuracy: 0.90043 | valid_logloss: 0.30207 | valid_accuracy: 0.8897  |  0:01:40s\n",
            "epoch 65 | loss: 0.26545 | train_logloss: 0.25596 | train_accuracy: 0.9018  | valid_logloss: 0.30314 | valid_accuracy: 0.89057 |  0:01:41s\n",
            "epoch 66 | loss: 0.27347 | train_logloss: 0.27191 | train_accuracy: 0.89647 | valid_logloss: 0.31043 | valid_accuracy: 0.8849  |  0:01:43s\n",
            "epoch 67 | loss: 0.27463 | train_logloss: 0.26093 | train_accuracy: 0.89967 | valid_logloss: 0.30095 | valid_accuracy: 0.8887  |  0:01:44s\n",
            "epoch 68 | loss: 0.2732  | train_logloss: 0.26007 | train_accuracy: 0.89983 | valid_logloss: 0.30447 | valid_accuracy: 0.88977 |  0:01:46s\n",
            "epoch 69 | loss: 0.26715 | train_logloss: 0.26003 | train_accuracy: 0.89913 | valid_logloss: 0.30357 | valid_accuracy: 0.8907  |  0:01:47s\n",
            "epoch 70 | loss: 0.26526 | train_logloss: 0.25775 | train_accuracy: 0.89827 | valid_logloss: 0.30687 | valid_accuracy: 0.8874  |  0:01:49s\n",
            "epoch 71 | loss: 0.26381 | train_logloss: 0.25809 | train_accuracy: 0.8989  | valid_logloss: 0.30276 | valid_accuracy: 0.88897 |  0:01:50s\n",
            "epoch 72 | loss: 0.26795 | train_logloss: 0.26317 | train_accuracy: 0.89927 | valid_logloss: 0.30418 | valid_accuracy: 0.88827 |  0:01:52s\n",
            "epoch 73 | loss: 0.27367 | train_logloss: 0.26511 | train_accuracy: 0.8993  | valid_logloss: 0.30393 | valid_accuracy: 0.88697 |  0:01:54s\n",
            "epoch 74 | loss: 0.26639 | train_logloss: 0.25721 | train_accuracy: 0.90067 | valid_logloss: 0.2999  | valid_accuracy: 0.8879  |  0:01:55s\n",
            "epoch 75 | loss: 0.26378 | train_logloss: 0.25425 | train_accuracy: 0.90253 | valid_logloss: 0.30035 | valid_accuracy: 0.88797 |  0:01:57s\n",
            "epoch 76 | loss: 0.25792 | train_logloss: 0.25115 | train_accuracy: 0.902   | valid_logloss: 0.3008  | valid_accuracy: 0.8882  |  0:01:58s\n",
            "epoch 77 | loss: 0.26436 | train_logloss: 0.25651 | train_accuracy: 0.9     | valid_logloss: 0.30032 | valid_accuracy: 0.8881  |  0:02:00s\n",
            "epoch 78 | loss: 0.26527 | train_logloss: 0.25935 | train_accuracy: 0.89927 | valid_logloss: 0.30663 | valid_accuracy: 0.8877  |  0:02:01s\n",
            "epoch 79 | loss: 0.26908 | train_logloss: 0.25957 | train_accuracy: 0.9004  | valid_logloss: 0.30442 | valid_accuracy: 0.88877 |  0:02:03s\n",
            "epoch 80 | loss: 0.26589 | train_logloss: 0.25318 | train_accuracy: 0.90167 | valid_logloss: 0.29869 | valid_accuracy: 0.88923 |  0:02:04s\n",
            "epoch 81 | loss: 0.26191 | train_logloss: 0.25068 | train_accuracy: 0.90473 | valid_logloss: 0.30308 | valid_accuracy: 0.8898  |  0:02:06s\n",
            "epoch 82 | loss: 0.25875 | train_logloss: 0.2494  | train_accuracy: 0.90243 | valid_logloss: 0.30122 | valid_accuracy: 0.88907 |  0:02:07s\n",
            "epoch 83 | loss: 0.26061 | train_logloss: 0.25016 | train_accuracy: 0.90397 | valid_logloss: 0.30494 | valid_accuracy: 0.8886  |  0:02:09s\n",
            "epoch 84 | loss: 0.25802 | train_logloss: 0.25794 | train_accuracy: 0.90233 | valid_logloss: 0.31054 | valid_accuracy: 0.88847 |  0:02:10s\n",
            "epoch 85 | loss: 0.25554 | train_logloss: 0.24602 | train_accuracy: 0.90613 | valid_logloss: 0.30207 | valid_accuracy: 0.8883  |  0:02:12s\n",
            "epoch 86 | loss: 0.25585 | train_logloss: 0.24692 | train_accuracy: 0.90403 | valid_logloss: 0.29975 | valid_accuracy: 0.88867 |  0:02:13s\n",
            "epoch 87 | loss: 0.2569  | train_logloss: 0.24624 | train_accuracy: 0.9049  | valid_logloss: 0.30132 | valid_accuracy: 0.88957 |  0:02:15s\n",
            "epoch 88 | loss: 0.25723 | train_logloss: 0.24913 | train_accuracy: 0.9033  | valid_logloss: 0.30141 | valid_accuracy: 0.8898  |  0:02:16s\n",
            "epoch 89 | loss: 0.26054 | train_logloss: 0.25715 | train_accuracy: 0.899   | valid_logloss: 0.30681 | valid_accuracy: 0.88613 |  0:02:18s\n",
            "epoch 90 | loss: 0.26143 | train_logloss: 0.25447 | train_accuracy: 0.90087 | valid_logloss: 0.30309 | valid_accuracy: 0.88873 |  0:02:20s\n",
            "epoch 91 | loss: 0.25789 | train_logloss: 0.24753 | train_accuracy: 0.9037  | valid_logloss: 0.30189 | valid_accuracy: 0.88967 |  0:02:21s\n",
            "epoch 92 | loss: 0.25618 | train_logloss: 0.25081 | train_accuracy: 0.9026  | valid_logloss: 0.30162 | valid_accuracy: 0.88937 |  0:02:23s\n",
            "epoch 93 | loss: 0.25435 | train_logloss: 0.24272 | train_accuracy: 0.90543 | valid_logloss: 0.30067 | valid_accuracy: 0.89083 |  0:02:24s\n",
            "epoch 94 | loss: 0.25375 | train_logloss: 0.24813 | train_accuracy: 0.90453 | valid_logloss: 0.30385 | valid_accuracy: 0.88837 |  0:02:26s\n",
            "epoch 95 | loss: 0.25352 | train_logloss: 0.24484 | train_accuracy: 0.906   | valid_logloss: 0.3066  | valid_accuracy: 0.8883  |  0:02:27s\n",
            "epoch 96 | loss: 0.25252 | train_logloss: 0.25087 | train_accuracy: 0.90437 | valid_logloss: 0.30855 | valid_accuracy: 0.8885  |  0:02:29s\n",
            "epoch 97 | loss: 0.25316 | train_logloss: 0.24618 | train_accuracy: 0.9039  | valid_logloss: 0.30244 | valid_accuracy: 0.88677 |  0:02:30s\n",
            "epoch 98 | loss: 0.25267 | train_logloss: 0.24243 | train_accuracy: 0.9053  | valid_logloss: 0.30481 | valid_accuracy: 0.8887  |  0:02:32s\n",
            "epoch 99 | loss: 0.25124 | train_logloss: 0.24125 | train_accuracy: 0.90623 | valid_logloss: 0.3034  | valid_accuracy: 0.89143 |  0:02:33s\n",
            "epoch 100| loss: 0.25141 | train_logloss: 0.24406 | train_accuracy: 0.90437 | valid_logloss: 0.29882 | valid_accuracy: 0.89083 |  0:02:35s\n",
            "epoch 101| loss: 0.25277 | train_logloss: 0.24592 | train_accuracy: 0.9058  | valid_logloss: 0.30613 | valid_accuracy: 0.88817 |  0:02:36s\n",
            "epoch 102| loss: 0.2543  | train_logloss: 0.24274 | train_accuracy: 0.90577 | valid_logloss: 0.30266 | valid_accuracy: 0.88903 |  0:02:38s\n",
            "epoch 103| loss: 0.25037 | train_logloss: 0.23867 | train_accuracy: 0.90707 | valid_logloss: 0.30091 | valid_accuracy: 0.8913  |  0:02:39s\n",
            "epoch 104| loss: 0.25198 | train_logloss: 0.24618 | train_accuracy: 0.9047  | valid_logloss: 0.30377 | valid_accuracy: 0.88723 |  0:02:41s\n",
            "epoch 105| loss: 0.25287 | train_logloss: 0.251   | train_accuracy: 0.9022  | valid_logloss: 0.31    | valid_accuracy: 0.88813 |  0:02:42s\n",
            "epoch 106| loss: 0.25893 | train_logloss: 0.2524  | train_accuracy: 0.904   | valid_logloss: 0.3056  | valid_accuracy: 0.88587 |  0:02:44s\n",
            "epoch 107| loss: 0.25778 | train_logloss: 0.24729 | train_accuracy: 0.9031  | valid_logloss: 0.30659 | valid_accuracy: 0.88857 |  0:02:45s\n",
            "epoch 108| loss: 0.25273 | train_logloss: 0.24576 | train_accuracy: 0.90547 | valid_logloss: 0.305   | valid_accuracy: 0.88837 |  0:02:47s\n",
            "epoch 109| loss: 0.24903 | train_logloss: 0.24056 | train_accuracy: 0.90757 | valid_logloss: 0.30553 | valid_accuracy: 0.88857 |  0:02:49s\n",
            "epoch 110| loss: 0.2471  | train_logloss: 0.23779 | train_accuracy: 0.90703 | valid_logloss: 0.30274 | valid_accuracy: 0.89127 |  0:02:50s\n",
            "epoch 111| loss: 0.24608 | train_logloss: 0.24077 | train_accuracy: 0.90623 | valid_logloss: 0.30153 | valid_accuracy: 0.89053 |  0:02:52s\n",
            "epoch 112| loss: 0.24807 | train_logloss: 0.23881 | train_accuracy: 0.9068  | valid_logloss: 0.30504 | valid_accuracy: 0.88963 |  0:02:53s\n",
            "epoch 113| loss: 0.24601 | train_logloss: 0.23626 | train_accuracy: 0.9078  | valid_logloss: 0.30533 | valid_accuracy: 0.89003 |  0:02:55s\n",
            "epoch 114| loss: 0.24773 | train_logloss: 0.24738 | train_accuracy: 0.90423 | valid_logloss: 0.31108 | valid_accuracy: 0.88707 |  0:02:56s\n",
            "epoch 115| loss: 0.25441 | train_logloss: 0.24633 | train_accuracy: 0.90403 | valid_logloss: 0.30651 | valid_accuracy: 0.888   |  0:02:58s\n",
            "epoch 116| loss: 0.25483 | train_logloss: 0.24342 | train_accuracy: 0.90617 | valid_logloss: 0.30087 | valid_accuracy: 0.89103 |  0:02:59s\n",
            "epoch 117| loss: 0.24957 | train_logloss: 0.2419  | train_accuracy: 0.9063  | valid_logloss: 0.30512 | valid_accuracy: 0.88903 |  0:03:01s\n",
            "epoch 118| loss: 0.24776 | train_logloss: 0.23813 | train_accuracy: 0.90657 | valid_logloss: 0.3028  | valid_accuracy: 0.88987 |  0:03:02s\n",
            "epoch 119| loss: 0.24632 | train_logloss: 0.23567 | train_accuracy: 0.90843 | valid_logloss: 0.30323 | valid_accuracy: 0.88957 |  0:03:04s\n",
            "epoch 120| loss: 0.2418  | train_logloss: 0.23482 | train_accuracy: 0.90747 | valid_logloss: 0.30375 | valid_accuracy: 0.89207 |  0:03:05s\n",
            "epoch 121| loss: 0.24134 | train_logloss: 0.23073 | train_accuracy: 0.90823 | valid_logloss: 0.30392 | valid_accuracy: 0.8903  |  0:03:07s\n",
            "epoch 122| loss: 0.24128 | train_logloss: 0.23312 | train_accuracy: 0.90893 | valid_logloss: 0.30388 | valid_accuracy: 0.88983 |  0:03:09s\n",
            "epoch 123| loss: 0.23984 | train_logloss: 0.22826 | train_accuracy: 0.90997 | valid_logloss: 0.30936 | valid_accuracy: 0.88947 |  0:03:10s\n",
            "epoch 124| loss: 0.2412  | train_logloss: 0.23221 | train_accuracy: 0.90897 | valid_logloss: 0.31104 | valid_accuracy: 0.88863 |  0:03:12s\n",
            "epoch 125| loss: 0.24393 | train_logloss: 0.2376  | train_accuracy: 0.90657 | valid_logloss: 0.31102 | valid_accuracy: 0.8879  |  0:03:13s\n",
            "epoch 126| loss: 0.24515 | train_logloss: 0.2328  | train_accuracy: 0.9099  | valid_logloss: 0.30726 | valid_accuracy: 0.89003 |  0:03:15s\n",
            "epoch 127| loss: 0.24908 | train_logloss: 0.23755 | train_accuracy: 0.90737 | valid_logloss: 0.29977 | valid_accuracy: 0.8895  |  0:03:16s\n",
            "epoch 128| loss: 0.24739 | train_logloss: 0.2366  | train_accuracy: 0.907   | valid_logloss: 0.30287 | valid_accuracy: 0.89057 |  0:03:18s\n",
            "epoch 129| loss: 0.24134 | train_logloss: 0.23071 | train_accuracy: 0.90887 | valid_logloss: 0.30577 | valid_accuracy: 0.89127 |  0:03:19s\n",
            "epoch 130| loss: 0.24695 | train_logloss: 0.23976 | train_accuracy: 0.9056  | valid_logloss: 0.30694 | valid_accuracy: 0.88707 |  0:03:21s\n",
            "epoch 131| loss: 0.24936 | train_logloss: 0.23911 | train_accuracy: 0.9064  | valid_logloss: 0.30423 | valid_accuracy: 0.88943 |  0:03:22s\n",
            "epoch 132| loss: 0.24343 | train_logloss: 0.23131 | train_accuracy: 0.91023 | valid_logloss: 0.30936 | valid_accuracy: 0.88827 |  0:03:24s\n",
            "epoch 133| loss: 0.2397  | train_logloss: 0.22531 | train_accuracy: 0.91003 | valid_logloss: 0.30362 | valid_accuracy: 0.89093 |  0:03:26s\n",
            "epoch 134| loss: 0.2363  | train_logloss: 0.22827 | train_accuracy: 0.9093  | valid_logloss: 0.30583 | valid_accuracy: 0.8912  |  0:03:27s\n",
            "epoch 135| loss: 0.24064 | train_logloss: 0.22879 | train_accuracy: 0.91007 | valid_logloss: 0.3016  | valid_accuracy: 0.89157 |  0:03:29s\n",
            "epoch 136| loss: 0.23885 | train_logloss: 0.22908 | train_accuracy: 0.9105  | valid_logloss: 0.30226 | valid_accuracy: 0.89033 |  0:03:30s\n",
            "epoch 137| loss: 0.24168 | train_logloss: 0.22891 | train_accuracy: 0.90913 | valid_logloss: 0.30636 | valid_accuracy: 0.8896  |  0:03:32s\n",
            "epoch 138| loss: 0.23919 | train_logloss: 0.22845 | train_accuracy: 0.91017 | valid_logloss: 0.31019 | valid_accuracy: 0.88917 |  0:03:33s\n",
            "epoch 139| loss: 0.23926 | train_logloss: 0.22759 | train_accuracy: 0.91073 | valid_logloss: 0.30982 | valid_accuracy: 0.89037 |  0:03:35s\n",
            "epoch 140| loss: 0.23578 | train_logloss: 0.22237 | train_accuracy: 0.91193 | valid_logloss: 0.30536 | valid_accuracy: 0.89037 |  0:03:36s\n",
            "epoch 141| loss: 0.23474 | train_logloss: 0.22349 | train_accuracy: 0.90977 | valid_logloss: 0.31374 | valid_accuracy: 0.88957 |  0:03:38s\n",
            "epoch 142| loss: 0.23461 | train_logloss: 0.22232 | train_accuracy: 0.9116  | valid_logloss: 0.312   | valid_accuracy: 0.88927 |  0:03:39s\n",
            "epoch 143| loss: 0.24456 | train_logloss: 0.24632 | train_accuracy: 0.9055  | valid_logloss: 0.31467 | valid_accuracy: 0.8866  |  0:03:41s\n",
            "epoch 144| loss: 0.25707 | train_logloss: 0.25366 | train_accuracy: 0.90197 | valid_logloss: 0.31261 | valid_accuracy: 0.88693 |  0:03:43s\n",
            "epoch 145| loss: 0.25845 | train_logloss: 0.24594 | train_accuracy: 0.90593 | valid_logloss: 0.304   | valid_accuracy: 0.88923 |  0:03:44s\n",
            "epoch 146| loss: 0.25488 | train_logloss: 0.24208 | train_accuracy: 0.90423 | valid_logloss: 0.30407 | valid_accuracy: 0.88907 |  0:03:46s\n",
            "epoch 147| loss: 0.2469  | train_logloss: 0.23981 | train_accuracy: 0.90697 | valid_logloss: 0.30802 | valid_accuracy: 0.89027 |  0:03:47s\n",
            "epoch 148| loss: 0.24471 | train_logloss: 0.24197 | train_accuracy: 0.90583 | valid_logloss: 0.30982 | valid_accuracy: 0.89067 |  0:03:49s\n",
            "epoch 149| loss: 0.25215 | train_logloss: 0.23835 | train_accuracy: 0.9065  | valid_logloss: 0.29975 | valid_accuracy: 0.8899  |  0:03:50s\n",
            "epoch 150| loss: 0.24835 | train_logloss: 0.23872 | train_accuracy: 0.90763 | valid_logloss: 0.3089  | valid_accuracy: 0.8887  |  0:03:52s\n",
            "epoch 151| loss: 0.24252 | train_logloss: 0.23228 | train_accuracy: 0.90983 | valid_logloss: 0.30675 | valid_accuracy: 0.89097 |  0:03:53s\n",
            "epoch 152| loss: 0.24256 | train_logloss: 0.23558 | train_accuracy: 0.90843 | valid_logloss: 0.30838 | valid_accuracy: 0.88933 |  0:03:55s\n",
            "epoch 153| loss: 0.24484 | train_logloss: 0.23796 | train_accuracy: 0.9075  | valid_logloss: 0.3149  | valid_accuracy: 0.8868  |  0:03:56s\n",
            "epoch 154| loss: 0.24339 | train_logloss: 0.23192 | train_accuracy: 0.9096  | valid_logloss: 0.30991 | valid_accuracy: 0.88977 |  0:03:58s\n",
            "epoch 155| loss: 0.24169 | train_logloss: 0.22907 | train_accuracy: 0.90953 | valid_logloss: 0.30817 | valid_accuracy: 0.8892  |  0:03:59s\n",
            "epoch 156| loss: 0.23804 | train_logloss: 0.23171 | train_accuracy: 0.90657 | valid_logloss: 0.30818 | valid_accuracy: 0.88793 |  0:04:01s\n",
            "epoch 157| loss: 0.2405  | train_logloss: 0.22884 | train_accuracy: 0.91083 | valid_logloss: 0.30404 | valid_accuracy: 0.89113 |  0:04:02s\n",
            "epoch 158| loss: 0.23597 | train_logloss: 0.22438 | train_accuracy: 0.91183 | valid_logloss: 0.30781 | valid_accuracy: 0.8925  |  0:04:04s\n",
            "epoch 159| loss: 0.23706 | train_logloss: 0.22953 | train_accuracy: 0.90933 | valid_logloss: 0.31326 | valid_accuracy: 0.8899  |  0:04:05s\n",
            "epoch 160| loss: 0.23535 | train_logloss: 0.22583 | train_accuracy: 0.9113  | valid_logloss: 0.31139 | valid_accuracy: 0.8891  |  0:04:07s\n",
            "epoch 161| loss: 0.23913 | train_logloss: 0.22783 | train_accuracy: 0.91057 | valid_logloss: 0.3064  | valid_accuracy: 0.8895  |  0:04:09s\n",
            "epoch 162| loss: 0.23865 | train_logloss: 0.23315 | train_accuracy: 0.90897 | valid_logloss: 0.31728 | valid_accuracy: 0.88907 |  0:04:10s\n",
            "epoch 163| loss: 0.24941 | train_logloss: 0.23757 | train_accuracy: 0.90717 | valid_logloss: 0.31608 | valid_accuracy: 0.8897  |  0:04:12s\n",
            "epoch 164| loss: 0.24377 | train_logloss: 0.2388  | train_accuracy: 0.90677 | valid_logloss: 0.31666 | valid_accuracy: 0.8873  |  0:04:13s\n",
            "epoch 165| loss: 0.23972 | train_logloss: 0.22732 | train_accuracy: 0.91087 | valid_logloss: 0.31201 | valid_accuracy: 0.89093 |  0:04:15s\n",
            "epoch 166| loss: 0.23504 | train_logloss: 0.22874 | train_accuracy: 0.91083 | valid_logloss: 0.31116 | valid_accuracy: 0.88787 |  0:04:16s\n",
            "epoch 167| loss: 0.23601 | train_logloss: 0.22221 | train_accuracy: 0.9131  | valid_logloss: 0.313   | valid_accuracy: 0.8911  |  0:04:18s\n",
            "epoch 168| loss: 0.23898 | train_logloss: 0.23517 | train_accuracy: 0.9096  | valid_logloss: 0.31452 | valid_accuracy: 0.89    |  0:04:19s\n",
            "epoch 169| loss: 0.2432  | train_logloss: 0.24004 | train_accuracy: 0.9086  | valid_logloss: 0.31299 | valid_accuracy: 0.89157 |  0:04:21s\n",
            "epoch 170| loss: 0.24225 | train_logloss: 0.22903 | train_accuracy: 0.911   | valid_logloss: 0.31045 | valid_accuracy: 0.88963 |  0:04:22s\n",
            "epoch 171| loss: 0.23954 | train_logloss: 0.22667 | train_accuracy: 0.91187 | valid_logloss: 0.30487 | valid_accuracy: 0.89033 |  0:04:24s\n",
            "epoch 172| loss: 0.23818 | train_logloss: 0.22834 | train_accuracy: 0.9093  | valid_logloss: 0.30486 | valid_accuracy: 0.8905  |  0:04:25s\n",
            "epoch 173| loss: 0.23557 | train_logloss: 0.22347 | train_accuracy: 0.91263 | valid_logloss: 0.31538 | valid_accuracy: 0.88827 |  0:04:27s\n",
            "epoch 174| loss: 0.23297 | train_logloss: 0.21929 | train_accuracy: 0.9142  | valid_logloss: 0.31219 | valid_accuracy: 0.89067 |  0:04:29s\n",
            "epoch 175| loss: 0.22813 | train_logloss: 0.22467 | train_accuracy: 0.91187 | valid_logloss: 0.32198 | valid_accuracy: 0.8887  |  0:04:30s\n",
            "epoch 176| loss: 0.23977 | train_logloss: 0.23295 | train_accuracy: 0.90897 | valid_logloss: 0.31616 | valid_accuracy: 0.88743 |  0:04:32s\n",
            "epoch 177| loss: 0.23664 | train_logloss: 0.22724 | train_accuracy: 0.91077 | valid_logloss: 0.3107  | valid_accuracy: 0.88857 |  0:04:33s\n",
            "epoch 178| loss: 0.23485 | train_logloss: 0.21956 | train_accuracy: 0.91287 | valid_logloss: 0.30904 | valid_accuracy: 0.8906  |  0:04:35s\n",
            "epoch 179| loss: 0.22915 | train_logloss: 0.21994 | train_accuracy: 0.9137  | valid_logloss: 0.31564 | valid_accuracy: 0.8903  |  0:04:36s\n",
            "epoch 180| loss: 0.22877 | train_logloss: 0.21766 | train_accuracy: 0.9143  | valid_logloss: 0.31631 | valid_accuracy: 0.88927 |  0:04:38s\n",
            "epoch 181| loss: 0.23286 | train_logloss: 0.22544 | train_accuracy: 0.91163 | valid_logloss: 0.32093 | valid_accuracy: 0.88897 |  0:04:39s\n",
            "epoch 182| loss: 0.23011 | train_logloss: 0.21684 | train_accuracy: 0.91397 | valid_logloss: 0.31288 | valid_accuracy: 0.89083 |  0:04:41s\n",
            "epoch 183| loss: 0.22786 | train_logloss: 0.22528 | train_accuracy: 0.912   | valid_logloss: 0.31803 | valid_accuracy: 0.88857 |  0:04:42s\n",
            "epoch 184| loss: 0.22409 | train_logloss: 0.21957 | train_accuracy: 0.9123  | valid_logloss: 0.32818 | valid_accuracy: 0.88777 |  0:04:44s\n",
            "epoch 185| loss: 0.22669 | train_logloss: 0.21284 | train_accuracy: 0.9167  | valid_logloss: 0.3188  | valid_accuracy: 0.89003 |  0:04:45s\n",
            "epoch 186| loss: 0.22412 | train_logloss: 0.20821 | train_accuracy: 0.91857 | valid_logloss: 0.31671 | valid_accuracy: 0.88827 |  0:04:47s\n",
            "epoch 187| loss: 0.21894 | train_logloss: 0.21058 | train_accuracy: 0.91637 | valid_logloss: 0.329   | valid_accuracy: 0.88727 |  0:04:49s\n",
            "epoch 188| loss: 0.21934 | train_logloss: 0.21459 | train_accuracy: 0.91517 | valid_logloss: 0.32813 | valid_accuracy: 0.8867  |  0:04:50s\n",
            "epoch 189| loss: 0.22145 | train_logloss: 0.21721 | train_accuracy: 0.91403 | valid_logloss: 0.32696 | valid_accuracy: 0.8886  |  0:04:52s\n",
            "epoch 190| loss: 0.22352 | train_logloss: 0.21759 | train_accuracy: 0.9142  | valid_logloss: 0.33474 | valid_accuracy: 0.88423 |  0:04:53s\n",
            "epoch 191| loss: 0.22335 | train_logloss: 0.21999 | train_accuracy: 0.91367 | valid_logloss: 0.32538 | valid_accuracy: 0.88463 |  0:04:55s\n",
            "epoch 192| loss: 0.22497 | train_logloss: 0.21326 | train_accuracy: 0.91453 | valid_logloss: 0.31897 | valid_accuracy: 0.891   |  0:04:56s\n",
            "epoch 193| loss: 0.22489 | train_logloss: 0.21387 | train_accuracy: 0.91573 | valid_logloss: 0.32016 | valid_accuracy: 0.8904  |  0:04:58s\n",
            "epoch 194| loss: 0.22194 | train_logloss: 0.21198 | train_accuracy: 0.91603 | valid_logloss: 0.32981 | valid_accuracy: 0.88707 |  0:04:59s\n",
            "epoch 195| loss: 0.21934 | train_logloss: 0.21175 | train_accuracy: 0.9169  | valid_logloss: 0.31579 | valid_accuracy: 0.88817 |  0:05:01s\n",
            "epoch 196| loss: 0.21785 | train_logloss: 0.20725 | train_accuracy: 0.91877 | valid_logloss: 0.32628 | valid_accuracy: 0.88717 |  0:05:02s\n",
            "epoch 197| loss: 0.21426 | train_logloss: 0.20935 | train_accuracy: 0.9159  | valid_logloss: 0.33375 | valid_accuracy: 0.88327 |  0:05:04s\n",
            "epoch 198| loss: 0.21448 | train_logloss: 0.20505 | train_accuracy: 0.91943 | valid_logloss: 0.32434 | valid_accuracy: 0.8899  |  0:05:05s\n",
            "epoch 199| loss: 0.21727 | train_logloss: 0.20868 | train_accuracy: 0.91737 | valid_logloss: 0.33426 | valid_accuracy: 0.8855  |  0:05:07s\n",
            "epoch 200| loss: 0.21591 | train_logloss: 0.20394 | train_accuracy: 0.91863 | valid_logloss: 0.32792 | valid_accuracy: 0.88743 |  0:05:08s\n",
            "epoch 201| loss: 0.22097 | train_logloss: 0.20538 | train_accuracy: 0.9187  | valid_logloss: 0.32933 | valid_accuracy: 0.8905  |  0:05:10s\n",
            "epoch 202| loss: 0.21583 | train_logloss: 0.20655 | train_accuracy: 0.9176  | valid_logloss: 0.33369 | valid_accuracy: 0.8875  |  0:05:11s\n",
            "epoch 203| loss: 0.21595 | train_logloss: 0.2086  | train_accuracy: 0.91753 | valid_logloss: 0.33986 | valid_accuracy: 0.88463 |  0:05:13s\n",
            "epoch 204| loss: 0.2193  | train_logloss: 0.21042 | train_accuracy: 0.9173  | valid_logloss: 0.33515 | valid_accuracy: 0.88873 |  0:05:15s\n",
            "epoch 205| loss: 0.22079 | train_logloss: 0.21426 | train_accuracy: 0.91717 | valid_logloss: 0.33813 | valid_accuracy: 0.88727 |  0:05:16s\n",
            "epoch 206| loss: 0.21797 | train_logloss: 0.20126 | train_accuracy: 0.91963 | valid_logloss: 0.32594 | valid_accuracy: 0.8889  |  0:05:18s\n",
            "epoch 207| loss: 0.21397 | train_logloss: 0.20226 | train_accuracy: 0.92007 | valid_logloss: 0.33421 | valid_accuracy: 0.88947 |  0:05:19s\n",
            "epoch 208| loss: 0.21547 | train_logloss: 0.20914 | train_accuracy: 0.91657 | valid_logloss: 0.33145 | valid_accuracy: 0.88693 |  0:05:21s\n",
            "\n",
            "Early stopping occurred at epoch 208 with best_epoch = 158 and best_valid_accuracy = 0.8925\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[410432  12896  56854]\n",
            " [ 15247 945089  39664]\n",
            " [ 27602  26910 378075]]\n",
            "Testing Score:  0.8925\n",
            "Confusion Matrix: \n",
            " [[419412  12587  48183]\n",
            " [ 13732 954252  32016]\n",
            " [ 35048  28128 369411]]\n",
            "Testing Score:  0.8917666666666667\n",
            "{'Rows': 30000, 'Nd': 16, 'Na': 16, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 321.8348882198334, 'time_tn': 22.63023042678833, 'accuracy_tn': 0.8925, 'time_learn_gb': 8.210687398910522, 'time_gb': 27.34357786178589, 'accuracy_gb': 0.8917666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpysm9FvWbi7",
        "outputId": "88e5101d-81c9-4381-b5db-8f9f8dfd7a2b"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1   9000   9000   9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2   9000   9000   9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3   9000   9000   9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4   9000   9000   9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "5  30000  30000  30000    8  ...  20.273622    0.891767      8.276289  27.093480\n",
              "6  30000  30000  30000   16  ...  22.630230    0.891767      8.210687  27.343578\n",
              "\n",
              "[7 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuG8w6M3BLAd",
        "outputId": "4557c78f-997c-4030-ac31-67cc0f67c086"
      },
      "source": [
        "time_model(number_exp=8, \n",
        "          Rows=30000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=2048,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.9988  | train_logloss: 18.71415| train_accuracy: 0.4233  | valid_logloss: 18.66625| valid_accuracy: 0.42407 |  0:00:02s\n",
            "epoch 1  | loss: 0.55887 | train_logloss: 21.24174| train_accuracy: 0.33337 | valid_logloss: 21.30052| valid_accuracy: 0.33323 |  0:00:04s\n",
            "epoch 2  | loss: 0.46631 | train_logloss: 21.49563| train_accuracy: 0.14713 | valid_logloss: 21.30088| valid_accuracy: 0.15167 |  0:00:06s\n",
            "epoch 3  | loss: 0.38887 | train_logloss: 18.99009| train_accuracy: 0.3336  | valid_logloss: 18.99406| valid_accuracy: 0.3338  |  0:00:08s\n",
            "epoch 4  | loss: 0.38343 | train_logloss: 5.08895 | train_accuracy: 0.3337  | valid_logloss: 5.09685 | valid_accuracy: 0.33367 |  0:00:10s\n",
            "epoch 5  | loss: 0.38242 | train_logloss: 2.6987  | train_accuracy: 0.32787 | valid_logloss: 2.72169 | valid_accuracy: 0.32823 |  0:00:12s\n",
            "epoch 6  | loss: 0.37912 | train_logloss: 7.61661 | train_accuracy: 0.27283 | valid_logloss: 7.67888 | valid_accuracy: 0.2721  |  0:00:14s\n",
            "epoch 7  | loss: 0.36877 | train_logloss: 1.99155 | train_accuracy: 0.32247 | valid_logloss: 1.99612 | valid_accuracy: 0.31933 |  0:00:16s\n",
            "epoch 8  | loss: 0.36487 | train_logloss: 1.68149 | train_accuracy: 0.5519  | valid_logloss: 1.69895 | valid_accuracy: 0.5482  |  0:00:18s\n",
            "epoch 9  | loss: 0.36244 | train_logloss: 2.16661 | train_accuracy: 0.3967  | valid_logloss: 2.19181 | valid_accuracy: 0.39303 |  0:00:20s\n",
            "epoch 10 | loss: 0.36322 | train_logloss: 1.55805 | train_accuracy: 0.6197  | valid_logloss: 1.57423 | valid_accuracy: 0.61213 |  0:00:22s\n",
            "epoch 11 | loss: 0.34851 | train_logloss: 1.29082 | train_accuracy: 0.65933 | valid_logloss: 1.30636 | valid_accuracy: 0.64963 |  0:00:24s\n",
            "epoch 12 | loss: 0.34055 | train_logloss: 0.80833 | train_accuracy: 0.72433 | valid_logloss: 0.82234 | valid_accuracy: 0.7174  |  0:00:26s\n",
            "epoch 13 | loss: 0.34248 | train_logloss: 0.68002 | train_accuracy: 0.73533 | valid_logloss: 0.70005 | valid_accuracy: 0.72457 |  0:00:28s\n",
            "epoch 14 | loss: 0.34085 | train_logloss: 0.72517 | train_accuracy: 0.67807 | valid_logloss: 0.74323 | valid_accuracy: 0.6675  |  0:00:30s\n",
            "epoch 15 | loss: 0.34447 | train_logloss: 0.51176 | train_accuracy: 0.82777 | valid_logloss: 0.52315 | valid_accuracy: 0.82257 |  0:00:33s\n",
            "epoch 16 | loss: 0.34231 | train_logloss: 0.45725 | train_accuracy: 0.84887 | valid_logloss: 0.46839 | valid_accuracy: 0.84343 |  0:00:35s\n",
            "epoch 17 | loss: 0.33834 | train_logloss: 0.46659 | train_accuracy: 0.8301  | valid_logloss: 0.48304 | valid_accuracy: 0.8241  |  0:00:37s\n",
            "epoch 18 | loss: 0.35725 | train_logloss: 0.42766 | train_accuracy: 0.84513 | valid_logloss: 0.43672 | valid_accuracy: 0.84123 |  0:00:39s\n",
            "epoch 19 | loss: 0.35246 | train_logloss: 0.39425 | train_accuracy: 0.86027 | valid_logloss: 0.39828 | valid_accuracy: 0.85887 |  0:00:41s\n",
            "epoch 20 | loss: 0.34395 | train_logloss: 0.38684 | train_accuracy: 0.86043 | valid_logloss: 0.39406 | valid_accuracy: 0.85953 |  0:00:43s\n",
            "epoch 21 | loss: 0.33759 | train_logloss: 0.36076 | train_accuracy: 0.8675  | valid_logloss: 0.36989 | valid_accuracy: 0.86317 |  0:00:45s\n",
            "epoch 22 | loss: 0.33512 | train_logloss: 0.35134 | train_accuracy: 0.86887 | valid_logloss: 0.35969 | valid_accuracy: 0.86733 |  0:00:47s\n",
            "epoch 23 | loss: 0.34218 | train_logloss: 0.36469 | train_accuracy: 0.8695  | valid_logloss: 0.37168 | valid_accuracy: 0.86783 |  0:00:49s\n",
            "epoch 24 | loss: 0.3519  | train_logloss: 0.36259 | train_accuracy: 0.86407 | valid_logloss: 0.37139 | valid_accuracy: 0.86343 |  0:00:51s\n",
            "epoch 25 | loss: 0.34891 | train_logloss: 0.35114 | train_accuracy: 0.86933 | valid_logloss: 0.35559 | valid_accuracy: 0.8684  |  0:00:53s\n",
            "epoch 26 | loss: 0.34215 | train_logloss: 0.35973 | train_accuracy: 0.865   | valid_logloss: 0.37227 | valid_accuracy: 0.8621  |  0:00:55s\n",
            "epoch 27 | loss: 0.34256 | train_logloss: 0.3438  | train_accuracy: 0.87107 | valid_logloss: 0.35119 | valid_accuracy: 0.86873 |  0:00:57s\n",
            "epoch 28 | loss: 0.34533 | train_logloss: 0.36799 | train_accuracy: 0.86353 | valid_logloss: 0.37616 | valid_accuracy: 0.8605  |  0:01:00s\n",
            "epoch 29 | loss: 0.36121 | train_logloss: 0.34388 | train_accuracy: 0.87543 | valid_logloss: 0.35151 | valid_accuracy: 0.87493 |  0:01:02s\n",
            "epoch 30 | loss: 0.33927 | train_logloss: 0.33409 | train_accuracy: 0.87747 | valid_logloss: 0.34101 | valid_accuracy: 0.87517 |  0:01:04s\n",
            "epoch 31 | loss: 0.33517 | train_logloss: 0.33266 | train_accuracy: 0.88163 | valid_logloss: 0.34456 | valid_accuracy: 0.87607 |  0:01:06s\n",
            "epoch 32 | loss: 0.34362 | train_logloss: 0.34358 | train_accuracy: 0.8715  | valid_logloss: 0.35297 | valid_accuracy: 0.86797 |  0:01:08s\n",
            "epoch 33 | loss: 0.3433  | train_logloss: 0.33955 | train_accuracy: 0.87877 | valid_logloss: 0.34997 | valid_accuracy: 0.8753  |  0:01:10s\n",
            "epoch 34 | loss: 0.3366  | train_logloss: 0.33053 | train_accuracy: 0.87883 | valid_logloss: 0.33786 | valid_accuracy: 0.87727 |  0:01:12s\n",
            "epoch 35 | loss: 0.33683 | train_logloss: 0.33099 | train_accuracy: 0.87767 | valid_logloss: 0.34133 | valid_accuracy: 0.87407 |  0:01:14s\n",
            "epoch 36 | loss: 0.34076 | train_logloss: 0.32712 | train_accuracy: 0.87933 | valid_logloss: 0.33726 | valid_accuracy: 0.87787 |  0:01:16s\n",
            "epoch 37 | loss: 0.32797 | train_logloss: 0.32413 | train_accuracy: 0.8785  | valid_logloss: 0.33662 | valid_accuracy: 0.8764  |  0:01:18s\n",
            "epoch 38 | loss: 0.32809 | train_logloss: 0.32244 | train_accuracy: 0.8803  | valid_logloss: 0.3389  | valid_accuracy: 0.87553 |  0:01:20s\n",
            "epoch 39 | loss: 0.32762 | train_logloss: 0.32444 | train_accuracy: 0.8812  | valid_logloss: 0.33626 | valid_accuracy: 0.87687 |  0:01:22s\n",
            "epoch 40 | loss: 0.34185 | train_logloss: 0.34095 | train_accuracy: 0.87277 | valid_logloss: 0.35234 | valid_accuracy: 0.86997 |  0:01:24s\n",
            "epoch 41 | loss: 0.35633 | train_logloss: 0.34735 | train_accuracy: 0.872   | valid_logloss: 0.35387 | valid_accuracy: 0.8702  |  0:01:26s\n",
            "epoch 42 | loss: 0.34228 | train_logloss: 0.33079 | train_accuracy: 0.8766  | valid_logloss: 0.34312 | valid_accuracy: 0.87113 |  0:01:28s\n",
            "epoch 43 | loss: 0.32905 | train_logloss: 0.32495 | train_accuracy: 0.87627 | valid_logloss: 0.33472 | valid_accuracy: 0.8727  |  0:01:30s\n",
            "epoch 44 | loss: 0.3286  | train_logloss: 0.31979 | train_accuracy: 0.88053 | valid_logloss: 0.33144 | valid_accuracy: 0.876   |  0:01:32s\n",
            "epoch 45 | loss: 0.32693 | train_logloss: 0.32388 | train_accuracy: 0.8786  | valid_logloss: 0.33932 | valid_accuracy: 0.87483 |  0:01:35s\n",
            "epoch 46 | loss: 0.32193 | train_logloss: 0.31717 | train_accuracy: 0.88097 | valid_logloss: 0.32993 | valid_accuracy: 0.87787 |  0:01:37s\n",
            "epoch 47 | loss: 0.32442 | train_logloss: 0.32444 | train_accuracy: 0.87613 | valid_logloss: 0.33606 | valid_accuracy: 0.874   |  0:01:39s\n",
            "epoch 48 | loss: 0.3282  | train_logloss: 0.32098 | train_accuracy: 0.8752  | valid_logloss: 0.33554 | valid_accuracy: 0.871   |  0:01:41s\n",
            "epoch 49 | loss: 0.32453 | train_logloss: 0.31711 | train_accuracy: 0.8785  | valid_logloss: 0.33324 | valid_accuracy: 0.87627 |  0:01:43s\n",
            "epoch 50 | loss: 0.31908 | train_logloss: 0.31234 | train_accuracy: 0.8799  | valid_logloss: 0.32576 | valid_accuracy: 0.8777  |  0:01:45s\n",
            "epoch 51 | loss: 0.31604 | train_logloss: 0.31302 | train_accuracy: 0.88133 | valid_logloss: 0.32629 | valid_accuracy: 0.87877 |  0:01:47s\n",
            "epoch 52 | loss: 0.32229 | train_logloss: 0.31395 | train_accuracy: 0.88127 | valid_logloss: 0.3267  | valid_accuracy: 0.87903 |  0:01:49s\n",
            "epoch 53 | loss: 0.31961 | train_logloss: 0.31111 | train_accuracy: 0.88423 | valid_logloss: 0.32599 | valid_accuracy: 0.8801  |  0:01:51s\n",
            "epoch 54 | loss: 0.3145  | train_logloss: 0.31147 | train_accuracy: 0.88403 | valid_logloss: 0.32903 | valid_accuracy: 0.877   |  0:01:53s\n",
            "epoch 55 | loss: 0.31461 | train_logloss: 0.30768 | train_accuracy: 0.88393 | valid_logloss: 0.31881 | valid_accuracy: 0.8807  |  0:01:55s\n",
            "epoch 56 | loss: 0.31656 | train_logloss: 0.32146 | train_accuracy: 0.87877 | valid_logloss: 0.33124 | valid_accuracy: 0.8753  |  0:01:57s\n",
            "epoch 57 | loss: 0.32664 | train_logloss: 0.31797 | train_accuracy: 0.87977 | valid_logloss: 0.32615 | valid_accuracy: 0.8776  |  0:01:59s\n",
            "epoch 58 | loss: 0.32322 | train_logloss: 0.3208  | train_accuracy: 0.87887 | valid_logloss: 0.33096 | valid_accuracy: 0.87597 |  0:02:01s\n",
            "epoch 59 | loss: 0.32855 | train_logloss: 0.32804 | train_accuracy: 0.87747 | valid_logloss: 0.33659 | valid_accuracy: 0.87457 |  0:02:03s\n",
            "epoch 60 | loss: 0.32835 | train_logloss: 0.31865 | train_accuracy: 0.88077 | valid_logloss: 0.3292  | valid_accuracy: 0.87793 |  0:02:05s\n",
            "epoch 61 | loss: 0.32129 | train_logloss: 0.31561 | train_accuracy: 0.88087 | valid_logloss: 0.32613 | valid_accuracy: 0.8771  |  0:02:07s\n",
            "epoch 62 | loss: 0.31862 | train_logloss: 0.31075 | train_accuracy: 0.88273 | valid_logloss: 0.32311 | valid_accuracy: 0.8787  |  0:02:09s\n",
            "epoch 63 | loss: 0.31503 | train_logloss: 0.31473 | train_accuracy: 0.8819  | valid_logloss: 0.32724 | valid_accuracy: 0.87907 |  0:02:11s\n",
            "epoch 64 | loss: 0.31685 | train_logloss: 0.31    | train_accuracy: 0.88227 | valid_logloss: 0.32019 | valid_accuracy: 0.88133 |  0:02:14s\n",
            "epoch 65 | loss: 0.31343 | train_logloss: 0.30779 | train_accuracy: 0.8849  | valid_logloss: 0.32173 | valid_accuracy: 0.87937 |  0:02:16s\n",
            "epoch 66 | loss: 0.31405 | train_logloss: 0.30697 | train_accuracy: 0.8837  | valid_logloss: 0.31965 | valid_accuracy: 0.87907 |  0:02:18s\n",
            "epoch 67 | loss: 0.31355 | train_logloss: 0.3135  | train_accuracy: 0.88227 | valid_logloss: 0.32352 | valid_accuracy: 0.87923 |  0:02:20s\n",
            "epoch 68 | loss: 0.31332 | train_logloss: 0.31156 | train_accuracy: 0.88407 | valid_logloss: 0.32266 | valid_accuracy: 0.87943 |  0:02:22s\n",
            "epoch 69 | loss: 0.31096 | train_logloss: 0.31094 | train_accuracy: 0.8838  | valid_logloss: 0.32356 | valid_accuracy: 0.8782  |  0:02:24s\n",
            "epoch 70 | loss: 0.31265 | train_logloss: 0.3069  | train_accuracy: 0.88373 | valid_logloss: 0.3212  | valid_accuracy: 0.87783 |  0:02:26s\n",
            "epoch 71 | loss: 0.31366 | train_logloss: 0.30249 | train_accuracy: 0.88577 | valid_logloss: 0.31688 | valid_accuracy: 0.87973 |  0:02:28s\n",
            "epoch 72 | loss: 0.31303 | train_logloss: 0.31427 | train_accuracy: 0.8818  | valid_logloss: 0.32702 | valid_accuracy: 0.87737 |  0:02:30s\n",
            "epoch 73 | loss: 0.32341 | train_logloss: 0.31771 | train_accuracy: 0.881   | valid_logloss: 0.32966 | valid_accuracy: 0.87523 |  0:02:32s\n",
            "epoch 74 | loss: 0.32962 | train_logloss: 0.33352 | train_accuracy: 0.87667 | valid_logloss: 0.34604 | valid_accuracy: 0.87253 |  0:02:34s\n",
            "epoch 75 | loss: 0.32998 | train_logloss: 0.32173 | train_accuracy: 0.87877 | valid_logloss: 0.33326 | valid_accuracy: 0.8732  |  0:02:36s\n",
            "epoch 76 | loss: 0.3236  | train_logloss: 0.31717 | train_accuracy: 0.88    | valid_logloss: 0.32856 | valid_accuracy: 0.8765  |  0:02:38s\n",
            "epoch 77 | loss: 0.33271 | train_logloss: 0.34043 | train_accuracy: 0.87657 | valid_logloss: 0.35199 | valid_accuracy: 0.8731  |  0:02:40s\n",
            "epoch 78 | loss: 0.3427  | train_logloss: 0.33372 | train_accuracy: 0.8763  | valid_logloss: 0.34534 | valid_accuracy: 0.8732  |  0:02:43s\n",
            "epoch 79 | loss: 0.33126 | train_logloss: 0.31381 | train_accuracy: 0.88237 | valid_logloss: 0.32522 | valid_accuracy: 0.87897 |  0:02:44s\n",
            "epoch 80 | loss: 0.31744 | train_logloss: 0.31068 | train_accuracy: 0.88417 | valid_logloss: 0.32459 | valid_accuracy: 0.88077 |  0:02:47s\n",
            "epoch 81 | loss: 0.3139  | train_logloss: 0.31043 | train_accuracy: 0.88427 | valid_logloss: 0.32276 | valid_accuracy: 0.87967 |  0:02:49s\n",
            "epoch 82 | loss: 0.3199  | train_logloss: 0.31647 | train_accuracy: 0.88113 | valid_logloss: 0.3285  | valid_accuracy: 0.87633 |  0:02:51s\n",
            "epoch 83 | loss: 0.31797 | train_logloss: 0.30936 | train_accuracy: 0.88223 | valid_logloss: 0.32243 | valid_accuracy: 0.8784  |  0:02:53s\n",
            "epoch 84 | loss: 0.31878 | train_logloss: 0.31387 | train_accuracy: 0.8828  | valid_logloss: 0.32391 | valid_accuracy: 0.87903 |  0:02:55s\n",
            "epoch 85 | loss: 0.32373 | train_logloss: 0.3204  | train_accuracy: 0.8795  | valid_logloss: 0.32877 | valid_accuracy: 0.87727 |  0:02:57s\n",
            "epoch 86 | loss: 0.33002 | train_logloss: 0.32777 | train_accuracy: 0.87957 | valid_logloss: 0.33744 | valid_accuracy: 0.87573 |  0:02:59s\n",
            "epoch 87 | loss: 0.33446 | train_logloss: 0.32718 | train_accuracy: 0.87857 | valid_logloss: 0.33945 | valid_accuracy: 0.87383 |  0:03:01s\n",
            "epoch 88 | loss: 0.33076 | train_logloss: 0.32292 | train_accuracy: 0.87883 | valid_logloss: 0.3344  | valid_accuracy: 0.87383 |  0:03:03s\n",
            "epoch 89 | loss: 0.32566 | train_logloss: 0.31886 | train_accuracy: 0.88067 | valid_logloss: 0.33057 | valid_accuracy: 0.87673 |  0:03:05s\n",
            "epoch 90 | loss: 0.32037 | train_logloss: 0.31362 | train_accuracy: 0.88233 | valid_logloss: 0.32649 | valid_accuracy: 0.8771  |  0:03:07s\n",
            "epoch 91 | loss: 0.31782 | train_logloss: 0.32043 | train_accuracy: 0.8798  | valid_logloss: 0.33119 | valid_accuracy: 0.8767  |  0:03:09s\n",
            "epoch 92 | loss: 0.31989 | train_logloss: 0.31484 | train_accuracy: 0.8812  | valid_logloss: 0.32517 | valid_accuracy: 0.87847 |  0:03:11s\n",
            "epoch 93 | loss: 0.31729 | train_logloss: 0.31342 | train_accuracy: 0.8823  | valid_logloss: 0.32234 | valid_accuracy: 0.8801  |  0:03:13s\n",
            "epoch 94 | loss: 0.31465 | train_logloss: 0.31108 | train_accuracy: 0.88203 | valid_logloss: 0.32281 | valid_accuracy: 0.878   |  0:03:15s\n",
            "epoch 95 | loss: 0.31289 | train_logloss: 0.30927 | train_accuracy: 0.8822  | valid_logloss: 0.32037 | valid_accuracy: 0.87867 |  0:03:17s\n",
            "epoch 96 | loss: 0.31219 | train_logloss: 0.30358 | train_accuracy: 0.8839  | valid_logloss: 0.31727 | valid_accuracy: 0.87993 |  0:03:19s\n",
            "epoch 97 | loss: 0.30746 | train_logloss: 0.30433 | train_accuracy: 0.88213 | valid_logloss: 0.31905 | valid_accuracy: 0.87713 |  0:03:21s\n",
            "epoch 98 | loss: 0.30456 | train_logloss: 0.3032  | train_accuracy: 0.88477 | valid_logloss: 0.31881 | valid_accuracy: 0.879   |  0:03:23s\n",
            "epoch 99 | loss: 0.30512 | train_logloss: 0.30239 | train_accuracy: 0.8859  | valid_logloss: 0.31496 | valid_accuracy: 0.88197 |  0:03:26s\n",
            "epoch 100| loss: 0.31212 | train_logloss: 0.31154 | train_accuracy: 0.88217 | valid_logloss: 0.32238 | valid_accuracy: 0.87723 |  0:03:28s\n",
            "epoch 101| loss: 0.32149 | train_logloss: 0.31802 | train_accuracy: 0.87603 | valid_logloss: 0.32953 | valid_accuracy: 0.87497 |  0:03:30s\n",
            "epoch 102| loss: 0.31159 | train_logloss: 0.30701 | train_accuracy: 0.88403 | valid_logloss: 0.31891 | valid_accuracy: 0.87947 |  0:03:32s\n",
            "epoch 103| loss: 0.30967 | train_logloss: 0.30035 | train_accuracy: 0.88533 | valid_logloss: 0.31495 | valid_accuracy: 0.88117 |  0:03:34s\n",
            "epoch 104| loss: 0.30405 | train_logloss: 0.30187 | train_accuracy: 0.8838  | valid_logloss: 0.31651 | valid_accuracy: 0.8809  |  0:03:36s\n",
            "epoch 105| loss: 0.31033 | train_logloss: 0.30821 | train_accuracy: 0.8831  | valid_logloss: 0.32183 | valid_accuracy: 0.879   |  0:03:38s\n",
            "epoch 106| loss: 0.31065 | train_logloss: 0.303   | train_accuracy: 0.88563 | valid_logloss: 0.31792 | valid_accuracy: 0.88017 |  0:03:40s\n",
            "epoch 107| loss: 0.30964 | train_logloss: 0.31283 | train_accuracy: 0.88027 | valid_logloss: 0.32605 | valid_accuracy: 0.8759  |  0:03:42s\n",
            "epoch 108| loss: 0.31466 | train_logloss: 0.30244 | train_accuracy: 0.88567 | valid_logloss: 0.3159  | valid_accuracy: 0.8801  |  0:03:44s\n",
            "epoch 109| loss: 0.30606 | train_logloss: 0.29918 | train_accuracy: 0.88693 | valid_logloss: 0.31336 | valid_accuracy: 0.8821  |  0:03:46s\n",
            "epoch 110| loss: 0.30415 | train_logloss: 0.30122 | train_accuracy: 0.88527 | valid_logloss: 0.31537 | valid_accuracy: 0.8812  |  0:03:48s\n",
            "epoch 111| loss: 0.30095 | train_logloss: 0.2987  | train_accuracy: 0.88533 | valid_logloss: 0.31316 | valid_accuracy: 0.88177 |  0:03:50s\n",
            "epoch 112| loss: 0.29929 | train_logloss: 0.29423 | train_accuracy: 0.88703 | valid_logloss: 0.3117  | valid_accuracy: 0.88257 |  0:03:52s\n",
            "epoch 113| loss: 0.30092 | train_logloss: 0.29497 | train_accuracy: 0.8882  | valid_logloss: 0.31276 | valid_accuracy: 0.8822  |  0:03:54s\n",
            "epoch 114| loss: 0.29942 | train_logloss: 0.29578 | train_accuracy: 0.88713 | valid_logloss: 0.31653 | valid_accuracy: 0.88033 |  0:03:57s\n",
            "epoch 115| loss: 0.30065 | train_logloss: 0.29576 | train_accuracy: 0.8877  | valid_logloss: 0.31173 | valid_accuracy: 0.88093 |  0:03:59s\n",
            "epoch 116| loss: 0.30111 | train_logloss: 0.29796 | train_accuracy: 0.8887  | valid_logloss: 0.31389 | valid_accuracy: 0.88243 |  0:04:01s\n",
            "epoch 117| loss: 0.30056 | train_logloss: 0.29402 | train_accuracy: 0.88887 | valid_logloss: 0.31334 | valid_accuracy: 0.88173 |  0:04:03s\n",
            "epoch 118| loss: 0.29569 | train_logloss: 0.28837 | train_accuracy: 0.8885  | valid_logloss: 0.30772 | valid_accuracy: 0.88253 |  0:04:05s\n",
            "epoch 119| loss: 0.29388 | train_logloss: 0.28766 | train_accuracy: 0.88967 | valid_logloss: 0.30604 | valid_accuracy: 0.8848  |  0:04:07s\n",
            "epoch 120| loss: 0.29407 | train_logloss: 0.2884  | train_accuracy: 0.88957 | valid_logloss: 0.3082  | valid_accuracy: 0.88317 |  0:04:09s\n",
            "epoch 121| loss: 0.29531 | train_logloss: 0.29211 | train_accuracy: 0.8891  | valid_logloss: 0.31173 | valid_accuracy: 0.8822  |  0:04:11s\n",
            "epoch 122| loss: 0.30927 | train_logloss: 0.30187 | train_accuracy: 0.88583 | valid_logloss: 0.31628 | valid_accuracy: 0.88197 |  0:04:13s\n",
            "epoch 123| loss: 0.30353 | train_logloss: 0.29981 | train_accuracy: 0.8876  | valid_logloss: 0.31931 | valid_accuracy: 0.87987 |  0:04:15s\n",
            "epoch 124| loss: 0.29873 | train_logloss: 0.29689 | train_accuracy: 0.88813 | valid_logloss: 0.31601 | valid_accuracy: 0.88277 |  0:04:17s\n",
            "epoch 125| loss: 0.29466 | train_logloss: 0.29244 | train_accuracy: 0.88977 | valid_logloss: 0.31224 | valid_accuracy: 0.88203 |  0:04:19s\n",
            "epoch 126| loss: 0.29781 | train_logloss: 0.29654 | train_accuracy: 0.88803 | valid_logloss: 0.31549 | valid_accuracy: 0.883   |  0:04:21s\n",
            "epoch 127| loss: 0.29632 | train_logloss: 0.29182 | train_accuracy: 0.88757 | valid_logloss: 0.30938 | valid_accuracy: 0.88357 |  0:04:23s\n",
            "epoch 128| loss: 0.29459 | train_logloss: 0.28762 | train_accuracy: 0.8897  | valid_logloss: 0.30732 | valid_accuracy: 0.88383 |  0:04:25s\n",
            "epoch 129| loss: 0.29156 | train_logloss: 0.28898 | train_accuracy: 0.89083 | valid_logloss: 0.30926 | valid_accuracy: 0.88357 |  0:04:28s\n",
            "epoch 130| loss: 0.29255 | train_logloss: 0.28913 | train_accuracy: 0.88973 | valid_logloss: 0.30853 | valid_accuracy: 0.88417 |  0:04:30s\n",
            "epoch 131| loss: 0.29396 | train_logloss: 0.28667 | train_accuracy: 0.89197 | valid_logloss: 0.31039 | valid_accuracy: 0.88393 |  0:04:32s\n",
            "epoch 132| loss: 0.29145 | train_logloss: 0.28511 | train_accuracy: 0.8912  | valid_logloss: 0.30672 | valid_accuracy: 0.88427 |  0:04:34s\n",
            "epoch 133| loss: 0.28863 | train_logloss: 0.28964 | train_accuracy: 0.89087 | valid_logloss: 0.31296 | valid_accuracy: 0.88177 |  0:04:36s\n",
            "epoch 134| loss: 0.29042 | train_logloss: 0.28386 | train_accuracy: 0.89303 | valid_logloss: 0.30937 | valid_accuracy: 0.8836  |  0:04:38s\n",
            "epoch 135| loss: 0.28717 | train_logloss: 0.28068 | train_accuracy: 0.89363 | valid_logloss: 0.30652 | valid_accuracy: 0.8842  |  0:04:40s\n",
            "epoch 136| loss: 0.28691 | train_logloss: 0.28247 | train_accuracy: 0.89413 | valid_logloss: 0.30788 | valid_accuracy: 0.8852  |  0:04:42s\n",
            "epoch 137| loss: 0.28415 | train_logloss: 0.28088 | train_accuracy: 0.89237 | valid_logloss: 0.30763 | valid_accuracy: 0.88543 |  0:04:44s\n",
            "epoch 138| loss: 0.28215 | train_logloss: 0.27865 | train_accuracy: 0.8938  | valid_logloss: 0.30529 | valid_accuracy: 0.8855  |  0:04:46s\n",
            "epoch 139| loss: 0.28392 | train_logloss: 0.27684 | train_accuracy: 0.89463 | valid_logloss: 0.30098 | valid_accuracy: 0.88803 |  0:04:48s\n",
            "epoch 140| loss: 0.2822  | train_logloss: 0.27544 | train_accuracy: 0.89387 | valid_logloss: 0.30164 | valid_accuracy: 0.88643 |  0:04:50s\n",
            "epoch 141| loss: 0.28914 | train_logloss: 0.29368 | train_accuracy: 0.88843 | valid_logloss: 0.31593 | valid_accuracy: 0.88163 |  0:04:52s\n",
            "epoch 142| loss: 0.29541 | train_logloss: 0.2859  | train_accuracy: 0.8898  | valid_logloss: 0.31039 | valid_accuracy: 0.88347 |  0:04:54s\n",
            "epoch 143| loss: 0.29046 | train_logloss: 0.28351 | train_accuracy: 0.89093 | valid_logloss: 0.30614 | valid_accuracy: 0.88293 |  0:04:56s\n",
            "epoch 144| loss: 0.28924 | train_logloss: 0.28657 | train_accuracy: 0.8914  | valid_logloss: 0.30924 | valid_accuracy: 0.88287 |  0:04:58s\n",
            "epoch 145| loss: 0.28946 | train_logloss: 0.2831  | train_accuracy: 0.89137 | valid_logloss: 0.30756 | valid_accuracy: 0.88297 |  0:05:00s\n",
            "epoch 146| loss: 0.28753 | train_logloss: 0.28952 | train_accuracy: 0.8919  | valid_logloss: 0.31509 | valid_accuracy: 0.88383 |  0:05:02s\n",
            "epoch 147| loss: 0.29262 | train_logloss: 0.28648 | train_accuracy: 0.89117 | valid_logloss: 0.31054 | valid_accuracy: 0.88373 |  0:05:04s\n",
            "epoch 148| loss: 0.29029 | train_logloss: 0.28447 | train_accuracy: 0.8944  | valid_logloss: 0.30553 | valid_accuracy: 0.8857  |  0:05:07s\n",
            "epoch 149| loss: 0.28738 | train_logloss: 0.28304 | train_accuracy: 0.89233 | valid_logloss: 0.30737 | valid_accuracy: 0.8839  |  0:05:09s\n",
            "epoch 150| loss: 0.28433 | train_logloss: 0.27925 | train_accuracy: 0.89407 | valid_logloss: 0.30452 | valid_accuracy: 0.8863  |  0:05:11s\n",
            "epoch 151| loss: 0.2908  | train_logloss: 0.28677 | train_accuracy: 0.89123 | valid_logloss: 0.31256 | valid_accuracy: 0.8835  |  0:05:13s\n",
            "epoch 152| loss: 0.29129 | train_logloss: 0.28199 | train_accuracy: 0.8926  | valid_logloss: 0.30909 | valid_accuracy: 0.88517 |  0:05:15s\n",
            "epoch 153| loss: 0.28435 | train_logloss: 0.27824 | train_accuracy: 0.89377 | valid_logloss: 0.30533 | valid_accuracy: 0.8863  |  0:05:17s\n",
            "epoch 154| loss: 0.28017 | train_logloss: 0.27614 | train_accuracy: 0.89507 | valid_logloss: 0.30269 | valid_accuracy: 0.88707 |  0:05:19s\n",
            "epoch 155| loss: 0.27931 | train_logloss: 0.27518 | train_accuracy: 0.89387 | valid_logloss: 0.30405 | valid_accuracy: 0.88527 |  0:05:21s\n",
            "epoch 156| loss: 0.27666 | train_logloss: 0.27433 | train_accuracy: 0.89593 | valid_logloss: 0.30425 | valid_accuracy: 0.88653 |  0:05:23s\n",
            "epoch 157| loss: 0.2769  | train_logloss: 0.27012 | train_accuracy: 0.89653 | valid_logloss: 0.30226 | valid_accuracy: 0.8875  |  0:05:25s\n",
            "epoch 158| loss: 0.27647 | train_logloss: 0.27281 | train_accuracy: 0.89533 | valid_logloss: 0.30258 | valid_accuracy: 0.88767 |  0:05:27s\n",
            "epoch 159| loss: 0.27477 | train_logloss: 0.26919 | train_accuracy: 0.89843 | valid_logloss: 0.30176 | valid_accuracy: 0.8881  |  0:05:29s\n",
            "epoch 160| loss: 0.27598 | train_logloss: 0.26701 | train_accuracy: 0.898   | valid_logloss: 0.30157 | valid_accuracy: 0.88857 |  0:05:32s\n",
            "epoch 161| loss: 0.27229 | train_logloss: 0.26717 | train_accuracy: 0.89817 | valid_logloss: 0.30268 | valid_accuracy: 0.88733 |  0:05:34s\n",
            "epoch 162| loss: 0.27573 | train_logloss: 0.27116 | train_accuracy: 0.89597 | valid_logloss: 0.30337 | valid_accuracy: 0.88677 |  0:05:36s\n",
            "epoch 163| loss: 0.27437 | train_logloss: 0.26764 | train_accuracy: 0.899   | valid_logloss: 0.30291 | valid_accuracy: 0.88823 |  0:05:38s\n",
            "epoch 164| loss: 0.2716  | train_logloss: 0.26498 | train_accuracy: 0.89957 | valid_logloss: 0.30188 | valid_accuracy: 0.8885  |  0:05:40s\n",
            "epoch 165| loss: 0.27016 | train_logloss: 0.26847 | train_accuracy: 0.89867 | valid_logloss: 0.30501 | valid_accuracy: 0.88713 |  0:05:42s\n",
            "epoch 166| loss: 0.26953 | train_logloss: 0.26583 | train_accuracy: 0.89733 | valid_logloss: 0.30109 | valid_accuracy: 0.88797 |  0:05:44s\n",
            "epoch 167| loss: 0.26969 | train_logloss: 0.26011 | train_accuracy: 0.90237 | valid_logloss: 0.29704 | valid_accuracy: 0.89107 |  0:05:46s\n",
            "epoch 168| loss: 0.26827 | train_logloss: 0.26198 | train_accuracy: 0.90113 | valid_logloss: 0.30164 | valid_accuracy: 0.88853 |  0:05:48s\n",
            "epoch 169| loss: 0.26687 | train_logloss: 0.26429 | train_accuracy: 0.90027 | valid_logloss: 0.30233 | valid_accuracy: 0.8887  |  0:05:50s\n",
            "epoch 170| loss: 0.26741 | train_logloss: 0.26076 | train_accuracy: 0.9014  | valid_logloss: 0.3026  | valid_accuracy: 0.88963 |  0:05:52s\n",
            "epoch 171| loss: 0.2659  | train_logloss: 0.26064 | train_accuracy: 0.90147 | valid_logloss: 0.29959 | valid_accuracy: 0.8896  |  0:05:54s\n",
            "epoch 172| loss: 0.26744 | train_logloss: 0.25977 | train_accuracy: 0.9018  | valid_logloss: 0.29886 | valid_accuracy: 0.8893  |  0:05:56s\n",
            "epoch 173| loss: 0.26636 | train_logloss: 0.25989 | train_accuracy: 0.9018  | valid_logloss: 0.30194 | valid_accuracy: 0.88743 |  0:05:58s\n",
            "epoch 174| loss: 0.26283 | train_logloss: 0.25476 | train_accuracy: 0.90447 | valid_logloss: 0.29899 | valid_accuracy: 0.8906  |  0:06:00s\n",
            "epoch 175| loss: 0.26369 | train_logloss: 0.2551  | train_accuracy: 0.9052  | valid_logloss: 0.30029 | valid_accuracy: 0.8901  |  0:06:02s\n",
            "epoch 176| loss: 0.26274 | train_logloss: 0.25822 | train_accuracy: 0.90203 | valid_logloss: 0.29899 | valid_accuracy: 0.88877 |  0:06:05s\n",
            "epoch 177| loss: 0.26513 | train_logloss: 0.26008 | train_accuracy: 0.9031  | valid_logloss: 0.30265 | valid_accuracy: 0.8882  |  0:06:07s\n",
            "epoch 178| loss: 0.26478 | train_logloss: 0.26038 | train_accuracy: 0.90323 | valid_logloss: 0.30194 | valid_accuracy: 0.88917 |  0:06:09s\n",
            "epoch 179| loss: 0.26233 | train_logloss: 0.25535 | train_accuracy: 0.90467 | valid_logloss: 0.30075 | valid_accuracy: 0.89063 |  0:06:11s\n",
            "epoch 180| loss: 0.26146 | train_logloss: 0.25704 | train_accuracy: 0.90403 | valid_logloss: 0.30462 | valid_accuracy: 0.88947 |  0:06:13s\n",
            "epoch 181| loss: 0.2616  | train_logloss: 0.26182 | train_accuracy: 0.90157 | valid_logloss: 0.30638 | valid_accuracy: 0.88557 |  0:06:15s\n",
            "epoch 182| loss: 0.26219 | train_logloss: 0.252   | train_accuracy: 0.9053  | valid_logloss: 0.30274 | valid_accuracy: 0.89017 |  0:06:17s\n",
            "epoch 183| loss: 0.27666 | train_logloss: 0.30844 | train_accuracy: 0.88407 | valid_logloss: 0.34002 | valid_accuracy: 0.8767  |  0:06:19s\n",
            "epoch 184| loss: 0.30765 | train_logloss: 0.29456 | train_accuracy: 0.88777 | valid_logloss: 0.31771 | valid_accuracy: 0.88033 |  0:06:21s\n",
            "epoch 185| loss: 0.29226 | train_logloss: 0.28377 | train_accuracy: 0.89133 | valid_logloss: 0.30915 | valid_accuracy: 0.88397 |  0:06:23s\n",
            "epoch 186| loss: 0.28503 | train_logloss: 0.27422 | train_accuracy: 0.89643 | valid_logloss: 0.30348 | valid_accuracy: 0.88897 |  0:06:25s\n",
            "epoch 187| loss: 0.27641 | train_logloss: 0.27179 | train_accuracy: 0.8977  | valid_logloss: 0.30572 | valid_accuracy: 0.88843 |  0:06:27s\n",
            "epoch 188| loss: 0.27369 | train_logloss: 0.26301 | train_accuracy: 0.90023 | valid_logloss: 0.29863 | valid_accuracy: 0.88917 |  0:06:29s\n",
            "epoch 189| loss: 0.26651 | train_logloss: 0.26152 | train_accuracy: 0.90073 | valid_logloss: 0.30181 | valid_accuracy: 0.88933 |  0:06:31s\n",
            "epoch 190| loss: 0.26585 | train_logloss: 0.25826 | train_accuracy: 0.90273 | valid_logloss: 0.30055 | valid_accuracy: 0.89027 |  0:06:33s\n",
            "epoch 191| loss: 0.27209 | train_logloss: 0.28463 | train_accuracy: 0.8937  | valid_logloss: 0.3148  | valid_accuracy: 0.88483 |  0:06:35s\n",
            "epoch 192| loss: 0.28767 | train_logloss: 0.27673 | train_accuracy: 0.8963  | valid_logloss: 0.31193 | valid_accuracy: 0.88607 |  0:06:37s\n",
            "epoch 193| loss: 0.27828 | train_logloss: 0.26922 | train_accuracy: 0.898   | valid_logloss: 0.30295 | valid_accuracy: 0.88647 |  0:06:39s\n",
            "epoch 194| loss: 0.27193 | train_logloss: 0.26228 | train_accuracy: 0.90127 | valid_logloss: 0.30069 | valid_accuracy: 0.88937 |  0:06:41s\n",
            "epoch 195| loss: 0.26912 | train_logloss: 0.26066 | train_accuracy: 0.90207 | valid_logloss: 0.29977 | valid_accuracy: 0.89037 |  0:06:43s\n",
            "epoch 196| loss: 0.26561 | train_logloss: 0.25947 | train_accuracy: 0.90263 | valid_logloss: 0.30158 | valid_accuracy: 0.88917 |  0:06:46s\n",
            "epoch 197| loss: 0.265   | train_logloss: 0.25861 | train_accuracy: 0.90273 | valid_logloss: 0.30127 | valid_accuracy: 0.89067 |  0:06:48s\n",
            "epoch 198| loss: 0.26427 | train_logloss: 0.25604 | train_accuracy: 0.9038  | valid_logloss: 0.29893 | valid_accuracy: 0.89007 |  0:06:50s\n",
            "epoch 199| loss: 0.26163 | train_logloss: 0.25374 | train_accuracy: 0.90443 | valid_logloss: 0.30155 | valid_accuracy: 0.8899  |  0:06:52s\n",
            "epoch 200| loss: 0.26161 | train_logloss: 0.25181 | train_accuracy: 0.90487 | valid_logloss: 0.29706 | valid_accuracy: 0.8913  |  0:06:54s\n",
            "epoch 201| loss: 0.26007 | train_logloss: 0.25082 | train_accuracy: 0.9052  | valid_logloss: 0.30228 | valid_accuracy: 0.8894  |  0:06:56s\n",
            "epoch 202| loss: 0.25653 | train_logloss: 0.25246 | train_accuracy: 0.90593 | valid_logloss: 0.30464 | valid_accuracy: 0.8907  |  0:06:58s\n",
            "epoch 203| loss: 0.25591 | train_logloss: 0.24445 | train_accuracy: 0.9072  | valid_logloss: 0.29953 | valid_accuracy: 0.8908  |  0:07:00s\n",
            "epoch 204| loss: 0.25178 | train_logloss: 0.24894 | train_accuracy: 0.90617 | valid_logloss: 0.30532 | valid_accuracy: 0.88927 |  0:07:02s\n",
            "epoch 205| loss: 0.25368 | train_logloss: 0.2496  | train_accuracy: 0.90707 | valid_logloss: 0.30698 | valid_accuracy: 0.88853 |  0:07:04s\n",
            "epoch 206| loss: 0.2521  | train_logloss: 0.2434  | train_accuracy: 0.9087  | valid_logloss: 0.30119 | valid_accuracy: 0.89073 |  0:07:06s\n",
            "epoch 207| loss: 0.25319 | train_logloss: 0.245   | train_accuracy: 0.90823 | valid_logloss: 0.30566 | valid_accuracy: 0.8909  |  0:07:08s\n",
            "epoch 208| loss: 0.25055 | train_logloss: 0.24752 | train_accuracy: 0.90747 | valid_logloss: 0.30616 | valid_accuracy: 0.88953 |  0:07:10s\n",
            "epoch 209| loss: 0.25102 | train_logloss: 0.24007 | train_accuracy: 0.91117 | valid_logloss: 0.30053 | valid_accuracy: 0.8899  |  0:07:12s\n",
            "epoch 210| loss: 0.24808 | train_logloss: 0.24337 | train_accuracy: 0.909   | valid_logloss: 0.3017  | valid_accuracy: 0.8891  |  0:07:15s\n",
            "epoch 211| loss: 0.24716 | train_logloss: 0.24139 | train_accuracy: 0.90783 | valid_logloss: 0.30538 | valid_accuracy: 0.8895  |  0:07:17s\n",
            "epoch 212| loss: 0.24684 | train_logloss: 0.24121 | train_accuracy: 0.909   | valid_logloss: 0.30431 | valid_accuracy: 0.88933 |  0:07:19s\n",
            "epoch 213| loss: 0.24634 | train_logloss: 0.23975 | train_accuracy: 0.90917 | valid_logloss: 0.30698 | valid_accuracy: 0.8884  |  0:07:21s\n",
            "epoch 214| loss: 0.24703 | train_logloss: 0.2434  | train_accuracy: 0.90733 | valid_logloss: 0.31063 | valid_accuracy: 0.88863 |  0:07:23s\n",
            "epoch 215| loss: 0.24798 | train_logloss: 0.23699 | train_accuracy: 0.91133 | valid_logloss: 0.30434 | valid_accuracy: 0.8901  |  0:07:25s\n",
            "epoch 216| loss: 0.24451 | train_logloss: 0.23564 | train_accuracy: 0.91103 | valid_logloss: 0.3044  | valid_accuracy: 0.88953 |  0:07:27s\n",
            "epoch 217| loss: 0.24439 | train_logloss: 0.23404 | train_accuracy: 0.91103 | valid_logloss: 0.3004  | valid_accuracy: 0.89107 |  0:07:29s\n",
            "epoch 218| loss: 0.24032 | train_logloss: 0.23157 | train_accuracy: 0.91177 | valid_logloss: 0.30863 | valid_accuracy: 0.89013 |  0:07:31s\n",
            "epoch 219| loss: 0.242   | train_logloss: 0.23787 | train_accuracy: 0.9085  | valid_logloss: 0.31083 | valid_accuracy: 0.88777 |  0:07:33s\n",
            "epoch 220| loss: 0.24521 | train_logloss: 0.2355  | train_accuracy: 0.90903 | valid_logloss: 0.3091  | valid_accuracy: 0.8886  |  0:07:35s\n",
            "epoch 221| loss: 0.24445 | train_logloss: 0.23723 | train_accuracy: 0.90907 | valid_logloss: 0.31596 | valid_accuracy: 0.88857 |  0:07:37s\n",
            "epoch 222| loss: 0.24098 | train_logloss: 0.24139 | train_accuracy: 0.9073  | valid_logloss: 0.31632 | valid_accuracy: 0.88757 |  0:07:39s\n",
            "epoch 223| loss: 0.24318 | train_logloss: 0.23054 | train_accuracy: 0.91257 | valid_logloss: 0.30857 | valid_accuracy: 0.8907  |  0:07:41s\n",
            "epoch 224| loss: 0.23964 | train_logloss: 0.22938 | train_accuracy: 0.91317 | valid_logloss: 0.30751 | valid_accuracy: 0.88897 |  0:07:43s\n",
            "epoch 225| loss: 0.2382  | train_logloss: 0.23651 | train_accuracy: 0.91177 | valid_logloss: 0.31613 | valid_accuracy: 0.88753 |  0:07:45s\n",
            "epoch 226| loss: 0.23954 | train_logloss: 0.22876 | train_accuracy: 0.9148  | valid_logloss: 0.31084 | valid_accuracy: 0.88987 |  0:07:47s\n",
            "epoch 227| loss: 0.23812 | train_logloss: 0.22845 | train_accuracy: 0.9116  | valid_logloss: 0.31793 | valid_accuracy: 0.8879  |  0:07:50s\n",
            "epoch 228| loss: 0.23639 | train_logloss: 0.22688 | train_accuracy: 0.91373 | valid_logloss: 0.31393 | valid_accuracy: 0.88647 |  0:07:52s\n",
            "epoch 229| loss: 0.23541 | train_logloss: 0.22589 | train_accuracy: 0.91377 | valid_logloss: 0.31341 | valid_accuracy: 0.88883 |  0:07:54s\n",
            "epoch 230| loss: 0.2365  | train_logloss: 0.22735 | train_accuracy: 0.9113  | valid_logloss: 0.31687 | valid_accuracy: 0.88763 |  0:07:56s\n",
            "epoch 231| loss: 0.23799 | train_logloss: 0.22582 | train_accuracy: 0.9143  | valid_logloss: 0.31497 | valid_accuracy: 0.8886  |  0:07:58s\n",
            "epoch 232| loss: 0.23353 | train_logloss: 0.22255 | train_accuracy: 0.9144  | valid_logloss: 0.31526 | valid_accuracy: 0.8876  |  0:08:00s\n",
            "epoch 233| loss: 0.23443 | train_logloss: 0.22177 | train_accuracy: 0.91577 | valid_logloss: 0.3129  | valid_accuracy: 0.88917 |  0:08:02s\n",
            "epoch 234| loss: 0.2322  | train_logloss: 0.22343 | train_accuracy: 0.915   | valid_logloss: 0.31582 | valid_accuracy: 0.889   |  0:08:04s\n",
            "epoch 235| loss: 0.23174 | train_logloss: 0.2231  | train_accuracy: 0.9148  | valid_logloss: 0.31758 | valid_accuracy: 0.887   |  0:08:06s\n",
            "epoch 236| loss: 0.22769 | train_logloss: 0.22173 | train_accuracy: 0.91513 | valid_logloss: 0.32345 | valid_accuracy: 0.88743 |  0:08:08s\n",
            "epoch 237| loss: 0.22685 | train_logloss: 0.22053 | train_accuracy: 0.91653 | valid_logloss: 0.32062 | valid_accuracy: 0.89047 |  0:08:10s\n",
            "epoch 238| loss: 0.22757 | train_logloss: 0.21671 | train_accuracy: 0.91673 | valid_logloss: 0.31976 | valid_accuracy: 0.88747 |  0:08:12s\n",
            "epoch 239| loss: 0.22697 | train_logloss: 0.21677 | train_accuracy: 0.9171  | valid_logloss: 0.32312 | valid_accuracy: 0.88903 |  0:08:14s\n",
            "epoch 240| loss: 0.22587 | train_logloss: 0.21358 | train_accuracy: 0.9194  | valid_logloss: 0.32609 | valid_accuracy: 0.88677 |  0:08:16s\n",
            "epoch 241| loss: 0.22498 | train_logloss: 0.21721 | train_accuracy: 0.91717 | valid_logloss: 0.33305 | valid_accuracy: 0.88587 |  0:08:18s\n",
            "epoch 242| loss: 0.22517 | train_logloss: 0.21639 | train_accuracy: 0.91803 | valid_logloss: 0.32804 | valid_accuracy: 0.88563 |  0:08:20s\n",
            "epoch 243| loss: 0.22546 | train_logloss: 0.21918 | train_accuracy: 0.9153  | valid_logloss: 0.33122 | valid_accuracy: 0.88677 |  0:08:23s\n",
            "epoch 244| loss: 0.22667 | train_logloss: 0.21625 | train_accuracy: 0.91697 | valid_logloss: 0.33041 | valid_accuracy: 0.88457 |  0:08:25s\n",
            "epoch 245| loss: 0.22324 | train_logloss: 0.21157 | train_accuracy: 0.91907 | valid_logloss: 0.32533 | valid_accuracy: 0.8878  |  0:08:27s\n",
            "epoch 246| loss: 0.22569 | train_logloss: 0.21645 | train_accuracy: 0.9172  | valid_logloss: 0.33567 | valid_accuracy: 0.88523 |  0:08:29s\n",
            "epoch 247| loss: 0.2214  | train_logloss: 0.21341 | train_accuracy: 0.91843 | valid_logloss: 0.32473 | valid_accuracy: 0.8863  |  0:08:31s\n",
            "epoch 248| loss: 0.22347 | train_logloss: 0.22452 | train_accuracy: 0.91467 | valid_logloss: 0.34226 | valid_accuracy: 0.88673 |  0:08:33s\n",
            "epoch 249| loss: 0.22538 | train_logloss: 0.21228 | train_accuracy: 0.91983 | valid_logloss: 0.33401 | valid_accuracy: 0.88647 |  0:08:35s\n",
            "epoch 250| loss: 0.2236  | train_logloss: 0.21141 | train_accuracy: 0.91903 | valid_logloss: 0.3431  | valid_accuracy: 0.88543 |  0:08:37s\n",
            "\n",
            "Early stopping occurred at epoch 250 with best_epoch = 200 and best_valid_accuracy = 0.8913\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[420029  13936  46217]\n",
            " [ 16883 953045  30072]\n",
            " [ 36963  29436 366188]]\n",
            "Testing Score:  0.8913\n",
            "Confusion Matrix: \n",
            " [[419412  12587  48183]\n",
            " [ 13732 954252  32016]\n",
            " [ 35048  28128 369411]]\n",
            "Testing Score:  0.8917666666666667\n",
            "{'Rows': 30000, 'Nd': 64, 'Na': 64, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 518.1420526504517, 'time_tn': 30.07811665534973, 'accuracy_tn': 0.8913, 'time_learn_gb': 8.244917631149292, 'time_gb': 28.10221552848816, 'accuracy_gb': 0.8917666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iTUqIHhWcYu",
        "outputId": "c972c6c2-0615-4bc6-bd74-df4aa6746c01"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1   9000   9000   9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2   9000   9000   9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3   9000   9000   9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4   9000   9000   9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "5  30000  30000  30000    8  ...  20.273622    0.891767      8.276289  27.093480\n",
              "6  30000  30000  30000   16  ...  22.630230    0.891767      8.210687  27.343578\n",
              "7  30000  30000  30000   64  ...  30.078117    0.891767      8.244918  28.102216\n",
              "\n",
              "[8 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ6c2Ct4BLAd",
        "outputId": "480cac9b-7405-4612-d4d6-b124ecba6650"
      },
      "source": [
        "time_model(number_exp=9, \n",
        "     Rows=30000, \n",
        "     Nd=32,\tNa=32,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=3, decision=3, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.78322 | train_logloss: 17.05534| train_accuracy: 0.3178  | valid_logloss: 17.05851| valid_accuracy: 0.3176  |  0:00:01s\n",
            "epoch 1  | loss: 0.44694 | train_logloss: 7.00825 | train_accuracy: 0.31507 | valid_logloss: 6.94097 | valid_accuracy: 0.31543 |  0:00:03s\n",
            "epoch 2  | loss: 0.3768  | train_logloss: 9.24496 | train_accuracy: 0.37033 | valid_logloss: 9.28357 | valid_accuracy: 0.3719  |  0:00:05s\n",
            "epoch 3  | loss: 0.35185 | train_logloss: 3.53121 | train_accuracy: 0.2996  | valid_logloss: 3.53458 | valid_accuracy: 0.3011  |  0:00:07s\n",
            "epoch 4  | loss: 0.3381  | train_logloss: 2.33125 | train_accuracy: 0.3094  | valid_logloss: 2.33064 | valid_accuracy: 0.3104  |  0:00:09s\n",
            "epoch 5  | loss: 0.32697 | train_logloss: 2.58933 | train_accuracy: 0.3338  | valid_logloss: 2.58154 | valid_accuracy: 0.33413 |  0:00:11s\n",
            "epoch 6  | loss: 0.32382 | train_logloss: 2.30059 | train_accuracy: 0.35147 | valid_logloss: 2.29008 | valid_accuracy: 0.35547 |  0:00:13s\n",
            "epoch 7  | loss: 0.32268 | train_logloss: 1.92177 | train_accuracy: 0.4181  | valid_logloss: 1.9175  | valid_accuracy: 0.42167 |  0:00:14s\n",
            "epoch 8  | loss: 0.31352 | train_logloss: 2.54485 | train_accuracy: 0.3583  | valid_logloss: 2.55033 | valid_accuracy: 0.3643  |  0:00:16s\n",
            "epoch 9  | loss: 0.3062  | train_logloss: 2.02991 | train_accuracy: 0.3441  | valid_logloss: 2.04914 | valid_accuracy: 0.34787 |  0:00:18s\n",
            "epoch 10 | loss: 0.30392 | train_logloss: 1.71316 | train_accuracy: 0.3048  | valid_logloss: 1.73129 | valid_accuracy: 0.30303 |  0:00:20s\n",
            "epoch 11 | loss: 0.30374 | train_logloss: 1.28921 | train_accuracy: 0.46373 | valid_logloss: 1.30434 | valid_accuracy: 0.4568  |  0:00:22s\n",
            "epoch 12 | loss: 0.29947 | train_logloss: 1.26145 | train_accuracy: 0.42843 | valid_logloss: 1.28387 | valid_accuracy: 0.4234  |  0:00:24s\n",
            "epoch 13 | loss: 0.29332 | train_logloss: 1.10834 | train_accuracy: 0.5528  | valid_logloss: 1.13241 | valid_accuracy: 0.5421  |  0:00:26s\n",
            "epoch 14 | loss: 0.29398 | train_logloss: 0.84805 | train_accuracy: 0.65793 | valid_logloss: 0.86066 | valid_accuracy: 0.65167 |  0:00:28s\n",
            "epoch 15 | loss: 0.29456 | train_logloss: 0.72295 | train_accuracy: 0.71297 | valid_logloss: 0.73455 | valid_accuracy: 0.7067  |  0:00:29s\n",
            "epoch 16 | loss: 0.29685 | train_logloss: 0.62046 | train_accuracy: 0.72763 | valid_logloss: 0.63022 | valid_accuracy: 0.72333 |  0:00:31s\n",
            "epoch 17 | loss: 0.29526 | train_logloss: 0.57751 | train_accuracy: 0.7741  | valid_logloss: 0.58725 | valid_accuracy: 0.76887 |  0:00:33s\n",
            "epoch 18 | loss: 0.28852 | train_logloss: 0.41095 | train_accuracy: 0.84827 | valid_logloss: 0.42145 | valid_accuracy: 0.84323 |  0:00:35s\n",
            "epoch 19 | loss: 0.28674 | train_logloss: 0.41568 | train_accuracy: 0.84567 | valid_logloss: 0.42191 | valid_accuracy: 0.84363 |  0:00:37s\n",
            "epoch 20 | loss: 0.28707 | train_logloss: 0.37984 | train_accuracy: 0.8529  | valid_logloss: 0.3913  | valid_accuracy: 0.85097 |  0:00:39s\n",
            "epoch 21 | loss: 0.28409 | train_logloss: 0.34077 | train_accuracy: 0.87083 | valid_logloss: 0.35391 | valid_accuracy: 0.86733 |  0:00:41s\n",
            "epoch 22 | loss: 0.28037 | train_logloss: 0.33809 | train_accuracy: 0.8693  | valid_logloss: 0.3527  | valid_accuracy: 0.86593 |  0:00:42s\n",
            "epoch 23 | loss: 0.28235 | train_logloss: 0.32372 | train_accuracy: 0.8714  | valid_logloss: 0.34123 | valid_accuracy: 0.86763 |  0:00:44s\n",
            "epoch 24 | loss: 0.27802 | train_logloss: 0.30509 | train_accuracy: 0.88583 | valid_logloss: 0.32594 | valid_accuracy: 0.88117 |  0:00:46s\n",
            "epoch 25 | loss: 0.27912 | train_logloss: 0.31057 | train_accuracy: 0.88003 | valid_logloss: 0.33003 | valid_accuracy: 0.87803 |  0:00:48s\n",
            "epoch 26 | loss: 0.27831 | train_logloss: 0.29203 | train_accuracy: 0.88747 | valid_logloss: 0.31307 | valid_accuracy: 0.8807  |  0:00:50s\n",
            "epoch 27 | loss: 0.28351 | train_logloss: 0.28526 | train_accuracy: 0.89097 | valid_logloss: 0.3055  | valid_accuracy: 0.8858  |  0:00:52s\n",
            "epoch 28 | loss: 0.30018 | train_logloss: 0.30239 | train_accuracy: 0.88473 | valid_logloss: 0.31376 | valid_accuracy: 0.88293 |  0:00:54s\n",
            "epoch 29 | loss: 0.2933  | train_logloss: 0.2918  | train_accuracy: 0.88947 | valid_logloss: 0.30556 | valid_accuracy: 0.88413 |  0:00:56s\n",
            "epoch 30 | loss: 0.28963 | train_logloss: 0.29099 | train_accuracy: 0.88933 | valid_logloss: 0.30724 | valid_accuracy: 0.88453 |  0:00:57s\n",
            "epoch 31 | loss: 0.28942 | train_logloss: 0.28432 | train_accuracy: 0.89163 | valid_logloss: 0.30337 | valid_accuracy: 0.886   |  0:00:59s\n",
            "epoch 32 | loss: 0.28165 | train_logloss: 0.28023 | train_accuracy: 0.89353 | valid_logloss: 0.30118 | valid_accuracy: 0.88763 |  0:01:01s\n",
            "epoch 33 | loss: 0.27944 | train_logloss: 0.27597 | train_accuracy: 0.895   | valid_logloss: 0.29995 | valid_accuracy: 0.88587 |  0:01:03s\n",
            "epoch 34 | loss: 0.27629 | train_logloss: 0.2677  | train_accuracy: 0.8968  | valid_logloss: 0.29621 | valid_accuracy: 0.8867  |  0:01:05s\n",
            "epoch 35 | loss: 0.27524 | train_logloss: 0.27161 | train_accuracy: 0.89517 | valid_logloss: 0.30046 | valid_accuracy: 0.88703 |  0:01:07s\n",
            "epoch 36 | loss: 0.27362 | train_logloss: 0.2733  | train_accuracy: 0.89267 | valid_logloss: 0.30453 | valid_accuracy: 0.88507 |  0:01:09s\n",
            "epoch 37 | loss: 0.2751  | train_logloss: 0.26324 | train_accuracy: 0.89843 | valid_logloss: 0.29363 | valid_accuracy: 0.8885  |  0:01:11s\n",
            "epoch 38 | loss: 0.27253 | train_logloss: 0.26488 | train_accuracy: 0.8978  | valid_logloss: 0.2967  | valid_accuracy: 0.8897  |  0:01:12s\n",
            "epoch 39 | loss: 0.27015 | train_logloss: 0.26141 | train_accuracy: 0.89923 | valid_logloss: 0.29359 | valid_accuracy: 0.89043 |  0:01:14s\n",
            "epoch 40 | loss: 0.27011 | train_logloss: 0.26204 | train_accuracy: 0.89877 | valid_logloss: 0.2917  | valid_accuracy: 0.8912  |  0:01:16s\n",
            "epoch 41 | loss: 0.26782 | train_logloss: 0.26258 | train_accuracy: 0.89937 | valid_logloss: 0.29811 | valid_accuracy: 0.89023 |  0:01:18s\n",
            "epoch 42 | loss: 0.26858 | train_logloss: 0.25766 | train_accuracy: 0.90107 | valid_logloss: 0.28904 | valid_accuracy: 0.8912  |  0:01:20s\n",
            "epoch 43 | loss: 0.26447 | train_logloss: 0.2555  | train_accuracy: 0.9002  | valid_logloss: 0.29342 | valid_accuracy: 0.8901  |  0:01:22s\n",
            "epoch 44 | loss: 0.26531 | train_logloss: 0.25299 | train_accuracy: 0.90237 | valid_logloss: 0.29005 | valid_accuracy: 0.894   |  0:01:24s\n",
            "epoch 45 | loss: 0.26494 | train_logloss: 0.25692 | train_accuracy: 0.9002  | valid_logloss: 0.29293 | valid_accuracy: 0.89033 |  0:01:25s\n",
            "epoch 46 | loss: 0.26255 | train_logloss: 0.25862 | train_accuracy: 0.90163 | valid_logloss: 0.296   | valid_accuracy: 0.89117 |  0:01:27s\n",
            "epoch 47 | loss: 0.26595 | train_logloss: 0.25387 | train_accuracy: 0.90257 | valid_logloss: 0.28997 | valid_accuracy: 0.8918  |  0:01:29s\n",
            "epoch 48 | loss: 0.26648 | train_logloss: 0.25843 | train_accuracy: 0.8983  | valid_logloss: 0.29624 | valid_accuracy: 0.88777 |  0:01:31s\n",
            "epoch 49 | loss: 0.266   | train_logloss: 0.25888 | train_accuracy: 0.89927 | valid_logloss: 0.29531 | valid_accuracy: 0.88817 |  0:01:33s\n",
            "epoch 50 | loss: 0.26019 | train_logloss: 0.25506 | train_accuracy: 0.90023 | valid_logloss: 0.2938  | valid_accuracy: 0.89207 |  0:01:35s\n",
            "epoch 51 | loss: 0.26249 | train_logloss: 0.25072 | train_accuracy: 0.9021  | valid_logloss: 0.29013 | valid_accuracy: 0.89203 |  0:01:37s\n",
            "epoch 52 | loss: 0.26089 | train_logloss: 0.25416 | train_accuracy: 0.90287 | valid_logloss: 0.29158 | valid_accuracy: 0.8926  |  0:01:39s\n",
            "epoch 53 | loss: 0.25868 | train_logloss: 0.24831 | train_accuracy: 0.90403 | valid_logloss: 0.29085 | valid_accuracy: 0.8918  |  0:01:40s\n",
            "epoch 54 | loss: 0.25888 | train_logloss: 0.25185 | train_accuracy: 0.90203 | valid_logloss: 0.29528 | valid_accuracy: 0.8918  |  0:01:42s\n",
            "epoch 55 | loss: 0.25915 | train_logloss: 0.24979 | train_accuracy: 0.9042  | valid_logloss: 0.29311 | valid_accuracy: 0.89213 |  0:01:44s\n",
            "epoch 56 | loss: 0.25937 | train_logloss: 0.25429 | train_accuracy: 0.90237 | valid_logloss: 0.29858 | valid_accuracy: 0.89223 |  0:01:46s\n",
            "epoch 57 | loss: 0.2593  | train_logloss: 0.25207 | train_accuracy: 0.90247 | valid_logloss: 0.29572 | valid_accuracy: 0.8905  |  0:01:48s\n",
            "epoch 58 | loss: 0.25903 | train_logloss: 0.24857 | train_accuracy: 0.90347 | valid_logloss: 0.29258 | valid_accuracy: 0.8922  |  0:01:50s\n",
            "epoch 59 | loss: 0.25499 | train_logloss: 0.24913 | train_accuracy: 0.90287 | valid_logloss: 0.29846 | valid_accuracy: 0.88913 |  0:01:52s\n",
            "epoch 60 | loss: 0.26044 | train_logloss: 0.25525 | train_accuracy: 0.90147 | valid_logloss: 0.29781 | valid_accuracy: 0.8906  |  0:01:54s\n",
            "epoch 61 | loss: 0.26556 | train_logloss: 0.25539 | train_accuracy: 0.90107 | valid_logloss: 0.30019 | valid_accuracy: 0.8904  |  0:01:56s\n",
            "epoch 62 | loss: 0.25934 | train_logloss: 0.25125 | train_accuracy: 0.90223 | valid_logloss: 0.29497 | valid_accuracy: 0.88923 |  0:01:57s\n",
            "epoch 63 | loss: 0.25637 | train_logloss: 0.24905 | train_accuracy: 0.90337 | valid_logloss: 0.29462 | valid_accuracy: 0.89073 |  0:01:59s\n",
            "epoch 64 | loss: 0.25741 | train_logloss: 0.24661 | train_accuracy: 0.90253 | valid_logloss: 0.29929 | valid_accuracy: 0.88937 |  0:02:01s\n",
            "epoch 65 | loss: 0.25546 | train_logloss: 0.24359 | train_accuracy: 0.9056  | valid_logloss: 0.29641 | valid_accuracy: 0.89147 |  0:02:03s\n",
            "epoch 66 | loss: 0.25109 | train_logloss: 0.23895 | train_accuracy: 0.90667 | valid_logloss: 0.2957  | valid_accuracy: 0.8924  |  0:02:05s\n",
            "epoch 67 | loss: 0.25633 | train_logloss: 0.24722 | train_accuracy: 0.904   | valid_logloss: 0.29648 | valid_accuracy: 0.89223 |  0:02:07s\n",
            "epoch 68 | loss: 0.25753 | train_logloss: 0.2544  | train_accuracy: 0.90187 | valid_logloss: 0.30023 | valid_accuracy: 0.88833 |  0:02:09s\n",
            "epoch 69 | loss: 0.25969 | train_logloss: 0.25441 | train_accuracy: 0.9018  | valid_logloss: 0.29565 | valid_accuracy: 0.89037 |  0:02:11s\n",
            "epoch 70 | loss: 0.26478 | train_logloss: 0.25644 | train_accuracy: 0.90007 | valid_logloss: 0.29818 | valid_accuracy: 0.88963 |  0:02:12s\n",
            "epoch 71 | loss: 0.25878 | train_logloss: 0.25378 | train_accuracy: 0.90337 | valid_logloss: 0.29533 | valid_accuracy: 0.89187 |  0:02:14s\n",
            "epoch 72 | loss: 0.25454 | train_logloss: 0.25045 | train_accuracy: 0.9029  | valid_logloss: 0.29578 | valid_accuracy: 0.89067 |  0:02:16s\n",
            "epoch 73 | loss: 0.2578  | train_logloss: 0.25465 | train_accuracy: 0.9017  | valid_logloss: 0.30289 | valid_accuracy: 0.88947 |  0:02:18s\n",
            "epoch 74 | loss: 0.2582  | train_logloss: 0.25104 | train_accuracy: 0.90273 | valid_logloss: 0.29988 | valid_accuracy: 0.8889  |  0:02:20s\n",
            "epoch 75 | loss: 0.25752 | train_logloss: 0.25215 | train_accuracy: 0.90263 | valid_logloss: 0.3011  | valid_accuracy: 0.89193 |  0:02:22s\n",
            "epoch 76 | loss: 0.26236 | train_logloss: 0.26266 | train_accuracy: 0.8989  | valid_logloss: 0.29815 | valid_accuracy: 0.88993 |  0:02:24s\n",
            "epoch 77 | loss: 0.27481 | train_logloss: 0.26834 | train_accuracy: 0.898   | valid_logloss: 0.30124 | valid_accuracy: 0.88663 |  0:02:26s\n",
            "epoch 78 | loss: 0.27108 | train_logloss: 0.26216 | train_accuracy: 0.9004  | valid_logloss: 0.3001  | valid_accuracy: 0.8893  |  0:02:27s\n",
            "epoch 79 | loss: 0.26493 | train_logloss: 0.25764 | train_accuracy: 0.90117 | valid_logloss: 0.30053 | valid_accuracy: 0.8904  |  0:02:29s\n",
            "epoch 80 | loss: 0.26415 | train_logloss: 0.25331 | train_accuracy: 0.90227 | valid_logloss: 0.2916  | valid_accuracy: 0.893   |  0:02:31s\n",
            "epoch 81 | loss: 0.25805 | train_logloss: 0.24418 | train_accuracy: 0.90527 | valid_logloss: 0.29314 | valid_accuracy: 0.89273 |  0:02:33s\n",
            "epoch 82 | loss: 0.25391 | train_logloss: 0.24363 | train_accuracy: 0.904   | valid_logloss: 0.28991 | valid_accuracy: 0.8912  |  0:02:35s\n",
            "epoch 83 | loss: 0.25191 | train_logloss: 0.24586 | train_accuracy: 0.90413 | valid_logloss: 0.29713 | valid_accuracy: 0.8916  |  0:02:37s\n",
            "epoch 84 | loss: 0.25034 | train_logloss: 0.24107 | train_accuracy: 0.90543 | valid_logloss: 0.291   | valid_accuracy: 0.89413 |  0:02:39s\n",
            "epoch 85 | loss: 0.24738 | train_logloss: 0.24353 | train_accuracy: 0.90813 | valid_logloss: 0.29813 | valid_accuracy: 0.8906  |  0:02:40s\n",
            "epoch 86 | loss: 0.2458  | train_logloss: 0.23289 | train_accuracy: 0.9092  | valid_logloss: 0.29288 | valid_accuracy: 0.8928  |  0:02:42s\n",
            "epoch 87 | loss: 0.2444  | train_logloss: 0.23756 | train_accuracy: 0.90763 | valid_logloss: 0.2964  | valid_accuracy: 0.89097 |  0:02:44s\n",
            "epoch 88 | loss: 0.24461 | train_logloss: 0.24205 | train_accuracy: 0.907   | valid_logloss: 0.303   | valid_accuracy: 0.89143 |  0:02:46s\n",
            "epoch 89 | loss: 0.24738 | train_logloss: 0.23906 | train_accuracy: 0.90663 | valid_logloss: 0.29886 | valid_accuracy: 0.8899  |  0:02:48s\n",
            "epoch 90 | loss: 0.25102 | train_logloss: 0.23766 | train_accuracy: 0.90823 | valid_logloss: 0.29418 | valid_accuracy: 0.89057 |  0:02:50s\n",
            "epoch 91 | loss: 0.24739 | train_logloss: 0.23988 | train_accuracy: 0.90647 | valid_logloss: 0.30315 | valid_accuracy: 0.8897  |  0:02:52s\n",
            "epoch 92 | loss: 0.24287 | train_logloss: 0.23416 | train_accuracy: 0.90823 | valid_logloss: 0.29829 | valid_accuracy: 0.8916  |  0:02:54s\n",
            "epoch 93 | loss: 0.24381 | train_logloss: 0.23259 | train_accuracy: 0.9084  | valid_logloss: 0.2989  | valid_accuracy: 0.89317 |  0:02:55s\n",
            "epoch 94 | loss: 0.24584 | train_logloss: 0.23928 | train_accuracy: 0.90667 | valid_logloss: 0.30226 | valid_accuracy: 0.89063 |  0:02:57s\n",
            "epoch 95 | loss: 0.24709 | train_logloss: 0.23323 | train_accuracy: 0.90843 | valid_logloss: 0.29743 | valid_accuracy: 0.89183 |  0:02:59s\n",
            "epoch 96 | loss: 0.24778 | train_logloss: 0.24131 | train_accuracy: 0.90643 | valid_logloss: 0.29865 | valid_accuracy: 0.8903  |  0:03:01s\n",
            "epoch 97 | loss: 0.24833 | train_logloss: 0.23689 | train_accuracy: 0.9071  | valid_logloss: 0.29703 | valid_accuracy: 0.89277 |  0:03:03s\n",
            "epoch 98 | loss: 0.2472  | train_logloss: 0.24025 | train_accuracy: 0.90557 | valid_logloss: 0.30079 | valid_accuracy: 0.89127 |  0:03:05s\n",
            "epoch 99 | loss: 0.24415 | train_logloss: 0.24082 | train_accuracy: 0.9048  | valid_logloss: 0.30645 | valid_accuracy: 0.8889  |  0:03:07s\n",
            "epoch 100| loss: 0.24265 | train_logloss: 0.23849 | train_accuracy: 0.90477 | valid_logloss: 0.30691 | valid_accuracy: 0.8881  |  0:03:09s\n",
            "epoch 101| loss: 0.2427  | train_logloss: 0.23454 | train_accuracy: 0.90887 | valid_logloss: 0.29806 | valid_accuracy: 0.89047 |  0:03:10s\n",
            "epoch 102| loss: 0.24139 | train_logloss: 0.22738 | train_accuracy: 0.9114  | valid_logloss: 0.30136 | valid_accuracy: 0.89293 |  0:03:12s\n",
            "epoch 103| loss: 0.23898 | train_logloss: 0.22711 | train_accuracy: 0.91177 | valid_logloss: 0.30377 | valid_accuracy: 0.89163 |  0:03:14s\n",
            "epoch 104| loss: 0.24173 | train_logloss: 0.24114 | train_accuracy: 0.90593 | valid_logloss: 0.30986 | valid_accuracy: 0.88993 |  0:03:16s\n",
            "epoch 105| loss: 0.24942 | train_logloss: 0.23532 | train_accuracy: 0.90883 | valid_logloss: 0.30585 | valid_accuracy: 0.8925  |  0:03:18s\n",
            "epoch 106| loss: 0.24447 | train_logloss: 0.23397 | train_accuracy: 0.90863 | valid_logloss: 0.30018 | valid_accuracy: 0.89233 |  0:03:20s\n",
            "epoch 107| loss: 0.23983 | train_logloss: 0.22835 | train_accuracy: 0.9102  | valid_logloss: 0.29792 | valid_accuracy: 0.89247 |  0:03:22s\n",
            "epoch 108| loss: 0.24138 | train_logloss: 0.25693 | train_accuracy: 0.90107 | valid_logloss: 0.32211 | valid_accuracy: 0.88587 |  0:03:24s\n",
            "epoch 109| loss: 0.25951 | train_logloss: 0.24097 | train_accuracy: 0.90633 | valid_logloss: 0.29487 | valid_accuracy: 0.89387 |  0:03:26s\n",
            "epoch 110| loss: 0.24582 | train_logloss: 0.24025 | train_accuracy: 0.90687 | valid_logloss: 0.30237 | valid_accuracy: 0.8903  |  0:03:28s\n",
            "epoch 111| loss: 0.24824 | train_logloss: 0.23313 | train_accuracy: 0.91    | valid_logloss: 0.29576 | valid_accuracy: 0.89243 |  0:03:30s\n",
            "epoch 112| loss: 0.24296 | train_logloss: 0.23243 | train_accuracy: 0.90923 | valid_logloss: 0.29832 | valid_accuracy: 0.89163 |  0:03:31s\n",
            "epoch 113| loss: 0.24162 | train_logloss: 0.2299  | train_accuracy: 0.9108  | valid_logloss: 0.29671 | valid_accuracy: 0.89247 |  0:03:33s\n",
            "epoch 114| loss: 0.23745 | train_logloss: 0.227   | train_accuracy: 0.9119  | valid_logloss: 0.29909 | valid_accuracy: 0.89303 |  0:03:35s\n",
            "epoch 115| loss: 0.23772 | train_logloss: 0.22301 | train_accuracy: 0.91217 | valid_logloss: 0.30156 | valid_accuracy: 0.89277 |  0:03:37s\n",
            "epoch 116| loss: 0.23655 | train_logloss: 0.22853 | train_accuracy: 0.90953 | valid_logloss: 0.30431 | valid_accuracy: 0.892   |  0:03:39s\n",
            "epoch 117| loss: 0.23594 | train_logloss: 0.21855 | train_accuracy: 0.914   | valid_logloss: 0.30046 | valid_accuracy: 0.89387 |  0:03:41s\n",
            "epoch 118| loss: 0.23263 | train_logloss: 0.22017 | train_accuracy: 0.9141  | valid_logloss: 0.30413 | valid_accuracy: 0.89213 |  0:03:43s\n",
            "epoch 119| loss: 0.23539 | train_logloss: 0.23431 | train_accuracy: 0.90937 | valid_logloss: 0.31035 | valid_accuracy: 0.88707 |  0:03:44s\n",
            "epoch 120| loss: 0.23879 | train_logloss: 0.22572 | train_accuracy: 0.9093  | valid_logloss: 0.30394 | valid_accuracy: 0.89323 |  0:03:46s\n",
            "epoch 121| loss: 0.23533 | train_logloss: 0.22468 | train_accuracy: 0.91403 | valid_logloss: 0.30478 | valid_accuracy: 0.89087 |  0:03:48s\n",
            "epoch 122| loss: 0.23629 | train_logloss: 0.22954 | train_accuracy: 0.9107  | valid_logloss: 0.30481 | valid_accuracy: 0.8911  |  0:03:50s\n",
            "epoch 123| loss: 0.23793 | train_logloss: 0.23403 | train_accuracy: 0.90843 | valid_logloss: 0.31074 | valid_accuracy: 0.8889  |  0:03:52s\n",
            "epoch 124| loss: 0.245   | train_logloss: 0.24614 | train_accuracy: 0.90593 | valid_logloss: 0.30602 | valid_accuracy: 0.8905  |  0:03:54s\n",
            "epoch 125| loss: 0.25281 | train_logloss: 0.2402  | train_accuracy: 0.90607 | valid_logloss: 0.30089 | valid_accuracy: 0.8895  |  0:03:56s\n",
            "epoch 126| loss: 0.24595 | train_logloss: 0.2283  | train_accuracy: 0.91193 | valid_logloss: 0.29871 | valid_accuracy: 0.89173 |  0:03:58s\n",
            "epoch 127| loss: 0.2394  | train_logloss: 0.22876 | train_accuracy: 0.91067 | valid_logloss: 0.30035 | valid_accuracy: 0.89267 |  0:03:59s\n",
            "epoch 128| loss: 0.24067 | train_logloss: 0.22883 | train_accuracy: 0.91153 | valid_logloss: 0.29761 | valid_accuracy: 0.8931  |  0:04:01s\n",
            "epoch 129| loss: 0.2347  | train_logloss: 0.22679 | train_accuracy: 0.9108  | valid_logloss: 0.3061  | valid_accuracy: 0.89127 |  0:04:03s\n",
            "epoch 130| loss: 0.23301 | train_logloss: 0.22342 | train_accuracy: 0.91183 | valid_logloss: 0.30513 | valid_accuracy: 0.89157 |  0:04:05s\n",
            "epoch 131| loss: 0.22942 | train_logloss: 0.22566 | train_accuracy: 0.91077 | valid_logloss: 0.30517 | valid_accuracy: 0.892   |  0:04:07s\n",
            "epoch 132| loss: 0.23571 | train_logloss: 0.22635 | train_accuracy: 0.91163 | valid_logloss: 0.3056  | valid_accuracy: 0.8937  |  0:04:09s\n",
            "epoch 133| loss: 0.23826 | train_logloss: 0.2284  | train_accuracy: 0.91173 | valid_logloss: 0.30476 | valid_accuracy: 0.89113 |  0:04:11s\n",
            "epoch 134| loss: 0.23728 | train_logloss: 0.22664 | train_accuracy: 0.91243 | valid_logloss: 0.30373 | valid_accuracy: 0.8928  |  0:04:13s\n",
            "\n",
            "Early stopping occurred at epoch 134 with best_epoch = 84 and best_valid_accuracy = 0.89413\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[425641  13076  41465]\n",
            " [ 16658 953038  30304]\n",
            " [ 41064  28312 363211]]\n",
            "Testing Score:  0.8941333333333333\n",
            "Confusion Matrix: \n",
            " [[419412  12587  48183]\n",
            " [ 13732 954252  32016]\n",
            " [ 35048  28128 369411]]\n",
            "Testing Score:  0.8917666666666667\n",
            "{'Rows': 30000, 'Nd': 32, 'Na': 32, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 3, 'decision': 3, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 253.57947421073914, 'time_tn': 26.655120849609375, 'accuracy_tn': 0.8941333333333333, 'time_learn_gb': 8.09342336654663, 'time_gb': 27.197022438049316, 'accuracy_gb': 0.8917666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqa-eziBWc2M",
        "outputId": "b39ac0fb-2536-4a55-bcfb-68521360af7c"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1   9000   9000   9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2   9000   9000   9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3   9000   9000   9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4   9000   9000   9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "5  30000  30000  30000    8  ...  20.273622    0.891767      8.276289  27.093480\n",
              "6  30000  30000  30000   16  ...  22.630230    0.891767      8.210687  27.343578\n",
              "7  30000  30000  30000   64  ...  30.078117    0.891767      8.244918  28.102216\n",
              "8  30000  30000  30000   32  ...  26.655121    0.891767      8.093423  27.197022\n",
              "\n",
              "[9 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DHTu9LrBLAd",
        "outputId": "39ed0403-447f-45ac-9c9a-c78c78940bfe"
      },
      "source": [
        "time_model(number_exp=10, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.19925 | train_logloss: 23.02097| train_accuracy: 0.3334  | valid_logloss: 23.01966| valid_accuracy: 0.33347 |  0:00:02s\n",
            "epoch 1  | loss: 0.6253  | train_logloss: 23.50548| train_accuracy: 0.27513 | valid_logloss: 23.47103| valid_accuracy: 0.27693 |  0:00:04s\n",
            "epoch 2  | loss: 0.4407  | train_logloss: 20.73321| train_accuracy: 0.33333 | valid_logloss: 20.80043| valid_accuracy: 0.33333 |  0:00:06s\n",
            "epoch 3  | loss: 0.38829 | train_logloss: 14.62514| train_accuracy: 0.19973 | valid_logloss: 14.65985| valid_accuracy: 0.20253 |  0:00:08s\n",
            "epoch 4  | loss: 0.37651 | train_logloss: 7.01168 | train_accuracy: 0.3337  | valid_logloss: 7.03836 | valid_accuracy: 0.3336  |  0:00:10s\n",
            "epoch 5  | loss: 0.36318 | train_logloss: 3.60119 | train_accuracy: 0.33437 | valid_logloss: 3.61014 | valid_accuracy: 0.33423 |  0:00:12s\n",
            "epoch 6  | loss: 0.35146 | train_logloss: 2.68802 | train_accuracy: 0.33497 | valid_logloss: 2.68986 | valid_accuracy: 0.33483 |  0:00:14s\n",
            "epoch 7  | loss: 0.34984 | train_logloss: 5.55152 | train_accuracy: 0.34923 | valid_logloss: 5.56538 | valid_accuracy: 0.35007 |  0:00:16s\n",
            "epoch 8  | loss: 0.34452 | train_logloss: 3.5552  | train_accuracy: 0.38903 | valid_logloss: 3.56006 | valid_accuracy: 0.38777 |  0:00:18s\n",
            "epoch 9  | loss: 0.35075 | train_logloss: 3.15407 | train_accuracy: 0.42077 | valid_logloss: 3.14914 | valid_accuracy: 0.42243 |  0:00:21s\n",
            "epoch 10 | loss: 0.36493 | train_logloss: 2.18332 | train_accuracy: 0.55617 | valid_logloss: 2.1975  | valid_accuracy: 0.55407 |  0:00:23s\n",
            "epoch 11 | loss: 0.35169 | train_logloss: 1.15302 | train_accuracy: 0.6471  | valid_logloss: 1.16148 | valid_accuracy: 0.64133 |  0:00:25s\n",
            "epoch 12 | loss: 0.36943 | train_logloss: 1.13685 | train_accuracy: 0.48217 | valid_logloss: 1.15745 | valid_accuracy: 0.4752  |  0:00:27s\n",
            "epoch 13 | loss: 0.39625 | train_logloss: 1.09554 | train_accuracy: 0.64637 | valid_logloss: 1.10323 | valid_accuracy: 0.6444  |  0:00:29s\n",
            "epoch 14 | loss: 0.35294 | train_logloss: 0.80379 | train_accuracy: 0.7182  | valid_logloss: 0.81189 | valid_accuracy: 0.7137  |  0:00:31s\n",
            "epoch 15 | loss: 0.34018 | train_logloss: 0.60287 | train_accuracy: 0.81473 | valid_logloss: 0.61261 | valid_accuracy: 0.80877 |  0:00:33s\n",
            "epoch 16 | loss: 0.33546 | train_logloss: 0.62088 | train_accuracy: 0.78817 | valid_logloss: 0.62841 | valid_accuracy: 0.78597 |  0:00:35s\n",
            "epoch 17 | loss: 0.33736 | train_logloss: 0.56163 | train_accuracy: 0.80433 | valid_logloss: 0.56994 | valid_accuracy: 0.79763 |  0:00:37s\n",
            "epoch 18 | loss: 0.33626 | train_logloss: 0.45739 | train_accuracy: 0.8275  | valid_logloss: 0.46763 | valid_accuracy: 0.82657 |  0:00:39s\n",
            "epoch 19 | loss: 0.3331  | train_logloss: 0.44323 | train_accuracy: 0.84057 | valid_logloss: 0.44992 | valid_accuracy: 0.84073 |  0:00:41s\n",
            "epoch 20 | loss: 0.33254 | train_logloss: 0.39777 | train_accuracy: 0.85447 | valid_logloss: 0.40506 | valid_accuracy: 0.85237 |  0:00:43s\n",
            "epoch 21 | loss: 0.33643 | train_logloss: 0.3545  | train_accuracy: 0.86357 | valid_logloss: 0.36385 | valid_accuracy: 0.85967 |  0:00:46s\n",
            "epoch 22 | loss: 0.32853 | train_logloss: 0.36086 | train_accuracy: 0.85967 | valid_logloss: 0.37218 | valid_accuracy: 0.85477 |  0:00:48s\n",
            "epoch 23 | loss: 0.32603 | train_logloss: 0.35828 | train_accuracy: 0.86787 | valid_logloss: 0.36819 | valid_accuracy: 0.86453 |  0:00:50s\n",
            "epoch 24 | loss: 0.32026 | train_logloss: 0.34675 | train_accuracy: 0.87007 | valid_logloss: 0.36128 | valid_accuracy: 0.8645  |  0:00:52s\n",
            "epoch 25 | loss: 0.31107 | train_logloss: 0.31102 | train_accuracy: 0.8839  | valid_logloss: 0.32479 | valid_accuracy: 0.8795  |  0:00:54s\n",
            "epoch 26 | loss: 0.30606 | train_logloss: 0.31659 | train_accuracy: 0.8777  | valid_logloss: 0.32934 | valid_accuracy: 0.8762  |  0:00:56s\n",
            "epoch 27 | loss: 0.30595 | train_logloss: 0.31173 | train_accuracy: 0.88023 | valid_logloss: 0.32638 | valid_accuracy: 0.8773  |  0:00:58s\n",
            "epoch 28 | loss: 0.30143 | train_logloss: 0.30909 | train_accuracy: 0.8825  | valid_logloss: 0.32597 | valid_accuracy: 0.87833 |  0:01:00s\n",
            "epoch 29 | loss: 0.29941 | train_logloss: 0.30102 | train_accuracy: 0.88493 | valid_logloss: 0.31915 | valid_accuracy: 0.88003 |  0:01:02s\n",
            "epoch 30 | loss: 0.29544 | train_logloss: 0.29641 | train_accuracy: 0.8868  | valid_logloss: 0.31441 | valid_accuracy: 0.88027 |  0:01:04s\n",
            "epoch 31 | loss: 0.29743 | train_logloss: 0.30324 | train_accuracy: 0.88173 | valid_logloss: 0.32022 | valid_accuracy: 0.87723 |  0:01:07s\n",
            "epoch 32 | loss: 0.30237 | train_logloss: 0.30256 | train_accuracy: 0.88543 | valid_logloss: 0.31996 | valid_accuracy: 0.87917 |  0:01:09s\n",
            "epoch 33 | loss: 0.30169 | train_logloss: 0.29218 | train_accuracy: 0.88927 | valid_logloss: 0.3114  | valid_accuracy: 0.88463 |  0:01:11s\n",
            "epoch 34 | loss: 0.29373 | train_logloss: 0.28458 | train_accuracy: 0.89163 | valid_logloss: 0.30529 | valid_accuracy: 0.88653 |  0:01:13s\n",
            "epoch 35 | loss: 0.29646 | train_logloss: 0.29362 | train_accuracy: 0.88797 | valid_logloss: 0.3149  | valid_accuracy: 0.8824  |  0:01:15s\n",
            "epoch 36 | loss: 0.29273 | train_logloss: 0.28199 | train_accuracy: 0.89313 | valid_logloss: 0.30473 | valid_accuracy: 0.88713 |  0:01:17s\n",
            "epoch 37 | loss: 0.28614 | train_logloss: 0.28215 | train_accuracy: 0.89103 | valid_logloss: 0.30468 | valid_accuracy: 0.88547 |  0:01:19s\n",
            "epoch 38 | loss: 0.28647 | train_logloss: 0.27979 | train_accuracy: 0.8932  | valid_logloss: 0.30369 | valid_accuracy: 0.88623 |  0:01:21s\n",
            "epoch 39 | loss: 0.28736 | train_logloss: 0.28194 | train_accuracy: 0.89303 | valid_logloss: 0.3076  | valid_accuracy: 0.88507 |  0:01:23s\n",
            "epoch 40 | loss: 0.28396 | train_logloss: 0.27247 | train_accuracy: 0.8955  | valid_logloss: 0.30307 | valid_accuracy: 0.8875  |  0:01:25s\n",
            "epoch 41 | loss: 0.28067 | train_logloss: 0.27121 | train_accuracy: 0.8952  | valid_logloss: 0.2995  | valid_accuracy: 0.88877 |  0:01:27s\n",
            "epoch 42 | loss: 0.28192 | train_logloss: 0.27186 | train_accuracy: 0.89653 | valid_logloss: 0.30333 | valid_accuracy: 0.88693 |  0:01:30s\n",
            "epoch 43 | loss: 0.27838 | train_logloss: 0.27228 | train_accuracy: 0.89423 | valid_logloss: 0.30339 | valid_accuracy: 0.88713 |  0:01:32s\n",
            "epoch 44 | loss: 0.27911 | train_logloss: 0.27303 | train_accuracy: 0.89597 | valid_logloss: 0.30523 | valid_accuracy: 0.88703 |  0:01:34s\n",
            "epoch 45 | loss: 0.27861 | train_logloss: 0.26669 | train_accuracy: 0.89793 | valid_logloss: 0.30099 | valid_accuracy: 0.88733 |  0:01:36s\n",
            "epoch 46 | loss: 0.27508 | train_logloss: 0.268   | train_accuracy: 0.89753 | valid_logloss: 0.30124 | valid_accuracy: 0.8871  |  0:01:38s\n",
            "epoch 47 | loss: 0.27671 | train_logloss: 0.27989 | train_accuracy: 0.89257 | valid_logloss: 0.31138 | valid_accuracy: 0.88367 |  0:01:40s\n",
            "epoch 48 | loss: 0.28692 | train_logloss: 0.27903 | train_accuracy: 0.89187 | valid_logloss: 0.30609 | valid_accuracy: 0.8855  |  0:01:42s\n",
            "epoch 49 | loss: 0.29083 | train_logloss: 0.30344 | train_accuracy: 0.88653 | valid_logloss: 0.32728 | valid_accuracy: 0.88103 |  0:01:44s\n",
            "epoch 50 | loss: 0.29988 | train_logloss: 0.29638 | train_accuracy: 0.8889  | valid_logloss: 0.31913 | valid_accuracy: 0.88163 |  0:01:46s\n",
            "epoch 51 | loss: 0.30712 | train_logloss: 0.29596 | train_accuracy: 0.88753 | valid_logloss: 0.31506 | valid_accuracy: 0.88227 |  0:01:48s\n",
            "epoch 52 | loss: 0.30217 | train_logloss: 0.31522 | train_accuracy: 0.88673 | valid_logloss: 0.33409 | valid_accuracy: 0.8808  |  0:01:50s\n",
            "epoch 53 | loss: 0.30519 | train_logloss: 0.28958 | train_accuracy: 0.88927 | valid_logloss: 0.30619 | valid_accuracy: 0.883   |  0:01:52s\n",
            "epoch 54 | loss: 0.29198 | train_logloss: 0.28356 | train_accuracy: 0.892   | valid_logloss: 0.3035  | valid_accuracy: 0.8871  |  0:01:55s\n",
            "epoch 55 | loss: 0.29748 | train_logloss: 0.31066 | train_accuracy: 0.8799  | valid_logloss: 0.32978 | valid_accuracy: 0.875   |  0:01:56s\n",
            "epoch 56 | loss: 0.30849 | train_logloss: 0.29176 | train_accuracy: 0.88907 | valid_logloss: 0.30734 | valid_accuracy: 0.88427 |  0:01:59s\n",
            "epoch 57 | loss: 0.29412 | train_logloss: 0.28296 | train_accuracy: 0.89293 | valid_logloss: 0.30584 | valid_accuracy: 0.88593 |  0:02:01s\n",
            "epoch 58 | loss: 0.29396 | train_logloss: 0.29154 | train_accuracy: 0.88687 | valid_logloss: 0.31185 | valid_accuracy: 0.88043 |  0:02:03s\n",
            "epoch 59 | loss: 0.29861 | train_logloss: 0.29244 | train_accuracy: 0.89063 | valid_logloss: 0.31266 | valid_accuracy: 0.88467 |  0:02:05s\n",
            "epoch 60 | loss: 0.29137 | train_logloss: 0.27518 | train_accuracy: 0.89647 | valid_logloss: 0.29689 | valid_accuracy: 0.88947 |  0:02:07s\n",
            "epoch 61 | loss: 0.28426 | train_logloss: 0.2777  | train_accuracy: 0.89387 | valid_logloss: 0.30033 | valid_accuracy: 0.88727 |  0:02:09s\n",
            "epoch 62 | loss: 0.2822  | train_logloss: 0.27371 | train_accuracy: 0.89543 | valid_logloss: 0.29807 | valid_accuracy: 0.88843 |  0:02:11s\n",
            "epoch 63 | loss: 0.2773  | train_logloss: 0.27042 | train_accuracy: 0.89737 | valid_logloss: 0.29775 | valid_accuracy: 0.88943 |  0:02:13s\n",
            "epoch 64 | loss: 0.27734 | train_logloss: 0.27441 | train_accuracy: 0.89577 | valid_logloss: 0.29975 | valid_accuracy: 0.88897 |  0:02:15s\n",
            "epoch 65 | loss: 0.27661 | train_logloss: 0.26988 | train_accuracy: 0.89727 | valid_logloss: 0.29606 | valid_accuracy: 0.8894  |  0:02:17s\n",
            "epoch 66 | loss: 0.27323 | train_logloss: 0.26717 | train_accuracy: 0.8977  | valid_logloss: 0.29728 | valid_accuracy: 0.89043 |  0:02:19s\n",
            "epoch 67 | loss: 0.27673 | train_logloss: 0.27163 | train_accuracy: 0.89537 | valid_logloss: 0.30056 | valid_accuracy: 0.8881  |  0:02:22s\n",
            "epoch 68 | loss: 0.27499 | train_logloss: 0.30033 | train_accuracy: 0.88923 | valid_logloss: 0.32461 | valid_accuracy: 0.8818  |  0:02:24s\n",
            "epoch 69 | loss: 0.30421 | train_logloss: 0.32069 | train_accuracy: 0.88073 | valid_logloss: 0.33645 | valid_accuracy: 0.87713 |  0:02:26s\n",
            "epoch 70 | loss: 0.31171 | train_logloss: 0.28867 | train_accuracy: 0.88917 | valid_logloss: 0.30551 | valid_accuracy: 0.88607 |  0:02:28s\n",
            "epoch 71 | loss: 0.29416 | train_logloss: 0.28266 | train_accuracy: 0.8936  | valid_logloss: 0.30302 | valid_accuracy: 0.8857  |  0:02:30s\n",
            "epoch 72 | loss: 0.29871 | train_logloss: 0.29835 | train_accuracy: 0.8918  | valid_logloss: 0.31797 | valid_accuracy: 0.8857  |  0:02:32s\n",
            "epoch 73 | loss: 0.30674 | train_logloss: 0.31055 | train_accuracy: 0.8871  | valid_logloss: 0.3223  | valid_accuracy: 0.88223 |  0:02:34s\n",
            "epoch 74 | loss: 0.30714 | train_logloss: 0.29545 | train_accuracy: 0.89063 | valid_logloss: 0.31332 | valid_accuracy: 0.88517 |  0:02:36s\n",
            "epoch 75 | loss: 0.29998 | train_logloss: 0.30874 | train_accuracy: 0.87623 | valid_logloss: 0.32723 | valid_accuracy: 0.8728  |  0:02:38s\n",
            "epoch 76 | loss: 0.30514 | train_logloss: 0.29178 | train_accuracy: 0.89097 | valid_logloss: 0.30633 | valid_accuracy: 0.8854  |  0:02:40s\n",
            "epoch 77 | loss: 0.31832 | train_logloss: 0.33005 | train_accuracy: 0.88153 | valid_logloss: 0.34057 | valid_accuracy: 0.87637 |  0:02:42s\n",
            "epoch 78 | loss: 0.31721 | train_logloss: 0.29619 | train_accuracy: 0.88687 | valid_logloss: 0.31114 | valid_accuracy: 0.88357 |  0:02:44s\n",
            "epoch 79 | loss: 0.30206 | train_logloss: 0.29616 | train_accuracy: 0.89063 | valid_logloss: 0.31066 | valid_accuracy: 0.88383 |  0:02:46s\n",
            "epoch 80 | loss: 0.3028  | train_logloss: 0.32303 | train_accuracy: 0.87803 | valid_logloss: 0.33625 | valid_accuracy: 0.87367 |  0:02:49s\n",
            "epoch 81 | loss: 0.31757 | train_logloss: 0.3062  | train_accuracy: 0.88217 | valid_logloss: 0.31873 | valid_accuracy: 0.8786  |  0:02:51s\n",
            "epoch 82 | loss: 0.31255 | train_logloss: 0.30612 | train_accuracy: 0.8828  | valid_logloss: 0.31808 | valid_accuracy: 0.87873 |  0:02:53s\n",
            "epoch 83 | loss: 0.3118  | train_logloss: 0.30784 | train_accuracy: 0.88277 | valid_logloss: 0.321   | valid_accuracy: 0.8789  |  0:02:55s\n",
            "epoch 84 | loss: 0.30527 | train_logloss: 0.29691 | train_accuracy: 0.888   | valid_logloss: 0.31168 | valid_accuracy: 0.88393 |  0:02:57s\n",
            "epoch 85 | loss: 0.29969 | train_logloss: 0.28547 | train_accuracy: 0.8903  | valid_logloss: 0.30092 | valid_accuracy: 0.88663 |  0:02:59s\n",
            "epoch 86 | loss: 0.29018 | train_logloss: 0.28663 | train_accuracy: 0.8855  | valid_logloss: 0.30505 | valid_accuracy: 0.88373 |  0:03:01s\n",
            "epoch 87 | loss: 0.2888  | train_logloss: 0.27834 | train_accuracy: 0.89427 | valid_logloss: 0.29981 | valid_accuracy: 0.88727 |  0:03:03s\n",
            "epoch 88 | loss: 0.28423 | train_logloss: 0.28015 | train_accuracy: 0.8917  | valid_logloss: 0.30247 | valid_accuracy: 0.8879  |  0:03:05s\n",
            "epoch 89 | loss: 0.28218 | train_logloss: 0.27579 | train_accuracy: 0.89437 | valid_logloss: 0.29795 | valid_accuracy: 0.88797 |  0:03:07s\n",
            "epoch 90 | loss: 0.28077 | train_logloss: 0.2751  | train_accuracy: 0.8946  | valid_logloss: 0.29623 | valid_accuracy: 0.88917 |  0:03:09s\n",
            "epoch 91 | loss: 0.28224 | train_logloss: 0.27272 | train_accuracy: 0.89387 | valid_logloss: 0.29708 | valid_accuracy: 0.89047 |  0:03:11s\n",
            "epoch 92 | loss: 0.28064 | train_logloss: 0.27128 | train_accuracy: 0.89627 | valid_logloss: 0.29447 | valid_accuracy: 0.88967 |  0:03:13s\n",
            "epoch 93 | loss: 0.27755 | train_logloss: 0.27072 | train_accuracy: 0.89623 | valid_logloss: 0.29418 | valid_accuracy: 0.88993 |  0:03:16s\n",
            "epoch 94 | loss: 0.27565 | train_logloss: 0.27099 | train_accuracy: 0.89513 | valid_logloss: 0.29512 | valid_accuracy: 0.88883 |  0:03:18s\n",
            "epoch 95 | loss: 0.27532 | train_logloss: 0.27262 | train_accuracy: 0.89617 | valid_logloss: 0.29904 | valid_accuracy: 0.89063 |  0:03:20s\n",
            "epoch 96 | loss: 0.28017 | train_logloss: 0.28125 | train_accuracy: 0.8933  | valid_logloss: 0.30691 | valid_accuracy: 0.88697 |  0:03:22s\n",
            "epoch 97 | loss: 0.28146 | train_logloss: 0.27116 | train_accuracy: 0.89553 | valid_logloss: 0.29865 | valid_accuracy: 0.89023 |  0:03:24s\n",
            "epoch 98 | loss: 0.27657 | train_logloss: 0.26514 | train_accuracy: 0.89843 | valid_logloss: 0.29394 | valid_accuracy: 0.891   |  0:03:26s\n",
            "epoch 99 | loss: 0.27198 | train_logloss: 0.26508 | train_accuracy: 0.89907 | valid_logloss: 0.29465 | valid_accuracy: 0.8903  |  0:03:28s\n",
            "epoch 100| loss: 0.27005 | train_logloss: 0.26268 | train_accuracy: 0.8995  | valid_logloss: 0.29834 | valid_accuracy: 0.8906  |  0:03:30s\n",
            "epoch 101| loss: 0.27519 | train_logloss: 0.26833 | train_accuracy: 0.89577 | valid_logloss: 0.29919 | valid_accuracy: 0.88877 |  0:03:32s\n",
            "epoch 102| loss: 0.27303 | train_logloss: 0.27533 | train_accuracy: 0.89747 | valid_logloss: 0.30537 | valid_accuracy: 0.88857 |  0:03:34s\n",
            "epoch 103| loss: 0.2745  | train_logloss: 0.26962 | train_accuracy: 0.89727 | valid_logloss: 0.30126 | valid_accuracy: 0.88937 |  0:03:36s\n",
            "epoch 104| loss: 0.27344 | train_logloss: 0.26384 | train_accuracy: 0.89897 | valid_logloss: 0.29454 | valid_accuracy: 0.88937 |  0:03:39s\n",
            "epoch 105| loss: 0.26645 | train_logloss: 0.26018 | train_accuracy: 0.89933 | valid_logloss: 0.29348 | valid_accuracy: 0.8908  |  0:03:41s\n",
            "epoch 106| loss: 0.26714 | train_logloss: 0.2692  | train_accuracy: 0.89707 | valid_logloss: 0.30232 | valid_accuracy: 0.8868  |  0:03:43s\n",
            "epoch 107| loss: 0.26608 | train_logloss: 0.26301 | train_accuracy: 0.89883 | valid_logloss: 0.29811 | valid_accuracy: 0.8883  |  0:03:45s\n",
            "epoch 108| loss: 0.26236 | train_logloss: 0.2552  | train_accuracy: 0.9015  | valid_logloss: 0.29184 | valid_accuracy: 0.8915  |  0:03:47s\n",
            "epoch 109| loss: 0.26325 | train_logloss: 0.25763 | train_accuracy: 0.902   | valid_logloss: 0.29535 | valid_accuracy: 0.8914  |  0:03:49s\n",
            "epoch 110| loss: 0.27766 | train_logloss: 0.27162 | train_accuracy: 0.89603 | valid_logloss: 0.30298 | valid_accuracy: 0.8874  |  0:03:51s\n",
            "epoch 111| loss: 0.27383 | train_logloss: 0.26378 | train_accuracy: 0.89953 | valid_logloss: 0.29443 | valid_accuracy: 0.89077 |  0:03:53s\n",
            "epoch 112| loss: 0.27032 | train_logloss: 0.25772 | train_accuracy: 0.90133 | valid_logloss: 0.28907 | valid_accuracy: 0.89193 |  0:03:55s\n",
            "epoch 113| loss: 0.26929 | train_logloss: 0.26541 | train_accuracy: 0.8973  | valid_logloss: 0.29431 | valid_accuracy: 0.88767 |  0:03:57s\n",
            "epoch 114| loss: 0.26864 | train_logloss: 0.26035 | train_accuracy: 0.89863 | valid_logloss: 0.29181 | valid_accuracy: 0.89033 |  0:03:59s\n",
            "epoch 115| loss: 0.26591 | train_logloss: 0.26025 | train_accuracy: 0.89953 | valid_logloss: 0.29205 | valid_accuracy: 0.89097 |  0:04:02s\n",
            "epoch 116| loss: 0.26897 | train_logloss: 0.26005 | train_accuracy: 0.89987 | valid_logloss: 0.29159 | valid_accuracy: 0.89173 |  0:04:04s\n",
            "epoch 117| loss: 0.26608 | train_logloss: 0.25794 | train_accuracy: 0.90173 | valid_logloss: 0.29234 | valid_accuracy: 0.8916  |  0:04:06s\n",
            "epoch 118| loss: 0.26899 | train_logloss: 0.26043 | train_accuracy: 0.9001  | valid_logloss: 0.29354 | valid_accuracy: 0.89027 |  0:04:08s\n",
            "epoch 119| loss: 0.268   | train_logloss: 0.25978 | train_accuracy: 0.8999  | valid_logloss: 0.29449 | valid_accuracy: 0.89143 |  0:04:10s\n",
            "epoch 120| loss: 0.26831 | train_logloss: 0.26882 | train_accuracy: 0.89397 | valid_logloss: 0.29864 | valid_accuracy: 0.88817 |  0:04:12s\n",
            "epoch 121| loss: 0.2757  | train_logloss: 0.27224 | train_accuracy: 0.89547 | valid_logloss: 0.30857 | valid_accuracy: 0.8846  |  0:04:14s\n",
            "epoch 122| loss: 0.27122 | train_logloss: 0.26184 | train_accuracy: 0.89767 | valid_logloss: 0.29813 | valid_accuracy: 0.8882  |  0:04:16s\n",
            "epoch 123| loss: 0.26641 | train_logloss: 0.26483 | train_accuracy: 0.89663 | valid_logloss: 0.3009  | valid_accuracy: 0.88847 |  0:04:18s\n",
            "epoch 124| loss: 0.26542 | train_logloss: 0.25717 | train_accuracy: 0.89973 | valid_logloss: 0.29486 | valid_accuracy: 0.8901  |  0:04:20s\n",
            "epoch 125| loss: 0.26601 | train_logloss: 0.26056 | train_accuracy: 0.89853 | valid_logloss: 0.29805 | valid_accuracy: 0.89003 |  0:04:22s\n",
            "epoch 126| loss: 0.27406 | train_logloss: 0.28558 | train_accuracy: 0.89087 | valid_logloss: 0.31514 | valid_accuracy: 0.8857  |  0:04:24s\n",
            "epoch 127| loss: 0.2843  | train_logloss: 0.27131 | train_accuracy: 0.89613 | valid_logloss: 0.30611 | valid_accuracy: 0.8863  |  0:04:27s\n",
            "epoch 128| loss: 0.27426 | train_logloss: 0.26878 | train_accuracy: 0.89733 | valid_logloss: 0.30361 | valid_accuracy: 0.888   |  0:04:29s\n",
            "epoch 129| loss: 0.26627 | train_logloss: 0.25929 | train_accuracy: 0.9001  | valid_logloss: 0.29595 | valid_accuracy: 0.89077 |  0:04:31s\n",
            "epoch 130| loss: 0.26942 | train_logloss: 0.25574 | train_accuracy: 0.902   | valid_logloss: 0.29193 | valid_accuracy: 0.89127 |  0:04:33s\n",
            "epoch 131| loss: 0.26523 | train_logloss: 0.25409 | train_accuracy: 0.9021  | valid_logloss: 0.29203 | valid_accuracy: 0.89157 |  0:04:35s\n",
            "epoch 132| loss: 0.26042 | train_logloss: 0.25758 | train_accuracy: 0.90143 | valid_logloss: 0.2988  | valid_accuracy: 0.89007 |  0:04:37s\n",
            "epoch 133| loss: 0.26341 | train_logloss: 0.25892 | train_accuracy: 0.89763 | valid_logloss: 0.29866 | valid_accuracy: 0.88857 |  0:04:39s\n",
            "epoch 134| loss: 0.26209 | train_logloss: 0.2509  | train_accuracy: 0.90327 | valid_logloss: 0.29018 | valid_accuracy: 0.89243 |  0:04:41s\n",
            "epoch 135| loss: 0.25645 | train_logloss: 0.25109 | train_accuracy: 0.90323 | valid_logloss: 0.29417 | valid_accuracy: 0.89193 |  0:04:43s\n",
            "epoch 136| loss: 0.25318 | train_logloss: 0.2428  | train_accuracy: 0.90487 | valid_logloss: 0.29175 | valid_accuracy: 0.89317 |  0:04:45s\n",
            "epoch 137| loss: 0.25487 | train_logloss: 0.2492  | train_accuracy: 0.90323 | valid_logloss: 0.29267 | valid_accuracy: 0.89227 |  0:04:47s\n",
            "epoch 138| loss: 0.25872 | train_logloss: 0.26284 | train_accuracy: 0.8992  | valid_logloss: 0.3023  | valid_accuracy: 0.88963 |  0:04:49s\n",
            "epoch 139| loss: 0.26354 | train_logloss: 0.25124 | train_accuracy: 0.90287 | valid_logloss: 0.29817 | valid_accuracy: 0.88993 |  0:04:51s\n",
            "epoch 140| loss: 0.28059 | train_logloss: 0.29922 | train_accuracy: 0.88117 | valid_logloss: 0.32557 | valid_accuracy: 0.87817 |  0:04:53s\n",
            "epoch 141| loss: 0.29211 | train_logloss: 0.27327 | train_accuracy: 0.89517 | valid_logloss: 0.30356 | valid_accuracy: 0.8859  |  0:04:56s\n",
            "epoch 142| loss: 0.27434 | train_logloss: 0.26516 | train_accuracy: 0.89687 | valid_logloss: 0.30064 | valid_accuracy: 0.8888  |  0:04:58s\n",
            "epoch 143| loss: 0.26915 | train_logloss: 0.26513 | train_accuracy: 0.89733 | valid_logloss: 0.30032 | valid_accuracy: 0.8889  |  0:05:00s\n",
            "epoch 144| loss: 0.2666  | train_logloss: 0.25623 | train_accuracy: 0.90037 | valid_logloss: 0.29571 | valid_accuracy: 0.89003 |  0:05:02s\n",
            "epoch 145| loss: 0.26492 | train_logloss: 0.2744  | train_accuracy: 0.89313 | valid_logloss: 0.31036 | valid_accuracy: 0.88323 |  0:05:04s\n",
            "epoch 146| loss: 0.27812 | train_logloss: 0.26347 | train_accuracy: 0.89953 | valid_logloss: 0.29662 | valid_accuracy: 0.89057 |  0:05:06s\n",
            "epoch 147| loss: 0.26604 | train_logloss: 0.26661 | train_accuracy: 0.89823 | valid_logloss: 0.30405 | valid_accuracy: 0.88947 |  0:05:08s\n",
            "epoch 148| loss: 0.27219 | train_logloss: 0.26476 | train_accuracy: 0.89997 | valid_logloss: 0.29673 | valid_accuracy: 0.8909  |  0:05:10s\n",
            "epoch 149| loss: 0.2696  | train_logloss: 0.27504 | train_accuracy: 0.8943  | valid_logloss: 0.30412 | valid_accuracy: 0.88957 |  0:05:12s\n",
            "epoch 150| loss: 0.27486 | train_logloss: 0.2626  | train_accuracy: 0.89827 | valid_logloss: 0.29758 | valid_accuracy: 0.88967 |  0:05:14s\n",
            "epoch 151| loss: 0.26733 | train_logloss: 0.26129 | train_accuracy: 0.8994  | valid_logloss: 0.29904 | valid_accuracy: 0.89023 |  0:05:16s\n",
            "epoch 152| loss: 0.26434 | train_logloss: 0.25371 | train_accuracy: 0.9032  | valid_logloss: 0.29535 | valid_accuracy: 0.8928  |  0:05:18s\n",
            "epoch 153| loss: 0.25942 | train_logloss: 0.24944 | train_accuracy: 0.90363 | valid_logloss: 0.29245 | valid_accuracy: 0.89283 |  0:05:21s\n",
            "epoch 154| loss: 0.25659 | train_logloss: 0.24874 | train_accuracy: 0.90273 | valid_logloss: 0.2932  | valid_accuracy: 0.8938  |  0:05:23s\n",
            "epoch 155| loss: 0.25453 | train_logloss: 0.24807 | train_accuracy: 0.9039  | valid_logloss: 0.29193 | valid_accuracy: 0.89263 |  0:05:25s\n",
            "epoch 156| loss: 0.25215 | train_logloss: 0.25352 | train_accuracy: 0.90147 | valid_logloss: 0.29953 | valid_accuracy: 0.8901  |  0:05:27s\n",
            "epoch 157| loss: 0.27008 | train_logloss: 0.28904 | train_accuracy: 0.8877  | valid_logloss: 0.31996 | valid_accuracy: 0.87937 |  0:05:29s\n",
            "epoch 158| loss: 0.3278  | train_logloss: 0.34912 | train_accuracy: 0.8647  | valid_logloss: 0.35829 | valid_accuracy: 0.86197 |  0:05:31s\n",
            "epoch 159| loss: 0.32444 | train_logloss: 0.30515 | train_accuracy: 0.88587 | valid_logloss: 0.31865 | valid_accuracy: 0.88183 |  0:05:33s\n",
            "epoch 160| loss: 0.31157 | train_logloss: 0.30308 | train_accuracy: 0.8838  | valid_logloss: 0.31854 | valid_accuracy: 0.8787  |  0:05:35s\n",
            "epoch 161| loss: 0.30111 | train_logloss: 0.28147 | train_accuracy: 0.89253 | valid_logloss: 0.29926 | valid_accuracy: 0.88783 |  0:05:37s\n",
            "epoch 162| loss: 0.29233 | train_logloss: 0.28634 | train_accuracy: 0.8905  | valid_logloss: 0.30518 | valid_accuracy: 0.8857  |  0:05:39s\n",
            "epoch 163| loss: 0.2886  | train_logloss: 0.28178 | train_accuracy: 0.89253 | valid_logloss: 0.30285 | valid_accuracy: 0.88483 |  0:05:41s\n",
            "epoch 164| loss: 0.28558 | train_logloss: 0.27528 | train_accuracy: 0.8943  | valid_logloss: 0.29722 | valid_accuracy: 0.88927 |  0:05:43s\n",
            "epoch 165| loss: 0.27821 | train_logloss: 0.27059 | train_accuracy: 0.8962  | valid_logloss: 0.29428 | valid_accuracy: 0.88937 |  0:05:45s\n",
            "epoch 166| loss: 0.27859 | train_logloss: 0.28015 | train_accuracy: 0.8936  | valid_logloss: 0.30235 | valid_accuracy: 0.8863  |  0:05:47s\n",
            "epoch 167| loss: 0.27813 | train_logloss: 0.26845 | train_accuracy: 0.89593 | valid_logloss: 0.2931  | valid_accuracy: 0.89047 |  0:05:50s\n",
            "epoch 168| loss: 0.27432 | train_logloss: 0.27463 | train_accuracy: 0.89517 | valid_logloss: 0.30015 | valid_accuracy: 0.88847 |  0:05:52s\n",
            "epoch 169| loss: 0.27308 | train_logloss: 0.26443 | train_accuracy: 0.8981  | valid_logloss: 0.29    | valid_accuracy: 0.89193 |  0:05:54s\n",
            "epoch 170| loss: 0.2685  | train_logloss: 0.26126 | train_accuracy: 0.90017 | valid_logloss: 0.28925 | valid_accuracy: 0.8923  |  0:05:56s\n",
            "epoch 171| loss: 0.26446 | train_logloss: 0.25737 | train_accuracy: 0.9008  | valid_logloss: 0.28936 | valid_accuracy: 0.89213 |  0:05:58s\n",
            "epoch 172| loss: 0.26296 | train_logloss: 0.25548 | train_accuracy: 0.90267 | valid_logloss: 0.28906 | valid_accuracy: 0.89353 |  0:06:00s\n",
            "epoch 173| loss: 0.26137 | train_logloss: 0.25447 | train_accuracy: 0.9021  | valid_logloss: 0.29099 | valid_accuracy: 0.8909  |  0:06:02s\n",
            "epoch 174| loss: 0.26196 | train_logloss: 0.28009 | train_accuracy: 0.8941  | valid_logloss: 0.31012 | valid_accuracy: 0.88643 |  0:06:04s\n",
            "epoch 175| loss: 0.28314 | train_logloss: 0.26955 | train_accuracy: 0.89743 | valid_logloss: 0.30264 | valid_accuracy: 0.88853 |  0:06:06s\n",
            "epoch 176| loss: 0.2775  | train_logloss: 0.28121 | train_accuracy: 0.89373 | valid_logloss: 0.30877 | valid_accuracy: 0.88533 |  0:06:08s\n",
            "epoch 177| loss: 0.2801  | train_logloss: 0.26671 | train_accuracy: 0.89723 | valid_logloss: 0.29582 | valid_accuracy: 0.8899  |  0:06:11s\n",
            "epoch 178| loss: 0.26902 | train_logloss: 0.2612  | train_accuracy: 0.9007  | valid_logloss: 0.29458 | valid_accuracy: 0.8917  |  0:06:13s\n",
            "epoch 179| loss: 0.26903 | train_logloss: 0.26286 | train_accuracy: 0.898   | valid_logloss: 0.29736 | valid_accuracy: 0.8887  |  0:06:15s\n",
            "epoch 180| loss: 0.26719 | train_logloss: 0.25901 | train_accuracy: 0.90017 | valid_logloss: 0.29406 | valid_accuracy: 0.8912  |  0:06:17s\n",
            "epoch 181| loss: 0.26516 | train_logloss: 0.25636 | train_accuracy: 0.90123 | valid_logloss: 0.29198 | valid_accuracy: 0.89053 |  0:06:19s\n",
            "epoch 182| loss: 0.26022 | train_logloss: 0.25286 | train_accuracy: 0.90247 | valid_logloss: 0.29108 | valid_accuracy: 0.89357 |  0:06:21s\n",
            "epoch 183| loss: 0.26044 | train_logloss: 0.25118 | train_accuracy: 0.90273 | valid_logloss: 0.28829 | valid_accuracy: 0.8927  |  0:06:23s\n",
            "epoch 184| loss: 0.264   | train_logloss: 0.25206 | train_accuracy: 0.90033 | valid_logloss: 0.2892  | valid_accuracy: 0.8929  |  0:06:25s\n",
            "epoch 185| loss: 0.25689 | train_logloss: 0.2481  | train_accuracy: 0.90387 | valid_logloss: 0.28753 | valid_accuracy: 0.89397 |  0:06:27s\n",
            "epoch 186| loss: 0.25501 | train_logloss: 0.2509  | train_accuracy: 0.90187 | valid_logloss: 0.29719 | valid_accuracy: 0.8895  |  0:06:29s\n",
            "epoch 187| loss: 0.25228 | train_logloss: 0.2481  | train_accuracy: 0.90263 | valid_logloss: 0.29388 | valid_accuracy: 0.89043 |  0:06:31s\n",
            "epoch 188| loss: 0.25295 | train_logloss: 0.24864 | train_accuracy: 0.9039  | valid_logloss: 0.29817 | valid_accuracy: 0.8883  |  0:06:33s\n",
            "epoch 189| loss: 0.25224 | train_logloss: 0.24536 | train_accuracy: 0.90543 | valid_logloss: 0.29417 | valid_accuracy: 0.8912  |  0:06:36s\n",
            "epoch 190| loss: 0.24679 | train_logloss: 0.24283 | train_accuracy: 0.9056  | valid_logloss: 0.29577 | valid_accuracy: 0.8914  |  0:06:38s\n",
            "epoch 191| loss: 0.24564 | train_logloss: 0.2359  | train_accuracy: 0.91    | valid_logloss: 0.29078 | valid_accuracy: 0.89363 |  0:06:40s\n",
            "epoch 192| loss: 0.24647 | train_logloss: 0.23665 | train_accuracy: 0.9083  | valid_logloss: 0.29731 | valid_accuracy: 0.8927  |  0:06:42s\n",
            "epoch 193| loss: 0.24712 | train_logloss: 0.24261 | train_accuracy: 0.90623 | valid_logloss: 0.29441 | valid_accuracy: 0.89187 |  0:06:44s\n",
            "epoch 194| loss: 0.24942 | train_logloss: 0.23543 | train_accuracy: 0.90793 | valid_logloss: 0.29361 | valid_accuracy: 0.89403 |  0:06:46s\n",
            "epoch 195| loss: 0.2509  | train_logloss: 0.23934 | train_accuracy: 0.90807 | valid_logloss: 0.29257 | valid_accuracy: 0.89247 |  0:06:48s\n",
            "epoch 196| loss: 0.24785 | train_logloss: 0.23724 | train_accuracy: 0.9078  | valid_logloss: 0.29231 | valid_accuracy: 0.89123 |  0:06:50s\n",
            "epoch 197| loss: 0.24162 | train_logloss: 0.23153 | train_accuracy: 0.91013 | valid_logloss: 0.29093 | valid_accuracy: 0.8935  |  0:06:53s\n",
            "epoch 198| loss: 0.24069 | train_logloss: 0.23217 | train_accuracy: 0.90887 | valid_logloss: 0.29476 | valid_accuracy: 0.89237 |  0:06:55s\n",
            "epoch 199| loss: 0.23813 | train_logloss: 0.23101 | train_accuracy: 0.9102  | valid_logloss: 0.29121 | valid_accuracy: 0.8928  |  0:06:57s\n",
            "epoch 200| loss: 0.236   | train_logloss: 0.23425 | train_accuracy: 0.9072  | valid_logloss: 0.30402 | valid_accuracy: 0.89033 |  0:06:59s\n",
            "epoch 201| loss: 0.23545 | train_logloss: 0.22367 | train_accuracy: 0.91347 | valid_logloss: 0.2987  | valid_accuracy: 0.89243 |  0:07:01s\n",
            "epoch 202| loss: 0.23371 | train_logloss: 0.22389 | train_accuracy: 0.9118  | valid_logloss: 0.29881 | valid_accuracy: 0.89323 |  0:07:03s\n",
            "epoch 203| loss: 0.23213 | train_logloss: 0.22945 | train_accuracy: 0.91087 | valid_logloss: 0.30477 | valid_accuracy: 0.893   |  0:07:05s\n",
            "epoch 204| loss: 0.23492 | train_logloss: 0.22034 | train_accuracy: 0.9146  | valid_logloss: 0.30341 | valid_accuracy: 0.8917  |  0:07:07s\n",
            "epoch 205| loss: 0.24398 | train_logloss: 0.24735 | train_accuracy: 0.90433 | valid_logloss: 0.31627 | valid_accuracy: 0.8895  |  0:07:09s\n",
            "epoch 206| loss: 0.24848 | train_logloss: 0.23563 | train_accuracy: 0.90673 | valid_logloss: 0.30583 | valid_accuracy: 0.89053 |  0:07:11s\n",
            "epoch 207| loss: 0.2445  | train_logloss: 0.23135 | train_accuracy: 0.9093  | valid_logloss: 0.30015 | valid_accuracy: 0.89173 |  0:07:13s\n",
            "epoch 208| loss: 0.24194 | train_logloss: 0.23072 | train_accuracy: 0.91037 | valid_logloss: 0.29977 | valid_accuracy: 0.89443 |  0:07:15s\n",
            "epoch 209| loss: 0.23623 | train_logloss: 0.22688 | train_accuracy: 0.90993 | valid_logloss: 0.30017 | valid_accuracy: 0.8932  |  0:07:17s\n",
            "epoch 210| loss: 0.23263 | train_logloss: 0.22281 | train_accuracy: 0.91263 | valid_logloss: 0.30135 | valid_accuracy: 0.89123 |  0:07:20s\n",
            "epoch 211| loss: 0.23171 | train_logloss: 0.22432 | train_accuracy: 0.9119  | valid_logloss: 0.30313 | valid_accuracy: 0.891   |  0:07:22s\n",
            "epoch 212| loss: 0.23032 | train_logloss: 0.21933 | train_accuracy: 0.91287 | valid_logloss: 0.30361 | valid_accuracy: 0.89157 |  0:07:24s\n",
            "epoch 213| loss: 0.23031 | train_logloss: 0.22074 | train_accuracy: 0.91277 | valid_logloss: 0.30375 | valid_accuracy: 0.89407 |  0:07:26s\n",
            "epoch 214| loss: 0.2277  | train_logloss: 0.21843 | train_accuracy: 0.9141  | valid_logloss: 0.30805 | valid_accuracy: 0.8914  |  0:07:28s\n",
            "epoch 215| loss: 0.22373 | train_logloss: 0.21538 | train_accuracy: 0.91513 | valid_logloss: 0.31386 | valid_accuracy: 0.88917 |  0:07:30s\n",
            "epoch 216| loss: 0.22473 | train_logloss: 0.21715 | train_accuracy: 0.91377 | valid_logloss: 0.3145  | valid_accuracy: 0.8892  |  0:07:32s\n",
            "epoch 217| loss: 0.22827 | train_logloss: 0.21372 | train_accuracy: 0.91643 | valid_logloss: 0.30965 | valid_accuracy: 0.88973 |  0:07:34s\n",
            "epoch 218| loss: 0.22132 | train_logloss: 0.2131  | train_accuracy: 0.91497 | valid_logloss: 0.31747 | valid_accuracy: 0.89127 |  0:07:36s\n",
            "epoch 219| loss: 0.21944 | train_logloss: 0.20609 | train_accuracy: 0.9177  | valid_logloss: 0.31435 | valid_accuracy: 0.89037 |  0:07:38s\n",
            "epoch 220| loss: 0.22325 | train_logloss: 0.25469 | train_accuracy: 0.90013 | valid_logloss: 0.34326 | valid_accuracy: 0.88357 |  0:07:41s\n",
            "epoch 221| loss: 0.25618 | train_logloss: 0.23744 | train_accuracy: 0.90403 | valid_logloss: 0.30487 | valid_accuracy: 0.89177 |  0:07:43s\n",
            "epoch 222| loss: 0.24636 | train_logloss: 0.23373 | train_accuracy: 0.908   | valid_logloss: 0.30772 | valid_accuracy: 0.89077 |  0:07:45s\n",
            "epoch 223| loss: 0.23895 | train_logloss: 0.22712 | train_accuracy: 0.90907 | valid_logloss: 0.30663 | valid_accuracy: 0.89027 |  0:07:47s\n",
            "epoch 224| loss: 0.23167 | train_logloss: 0.21728 | train_accuracy: 0.9136  | valid_logloss: 0.3031  | valid_accuracy: 0.89247 |  0:07:49s\n",
            "epoch 225| loss: 0.22932 | train_logloss: 0.21897 | train_accuracy: 0.91353 | valid_logloss: 0.31613 | valid_accuracy: 0.8901  |  0:07:51s\n",
            "epoch 226| loss: 0.22272 | train_logloss: 0.21951 | train_accuracy: 0.91327 | valid_logloss: 0.31793 | valid_accuracy: 0.8884  |  0:07:53s\n",
            "epoch 227| loss: 0.21932 | train_logloss: 0.21281 | train_accuracy: 0.9173  | valid_logloss: 0.31855 | valid_accuracy: 0.88797 |  0:07:55s\n",
            "epoch 228| loss: 0.21524 | train_logloss: 0.20879 | train_accuracy: 0.91597 | valid_logloss: 0.31724 | valid_accuracy: 0.88817 |  0:07:57s\n",
            "epoch 229| loss: 0.21681 | train_logloss: 0.2075  | train_accuracy: 0.918   | valid_logloss: 0.32505 | valid_accuracy: 0.89253 |  0:07:59s\n",
            "epoch 230| loss: 0.24419 | train_logloss: 0.24991 | train_accuracy: 0.9045  | valid_logloss: 0.32561 | valid_accuracy: 0.8862  |  0:08:02s\n",
            "epoch 231| loss: 0.24831 | train_logloss: 0.23161 | train_accuracy: 0.90983 | valid_logloss: 0.31029 | valid_accuracy: 0.8901  |  0:08:04s\n",
            "epoch 232| loss: 0.23591 | train_logloss: 0.22841 | train_accuracy: 0.91013 | valid_logloss: 0.3116  | valid_accuracy: 0.892   |  0:08:06s\n",
            "epoch 233| loss: 0.23757 | train_logloss: 0.22148 | train_accuracy: 0.91327 | valid_logloss: 0.30676 | valid_accuracy: 0.8927  |  0:08:08s\n",
            "epoch 234| loss: 0.23179 | train_logloss: 0.22695 | train_accuracy: 0.91117 | valid_logloss: 0.32232 | valid_accuracy: 0.88613 |  0:08:10s\n",
            "epoch 235| loss: 0.22618 | train_logloss: 0.21199 | train_accuracy: 0.9166  | valid_logloss: 0.30933 | valid_accuracy: 0.89197 |  0:08:12s\n",
            "epoch 236| loss: 0.22226 | train_logloss: 0.20929 | train_accuracy: 0.91553 | valid_logloss: 0.31122 | valid_accuracy: 0.89213 |  0:08:14s\n",
            "epoch 237| loss: 0.21925 | train_logloss: 0.20558 | train_accuracy: 0.9175  | valid_logloss: 0.31463 | valid_accuracy: 0.89163 |  0:08:16s\n",
            "epoch 238| loss: 0.21591 | train_logloss: 0.20339 | train_accuracy: 0.92    | valid_logloss: 0.31255 | valid_accuracy: 0.89047 |  0:08:18s\n",
            "epoch 239| loss: 0.22032 | train_logloss: 0.24073 | train_accuracy: 0.90577 | valid_logloss: 0.33038 | valid_accuracy: 0.88833 |  0:08:20s\n",
            "epoch 240| loss: 0.2453  | train_logloss: 0.22893 | train_accuracy: 0.9097  | valid_logloss: 0.31663 | valid_accuracy: 0.88887 |  0:08:22s\n",
            "epoch 241| loss: 0.23556 | train_logloss: 0.21789 | train_accuracy: 0.91427 | valid_logloss: 0.30881 | valid_accuracy: 0.8907  |  0:08:24s\n",
            "epoch 242| loss: 0.2274  | train_logloss: 0.21681 | train_accuracy: 0.91207 | valid_logloss: 0.319   | valid_accuracy: 0.8878  |  0:08:26s\n",
            "epoch 243| loss: 0.24234 | train_logloss: 0.26277 | train_accuracy: 0.89923 | valid_logloss: 0.32856 | valid_accuracy: 0.8859  |  0:08:29s\n",
            "epoch 244| loss: 0.26346 | train_logloss: 0.2479  | train_accuracy: 0.90373 | valid_logloss: 0.31157 | valid_accuracy: 0.88993 |  0:08:31s\n",
            "epoch 245| loss: 0.24955 | train_logloss: 0.23455 | train_accuracy: 0.9096  | valid_logloss: 0.30512 | valid_accuracy: 0.8899  |  0:08:33s\n",
            "epoch 246| loss: 0.23889 | train_logloss: 0.2233  | train_accuracy: 0.9132  | valid_logloss: 0.29838 | valid_accuracy: 0.8923  |  0:08:35s\n",
            "epoch 247| loss: 0.23466 | train_logloss: 0.22327 | train_accuracy: 0.91243 | valid_logloss: 0.31478 | valid_accuracy: 0.88853 |  0:08:37s\n",
            "epoch 248| loss: 0.22791 | train_logloss: 0.21368 | train_accuracy: 0.91663 | valid_logloss: 0.31038 | valid_accuracy: 0.8899  |  0:08:39s\n",
            "epoch 249| loss: 0.22244 | train_logloss: 0.20787 | train_accuracy: 0.9177  | valid_logloss: 0.30911 | valid_accuracy: 0.8915  |  0:08:41s\n",
            "epoch 250| loss: 0.21771 | train_logloss: 0.20539 | train_accuracy: 0.91883 | valid_logloss: 0.31998 | valid_accuracy: 0.88977 |  0:08:43s\n",
            "epoch 251| loss: 0.22188 | train_logloss: 0.24465 | train_accuracy: 0.90633 | valid_logloss: 0.32946 | valid_accuracy: 0.88493 |  0:08:45s\n",
            "epoch 252| loss: 0.25173 | train_logloss: 0.23331 | train_accuracy: 0.91153 | valid_logloss: 0.30991 | valid_accuracy: 0.89183 |  0:08:47s\n",
            "epoch 253| loss: 0.23553 | train_logloss: 0.22184 | train_accuracy: 0.914   | valid_logloss: 0.30322 | valid_accuracy: 0.89253 |  0:08:49s\n",
            "epoch 254| loss: 0.23044 | train_logloss: 0.21712 | train_accuracy: 0.91453 | valid_logloss: 0.3112  | valid_accuracy: 0.8914  |  0:08:51s\n",
            "epoch 255| loss: 0.2264  | train_logloss: 0.21344 | train_accuracy: 0.9164  | valid_logloss: 0.31918 | valid_accuracy: 0.89043 |  0:08:53s\n",
            "epoch 256| loss: 0.22058 | train_logloss: 0.20784 | train_accuracy: 0.91807 | valid_logloss: 0.3181  | valid_accuracy: 0.88953 |  0:08:55s\n",
            "epoch 257| loss: 0.21392 | train_logloss: 0.20003 | train_accuracy: 0.92127 | valid_logloss: 0.31966 | valid_accuracy: 0.8913  |  0:08:57s\n",
            "epoch 258| loss: 0.21075 | train_logloss: 0.19757 | train_accuracy: 0.92163 | valid_logloss: 0.33211 | valid_accuracy: 0.8894  |  0:08:59s\n",
            "\n",
            "Early stopping occurred at epoch 258 with best_epoch = 208 and best_valid_accuracy = 0.89443\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[408173  13469  58540]\n",
            " [ 14337 951841  33822]\n",
            " [ 24368  28870 379349]]\n",
            "Testing Score:  0.8944333333333333\n",
            "Confusion Matrix: \n",
            " [[419412  12587  48183]\n",
            " [ 13732 954252  32016]\n",
            " [ 35048  28128 369411]]\n",
            "Testing Score:  0.8917666666666667\n",
            "{'Rows': 30000, 'Nd': 128, 'Na': 128, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 539.9478311538696, 'time_tn': 28.425750255584717, 'accuracy_tn': 0.8944333333333333, 'time_learn_gb': 7.857594013214111, 'time_gb': 27.867676973342896, 'accuracy_gb': 0.8917666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wP-3LQlWeXC",
        "outputId": "816bff5f-26b8-4608-fc70-2d5c92507a9c"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...  55.350941    0.883778      3.483706  16.722444\n",
              "1   9000   9000   9000   16  ...  61.207308    0.883778      3.464101  17.349063\n",
              "2   9000   9000   9000   64  ...  90.066453    0.883778      3.411332  16.978554\n",
              "3   9000   9000   9000   32  ...  77.371469    0.883778      3.509229  17.331297\n",
              "4   9000   9000   9000  128  ...  90.364384    0.883778      3.475400  17.451218\n",
              "5  30000  30000  30000    8  ...  20.273622    0.891767      8.276289  27.093480\n",
              "6  30000  30000  30000   16  ...  22.630230    0.891767      8.210687  27.343578\n",
              "7  30000  30000  30000   64  ...  30.078117    0.891767      8.244918  28.102216\n",
              "8  30000  30000  30000   32  ...  26.655121    0.891767      8.093423  27.197022\n",
              "9  30000  30000  30000  128  ...  28.425750    0.891767      7.857594  27.867677\n",
              "\n",
              "[10 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QGMDAZ-rEW"
      },
      "source": [
        "##300000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4rLckKpRWYR",
        "outputId": "f85b378b-3756-4414-c85c-d95d7dbe3444"
      },
      "source": [
        "time_model(number_exp=11, \n",
        "     Rows=300000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.39079 | train_logloss: 2.59139 | train_accuracy: 0.24259 | valid_logloss: 2.58263 | valid_accuracy: 0.24365 |  0:00:12s\n",
            "epoch 1  | loss: 0.31362 | train_logloss: 0.42557 | train_accuracy: 0.84944 | valid_logloss: 0.42434 | valid_accuracy: 0.84993 |  0:00:25s\n",
            "epoch 2  | loss: 0.30036 | train_logloss: 0.29473 | train_accuracy: 0.88865 | valid_logloss: 0.29652 | valid_accuracy: 0.88872 |  0:00:38s\n",
            "epoch 3  | loss: 0.29274 | train_logloss: 0.28379 | train_accuracy: 0.89345 | valid_logloss: 0.28558 | valid_accuracy: 0.89349 |  0:00:51s\n",
            "epoch 4  | loss: 0.28983 | train_logloss: 0.28599 | train_accuracy: 0.89053 | valid_logloss: 0.28848 | valid_accuracy: 0.89021 |  0:01:04s\n",
            "epoch 5  | loss: 0.28811 | train_logloss: 0.28661 | train_accuracy: 0.89028 | valid_logloss: 0.28839 | valid_accuracy: 0.89032 |  0:01:17s\n",
            "epoch 6  | loss: 0.28422 | train_logloss: 0.27525 | train_accuracy: 0.89517 | valid_logloss: 0.27846 | valid_accuracy: 0.89443 |  0:01:30s\n",
            "epoch 7  | loss: 0.28055 | train_logloss: 0.27252 | train_accuracy: 0.89595 | valid_logloss: 0.27606 | valid_accuracy: 0.89496 |  0:01:43s\n",
            "epoch 8  | loss: 0.27862 | train_logloss: 0.27415 | train_accuracy: 0.89536 | valid_logloss: 0.27781 | valid_accuracy: 0.89433 |  0:01:56s\n",
            "epoch 9  | loss: 0.27742 | train_logloss: 0.27102 | train_accuracy: 0.89732 | valid_logloss: 0.27464 | valid_accuracy: 0.89624 |  0:02:09s\n",
            "epoch 10 | loss: 0.27591 | train_logloss: 0.2685  | train_accuracy: 0.89783 | valid_logloss: 0.27346 | valid_accuracy: 0.89733 |  0:02:22s\n",
            "epoch 11 | loss: 0.27525 | train_logloss: 0.26915 | train_accuracy: 0.8971  | valid_logloss: 0.27334 | valid_accuracy: 0.89587 |  0:02:35s\n",
            "epoch 12 | loss: 0.27484 | train_logloss: 0.27059 | train_accuracy: 0.89666 | valid_logloss: 0.27441 | valid_accuracy: 0.89568 |  0:02:48s\n",
            "epoch 13 | loss: 0.27285 | train_logloss: 0.26876 | train_accuracy: 0.89738 | valid_logloss: 0.27349 | valid_accuracy: 0.89619 |  0:03:01s\n",
            "epoch 14 | loss: 0.273   | train_logloss: 0.26925 | train_accuracy: 0.89768 | valid_logloss: 0.27319 | valid_accuracy: 0.89679 |  0:03:13s\n",
            "epoch 15 | loss: 0.27229 | train_logloss: 0.26834 | train_accuracy: 0.89733 | valid_logloss: 0.27247 | valid_accuracy: 0.89688 |  0:03:27s\n",
            "epoch 16 | loss: 0.27177 | train_logloss: 0.26637 | train_accuracy: 0.8991  | valid_logloss: 0.27143 | valid_accuracy: 0.89783 |  0:03:39s\n",
            "epoch 17 | loss: 0.2715  | train_logloss: 0.26683 | train_accuracy: 0.89876 | valid_logloss: 0.27138 | valid_accuracy: 0.89776 |  0:03:52s\n",
            "epoch 18 | loss: 0.27183 | train_logloss: 0.26562 | train_accuracy: 0.89899 | valid_logloss: 0.27049 | valid_accuracy: 0.89761 |  0:04:05s\n",
            "epoch 19 | loss: 0.2756  | train_logloss: 0.26647 | train_accuracy: 0.89855 | valid_logloss: 0.27082 | valid_accuracy: 0.89734 |  0:04:18s\n",
            "epoch 20 | loss: 0.27181 | train_logloss: 0.26546 | train_accuracy: 0.89904 | valid_logloss: 0.27148 | valid_accuracy: 0.89753 |  0:04:31s\n",
            "epoch 21 | loss: 0.27517 | train_logloss: 0.2655  | train_accuracy: 0.8992  | valid_logloss: 0.27017 | valid_accuracy: 0.89798 |  0:04:44s\n",
            "epoch 22 | loss: 0.27058 | train_logloss: 0.26747 | train_accuracy: 0.89785 | valid_logloss: 0.27356 | valid_accuracy: 0.89713 |  0:04:57s\n",
            "epoch 23 | loss: 0.26962 | train_logloss: 0.26425 | train_accuracy: 0.89977 | valid_logloss: 0.26975 | valid_accuracy: 0.89814 |  0:05:10s\n",
            "epoch 24 | loss: 0.26893 | train_logloss: 0.2627  | train_accuracy: 0.90007 | valid_logloss: 0.26856 | valid_accuracy: 0.89855 |  0:05:23s\n",
            "epoch 25 | loss: 0.26824 | train_logloss: 0.26702 | train_accuracy: 0.89873 | valid_logloss: 0.27241 | valid_accuracy: 0.89747 |  0:05:35s\n",
            "epoch 26 | loss: 0.26829 | train_logloss: 0.26277 | train_accuracy: 0.90032 | valid_logloss: 0.2684  | valid_accuracy: 0.89865 |  0:05:48s\n",
            "epoch 27 | loss: 0.26806 | train_logloss: 0.26469 | train_accuracy: 0.89956 | valid_logloss: 0.27079 | valid_accuracy: 0.89852 |  0:06:01s\n",
            "epoch 28 | loss: 0.26724 | train_logloss: 0.26401 | train_accuracy: 0.89936 | valid_logloss: 0.26984 | valid_accuracy: 0.89767 |  0:06:14s\n",
            "epoch 29 | loss: 0.26693 | train_logloss: 0.26244 | train_accuracy: 0.90024 | valid_logloss: 0.26814 | valid_accuracy: 0.89844 |  0:06:27s\n",
            "epoch 30 | loss: 0.26693 | train_logloss: 0.26178 | train_accuracy: 0.90055 | valid_logloss: 0.26839 | valid_accuracy: 0.89847 |  0:06:40s\n",
            "epoch 31 | loss: 0.26614 | train_logloss: 0.26477 | train_accuracy: 0.89864 | valid_logloss: 0.27103 | valid_accuracy: 0.89689 |  0:06:53s\n",
            "epoch 32 | loss: 0.26728 | train_logloss: 0.26689 | train_accuracy: 0.89895 | valid_logloss: 0.27324 | valid_accuracy: 0.89747 |  0:07:05s\n",
            "epoch 33 | loss: 0.26646 | train_logloss: 0.26323 | train_accuracy: 0.8997  | valid_logloss: 0.26957 | valid_accuracy: 0.89809 |  0:07:18s\n",
            "epoch 34 | loss: 0.26693 | train_logloss: 0.26147 | train_accuracy: 0.9007  | valid_logloss: 0.2676  | valid_accuracy: 0.89913 |  0:07:31s\n",
            "epoch 35 | loss: 0.26685 | train_logloss: 0.26208 | train_accuracy: 0.90035 | valid_logloss: 0.26881 | valid_accuracy: 0.89846 |  0:07:44s\n",
            "epoch 36 | loss: 0.26504 | train_logloss: 0.26114 | train_accuracy: 0.90025 | valid_logloss: 0.2683  | valid_accuracy: 0.89824 |  0:07:57s\n",
            "epoch 37 | loss: 0.265   | train_logloss: 0.25757 | train_accuracy: 0.90185 | valid_logloss: 0.26464 | valid_accuracy: 0.89979 |  0:08:10s\n",
            "epoch 38 | loss: 0.26442 | train_logloss: 0.26127 | train_accuracy: 0.90073 | valid_logloss: 0.2676  | valid_accuracy: 0.89907 |  0:08:23s\n",
            "epoch 39 | loss: 0.26476 | train_logloss: 0.26026 | train_accuracy: 0.90081 | valid_logloss: 0.26749 | valid_accuracy: 0.89861 |  0:08:36s\n",
            "epoch 40 | loss: 0.26456 | train_logloss: 0.26015 | train_accuracy: 0.90044 | valid_logloss: 0.267   | valid_accuracy: 0.89861 |  0:08:49s\n",
            "epoch 41 | loss: 0.26599 | train_logloss: 0.26287 | train_accuracy: 0.89964 | valid_logloss: 0.26876 | valid_accuracy: 0.89754 |  0:09:02s\n",
            "epoch 42 | loss: 0.26668 | train_logloss: 0.26193 | train_accuracy: 0.90058 | valid_logloss: 0.26827 | valid_accuracy: 0.89852 |  0:09:14s\n",
            "epoch 43 | loss: 0.26502 | train_logloss: 0.26365 | train_accuracy: 0.89979 | valid_logloss: 0.27052 | valid_accuracy: 0.89733 |  0:09:27s\n",
            "epoch 44 | loss: 0.26487 | train_logloss: 0.26327 | train_accuracy: 0.89872 | valid_logloss: 0.2698  | valid_accuracy: 0.89694 |  0:09:40s\n",
            "epoch 45 | loss: 0.26506 | train_logloss: 0.25981 | train_accuracy: 0.9006  | valid_logloss: 0.26604 | valid_accuracy: 0.89896 |  0:09:53s\n",
            "epoch 46 | loss: 0.2635  | train_logloss: 0.26256 | train_accuracy: 0.89974 | valid_logloss: 0.26908 | valid_accuracy: 0.89816 |  0:10:06s\n",
            "epoch 47 | loss: 0.26399 | train_logloss: 0.25955 | train_accuracy: 0.90118 | valid_logloss: 0.2678  | valid_accuracy: 0.89898 |  0:10:19s\n",
            "epoch 48 | loss: 0.26328 | train_logloss: 0.26219 | train_accuracy: 0.90024 | valid_logloss: 0.26803 | valid_accuracy: 0.8987  |  0:10:32s\n",
            "epoch 49 | loss: 0.26381 | train_logloss: 0.25723 | train_accuracy: 0.90198 | valid_logloss: 0.26474 | valid_accuracy: 0.89958 |  0:10:45s\n",
            "epoch 50 | loss: 0.26378 | train_logloss: 0.25898 | train_accuracy: 0.901   | valid_logloss: 0.26564 | valid_accuracy: 0.89903 |  0:10:58s\n",
            "epoch 51 | loss: 0.26487 | train_logloss: 0.26269 | train_accuracy: 0.90014 | valid_logloss: 0.26955 | valid_accuracy: 0.8985  |  0:11:11s\n",
            "epoch 52 | loss: 0.2648  | train_logloss: 0.25968 | train_accuracy: 0.9011  | valid_logloss: 0.26695 | valid_accuracy: 0.899   |  0:11:23s\n",
            "epoch 53 | loss: 0.26312 | train_logloss: 0.26093 | train_accuracy: 0.90068 | valid_logloss: 0.2673  | valid_accuracy: 0.89897 |  0:11:36s\n",
            "epoch 54 | loss: 0.26313 | train_logloss: 0.25587 | train_accuracy: 0.90273 | valid_logloss: 0.26433 | valid_accuracy: 0.90062 |  0:11:49s\n",
            "epoch 55 | loss: 0.26228 | train_logloss: 0.25735 | train_accuracy: 0.90192 | valid_logloss: 0.26406 | valid_accuracy: 0.89995 |  0:12:02s\n",
            "epoch 56 | loss: 0.26261 | train_logloss: 0.26107 | train_accuracy: 0.90033 | valid_logloss: 0.26764 | valid_accuracy: 0.89818 |  0:12:15s\n",
            "epoch 57 | loss: 0.26241 | train_logloss: 0.25844 | train_accuracy: 0.90138 | valid_logloss: 0.26623 | valid_accuracy: 0.899   |  0:12:28s\n",
            "epoch 58 | loss: 0.26224 | train_logloss: 0.25958 | train_accuracy: 0.9009  | valid_logloss: 0.26653 | valid_accuracy: 0.89873 |  0:12:41s\n",
            "epoch 59 | loss: 0.26366 | train_logloss: 0.25969 | train_accuracy: 0.9009  | valid_logloss: 0.26701 | valid_accuracy: 0.899   |  0:12:54s\n",
            "epoch 60 | loss: 0.2632  | train_logloss: 0.25851 | train_accuracy: 0.90117 | valid_logloss: 0.26641 | valid_accuracy: 0.89907 |  0:13:08s\n",
            "epoch 61 | loss: 0.26201 | train_logloss: 0.2575  | train_accuracy: 0.90207 | valid_logloss: 0.26585 | valid_accuracy: 0.89972 |  0:13:21s\n",
            "epoch 62 | loss: 0.26172 | train_logloss: 0.25843 | train_accuracy: 0.90164 | valid_logloss: 0.26618 | valid_accuracy: 0.8995  |  0:13:34s\n",
            "epoch 63 | loss: 0.26592 | train_logloss: 0.26643 | train_accuracy: 0.89845 | valid_logloss: 0.27209 | valid_accuracy: 0.89743 |  0:13:47s\n",
            "epoch 64 | loss: 0.2648  | train_logloss: 0.25942 | train_accuracy: 0.90122 | valid_logloss: 0.26651 | valid_accuracy: 0.89954 |  0:14:00s\n",
            "epoch 65 | loss: 0.26283 | train_logloss: 0.26083 | train_accuracy: 0.9011  | valid_logloss: 0.26781 | valid_accuracy: 0.89927 |  0:14:13s\n",
            "epoch 66 | loss: 0.26247 | train_logloss: 0.25963 | train_accuracy: 0.90068 | valid_logloss: 0.26769 | valid_accuracy: 0.89831 |  0:14:25s\n",
            "epoch 67 | loss: 0.26293 | train_logloss: 0.25872 | train_accuracy: 0.90199 | valid_logloss: 0.26591 | valid_accuracy: 0.89993 |  0:14:38s\n",
            "epoch 68 | loss: 0.26222 | train_logloss: 0.25719 | train_accuracy: 0.9024  | valid_logloss: 0.26531 | valid_accuracy: 0.89944 |  0:14:52s\n",
            "epoch 69 | loss: 0.26162 | train_logloss: 0.26071 | train_accuracy: 0.90085 | valid_logloss: 0.2675  | valid_accuracy: 0.89877 |  0:15:05s\n",
            "epoch 70 | loss: 0.26067 | train_logloss: 0.25774 | train_accuracy: 0.90184 | valid_logloss: 0.26584 | valid_accuracy: 0.89911 |  0:15:18s\n",
            "epoch 71 | loss: 0.26012 | train_logloss: 0.25804 | train_accuracy: 0.90132 | valid_logloss: 0.26605 | valid_accuracy: 0.89928 |  0:15:31s\n",
            "epoch 72 | loss: 0.2597  | train_logloss: 0.25827 | train_accuracy: 0.90123 | valid_logloss: 0.26717 | valid_accuracy: 0.8986  |  0:15:44s\n",
            "epoch 73 | loss: 0.26006 | train_logloss: 0.2555  | train_accuracy: 0.90233 | valid_logloss: 0.2642  | valid_accuracy: 0.89955 |  0:15:57s\n",
            "epoch 74 | loss: 0.26035 | train_logloss: 0.25537 | train_accuracy: 0.90285 | valid_logloss: 0.2649  | valid_accuracy: 0.89988 |  0:16:10s\n",
            "epoch 75 | loss: 0.26029 | train_logloss: 0.2587  | train_accuracy: 0.90139 | valid_logloss: 0.26691 | valid_accuracy: 0.89894 |  0:16:24s\n",
            "epoch 76 | loss: 0.26015 | train_logloss: 0.25482 | train_accuracy: 0.90264 | valid_logloss: 0.26377 | valid_accuracy: 0.90002 |  0:16:37s\n",
            "epoch 77 | loss: 0.25912 | train_logloss: 0.25473 | train_accuracy: 0.90307 | valid_logloss: 0.26375 | valid_accuracy: 0.90039 |  0:16:50s\n",
            "epoch 78 | loss: 0.26016 | train_logloss: 0.25811 | train_accuracy: 0.90175 | valid_logloss: 0.26702 | valid_accuracy: 0.89952 |  0:17:03s\n",
            "epoch 79 | loss: 0.25963 | train_logloss: 0.25729 | train_accuracy: 0.90146 | valid_logloss: 0.26689 | valid_accuracy: 0.89879 |  0:17:16s\n",
            "epoch 80 | loss: 0.25919 | train_logloss: 0.25823 | train_accuracy: 0.90173 | valid_logloss: 0.26803 | valid_accuracy: 0.89897 |  0:17:29s\n",
            "epoch 81 | loss: 0.25909 | train_logloss: 0.25646 | train_accuracy: 0.90226 | valid_logloss: 0.26617 | valid_accuracy: 0.89915 |  0:17:42s\n",
            "epoch 82 | loss: 0.25921 | train_logloss: 0.25669 | train_accuracy: 0.9023  | valid_logloss: 0.26589 | valid_accuracy: 0.89943 |  0:17:56s\n",
            "epoch 83 | loss: 0.25852 | train_logloss: 0.25383 | train_accuracy: 0.90297 | valid_logloss: 0.26323 | valid_accuracy: 0.90036 |  0:18:09s\n",
            "epoch 84 | loss: 0.25963 | train_logloss: 0.25605 | train_accuracy: 0.90195 | valid_logloss: 0.2653  | valid_accuracy: 0.89911 |  0:18:22s\n",
            "epoch 85 | loss: 0.25877 | train_logloss: 0.25998 | train_accuracy: 0.90183 | valid_logloss: 0.26877 | valid_accuracy: 0.89911 |  0:18:35s\n",
            "epoch 86 | loss: 0.25874 | train_logloss: 0.25496 | train_accuracy: 0.90264 | valid_logloss: 0.26362 | valid_accuracy: 0.9001  |  0:18:48s\n",
            "epoch 87 | loss: 0.25878 | train_logloss: 0.25437 | train_accuracy: 0.90322 | valid_logloss: 0.26289 | valid_accuracy: 0.90055 |  0:19:02s\n",
            "epoch 88 | loss: 0.25841 | train_logloss: 0.255   | train_accuracy: 0.90265 | valid_logloss: 0.26375 | valid_accuracy: 0.90003 |  0:19:15s\n",
            "epoch 89 | loss: 0.25865 | train_logloss: 0.25432 | train_accuracy: 0.90325 | valid_logloss: 0.26354 | valid_accuracy: 0.90045 |  0:19:28s\n",
            "epoch 90 | loss: 0.25831 | train_logloss: 0.25613 | train_accuracy: 0.90145 | valid_logloss: 0.26496 | valid_accuracy: 0.89864 |  0:19:41s\n",
            "epoch 91 | loss: 0.25848 | train_logloss: 0.25632 | train_accuracy: 0.90235 | valid_logloss: 0.26633 | valid_accuracy: 0.89939 |  0:19:54s\n",
            "epoch 92 | loss: 0.25825 | train_logloss: 0.25328 | train_accuracy: 0.90377 | valid_logloss: 0.26229 | valid_accuracy: 0.90068 |  0:20:08s\n",
            "epoch 93 | loss: 0.25822 | train_logloss: 0.25348 | train_accuracy: 0.90317 | valid_logloss: 0.26252 | valid_accuracy: 0.90064 |  0:20:21s\n",
            "epoch 94 | loss: 0.25767 | train_logloss: 0.25206 | train_accuracy: 0.90401 | valid_logloss: 0.26215 | valid_accuracy: 0.90117 |  0:20:34s\n",
            "epoch 95 | loss: 0.2574  | train_logloss: 0.25487 | train_accuracy: 0.90244 | valid_logloss: 0.26424 | valid_accuracy: 0.89989 |  0:20:47s\n",
            "epoch 96 | loss: 0.25785 | train_logloss: 0.25605 | train_accuracy: 0.9023  | valid_logloss: 0.26504 | valid_accuracy: 0.89949 |  0:21:00s\n",
            "epoch 97 | loss: 0.25777 | train_logloss: 0.25201 | train_accuracy: 0.90403 | valid_logloss: 0.26195 | valid_accuracy: 0.90062 |  0:21:13s\n",
            "epoch 98 | loss: 0.25863 | train_logloss: 0.25475 | train_accuracy: 0.90289 | valid_logloss: 0.26427 | valid_accuracy: 0.90013 |  0:21:26s\n",
            "epoch 99 | loss: 0.26149 | train_logloss: 0.25711 | train_accuracy: 0.90194 | valid_logloss: 0.26515 | valid_accuracy: 0.89962 |  0:21:39s\n",
            "epoch 100| loss: 0.26088 | train_logloss: 0.25521 | train_accuracy: 0.90336 | valid_logloss: 0.2642  | valid_accuracy: 0.90077 |  0:21:53s\n",
            "epoch 101| loss: 0.25936 | train_logloss: 0.25525 | train_accuracy: 0.90312 | valid_logloss: 0.26405 | valid_accuracy: 0.90099 |  0:22:06s\n",
            "epoch 102| loss: 0.25794 | train_logloss: 0.25365 | train_accuracy: 0.90358 | valid_logloss: 0.26319 | valid_accuracy: 0.9003  |  0:22:19s\n",
            "epoch 103| loss: 0.25826 | train_logloss: 0.25562 | train_accuracy: 0.90292 | valid_logloss: 0.26434 | valid_accuracy: 0.90019 |  0:22:32s\n",
            "epoch 104| loss: 0.2575  | train_logloss: 0.25362 | train_accuracy: 0.90367 | valid_logloss: 0.26293 | valid_accuracy: 0.9006  |  0:22:45s\n",
            "epoch 105| loss: 0.25812 | train_logloss: 0.25924 | train_accuracy: 0.90101 | valid_logloss: 0.26926 | valid_accuracy: 0.89787 |  0:22:58s\n",
            "epoch 106| loss: 0.25783 | train_logloss: 0.25852 | train_accuracy: 0.90158 | valid_logloss: 0.26742 | valid_accuracy: 0.89899 |  0:23:12s\n",
            "epoch 107| loss: 0.25769 | train_logloss: 0.25391 | train_accuracy: 0.90368 | valid_logloss: 0.26362 | valid_accuracy: 0.90083 |  0:23:25s\n",
            "epoch 108| loss: 0.25851 | train_logloss: 0.25245 | train_accuracy: 0.90347 | valid_logloss: 0.26277 | valid_accuracy: 0.90037 |  0:23:38s\n",
            "epoch 109| loss: 0.2572  | train_logloss: 0.25326 | train_accuracy: 0.90385 | valid_logloss: 0.26265 | valid_accuracy: 0.90075 |  0:23:51s\n",
            "epoch 110| loss: 0.25696 | train_logloss: 0.25344 | train_accuracy: 0.90338 | valid_logloss: 0.2632  | valid_accuracy: 0.90019 |  0:24:04s\n",
            "epoch 111| loss: 0.25748 | train_logloss: 0.25273 | train_accuracy: 0.90375 | valid_logloss: 0.26234 | valid_accuracy: 0.90084 |  0:24:18s\n",
            "epoch 112| loss: 0.25688 | train_logloss: 0.2538  | train_accuracy: 0.90266 | valid_logloss: 0.26476 | valid_accuracy: 0.89878 |  0:24:31s\n",
            "epoch 113| loss: 0.25745 | train_logloss: 0.25377 | train_accuracy: 0.90304 | valid_logloss: 0.26351 | valid_accuracy: 0.90009 |  0:24:44s\n",
            "epoch 114| loss: 0.25732 | train_logloss: 0.25173 | train_accuracy: 0.90401 | valid_logloss: 0.26239 | valid_accuracy: 0.90069 |  0:24:57s\n",
            "epoch 115| loss: 0.25665 | train_logloss: 0.25309 | train_accuracy: 0.90307 | valid_logloss: 0.2638  | valid_accuracy: 0.89994 |  0:25:10s\n",
            "epoch 116| loss: 0.25673 | train_logloss: 0.25317 | train_accuracy: 0.90291 | valid_logloss: 0.26461 | valid_accuracy: 0.89954 |  0:25:23s\n",
            "epoch 117| loss: 0.25728 | train_logloss: 0.25613 | train_accuracy: 0.90206 | valid_logloss: 0.26628 | valid_accuracy: 0.89859 |  0:25:36s\n",
            "epoch 118| loss: 0.25645 | train_logloss: 0.25342 | train_accuracy: 0.90367 | valid_logloss: 0.26298 | valid_accuracy: 0.90035 |  0:25:50s\n",
            "epoch 119| loss: 0.25945 | train_logloss: 0.25417 | train_accuracy: 0.90257 | valid_logloss: 0.26448 | valid_accuracy: 0.89973 |  0:26:03s\n",
            "epoch 120| loss: 0.25764 | train_logloss: 0.25538 | train_accuracy: 0.90274 | valid_logloss: 0.26546 | valid_accuracy: 0.89999 |  0:26:16s\n",
            "epoch 121| loss: 0.25679 | train_logloss: 0.25118 | train_accuracy: 0.90411 | valid_logloss: 0.26097 | valid_accuracy: 0.90105 |  0:26:29s\n",
            "epoch 122| loss: 0.25534 | train_logloss: 0.25417 | train_accuracy: 0.90241 | valid_logloss: 0.26536 | valid_accuracy: 0.89931 |  0:26:43s\n",
            "epoch 123| loss: 0.25586 | train_logloss: 0.25444 | train_accuracy: 0.90315 | valid_logloss: 0.26507 | valid_accuracy: 0.89962 |  0:26:56s\n",
            "epoch 124| loss: 0.25577 | train_logloss: 0.25157 | train_accuracy: 0.9039  | valid_logloss: 0.26164 | valid_accuracy: 0.90122 |  0:27:09s\n",
            "epoch 125| loss: 0.25574 | train_logloss: 0.24988 | train_accuracy: 0.90391 | valid_logloss: 0.26111 | valid_accuracy: 0.90109 |  0:27:22s\n",
            "epoch 126| loss: 0.25618 | train_logloss: 0.25279 | train_accuracy: 0.90341 | valid_logloss: 0.2628  | valid_accuracy: 0.90018 |  0:27:35s\n",
            "epoch 127| loss: 0.256   | train_logloss: 0.25482 | train_accuracy: 0.90289 | valid_logloss: 0.26514 | valid_accuracy: 0.89949 |  0:27:48s\n",
            "epoch 128| loss: 0.25594 | train_logloss: 0.25277 | train_accuracy: 0.90306 | valid_logloss: 0.26331 | valid_accuracy: 0.89993 |  0:28:02s\n",
            "epoch 129| loss: 0.25539 | train_logloss: 0.24997 | train_accuracy: 0.90436 | valid_logloss: 0.26083 | valid_accuracy: 0.90112 |  0:28:15s\n",
            "epoch 130| loss: 0.2555  | train_logloss: 0.25035 | train_accuracy: 0.90427 | valid_logloss: 0.26207 | valid_accuracy: 0.90078 |  0:28:28s\n",
            "epoch 131| loss: 0.25626 | train_logloss: 0.25153 | train_accuracy: 0.904   | valid_logloss: 0.26232 | valid_accuracy: 0.90065 |  0:28:41s\n",
            "epoch 132| loss: 0.25595 | train_logloss: 0.2533  | train_accuracy: 0.90396 | valid_logloss: 0.26449 | valid_accuracy: 0.90064 |  0:28:54s\n",
            "epoch 133| loss: 0.25497 | train_logloss: 0.25169 | train_accuracy: 0.90441 | valid_logloss: 0.26376 | valid_accuracy: 0.90081 |  0:29:08s\n",
            "epoch 134| loss: 0.25553 | train_logloss: 0.25139 | train_accuracy: 0.90423 | valid_logloss: 0.26353 | valid_accuracy: 0.90041 |  0:29:21s\n",
            "epoch 135| loss: 0.25515 | train_logloss: 0.25235 | train_accuracy: 0.90337 | valid_logloss: 0.2639  | valid_accuracy: 0.89976 |  0:29:34s\n",
            "epoch 136| loss: 0.25591 | train_logloss: 0.25136 | train_accuracy: 0.90425 | valid_logloss: 0.26251 | valid_accuracy: 0.90085 |  0:29:47s\n",
            "epoch 137| loss: 0.2552  | train_logloss: 0.25142 | train_accuracy: 0.90363 | valid_logloss: 0.26224 | valid_accuracy: 0.90076 |  0:30:00s\n",
            "epoch 138| loss: 0.25658 | train_logloss: 0.25115 | train_accuracy: 0.9037  | valid_logloss: 0.26208 | valid_accuracy: 0.90078 |  0:30:13s\n",
            "epoch 139| loss: 0.25608 | train_logloss: 0.25426 | train_accuracy: 0.90283 | valid_logloss: 0.26674 | valid_accuracy: 0.89934 |  0:30:26s\n",
            "epoch 140| loss: 0.25547 | train_logloss: 0.25201 | train_accuracy: 0.90346 | valid_logloss: 0.26314 | valid_accuracy: 0.9001  |  0:30:39s\n",
            "epoch 141| loss: 0.25558 | train_logloss: 0.25009 | train_accuracy: 0.90449 | valid_logloss: 0.26231 | valid_accuracy: 0.90079 |  0:30:52s\n",
            "epoch 142| loss: 0.25626 | train_logloss: 0.25551 | train_accuracy: 0.90273 | valid_logloss: 0.26683 | valid_accuracy: 0.89937 |  0:31:06s\n",
            "epoch 143| loss: 0.25602 | train_logloss: 0.25065 | train_accuracy: 0.90385 | valid_logloss: 0.26251 | valid_accuracy: 0.90071 |  0:31:19s\n",
            "epoch 144| loss: 0.25599 | train_logloss: 0.2533  | train_accuracy: 0.90297 | valid_logloss: 0.26465 | valid_accuracy: 0.89952 |  0:31:32s\n",
            "epoch 145| loss: 0.25599 | train_logloss: 0.2532  | train_accuracy: 0.9033  | valid_logloss: 0.26418 | valid_accuracy: 0.90047 |  0:31:45s\n",
            "epoch 146| loss: 0.25586 | train_logloss: 0.25397 | train_accuracy: 0.903   | valid_logloss: 0.2649  | valid_accuracy: 0.90018 |  0:31:58s\n",
            "epoch 147| loss: 0.2549  | train_logloss: 0.25316 | train_accuracy: 0.90315 | valid_logloss: 0.26443 | valid_accuracy: 0.89999 |  0:32:11s\n",
            "epoch 148| loss: 0.25528 | train_logloss: 0.2532  | train_accuracy: 0.90315 | valid_logloss: 0.26439 | valid_accuracy: 0.89995 |  0:32:24s\n",
            "epoch 149| loss: 0.25558 | train_logloss: 0.25073 | train_accuracy: 0.90404 | valid_logloss: 0.26282 | valid_accuracy: 0.9011  |  0:32:37s\n",
            "epoch 150| loss: 0.25557 | train_logloss: 0.25125 | train_accuracy: 0.90389 | valid_logloss: 0.26307 | valid_accuracy: 0.90032 |  0:32:50s\n",
            "epoch 151| loss: 0.25543 | train_logloss: 0.25161 | train_accuracy: 0.90419 | valid_logloss: 0.26391 | valid_accuracy: 0.9006  |  0:33:03s\n",
            "epoch 152| loss: 0.25595 | train_logloss: 0.25205 | train_accuracy: 0.90373 | valid_logloss: 0.26337 | valid_accuracy: 0.89999 |  0:33:16s\n",
            "epoch 153| loss: 0.25563 | train_logloss: 0.25063 | train_accuracy: 0.90399 | valid_logloss: 0.26195 | valid_accuracy: 0.90095 |  0:33:30s\n",
            "epoch 154| loss: 0.25569 | train_logloss: 0.2526  | train_accuracy: 0.9035  | valid_logloss: 0.26383 | valid_accuracy: 0.89991 |  0:33:43s\n",
            "epoch 155| loss: 0.2547  | train_logloss: 0.24992 | train_accuracy: 0.90446 | valid_logloss: 0.26193 | valid_accuracy: 0.90113 |  0:33:56s\n",
            "epoch 156| loss: 0.25478 | train_logloss: 0.25124 | train_accuracy: 0.90399 | valid_logloss: 0.26335 | valid_accuracy: 0.9007  |  0:34:09s\n",
            "epoch 157| loss: 0.25541 | train_logloss: 0.25139 | train_accuracy: 0.90414 | valid_logloss: 0.26275 | valid_accuracy: 0.90085 |  0:34:22s\n",
            "epoch 158| loss: 0.25525 | train_logloss: 0.24961 | train_accuracy: 0.90496 | valid_logloss: 0.26188 | valid_accuracy: 0.90127 |  0:34:35s\n",
            "epoch 159| loss: 0.25461 | train_logloss: 0.25324 | train_accuracy: 0.90337 | valid_logloss: 0.26464 | valid_accuracy: 0.89983 |  0:34:48s\n",
            "epoch 160| loss: 0.25503 | train_logloss: 0.25289 | train_accuracy: 0.90382 | valid_logloss: 0.26338 | valid_accuracy: 0.9004  |  0:35:01s\n",
            "epoch 161| loss: 0.25485 | train_logloss: 0.25009 | train_accuracy: 0.90461 | valid_logloss: 0.26109 | valid_accuracy: 0.90126 |  0:35:14s\n",
            "epoch 162| loss: 0.25541 | train_logloss: 0.25217 | train_accuracy: 0.90365 | valid_logloss: 0.26429 | valid_accuracy: 0.90031 |  0:35:27s\n",
            "epoch 163| loss: 0.25483 | train_logloss: 0.2491  | train_accuracy: 0.90511 | valid_logloss: 0.26199 | valid_accuracy: 0.90117 |  0:35:40s\n",
            "epoch 164| loss: 0.25419 | train_logloss: 0.24961 | train_accuracy: 0.90484 | valid_logloss: 0.26185 | valid_accuracy: 0.90154 |  0:35:53s\n",
            "epoch 165| loss: 0.25619 | train_logloss: 0.25407 | train_accuracy: 0.90258 | valid_logloss: 0.2652  | valid_accuracy: 0.89952 |  0:36:06s\n",
            "epoch 166| loss: 0.25572 | train_logloss: 0.25249 | train_accuracy: 0.9034  | valid_logloss: 0.26286 | valid_accuracy: 0.90042 |  0:36:19s\n",
            "epoch 167| loss: 0.25615 | train_logloss: 0.25166 | train_accuracy: 0.90396 | valid_logloss: 0.2628  | valid_accuracy: 0.90074 |  0:36:33s\n",
            "epoch 168| loss: 0.25556 | train_logloss: 0.25239 | train_accuracy: 0.90392 | valid_logloss: 0.26312 | valid_accuracy: 0.90069 |  0:36:45s\n",
            "epoch 169| loss: 0.25464 | train_logloss: 0.2511  | train_accuracy: 0.90385 | valid_logloss: 0.26329 | valid_accuracy: 0.90059 |  0:36:59s\n",
            "epoch 170| loss: 0.25428 | train_logloss: 0.25053 | train_accuracy: 0.90452 | valid_logloss: 0.26127 | valid_accuracy: 0.90125 |  0:37:12s\n",
            "epoch 171| loss: 0.25525 | train_logloss: 0.24979 | train_accuracy: 0.90456 | valid_logloss: 0.26234 | valid_accuracy: 0.90106 |  0:37:24s\n",
            "epoch 172| loss: 0.25431 | train_logloss: 0.25066 | train_accuracy: 0.90397 | valid_logloss: 0.26275 | valid_accuracy: 0.90013 |  0:37:38s\n",
            "epoch 173| loss: 0.25579 | train_logloss: 0.25135 | train_accuracy: 0.90356 | valid_logloss: 0.26307 | valid_accuracy: 0.89995 |  0:37:50s\n",
            "epoch 174| loss: 0.25448 | train_logloss: 0.25051 | train_accuracy: 0.90406 | valid_logloss: 0.26349 | valid_accuracy: 0.90063 |  0:38:03s\n",
            "epoch 175| loss: 0.25436 | train_logloss: 0.25271 | train_accuracy: 0.90372 | valid_logloss: 0.26522 | valid_accuracy: 0.90074 |  0:38:17s\n",
            "epoch 176| loss: 0.25438 | train_logloss: 0.24983 | train_accuracy: 0.90432 | valid_logloss: 0.26219 | valid_accuracy: 0.90116 |  0:38:30s\n",
            "epoch 177| loss: 0.25467 | train_logloss: 0.25064 | train_accuracy: 0.90453 | valid_logloss: 0.26258 | valid_accuracy: 0.90106 |  0:38:43s\n",
            "epoch 178| loss: 0.25478 | train_logloss: 0.25178 | train_accuracy: 0.90359 | valid_logloss: 0.26419 | valid_accuracy: 0.90018 |  0:38:56s\n",
            "epoch 179| loss: 0.25408 | train_logloss: 0.24962 | train_accuracy: 0.90426 | valid_logloss: 0.26129 | valid_accuracy: 0.90083 |  0:39:09s\n",
            "epoch 180| loss: 0.25403 | train_logloss: 0.25415 | train_accuracy: 0.90292 | valid_logloss: 0.265   | valid_accuracy: 0.89983 |  0:39:22s\n",
            "epoch 181| loss: 0.25488 | train_logloss: 0.25292 | train_accuracy: 0.90386 | valid_logloss: 0.26398 | valid_accuracy: 0.90057 |  0:39:35s\n",
            "epoch 182| loss: 0.25516 | train_logloss: 0.25125 | train_accuracy: 0.90399 | valid_logloss: 0.26322 | valid_accuracy: 0.90046 |  0:39:48s\n",
            "epoch 183| loss: 0.25516 | train_logloss: 0.25616 | train_accuracy: 0.90257 | valid_logloss: 0.26707 | valid_accuracy: 0.89906 |  0:40:02s\n",
            "epoch 184| loss: 0.25501 | train_logloss: 0.25243 | train_accuracy: 0.90318 | valid_logloss: 0.26429 | valid_accuracy: 0.90057 |  0:40:15s\n",
            "epoch 185| loss: 0.25499 | train_logloss: 0.25429 | train_accuracy: 0.903   | valid_logloss: 0.2652  | valid_accuracy: 0.89976 |  0:40:28s\n",
            "epoch 186| loss: 0.25426 | train_logloss: 0.25157 | train_accuracy: 0.90352 | valid_logloss: 0.26281 | valid_accuracy: 0.90006 |  0:40:41s\n",
            "epoch 187| loss: 0.25399 | train_logloss: 0.24928 | train_accuracy: 0.9046  | valid_logloss: 0.26079 | valid_accuracy: 0.90126 |  0:40:54s\n",
            "epoch 188| loss: 0.25471 | train_logloss: 0.2495  | train_accuracy: 0.90486 | valid_logloss: 0.26168 | valid_accuracy: 0.90121 |  0:41:07s\n",
            "epoch 189| loss: 0.25492 | train_logloss: 0.24838 | train_accuracy: 0.90495 | valid_logloss: 0.26118 | valid_accuracy: 0.90141 |  0:41:20s\n",
            "epoch 190| loss: 0.25387 | train_logloss: 0.24945 | train_accuracy: 0.90468 | valid_logloss: 0.26098 | valid_accuracy: 0.9014  |  0:41:33s\n",
            "epoch 191| loss: 0.25449 | train_logloss: 0.24859 | train_accuracy: 0.90481 | valid_logloss: 0.26117 | valid_accuracy: 0.90105 |  0:41:46s\n",
            "epoch 192| loss: 0.25435 | train_logloss: 0.25049 | train_accuracy: 0.90425 | valid_logloss: 0.26267 | valid_accuracy: 0.90047 |  0:41:59s\n",
            "epoch 193| loss: 0.25386 | train_logloss: 0.2503  | train_accuracy: 0.90442 | valid_logloss: 0.26307 | valid_accuracy: 0.90046 |  0:42:12s\n",
            "epoch 194| loss: 0.25477 | train_logloss: 0.24976 | train_accuracy: 0.90471 | valid_logloss: 0.26247 | valid_accuracy: 0.90063 |  0:42:25s\n",
            "epoch 195| loss: 0.25361 | train_logloss: 0.24842 | train_accuracy: 0.90517 | valid_logloss: 0.26135 | valid_accuracy: 0.90147 |  0:42:38s\n",
            "epoch 196| loss: 0.25363 | train_logloss: 0.24995 | train_accuracy: 0.90391 | valid_logloss: 0.26397 | valid_accuracy: 0.90019 |  0:42:51s\n",
            "epoch 197| loss: 0.254   | train_logloss: 0.25144 | train_accuracy: 0.9039  | valid_logloss: 0.26524 | valid_accuracy: 0.90008 |  0:43:04s\n",
            "epoch 198| loss: 0.25355 | train_logloss: 0.24798 | train_accuracy: 0.90551 | valid_logloss: 0.26039 | valid_accuracy: 0.90194 |  0:43:17s\n",
            "epoch 199| loss: 0.25508 | train_logloss: 0.24963 | train_accuracy: 0.90507 | valid_logloss: 0.26248 | valid_accuracy: 0.90095 |  0:43:30s\n",
            "epoch 200| loss: 0.25345 | train_logloss: 0.24893 | train_accuracy: 0.90457 | valid_logloss: 0.26197 | valid_accuracy: 0.90117 |  0:43:43s\n",
            "epoch 201| loss: 0.25296 | train_logloss: 0.24871 | train_accuracy: 0.90466 | valid_logloss: 0.26244 | valid_accuracy: 0.90098 |  0:43:56s\n",
            "epoch 202| loss: 0.25347 | train_logloss: 0.24922 | train_accuracy: 0.90464 | valid_logloss: 0.26245 | valid_accuracy: 0.90119 |  0:44:09s\n",
            "epoch 203| loss: 0.25373 | train_logloss: 0.24988 | train_accuracy: 0.9046  | valid_logloss: 0.26189 | valid_accuracy: 0.90138 |  0:44:21s\n",
            "epoch 204| loss: 0.25377 | train_logloss: 0.25062 | train_accuracy: 0.90404 | valid_logloss: 0.26393 | valid_accuracy: 0.9005  |  0:44:34s\n",
            "epoch 205| loss: 0.25311 | train_logloss: 0.2508  | train_accuracy: 0.90392 | valid_logloss: 0.26499 | valid_accuracy: 0.90037 |  0:44:47s\n",
            "epoch 206| loss: 0.25331 | train_logloss: 0.24865 | train_accuracy: 0.90491 | valid_logloss: 0.2628  | valid_accuracy: 0.90098 |  0:45:00s\n",
            "epoch 207| loss: 0.25277 | train_logloss: 0.24793 | train_accuracy: 0.90502 | valid_logloss: 0.26278 | valid_accuracy: 0.90046 |  0:45:13s\n",
            "epoch 208| loss: 0.25298 | train_logloss: 0.24693 | train_accuracy: 0.90569 | valid_logloss: 0.26094 | valid_accuracy: 0.90174 |  0:45:26s\n",
            "epoch 209| loss: 0.25297 | train_logloss: 0.25309 | train_accuracy: 0.90282 | valid_logloss: 0.26544 | valid_accuracy: 0.89959 |  0:45:39s\n",
            "epoch 210| loss: 0.25637 | train_logloss: 0.25209 | train_accuracy: 0.90362 | valid_logloss: 0.26398 | valid_accuracy: 0.90037 |  0:45:52s\n",
            "epoch 211| loss: 0.25362 | train_logloss: 0.25134 | train_accuracy: 0.90343 | valid_logloss: 0.26485 | valid_accuracy: 0.89997 |  0:46:05s\n",
            "epoch 212| loss: 0.25365 | train_logloss: 0.24906 | train_accuracy: 0.90465 | valid_logloss: 0.26135 | valid_accuracy: 0.90109 |  0:46:18s\n",
            "epoch 213| loss: 0.25375 | train_logloss: 0.25475 | train_accuracy: 0.90353 | valid_logloss: 0.26626 | valid_accuracy: 0.89969 |  0:46:31s\n",
            "epoch 214| loss: 0.25328 | train_logloss: 0.24936 | train_accuracy: 0.90459 | valid_logloss: 0.2635  | valid_accuracy: 0.90053 |  0:46:44s\n",
            "epoch 215| loss: 0.25374 | train_logloss: 0.25321 | train_accuracy: 0.90334 | valid_logloss: 0.26561 | valid_accuracy: 0.90033 |  0:46:57s\n",
            "epoch 216| loss: 0.25383 | train_logloss: 0.24936 | train_accuracy: 0.90474 | valid_logloss: 0.26229 | valid_accuracy: 0.90124 |  0:47:10s\n",
            "epoch 217| loss: 0.25366 | train_logloss: 0.25086 | train_accuracy: 0.90365 | valid_logloss: 0.26454 | valid_accuracy: 0.90017 |  0:47:23s\n",
            "epoch 218| loss: 0.25349 | train_logloss: 0.24895 | train_accuracy: 0.90496 | valid_logloss: 0.26219 | valid_accuracy: 0.90112 |  0:47:36s\n",
            "epoch 219| loss: 0.25329 | train_logloss: 0.2504  | train_accuracy: 0.90387 | valid_logloss: 0.26393 | valid_accuracy: 0.9     |  0:47:49s\n",
            "epoch 220| loss: 0.2527  | train_logloss: 0.24703 | train_accuracy: 0.90567 | valid_logloss: 0.26076 | valid_accuracy: 0.90169 |  0:48:02s\n",
            "epoch 221| loss: 0.25255 | train_logloss: 0.24852 | train_accuracy: 0.90494 | valid_logloss: 0.26197 | valid_accuracy: 0.90114 |  0:48:15s\n",
            "epoch 222| loss: 0.25411 | train_logloss: 0.25171 | train_accuracy: 0.90384 | valid_logloss: 0.26561 | valid_accuracy: 0.90009 |  0:48:28s\n",
            "epoch 223| loss: 0.25382 | train_logloss: 0.24931 | train_accuracy: 0.90452 | valid_logloss: 0.2626  | valid_accuracy: 0.90063 |  0:48:42s\n",
            "epoch 224| loss: 0.25365 | train_logloss: 0.25254 | train_accuracy: 0.90378 | valid_logloss: 0.265   | valid_accuracy: 0.90034 |  0:48:55s\n",
            "epoch 225| loss: 0.25326 | train_logloss: 0.25086 | train_accuracy: 0.90376 | valid_logloss: 0.26404 | valid_accuracy: 0.90029 |  0:49:08s\n",
            "epoch 226| loss: 0.25237 | train_logloss: 0.24902 | train_accuracy: 0.90474 | valid_logloss: 0.26314 | valid_accuracy: 0.90084 |  0:49:21s\n",
            "epoch 227| loss: 0.25326 | train_logloss: 0.24853 | train_accuracy: 0.90503 | valid_logloss: 0.26198 | valid_accuracy: 0.90123 |  0:49:34s\n",
            "epoch 228| loss: 0.25294 | train_logloss: 0.24775 | train_accuracy: 0.90509 | valid_logloss: 0.26156 | valid_accuracy: 0.90188 |  0:49:47s\n",
            "epoch 229| loss: 0.25241 | train_logloss: 0.24976 | train_accuracy: 0.90422 | valid_logloss: 0.26375 | valid_accuracy: 0.90043 |  0:50:00s\n",
            "epoch 230| loss: 0.25323 | train_logloss: 0.24784 | train_accuracy: 0.90491 | valid_logloss: 0.26206 | valid_accuracy: 0.90101 |  0:50:13s\n",
            "epoch 231| loss: 0.25402 | train_logloss: 0.25001 | train_accuracy: 0.90456 | valid_logloss: 0.26256 | valid_accuracy: 0.90043 |  0:50:26s\n",
            "epoch 232| loss: 0.2538  | train_logloss: 0.25052 | train_accuracy: 0.90449 | valid_logloss: 0.2642  | valid_accuracy: 0.90101 |  0:50:39s\n",
            "epoch 233| loss: 0.25271 | train_logloss: 0.25078 | train_accuracy: 0.90455 | valid_logloss: 0.26307 | valid_accuracy: 0.90076 |  0:50:52s\n",
            "epoch 234| loss: 0.25366 | train_logloss: 0.25161 | train_accuracy: 0.90327 | valid_logloss: 0.26464 | valid_accuracy: 0.89973 |  0:51:05s\n",
            "epoch 235| loss: 0.25284 | train_logloss: 0.24994 | train_accuracy: 0.90437 | valid_logloss: 0.26422 | valid_accuracy: 0.90058 |  0:51:19s\n",
            "epoch 236| loss: 0.25244 | train_logloss: 0.24901 | train_accuracy: 0.90448 | valid_logloss: 0.26189 | valid_accuracy: 0.90076 |  0:51:32s\n",
            "epoch 237| loss: 0.25271 | train_logloss: 0.24963 | train_accuracy: 0.90422 | valid_logloss: 0.26378 | valid_accuracy: 0.9004  |  0:51:45s\n",
            "epoch 238| loss: 0.2524  | train_logloss: 0.24794 | train_accuracy: 0.90559 | valid_logloss: 0.2623  | valid_accuracy: 0.90154 |  0:51:58s\n",
            "epoch 239| loss: 0.25492 | train_logloss: 0.26107 | train_accuracy: 0.9016  | valid_logloss: 0.27102 | valid_accuracy: 0.89932 |  0:52:11s\n",
            "epoch 240| loss: 0.25841 | train_logloss: 0.25028 | train_accuracy: 0.90366 | valid_logloss: 0.26309 | valid_accuracy: 0.90085 |  0:52:24s\n",
            "epoch 241| loss: 0.25518 | train_logloss: 0.24953 | train_accuracy: 0.9043  | valid_logloss: 0.26229 | valid_accuracy: 0.90093 |  0:52:37s\n",
            "epoch 242| loss: 0.2536  | train_logloss: 0.24909 | train_accuracy: 0.90512 | valid_logloss: 0.26326 | valid_accuracy: 0.90084 |  0:52:50s\n",
            "epoch 243| loss: 0.25323 | train_logloss: 0.25129 | train_accuracy: 0.90413 | valid_logloss: 0.2649  | valid_accuracy: 0.90039 |  0:53:03s\n",
            "epoch 244| loss: 0.25294 | train_logloss: 0.25018 | train_accuracy: 0.90414 | valid_logloss: 0.26339 | valid_accuracy: 0.89996 |  0:53:16s\n",
            "epoch 245| loss: 0.2525  | train_logloss: 0.24886 | train_accuracy: 0.90489 | valid_logloss: 0.26193 | valid_accuracy: 0.90105 |  0:53:29s\n",
            "epoch 246| loss: 0.25227 | train_logloss: 0.24693 | train_accuracy: 0.90542 | valid_logloss: 0.26142 | valid_accuracy: 0.9014  |  0:53:42s\n",
            "epoch 247| loss: 0.25376 | train_logloss: 0.25602 | train_accuracy: 0.90204 | valid_logloss: 0.26736 | valid_accuracy: 0.8987  |  0:53:55s\n",
            "epoch 248| loss: 0.25411 | train_logloss: 0.25036 | train_accuracy: 0.90422 | valid_logloss: 0.26491 | valid_accuracy: 0.8998  |  0:54:08s\n",
            "\n",
            "Early stopping occurred at epoch 248 with best_epoch = 198 and best_valid_accuracy = 0.90194\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[425129  10426  44627]\n",
            " [ 13934 955378  30688]\n",
            " [ 31467  26935 374185]]\n",
            "Testing Score:  0.9019433333333333\n",
            "Confusion Matrix: \n",
            " [[426049  10341  43792]\n",
            " [ 14187 955965  29848]\n",
            " [ 32397  27130 373060]]\n",
            "Testing Score:  0.9015533333333333\n",
            "{'Rows': 300000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 300000, 'test': 300000, 'time_learn_tn': 3252.5826733112335, 'time_tn': 19.763668537139893, 'accuracy_tn': 0.9019433333333333, 'time_learn_gb': 140.28218722343445, 'time_gb': 95.82626795768738, 'accuracy_gb': 0.9015533333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "vyw5vT76WfDW",
        "outputId": "00db4771-ead5-429c-fe29-3e951d58e1a1"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ... accuracy_gb time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...    0.883778      3.483706  16.722444\n",
              "1     9000    9000    9000  ...    0.883778      3.464101  17.349063\n",
              "2     9000    9000    9000  ...    0.883778      3.411332  16.978554\n",
              "3     9000    9000    9000  ...    0.883778      3.509229  17.331297\n",
              "4     9000    9000    9000  ...    0.883778      3.475400  17.451218\n",
              "5    30000   30000   30000  ...    0.891767      8.276289  27.093480\n",
              "6    30000   30000   30000  ...    0.891767      8.210687  27.343578\n",
              "7    30000   30000   30000  ...    0.891767      8.244918  28.102216\n",
              "8    30000   30000   30000  ...    0.891767      8.093423  27.197022\n",
              "9    30000   30000   30000  ...    0.891767      7.857594  27.867677\n",
              "10  300000  300000  300000  ...    0.901553    140.282187  95.826268\n",
              "\n",
              "[11 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtomkZNzRWYZ",
        "outputId": "5b4e8ea3-ef61-45c6-9002-09bc5808ef26"
      },
      "source": [
        "time_model(number_exp=12, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.40504 | train_logloss: 1.40503 | train_accuracy: 0.41059 | valid_logloss: 1.40505 | valid_accuracy: 0.41046 |  0:00:14s\n",
            "epoch 1  | loss: 0.32269 | train_logloss: 0.42897 | train_accuracy: 0.8458  | valid_logloss: 0.42735 | valid_accuracy: 0.84628 |  0:00:28s\n",
            "epoch 2  | loss: 0.30726 | train_logloss: 0.30098 | train_accuracy: 0.88776 | valid_logloss: 0.3033  | valid_accuracy: 0.88717 |  0:00:43s\n",
            "epoch 3  | loss: 0.29507 | train_logloss: 0.28718 | train_accuracy: 0.89144 | valid_logloss: 0.28985 | valid_accuracy: 0.89076 |  0:00:57s\n",
            "epoch 4  | loss: 0.29087 | train_logloss: 0.28015 | train_accuracy: 0.8941  | valid_logloss: 0.28348 | valid_accuracy: 0.89389 |  0:01:12s\n",
            "epoch 5  | loss: 0.28837 | train_logloss: 0.28311 | train_accuracy: 0.89382 | valid_logloss: 0.28657 | valid_accuracy: 0.89334 |  0:01:26s\n",
            "epoch 6  | loss: 0.2863  | train_logloss: 0.27793 | train_accuracy: 0.89406 | valid_logloss: 0.28113 | valid_accuracy: 0.89375 |  0:01:41s\n",
            "epoch 7  | loss: 0.28258 | train_logloss: 0.27593 | train_accuracy: 0.89406 | valid_logloss: 0.27931 | valid_accuracy: 0.89365 |  0:01:55s\n",
            "epoch 8  | loss: 0.27844 | train_logloss: 0.27408 | train_accuracy: 0.89589 | valid_logloss: 0.27893 | valid_accuracy: 0.89467 |  0:02:09s\n",
            "epoch 9  | loss: 0.27562 | train_logloss: 0.27128 | train_accuracy: 0.89635 | valid_logloss: 0.27613 | valid_accuracy: 0.89502 |  0:02:24s\n",
            "epoch 10 | loss: 0.27574 | train_logloss: 0.26663 | train_accuracy: 0.89912 | valid_logloss: 0.27129 | valid_accuracy: 0.89739 |  0:02:38s\n",
            "epoch 11 | loss: 0.27501 | train_logloss: 0.2701  | train_accuracy: 0.89763 | valid_logloss: 0.2752  | valid_accuracy: 0.89584 |  0:02:52s\n",
            "epoch 12 | loss: 0.27672 | train_logloss: 0.27571 | train_accuracy: 0.89523 | valid_logloss: 0.2802  | valid_accuracy: 0.89437 |  0:03:07s\n",
            "epoch 13 | loss: 0.27419 | train_logloss: 0.26843 | train_accuracy: 0.89752 | valid_logloss: 0.2744  | valid_accuracy: 0.89649 |  0:03:21s\n",
            "epoch 14 | loss: 0.27244 | train_logloss: 0.26717 | train_accuracy: 0.89789 | valid_logloss: 0.27151 | valid_accuracy: 0.89697 |  0:03:35s\n",
            "epoch 15 | loss: 0.27133 | train_logloss: 0.26704 | train_accuracy: 0.89799 | valid_logloss: 0.27207 | valid_accuracy: 0.89651 |  0:03:50s\n",
            "epoch 16 | loss: 0.27049 | train_logloss: 0.26899 | train_accuracy: 0.89756 | valid_logloss: 0.27477 | valid_accuracy: 0.89632 |  0:04:04s\n",
            "epoch 17 | loss: 0.27405 | train_logloss: 0.27131 | train_accuracy: 0.89824 | valid_logloss: 0.27671 | valid_accuracy: 0.89686 |  0:04:19s\n",
            "epoch 18 | loss: 0.27382 | train_logloss: 0.27153 | train_accuracy: 0.89716 | valid_logloss: 0.27642 | valid_accuracy: 0.89547 |  0:04:33s\n",
            "epoch 19 | loss: 0.27496 | train_logloss: 0.27771 | train_accuracy: 0.89565 | valid_logloss: 0.28233 | valid_accuracy: 0.89396 |  0:04:47s\n",
            "epoch 20 | loss: 0.27675 | train_logloss: 0.27147 | train_accuracy: 0.89677 | valid_logloss: 0.27707 | valid_accuracy: 0.89512 |  0:05:02s\n",
            "epoch 21 | loss: 0.27277 | train_logloss: 0.27441 | train_accuracy: 0.89414 | valid_logloss: 0.27925 | valid_accuracy: 0.8931  |  0:05:16s\n",
            "epoch 22 | loss: 0.27142 | train_logloss: 0.27053 | train_accuracy: 0.89705 | valid_logloss: 0.27542 | valid_accuracy: 0.89593 |  0:05:31s\n",
            "epoch 23 | loss: 0.27251 | train_logloss: 0.27518 | train_accuracy: 0.89383 | valid_logloss: 0.28052 | valid_accuracy: 0.89222 |  0:05:45s\n",
            "epoch 24 | loss: 0.26964 | train_logloss: 0.26375 | train_accuracy: 0.89938 | valid_logloss: 0.26948 | valid_accuracy: 0.89793 |  0:05:59s\n",
            "epoch 25 | loss: 0.26829 | train_logloss: 0.26457 | train_accuracy: 0.89996 | valid_logloss: 0.27064 | valid_accuracy: 0.89836 |  0:06:14s\n",
            "epoch 26 | loss: 0.27376 | train_logloss: 0.27102 | train_accuracy: 0.89534 | valid_logloss: 0.27555 | valid_accuracy: 0.89457 |  0:06:28s\n",
            "epoch 27 | loss: 0.27037 | train_logloss: 0.26367 | train_accuracy: 0.90029 | valid_logloss: 0.26987 | valid_accuracy: 0.8982  |  0:06:43s\n",
            "epoch 28 | loss: 0.26759 | train_logloss: 0.26163 | train_accuracy: 0.90054 | valid_logloss: 0.2679  | valid_accuracy: 0.89856 |  0:06:57s\n",
            "epoch 29 | loss: 0.26545 | train_logloss: 0.26149 | train_accuracy: 0.9003  | valid_logloss: 0.26785 | valid_accuracy: 0.89797 |  0:07:11s\n",
            "epoch 30 | loss: 0.26755 | train_logloss: 0.26471 | train_accuracy: 0.89966 | valid_logloss: 0.26992 | valid_accuracy: 0.89807 |  0:07:26s\n",
            "epoch 31 | loss: 0.27066 | train_logloss: 0.26582 | train_accuracy: 0.89883 | valid_logloss: 0.27141 | valid_accuracy: 0.89736 |  0:07:40s\n",
            "epoch 32 | loss: 0.26739 | train_logloss: 0.26253 | train_accuracy: 0.90061 | valid_logloss: 0.26882 | valid_accuracy: 0.89843 |  0:07:54s\n",
            "epoch 33 | loss: 0.26729 | train_logloss: 0.27445 | train_accuracy: 0.89496 | valid_logloss: 0.27996 | valid_accuracy: 0.89362 |  0:08:09s\n",
            "epoch 34 | loss: 0.26853 | train_logloss: 0.26294 | train_accuracy: 0.90033 | valid_logloss: 0.26997 | valid_accuracy: 0.8981  |  0:08:23s\n",
            "epoch 35 | loss: 0.26482 | train_logloss: 0.26269 | train_accuracy: 0.90006 | valid_logloss: 0.26943 | valid_accuracy: 0.89795 |  0:08:38s\n",
            "epoch 36 | loss: 0.26435 | train_logloss: 0.25753 | train_accuracy: 0.90196 | valid_logloss: 0.26402 | valid_accuracy: 0.8999  |  0:08:52s\n",
            "epoch 37 | loss: 0.26365 | train_logloss: 0.2574  | train_accuracy: 0.90242 | valid_logloss: 0.26422 | valid_accuracy: 0.90004 |  0:09:06s\n",
            "epoch 38 | loss: 0.26253 | train_logloss: 0.25745 | train_accuracy: 0.90248 | valid_logloss: 0.26447 | valid_accuracy: 0.90032 |  0:09:21s\n",
            "epoch 39 | loss: 0.26292 | train_logloss: 0.25976 | train_accuracy: 0.90084 | valid_logloss: 0.26737 | valid_accuracy: 0.89894 |  0:09:35s\n",
            "epoch 40 | loss: 0.26194 | train_logloss: 0.25861 | train_accuracy: 0.90095 | valid_logloss: 0.26717 | valid_accuracy: 0.89867 |  0:09:49s\n",
            "epoch 41 | loss: 0.26294 | train_logloss: 0.25991 | train_accuracy: 0.90108 | valid_logloss: 0.26702 | valid_accuracy: 0.89881 |  0:10:04s\n",
            "epoch 42 | loss: 0.26276 | train_logloss: 0.27097 | train_accuracy: 0.89649 | valid_logloss: 0.2781  | valid_accuracy: 0.89427 |  0:10:18s\n",
            "epoch 43 | loss: 0.26425 | train_logloss: 0.25958 | train_accuracy: 0.90026 | valid_logloss: 0.26711 | valid_accuracy: 0.89755 |  0:10:32s\n",
            "epoch 44 | loss: 0.26148 | train_logloss: 0.25635 | train_accuracy: 0.90241 | valid_logloss: 0.26467 | valid_accuracy: 0.89917 |  0:10:47s\n",
            "epoch 45 | loss: 0.26284 | train_logloss: 0.26009 | train_accuracy: 0.90051 | valid_logloss: 0.2679  | valid_accuracy: 0.89827 |  0:11:01s\n",
            "epoch 46 | loss: 0.26147 | train_logloss: 0.25552 | train_accuracy: 0.90264 | valid_logloss: 0.26402 | valid_accuracy: 0.9002  |  0:11:15s\n",
            "epoch 47 | loss: 0.26042 | train_logloss: 0.25699 | train_accuracy: 0.90218 | valid_logloss: 0.26467 | valid_accuracy: 0.89986 |  0:11:30s\n",
            "epoch 48 | loss: 0.25943 | train_logloss: 0.25419 | train_accuracy: 0.90264 | valid_logloss: 0.26338 | valid_accuracy: 0.89984 |  0:11:44s\n",
            "epoch 49 | loss: 0.26008 | train_logloss: 0.25652 | train_accuracy: 0.90265 | valid_logloss: 0.26592 | valid_accuracy: 0.90007 |  0:11:58s\n",
            "epoch 50 | loss: 0.25975 | train_logloss: 0.25854 | train_accuracy: 0.9017  | valid_logloss: 0.26731 | valid_accuracy: 0.89919 |  0:12:13s\n",
            "epoch 51 | loss: 0.26102 | train_logloss: 0.25556 | train_accuracy: 0.90269 | valid_logloss: 0.26419 | valid_accuracy: 0.90008 |  0:12:27s\n",
            "epoch 52 | loss: 0.26068 | train_logloss: 0.25381 | train_accuracy: 0.90352 | valid_logloss: 0.26355 | valid_accuracy: 0.90066 |  0:12:41s\n",
            "epoch 53 | loss: 0.25976 | train_logloss: 0.25525 | train_accuracy: 0.90289 | valid_logloss: 0.26419 | valid_accuracy: 0.90002 |  0:12:56s\n",
            "epoch 54 | loss: 0.25847 | train_logloss: 0.25398 | train_accuracy: 0.9034  | valid_logloss: 0.2634  | valid_accuracy: 0.90061 |  0:13:10s\n",
            "epoch 55 | loss: 0.26097 | train_logloss: 0.27098 | train_accuracy: 0.89803 | valid_logloss: 0.27731 | valid_accuracy: 0.89659 |  0:13:24s\n",
            "epoch 56 | loss: 0.26377 | train_logloss: 0.25887 | train_accuracy: 0.90162 | valid_logloss: 0.26669 | valid_accuracy: 0.89969 |  0:13:39s\n",
            "epoch 57 | loss: 0.26071 | train_logloss: 0.25583 | train_accuracy: 0.90221 | valid_logloss: 0.26478 | valid_accuracy: 0.8998  |  0:13:53s\n",
            "epoch 58 | loss: 0.25881 | train_logloss: 0.25347 | train_accuracy: 0.90372 | valid_logloss: 0.26282 | valid_accuracy: 0.90107 |  0:14:08s\n",
            "epoch 59 | loss: 0.25952 | train_logloss: 0.25754 | train_accuracy: 0.90217 | valid_logloss: 0.26514 | valid_accuracy: 0.89986 |  0:14:22s\n",
            "epoch 60 | loss: 0.2589  | train_logloss: 0.25255 | train_accuracy: 0.90355 | valid_logloss: 0.26209 | valid_accuracy: 0.90033 |  0:14:37s\n",
            "epoch 61 | loss: 0.26422 | train_logloss: 0.26209 | train_accuracy: 0.90093 | valid_logloss: 0.26999 | valid_accuracy: 0.89908 |  0:14:51s\n",
            "epoch 62 | loss: 0.26208 | train_logloss: 0.25611 | train_accuracy: 0.90299 | valid_logloss: 0.26419 | valid_accuracy: 0.90026 |  0:15:06s\n",
            "epoch 63 | loss: 0.26529 | train_logloss: 0.25982 | train_accuracy: 0.90177 | valid_logloss: 0.26789 | valid_accuracy: 0.89924 |  0:15:20s\n",
            "epoch 64 | loss: 0.26124 | train_logloss: 0.25494 | train_accuracy: 0.90309 | valid_logloss: 0.26418 | valid_accuracy: 0.9005  |  0:15:34s\n",
            "epoch 65 | loss: 0.25924 | train_logloss: 0.2526  | train_accuracy: 0.90359 | valid_logloss: 0.26218 | valid_accuracy: 0.90084 |  0:15:49s\n",
            "epoch 66 | loss: 0.25878 | train_logloss: 0.25603 | train_accuracy: 0.90266 | valid_logloss: 0.26506 | valid_accuracy: 0.90013 |  0:16:03s\n",
            "epoch 67 | loss: 0.26006 | train_logloss: 0.25726 | train_accuracy: 0.90249 | valid_logloss: 0.26596 | valid_accuracy: 0.90036 |  0:16:18s\n",
            "epoch 68 | loss: 0.2585  | train_logloss: 0.25146 | train_accuracy: 0.90421 | valid_logloss: 0.26193 | valid_accuracy: 0.90118 |  0:16:32s\n",
            "epoch 69 | loss: 0.25636 | train_logloss: 0.25206 | train_accuracy: 0.90397 | valid_logloss: 0.26318 | valid_accuracy: 0.90072 |  0:16:46s\n",
            "epoch 70 | loss: 0.25756 | train_logloss: 0.25431 | train_accuracy: 0.90294 | valid_logloss: 0.26482 | valid_accuracy: 0.89953 |  0:17:01s\n",
            "epoch 71 | loss: 0.25592 | train_logloss: 0.25504 | train_accuracy: 0.90254 | valid_logloss: 0.26615 | valid_accuracy: 0.89976 |  0:17:15s\n",
            "epoch 72 | loss: 0.25601 | train_logloss: 0.25388 | train_accuracy: 0.90224 | valid_logloss: 0.26499 | valid_accuracy: 0.89945 |  0:17:29s\n",
            "epoch 73 | loss: 0.25621 | train_logloss: 0.25204 | train_accuracy: 0.90412 | valid_logloss: 0.26319 | valid_accuracy: 0.90073 |  0:17:44s\n",
            "epoch 74 | loss: 0.25558 | train_logloss: 0.25079 | train_accuracy: 0.90469 | valid_logloss: 0.26383 | valid_accuracy: 0.90137 |  0:17:58s\n",
            "epoch 75 | loss: 0.25768 | train_logloss: 0.25842 | train_accuracy: 0.90177 | valid_logloss: 0.26848 | valid_accuracy: 0.89911 |  0:18:12s\n",
            "epoch 76 | loss: 0.25874 | train_logloss: 0.25677 | train_accuracy: 0.90275 | valid_logloss: 0.26779 | valid_accuracy: 0.89908 |  0:18:27s\n",
            "epoch 77 | loss: 0.2564  | train_logloss: 0.25235 | train_accuracy: 0.90393 | valid_logloss: 0.26437 | valid_accuracy: 0.90047 |  0:18:41s\n",
            "epoch 78 | loss: 0.25527 | train_logloss: 0.25302 | train_accuracy: 0.90312 | valid_logloss: 0.26495 | valid_accuracy: 0.89946 |  0:18:55s\n",
            "epoch 79 | loss: 0.25708 | train_logloss: 0.2505  | train_accuracy: 0.90401 | valid_logloss: 0.26322 | valid_accuracy: 0.90072 |  0:19:09s\n",
            "epoch 80 | loss: 0.25565 | train_logloss: 0.25049 | train_accuracy: 0.90431 | valid_logloss: 0.26247 | valid_accuracy: 0.90041 |  0:19:24s\n",
            "epoch 81 | loss: 0.25664 | train_logloss: 0.25426 | train_accuracy: 0.90333 | valid_logloss: 0.26561 | valid_accuracy: 0.90066 |  0:19:38s\n",
            "epoch 82 | loss: 0.25747 | train_logloss: 0.25217 | train_accuracy: 0.90388 | valid_logloss: 0.26474 | valid_accuracy: 0.9002  |  0:19:53s\n",
            "epoch 83 | loss: 0.25528 | train_logloss: 0.25031 | train_accuracy: 0.90431 | valid_logloss: 0.26247 | valid_accuracy: 0.90093 |  0:20:07s\n",
            "epoch 84 | loss: 0.25484 | train_logloss: 0.25255 | train_accuracy: 0.90331 | valid_logloss: 0.26492 | valid_accuracy: 0.89952 |  0:20:22s\n",
            "epoch 85 | loss: 0.25453 | train_logloss: 0.25323 | train_accuracy: 0.90337 | valid_logloss: 0.2653  | valid_accuracy: 0.90003 |  0:20:37s\n",
            "epoch 86 | loss: 0.25386 | train_logloss: 0.24866 | train_accuracy: 0.90504 | valid_logloss: 0.26186 | valid_accuracy: 0.90121 |  0:20:51s\n",
            "epoch 87 | loss: 0.25351 | train_logloss: 0.25603 | train_accuracy: 0.90223 | valid_logloss: 0.26917 | valid_accuracy: 0.8985  |  0:21:06s\n",
            "epoch 88 | loss: 0.25388 | train_logloss: 0.24804 | train_accuracy: 0.90492 | valid_logloss: 0.26138 | valid_accuracy: 0.90135 |  0:21:20s\n",
            "epoch 89 | loss: 0.25281 | train_logloss: 0.24649 | train_accuracy: 0.90576 | valid_logloss: 0.26097 | valid_accuracy: 0.901   |  0:21:35s\n",
            "epoch 90 | loss: 0.25362 | train_logloss: 0.24985 | train_accuracy: 0.90401 | valid_logloss: 0.26356 | valid_accuracy: 0.90016 |  0:21:50s\n",
            "epoch 91 | loss: 0.25684 | train_logloss: 0.24917 | train_accuracy: 0.90502 | valid_logloss: 0.26191 | valid_accuracy: 0.90121 |  0:22:04s\n",
            "epoch 92 | loss: 0.25292 | train_logloss: 0.24924 | train_accuracy: 0.9045  | valid_logloss: 0.26407 | valid_accuracy: 0.90032 |  0:22:19s\n",
            "epoch 93 | loss: 0.25282 | train_logloss: 0.24953 | train_accuracy: 0.90458 | valid_logloss: 0.26464 | valid_accuracy: 0.90003 |  0:22:33s\n",
            "epoch 94 | loss: 0.25189 | train_logloss: 0.24857 | train_accuracy: 0.90494 | valid_logloss: 0.26402 | valid_accuracy: 0.90026 |  0:22:48s\n",
            "epoch 95 | loss: 0.25179 | train_logloss: 0.25125 | train_accuracy: 0.9038  | valid_logloss: 0.26673 | valid_accuracy: 0.89889 |  0:23:02s\n",
            "epoch 96 | loss: 0.25213 | train_logloss: 0.25209 | train_accuracy: 0.90372 | valid_logloss: 0.26693 | valid_accuracy: 0.8996  |  0:23:17s\n",
            "epoch 97 | loss: 0.25299 | train_logloss: 0.25081 | train_accuracy: 0.90429 | valid_logloss: 0.26503 | valid_accuracy: 0.89939 |  0:23:32s\n",
            "epoch 98 | loss: 0.25292 | train_logloss: 0.24622 | train_accuracy: 0.90595 | valid_logloss: 0.26113 | valid_accuracy: 0.90173 |  0:23:46s\n",
            "epoch 99 | loss: 0.2525  | train_logloss: 0.24778 | train_accuracy: 0.90557 | valid_logloss: 0.26217 | valid_accuracy: 0.90099 |  0:24:01s\n",
            "epoch 100| loss: 0.25172 | train_logloss: 0.24918 | train_accuracy: 0.90424 | valid_logloss: 0.26339 | valid_accuracy: 0.90021 |  0:24:15s\n",
            "epoch 101| loss: 0.25232 | train_logloss: 0.24852 | train_accuracy: 0.90451 | valid_logloss: 0.26293 | valid_accuracy: 0.90061 |  0:24:30s\n",
            "epoch 102| loss: 0.25186 | train_logloss: 0.24817 | train_accuracy: 0.90519 | valid_logloss: 0.26307 | valid_accuracy: 0.90083 |  0:24:44s\n",
            "epoch 103| loss: 0.25143 | train_logloss: 0.24618 | train_accuracy: 0.90614 | valid_logloss: 0.26158 | valid_accuracy: 0.9012  |  0:24:59s\n",
            "epoch 104| loss: 0.25075 | train_logloss: 0.24534 | train_accuracy: 0.90636 | valid_logloss: 0.26168 | valid_accuracy: 0.90158 |  0:25:14s\n",
            "epoch 105| loss: 0.25069 | train_logloss: 0.24811 | train_accuracy: 0.90534 | valid_logloss: 0.26472 | valid_accuracy: 0.9008  |  0:25:28s\n",
            "epoch 106| loss: 0.25118 | train_logloss: 0.25273 | train_accuracy: 0.90288 | valid_logloss: 0.26843 | valid_accuracy: 0.89829 |  0:25:43s\n",
            "epoch 107| loss: 0.25185 | train_logloss: 0.249   | train_accuracy: 0.90513 | valid_logloss: 0.26681 | valid_accuracy: 0.89974 |  0:25:58s\n",
            "epoch 108| loss: 0.25056 | train_logloss: 0.2474  | train_accuracy: 0.90572 | valid_logloss: 0.26438 | valid_accuracy: 0.90049 |  0:26:12s\n",
            "epoch 109| loss: 0.25127 | train_logloss: 0.24788 | train_accuracy: 0.90547 | valid_logloss: 0.26404 | valid_accuracy: 0.90089 |  0:26:27s\n",
            "epoch 110| loss: 0.25022 | train_logloss: 0.25039 | train_accuracy: 0.90389 | valid_logloss: 0.2661  | valid_accuracy: 0.8993  |  0:26:42s\n",
            "epoch 111| loss: 0.25039 | train_logloss: 0.24633 | train_accuracy: 0.9055  | valid_logloss: 0.26341 | valid_accuracy: 0.90051 |  0:26:56s\n",
            "epoch 112| loss: 0.24966 | train_logloss: 0.24523 | train_accuracy: 0.90621 | valid_logloss: 0.26294 | valid_accuracy: 0.90092 |  0:27:11s\n",
            "epoch 113| loss: 0.25263 | train_logloss: 0.25366 | train_accuracy: 0.90417 | valid_logloss: 0.26599 | valid_accuracy: 0.90015 |  0:27:26s\n",
            "epoch 114| loss: 0.2546  | train_logloss: 0.24588 | train_accuracy: 0.90606 | valid_logloss: 0.26137 | valid_accuracy: 0.90138 |  0:27:40s\n",
            "epoch 115| loss: 0.25193 | train_logloss: 0.24815 | train_accuracy: 0.90519 | valid_logloss: 0.26453 | valid_accuracy: 0.90053 |  0:27:55s\n",
            "epoch 116| loss: 0.25141 | train_logloss: 0.24904 | train_accuracy: 0.90507 | valid_logloss: 0.26542 | valid_accuracy: 0.90006 |  0:28:09s\n",
            "epoch 117| loss: 0.25272 | train_logloss: 0.24582 | train_accuracy: 0.90613 | valid_logloss: 0.26211 | valid_accuracy: 0.90118 |  0:28:24s\n",
            "epoch 118| loss: 0.25166 | train_logloss: 0.24917 | train_accuracy: 0.90471 | valid_logloss: 0.26477 | valid_accuracy: 0.90026 |  0:28:38s\n",
            "epoch 119| loss: 0.25051 | train_logloss: 0.2503  | train_accuracy: 0.90368 | valid_logloss: 0.26658 | valid_accuracy: 0.89888 |  0:28:53s\n",
            "epoch 120| loss: 0.24966 | train_logloss: 0.2473  | train_accuracy: 0.90547 | valid_logloss: 0.2655  | valid_accuracy: 0.90007 |  0:29:07s\n",
            "epoch 121| loss: 0.24944 | train_logloss: 0.24326 | train_accuracy: 0.90719 | valid_logloss: 0.26113 | valid_accuracy: 0.90183 |  0:29:22s\n",
            "epoch 122| loss: 0.24932 | train_logloss: 0.24442 | train_accuracy: 0.90701 | valid_logloss: 0.26259 | valid_accuracy: 0.90109 |  0:29:36s\n",
            "epoch 123| loss: 0.24909 | train_logloss: 0.24625 | train_accuracy: 0.90601 | valid_logloss: 0.26502 | valid_accuracy: 0.90059 |  0:29:51s\n",
            "epoch 124| loss: 0.24927 | train_logloss: 0.24432 | train_accuracy: 0.90626 | valid_logloss: 0.26321 | valid_accuracy: 0.90058 |  0:30:05s\n",
            "epoch 125| loss: 0.24897 | train_logloss: 0.24298 | train_accuracy: 0.90729 | valid_logloss: 0.26329 | valid_accuracy: 0.90096 |  0:30:19s\n",
            "epoch 126| loss: 0.24917 | train_logloss: 0.24257 | train_accuracy: 0.90704 | valid_logloss: 0.2618  | valid_accuracy: 0.90147 |  0:30:34s\n",
            "epoch 127| loss: 0.24813 | train_logloss: 0.2433  | train_accuracy: 0.90705 | valid_logloss: 0.26136 | valid_accuracy: 0.90108 |  0:30:48s\n",
            "epoch 128| loss: 0.24818 | train_logloss: 0.24351 | train_accuracy: 0.90658 | valid_logloss: 0.26424 | valid_accuracy: 0.90125 |  0:31:03s\n",
            "epoch 129| loss: 0.24918 | train_logloss: 0.2434  | train_accuracy: 0.90696 | valid_logloss: 0.26385 | valid_accuracy: 0.90123 |  0:31:17s\n",
            "epoch 130| loss: 0.24913 | train_logloss: 0.24379 | train_accuracy: 0.90698 | valid_logloss: 0.26292 | valid_accuracy: 0.90145 |  0:31:32s\n",
            "epoch 131| loss: 0.24916 | train_logloss: 0.24284 | train_accuracy: 0.9071  | valid_logloss: 0.26251 | valid_accuracy: 0.90141 |  0:31:46s\n",
            "epoch 132| loss: 0.24827 | train_logloss: 0.24385 | train_accuracy: 0.90699 | valid_logloss: 0.26352 | valid_accuracy: 0.90092 |  0:32:01s\n",
            "epoch 133| loss: 0.24809 | train_logloss: 0.24381 | train_accuracy: 0.9069  | valid_logloss: 0.26314 | valid_accuracy: 0.90141 |  0:32:15s\n",
            "epoch 134| loss: 0.24846 | train_logloss: 0.24604 | train_accuracy: 0.90591 | valid_logloss: 0.26579 | valid_accuracy: 0.89993 |  0:32:31s\n",
            "epoch 135| loss: 0.2493  | train_logloss: 0.24268 | train_accuracy: 0.90779 | valid_logloss: 0.26312 | valid_accuracy: 0.90135 |  0:32:46s\n",
            "epoch 136| loss: 0.24803 | train_logloss: 0.24737 | train_accuracy: 0.90553 | valid_logloss: 0.26609 | valid_accuracy: 0.89983 |  0:33:01s\n",
            "epoch 137| loss: 0.24739 | train_logloss: 0.24414 | train_accuracy: 0.90653 | valid_logloss: 0.26554 | valid_accuracy: 0.90028 |  0:33:17s\n",
            "epoch 138| loss: 0.2485  | train_logloss: 0.24507 | train_accuracy: 0.90586 | valid_logloss: 0.26425 | valid_accuracy: 0.90114 |  0:33:32s\n",
            "epoch 139| loss: 0.24711 | train_logloss: 0.24158 | train_accuracy: 0.90728 | valid_logloss: 0.26314 | valid_accuracy: 0.90106 |  0:33:46s\n",
            "epoch 140| loss: 0.24771 | train_logloss: 0.24402 | train_accuracy: 0.90665 | valid_logloss: 0.26421 | valid_accuracy: 0.90065 |  0:34:01s\n",
            "epoch 141| loss: 0.24648 | train_logloss: 0.24258 | train_accuracy: 0.90712 | valid_logloss: 0.2642  | valid_accuracy: 0.90127 |  0:34:16s\n",
            "epoch 142| loss: 0.24625 | train_logloss: 0.24241 | train_accuracy: 0.90737 | valid_logloss: 0.26367 | valid_accuracy: 0.90114 |  0:34:30s\n",
            "epoch 143| loss: 0.24717 | train_logloss: 0.24472 | train_accuracy: 0.90661 | valid_logloss: 0.26719 | valid_accuracy: 0.89962 |  0:34:45s\n",
            "epoch 144| loss: 0.24758 | train_logloss: 0.24877 | train_accuracy: 0.90473 | valid_logloss: 0.26956 | valid_accuracy: 0.89932 |  0:34:59s\n",
            "epoch 145| loss: 0.24636 | train_logloss: 0.24148 | train_accuracy: 0.90791 | valid_logloss: 0.26432 | valid_accuracy: 0.90133 |  0:35:14s\n",
            "epoch 146| loss: 0.24624 | train_logloss: 0.24294 | train_accuracy: 0.90653 | valid_logloss: 0.26474 | valid_accuracy: 0.90058 |  0:35:29s\n",
            "epoch 147| loss: 0.24732 | train_logloss: 0.24907 | train_accuracy: 0.90509 | valid_logloss: 0.268   | valid_accuracy: 0.90008 |  0:35:43s\n",
            "epoch 148| loss: 0.24702 | train_logloss: 0.24498 | train_accuracy: 0.90642 | valid_logloss: 0.26577 | valid_accuracy: 0.90053 |  0:35:58s\n",
            "epoch 149| loss: 0.24677 | train_logloss: 0.24407 | train_accuracy: 0.90627 | valid_logloss: 0.26696 | valid_accuracy: 0.9002  |  0:36:13s\n",
            "epoch 150| loss: 0.24604 | train_logloss: 0.24032 | train_accuracy: 0.90786 | valid_logloss: 0.26391 | valid_accuracy: 0.90087 |  0:36:28s\n",
            "epoch 151| loss: 0.24554 | train_logloss: 0.23959 | train_accuracy: 0.90801 | valid_logloss: 0.26283 | valid_accuracy: 0.90145 |  0:36:42s\n",
            "epoch 152| loss: 0.24592 | train_logloss: 0.24306 | train_accuracy: 0.9072  | valid_logloss: 0.26671 | valid_accuracy: 0.90059 |  0:36:57s\n",
            "epoch 153| loss: 0.24534 | train_logloss: 0.24358 | train_accuracy: 0.90715 | valid_logloss: 0.26832 | valid_accuracy: 0.90036 |  0:37:12s\n",
            "epoch 154| loss: 0.24603 | train_logloss: 0.24712 | train_accuracy: 0.90568 | valid_logloss: 0.26802 | valid_accuracy: 0.90052 |  0:37:26s\n",
            "epoch 155| loss: 0.24774 | train_logloss: 0.24252 | train_accuracy: 0.90724 | valid_logloss: 0.26461 | valid_accuracy: 0.9014  |  0:37:41s\n",
            "epoch 156| loss: 0.24534 | train_logloss: 0.24284 | train_accuracy: 0.90683 | valid_logloss: 0.26748 | valid_accuracy: 0.89944 |  0:37:55s\n",
            "epoch 157| loss: 0.24704 | train_logloss: 0.24061 | train_accuracy: 0.90836 | valid_logloss: 0.26481 | valid_accuracy: 0.90088 |  0:38:10s\n",
            "epoch 158| loss: 0.24563 | train_logloss: 0.24273 | train_accuracy: 0.90702 | valid_logloss: 0.26575 | valid_accuracy: 0.90079 |  0:38:25s\n",
            "epoch 159| loss: 0.24689 | train_logloss: 0.24089 | train_accuracy: 0.9081  | valid_logloss: 0.26378 | valid_accuracy: 0.90114 |  0:38:39s\n",
            "epoch 160| loss: 0.24669 | train_logloss: 0.24039 | train_accuracy: 0.90816 | valid_logloss: 0.26183 | valid_accuracy: 0.90147 |  0:38:54s\n",
            "epoch 161| loss: 0.24695 | train_logloss: 0.24381 | train_accuracy: 0.90679 | valid_logloss: 0.26615 | valid_accuracy: 0.90074 |  0:39:09s\n",
            "epoch 162| loss: 0.24738 | train_logloss: 0.2424  | train_accuracy: 0.90764 | valid_logloss: 0.26392 | valid_accuracy: 0.90129 |  0:39:23s\n",
            "epoch 163| loss: 0.24587 | train_logloss: 0.24079 | train_accuracy: 0.90798 | valid_logloss: 0.26477 | valid_accuracy: 0.90099 |  0:39:38s\n",
            "epoch 164| loss: 0.24507 | train_logloss: 0.2479  | train_accuracy: 0.90441 | valid_logloss: 0.27184 | valid_accuracy: 0.89779 |  0:39:53s\n",
            "epoch 165| loss: 0.24554 | train_logloss: 0.23945 | train_accuracy: 0.908   | valid_logloss: 0.26234 | valid_accuracy: 0.90162 |  0:40:07s\n",
            "epoch 166| loss: 0.24552 | train_logloss: 0.25379 | train_accuracy: 0.90163 | valid_logloss: 0.27734 | valid_accuracy: 0.89493 |  0:40:22s\n",
            "epoch 167| loss: 0.24632 | train_logloss: 0.24233 | train_accuracy: 0.90677 | valid_logloss: 0.26596 | valid_accuracy: 0.89977 |  0:40:36s\n",
            "epoch 168| loss: 0.24571 | train_logloss: 0.24028 | train_accuracy: 0.90809 | valid_logloss: 0.2637  | valid_accuracy: 0.90098 |  0:40:51s\n",
            "epoch 169| loss: 0.24586 | train_logloss: 0.23965 | train_accuracy: 0.9082  | valid_logloss: 0.2645  | valid_accuracy: 0.90115 |  0:41:06s\n",
            "epoch 170| loss: 0.24532 | train_logloss: 0.24115 | train_accuracy: 0.90763 | valid_logloss: 0.26463 | valid_accuracy: 0.90098 |  0:41:20s\n",
            "epoch 171| loss: 0.2446  | train_logloss: 0.24207 | train_accuracy: 0.90697 | valid_logloss: 0.26669 | valid_accuracy: 0.89993 |  0:41:35s\n",
            "\n",
            "Early stopping occurred at epoch 171 with best_epoch = 121 and best_valid_accuracy = 0.90183\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[422879  10514  46789]\n",
            " [ 14005 956084  29911]\n",
            " [ 29086  27374 376127]]\n",
            "Testing Score:  0.9018333333333334\n",
            "Confusion Matrix: \n",
            " [[425621  10355  44206]\n",
            " [ 14063 956159  29778]\n",
            " [ 32309  27156 373122]]\n",
            "Testing Score:  0.90149\n",
            "{'Rows': 300000, 'Nd': 16, 'Na': 16, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 300000, 'test': 300000, 'time_learn_tn': 2499.8729372024536, 'time_tn': 21.90016508102417, 'accuracy_tn': 0.9018333333333334, 'time_learn_gb': 122.02576994895935, 'time_gb': 81.67052435874939, 'accuracy_gb': 0.90149}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "KEGbhxjyWfq5",
        "outputId": "994a04bc-1821-4216-8d7c-684824d0c507"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ... accuracy_gb time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...    0.883778      3.483706  16.722444\n",
              "1     9000    9000    9000  ...    0.883778      3.464101  17.349063\n",
              "2     9000    9000    9000  ...    0.883778      3.411332  16.978554\n",
              "3     9000    9000    9000  ...    0.883778      3.509229  17.331297\n",
              "4     9000    9000    9000  ...    0.883778      3.475400  17.451218\n",
              "5    30000   30000   30000  ...    0.891767      8.276289  27.093480\n",
              "6    30000   30000   30000  ...    0.891767      8.210687  27.343578\n",
              "7    30000   30000   30000  ...    0.891767      8.244918  28.102216\n",
              "8    30000   30000   30000  ...    0.891767      8.093423  27.197022\n",
              "9    30000   30000   30000  ...    0.891767      7.857594  27.867677\n",
              "10  300000  300000  300000  ...    0.901553    140.282187  95.826268\n",
              "11  300000  300000  300000  ...    0.901490    122.025770  81.670524\n",
              "\n",
              "[12 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwxdlHHgRWYZ",
        "outputId": "524b628b-b1ae-4daf-f562-fb59eada0560"
      },
      "source": [
        "time_model(number_exp=13, \n",
        "          Rows=300000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=2048,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.45852 | train_logloss: 1.23316 | train_accuracy: 0.49431 | valid_logloss: 1.23162 | valid_accuracy: 0.49515 |  0:00:19s\n",
            "epoch 1  | loss: 0.33774 | train_logloss: 0.43007 | train_accuracy: 0.84012 | valid_logloss: 0.42901 | valid_accuracy: 0.84103 |  0:00:38s\n",
            "epoch 2  | loss: 0.32425 | train_logloss: 0.34812 | train_accuracy: 0.874   | valid_logloss: 0.34836 | valid_accuracy: 0.87473 |  0:00:58s\n",
            "epoch 3  | loss: 0.31809 | train_logloss: 0.30659 | train_accuracy: 0.88468 | valid_logloss: 0.30751 | valid_accuracy: 0.88546 |  0:01:17s\n",
            "epoch 4  | loss: 0.3051  | train_logloss: 0.30431 | train_accuracy: 0.88572 | valid_logloss: 0.30674 | valid_accuracy: 0.8856  |  0:01:37s\n",
            "epoch 5  | loss: 0.30299 | train_logloss: 0.29393 | train_accuracy: 0.88884 | valid_logloss: 0.29667 | valid_accuracy: 0.8891  |  0:01:56s\n",
            "epoch 6  | loss: 0.31306 | train_logloss: 0.31689 | train_accuracy: 0.88122 | valid_logloss: 0.31775 | valid_accuracy: 0.88101 |  0:02:16s\n",
            "epoch 7  | loss: 0.30928 | train_logloss: 0.29149 | train_accuracy: 0.88917 | valid_logloss: 0.29262 | valid_accuracy: 0.88964 |  0:02:35s\n",
            "epoch 8  | loss: 0.30481 | train_logloss: 0.3039  | train_accuracy: 0.88767 | valid_logloss: 0.30484 | valid_accuracy: 0.88764 |  0:02:55s\n",
            "epoch 9  | loss: 0.3035  | train_logloss: 0.29511 | train_accuracy: 0.88944 | valid_logloss: 0.29686 | valid_accuracy: 0.88918 |  0:03:14s\n",
            "epoch 10 | loss: 0.30247 | train_logloss: 0.2904  | train_accuracy: 0.88998 | valid_logloss: 0.29231 | valid_accuracy: 0.88985 |  0:03:34s\n",
            "epoch 11 | loss: 0.2986  | train_logloss: 0.29534 | train_accuracy: 0.88871 | valid_logloss: 0.29818 | valid_accuracy: 0.88843 |  0:03:53s\n",
            "epoch 12 | loss: 0.2908  | train_logloss: 0.28318 | train_accuracy: 0.89245 | valid_logloss: 0.28638 | valid_accuracy: 0.89176 |  0:04:12s\n",
            "epoch 13 | loss: 0.28534 | train_logloss: 0.27978 | train_accuracy: 0.89338 | valid_logloss: 0.28318 | valid_accuracy: 0.89303 |  0:04:32s\n",
            "epoch 14 | loss: 0.28386 | train_logloss: 0.29588 | train_accuracy: 0.88914 | valid_logloss: 0.29873 | valid_accuracy: 0.88787 |  0:04:51s\n",
            "epoch 15 | loss: 0.28872 | train_logloss: 0.27795 | train_accuracy: 0.89451 | valid_logloss: 0.28145 | valid_accuracy: 0.8937  |  0:05:10s\n",
            "epoch 16 | loss: 0.28013 | train_logloss: 0.27393 | train_accuracy: 0.89578 | valid_logloss: 0.27741 | valid_accuracy: 0.89509 |  0:05:29s\n",
            "epoch 17 | loss: 0.27604 | train_logloss: 0.27103 | train_accuracy: 0.89662 | valid_logloss: 0.27437 | valid_accuracy: 0.89612 |  0:05:49s\n",
            "epoch 18 | loss: 0.27623 | train_logloss: 0.29452 | train_accuracy: 0.88731 | valid_logloss: 0.29709 | valid_accuracy: 0.88729 |  0:06:08s\n",
            "epoch 19 | loss: 0.27781 | train_logloss: 0.27213 | train_accuracy: 0.89712 | valid_logloss: 0.27716 | valid_accuracy: 0.89578 |  0:06:28s\n",
            "epoch 20 | loss: 0.27248 | train_logloss: 0.27023 | train_accuracy: 0.89739 | valid_logloss: 0.27443 | valid_accuracy: 0.89627 |  0:06:47s\n",
            "epoch 21 | loss: 0.27062 | train_logloss: 0.26881 | train_accuracy: 0.89806 | valid_logloss: 0.27307 | valid_accuracy: 0.89701 |  0:07:06s\n",
            "epoch 22 | loss: 0.27658 | train_logloss: 0.31248 | train_accuracy: 0.87445 | valid_logloss: 0.31536 | valid_accuracy: 0.8742  |  0:07:26s\n",
            "epoch 23 | loss: 0.27314 | train_logloss: 0.26705 | train_accuracy: 0.89806 | valid_logloss: 0.27158 | valid_accuracy: 0.89743 |  0:07:45s\n",
            "epoch 24 | loss: 0.27066 | train_logloss: 0.267   | train_accuracy: 0.89877 | valid_logloss: 0.27107 | valid_accuracy: 0.89807 |  0:08:04s\n",
            "epoch 25 | loss: 0.26899 | train_logloss: 0.26307 | train_accuracy: 0.89986 | valid_logloss: 0.26739 | valid_accuracy: 0.89873 |  0:08:24s\n",
            "epoch 26 | loss: 0.27296 | train_logloss: 0.26488 | train_accuracy: 0.89925 | valid_logloss: 0.26906 | valid_accuracy: 0.89785 |  0:08:43s\n",
            "epoch 27 | loss: 0.26906 | train_logloss: 0.26755 | train_accuracy: 0.89908 | valid_logloss: 0.27253 | valid_accuracy: 0.89768 |  0:09:02s\n",
            "epoch 28 | loss: 0.26818 | train_logloss: 0.26321 | train_accuracy: 0.89983 | valid_logloss: 0.26835 | valid_accuracy: 0.8982  |  0:09:22s\n",
            "epoch 29 | loss: 0.26567 | train_logloss: 0.26156 | train_accuracy: 0.90016 | valid_logloss: 0.26733 | valid_accuracy: 0.89816 |  0:09:41s\n",
            "epoch 30 | loss: 0.26492 | train_logloss: 0.26107 | train_accuracy: 0.90036 | valid_logloss: 0.26707 | valid_accuracy: 0.89885 |  0:10:00s\n",
            "epoch 31 | loss: 0.26341 | train_logloss: 0.25932 | train_accuracy: 0.90159 | valid_logloss: 0.26556 | valid_accuracy: 0.90007 |  0:10:19s\n",
            "epoch 32 | loss: 0.26391 | train_logloss: 0.25862 | train_accuracy: 0.90137 | valid_logloss: 0.26528 | valid_accuracy: 0.89976 |  0:10:39s\n",
            "epoch 33 | loss: 0.2627  | train_logloss: 0.25969 | train_accuracy: 0.90178 | valid_logloss: 0.26682 | valid_accuracy: 0.89937 |  0:10:58s\n",
            "epoch 34 | loss: 0.26421 | train_logloss: 0.26539 | train_accuracy: 0.89911 | valid_logloss: 0.27225 | valid_accuracy: 0.89677 |  0:11:18s\n",
            "epoch 35 | loss: 0.2625  | train_logloss: 0.26036 | train_accuracy: 0.89984 | valid_logloss: 0.26715 | valid_accuracy: 0.89769 |  0:11:37s\n",
            "epoch 36 | loss: 0.26104 | train_logloss: 0.25878 | train_accuracy: 0.90093 | valid_logloss: 0.26557 | valid_accuracy: 0.89875 |  0:11:56s\n",
            "epoch 37 | loss: 0.26022 | train_logloss: 0.25589 | train_accuracy: 0.90224 | valid_logloss: 0.26312 | valid_accuracy: 0.90016 |  0:12:15s\n",
            "epoch 38 | loss: 0.25945 | train_logloss: 0.25728 | train_accuracy: 0.90135 | valid_logloss: 0.26495 | valid_accuracy: 0.8988  |  0:12:35s\n",
            "epoch 39 | loss: 0.25908 | train_logloss: 0.25548 | train_accuracy: 0.90223 | valid_logloss: 0.26462 | valid_accuracy: 0.89985 |  0:12:54s\n",
            "epoch 40 | loss: 0.2615  | train_logloss: 0.26029 | train_accuracy: 0.89998 | valid_logloss: 0.2667  | valid_accuracy: 0.89834 |  0:13:13s\n",
            "epoch 41 | loss: 0.26076 | train_logloss: 0.25883 | train_accuracy: 0.90067 | valid_logloss: 0.26537 | valid_accuracy: 0.89916 |  0:13:33s\n",
            "epoch 42 | loss: 0.25959 | train_logloss: 0.25671 | train_accuracy: 0.90203 | valid_logloss: 0.26547 | valid_accuracy: 0.89946 |  0:13:52s\n",
            "epoch 43 | loss: 0.25883 | train_logloss: 0.25748 | train_accuracy: 0.90153 | valid_logloss: 0.26566 | valid_accuracy: 0.89893 |  0:14:11s\n",
            "epoch 44 | loss: 0.2582  | train_logloss: 0.25295 | train_accuracy: 0.9036  | valid_logloss: 0.26179 | valid_accuracy: 0.9011  |  0:14:31s\n",
            "epoch 45 | loss: 0.25647 | train_logloss: 0.2526  | train_accuracy: 0.9036  | valid_logloss: 0.26185 | valid_accuracy: 0.90105 |  0:14:50s\n",
            "epoch 46 | loss: 0.25599 | train_logloss: 0.25223 | train_accuracy: 0.90372 | valid_logloss: 0.26314 | valid_accuracy: 0.90012 |  0:15:10s\n",
            "epoch 47 | loss: 0.25579 | train_logloss: 0.26054 | train_accuracy: 0.90003 | valid_logloss: 0.27068 | valid_accuracy: 0.89643 |  0:15:29s\n",
            "epoch 48 | loss: 0.25629 | train_logloss: 0.25116 | train_accuracy: 0.90412 | valid_logloss: 0.26175 | valid_accuracy: 0.90089 |  0:15:48s\n",
            "epoch 49 | loss: 0.25505 | train_logloss: 0.25352 | train_accuracy: 0.90359 | valid_logloss: 0.26428 | valid_accuracy: 0.90039 |  0:16:08s\n",
            "epoch 50 | loss: 0.2539  | train_logloss: 0.24979 | train_accuracy: 0.90419 | valid_logloss: 0.26055 | valid_accuracy: 0.90139 |  0:16:27s\n",
            "epoch 51 | loss: 0.25362 | train_logloss: 0.25032 | train_accuracy: 0.90417 | valid_logloss: 0.26208 | valid_accuracy: 0.90084 |  0:16:46s\n",
            "epoch 52 | loss: 0.25314 | train_logloss: 0.25128 | train_accuracy: 0.90458 | valid_logloss: 0.26287 | valid_accuracy: 0.90101 |  0:17:06s\n",
            "epoch 53 | loss: 0.25277 | train_logloss: 0.25045 | train_accuracy: 0.90397 | valid_logloss: 0.2626  | valid_accuracy: 0.90004 |  0:17:25s\n",
            "epoch 54 | loss: 0.25332 | train_logloss: 0.24783 | train_accuracy: 0.90489 | valid_logloss: 0.26153 | valid_accuracy: 0.90063 |  0:17:44s\n",
            "epoch 55 | loss: 0.25336 | train_logloss: 0.24784 | train_accuracy: 0.90549 | valid_logloss: 0.26042 | valid_accuracy: 0.90154 |  0:18:04s\n",
            "epoch 56 | loss: 0.25224 | train_logloss: 0.24672 | train_accuracy: 0.90553 | valid_logloss: 0.26029 | valid_accuracy: 0.90132 |  0:18:23s\n",
            "epoch 57 | loss: 0.25901 | train_logloss: 0.25034 | train_accuracy: 0.90419 | valid_logloss: 0.26196 | valid_accuracy: 0.90091 |  0:18:42s\n",
            "epoch 58 | loss: 0.2579  | train_logloss: 0.25252 | train_accuracy: 0.90352 | valid_logloss: 0.26319 | valid_accuracy: 0.90007 |  0:19:02s\n",
            "epoch 59 | loss: 0.25367 | train_logloss: 0.24886 | train_accuracy: 0.90508 | valid_logloss: 0.26122 | valid_accuracy: 0.90132 |  0:19:21s\n",
            "epoch 60 | loss: 0.25122 | train_logloss: 0.24663 | train_accuracy: 0.90576 | valid_logloss: 0.26059 | valid_accuracy: 0.9017  |  0:19:40s\n",
            "epoch 61 | loss: 0.25012 | train_logloss: 0.2462  | train_accuracy: 0.90542 | valid_logloss: 0.26125 | valid_accuracy: 0.90108 |  0:20:00s\n",
            "epoch 62 | loss: 0.25066 | train_logloss: 0.24787 | train_accuracy: 0.90495 | valid_logloss: 0.26253 | valid_accuracy: 0.90059 |  0:20:19s\n",
            "epoch 63 | loss: 0.24812 | train_logloss: 0.24516 | train_accuracy: 0.90617 | valid_logloss: 0.2622  | valid_accuracy: 0.90091 |  0:20:39s\n",
            "epoch 64 | loss: 0.24764 | train_logloss: 0.2447  | train_accuracy: 0.90639 | valid_logloss: 0.26279 | valid_accuracy: 0.90072 |  0:20:58s\n",
            "epoch 65 | loss: 0.24734 | train_logloss: 0.24027 | train_accuracy: 0.90814 | valid_logloss: 0.25974 | valid_accuracy: 0.90221 |  0:21:18s\n",
            "epoch 66 | loss: 0.24696 | train_logloss: 0.2415  | train_accuracy: 0.90699 | valid_logloss: 0.26233 | valid_accuracy: 0.90112 |  0:21:37s\n",
            "epoch 67 | loss: 0.24685 | train_logloss: 0.24409 | train_accuracy: 0.90702 | valid_logloss: 0.26486 | valid_accuracy: 0.9011  |  0:21:56s\n",
            "epoch 68 | loss: 0.24762 | train_logloss: 0.24385 | train_accuracy: 0.90655 | valid_logloss: 0.26208 | valid_accuracy: 0.90082 |  0:22:15s\n",
            "epoch 69 | loss: 0.24704 | train_logloss: 0.24151 | train_accuracy: 0.90783 | valid_logloss: 0.26156 | valid_accuracy: 0.90111 |  0:22:35s\n",
            "epoch 70 | loss: 0.2469  | train_logloss: 0.24476 | train_accuracy: 0.90641 | valid_logloss: 0.26297 | valid_accuracy: 0.90135 |  0:22:54s\n",
            "epoch 71 | loss: 0.24554 | train_logloss: 0.2391  | train_accuracy: 0.90821 | valid_logloss: 0.26193 | valid_accuracy: 0.90146 |  0:23:13s\n",
            "epoch 72 | loss: 0.24455 | train_logloss: 0.24098 | train_accuracy: 0.90799 | valid_logloss: 0.26411 | valid_accuracy: 0.9008  |  0:23:33s\n",
            "epoch 73 | loss: 0.24388 | train_logloss: 0.23744 | train_accuracy: 0.90881 | valid_logloss: 0.26093 | valid_accuracy: 0.90168 |  0:23:52s\n",
            "epoch 74 | loss: 0.24721 | train_logloss: 0.24022 | train_accuracy: 0.90824 | valid_logloss: 0.26358 | valid_accuracy: 0.90122 |  0:24:11s\n",
            "epoch 75 | loss: 0.24532 | train_logloss: 0.24215 | train_accuracy: 0.90771 | valid_logloss: 0.26466 | valid_accuracy: 0.90128 |  0:24:30s\n",
            "epoch 76 | loss: 0.24364 | train_logloss: 0.24179 | train_accuracy: 0.90793 | valid_logloss: 0.26572 | valid_accuracy: 0.90078 |  0:24:50s\n",
            "epoch 77 | loss: 0.24369 | train_logloss: 0.23765 | train_accuracy: 0.90886 | valid_logloss: 0.26272 | valid_accuracy: 0.9004  |  0:25:09s\n",
            "epoch 78 | loss: 0.24188 | train_logloss: 0.23732 | train_accuracy: 0.90908 | valid_logloss: 0.26103 | valid_accuracy: 0.9023  |  0:25:28s\n",
            "epoch 79 | loss: 0.24099 | train_logloss: 0.23676 | train_accuracy: 0.90899 | valid_logloss: 0.26415 | valid_accuracy: 0.90099 |  0:25:48s\n",
            "epoch 80 | loss: 0.24099 | train_logloss: 0.23906 | train_accuracy: 0.90776 | valid_logloss: 0.26579 | valid_accuracy: 0.90027 |  0:26:07s\n",
            "epoch 81 | loss: 0.24068 | train_logloss: 0.23698 | train_accuracy: 0.90882 | valid_logloss: 0.26446 | valid_accuracy: 0.90058 |  0:26:26s\n",
            "epoch 82 | loss: 0.24168 | train_logloss: 0.23723 | train_accuracy: 0.90849 | valid_logloss: 0.26404 | valid_accuracy: 0.90055 |  0:26:46s\n",
            "epoch 83 | loss: 0.24014 | train_logloss: 0.23332 | train_accuracy: 0.91013 | valid_logloss: 0.26387 | valid_accuracy: 0.90075 |  0:27:05s\n",
            "epoch 84 | loss: 0.23903 | train_logloss: 0.23184 | train_accuracy: 0.91072 | valid_logloss: 0.26168 | valid_accuracy: 0.9014  |  0:27:24s\n",
            "epoch 85 | loss: 0.23795 | train_logloss: 0.2322  | train_accuracy: 0.91084 | valid_logloss: 0.26366 | valid_accuracy: 0.90145 |  0:27:44s\n",
            "epoch 86 | loss: 0.24019 | train_logloss: 0.23699 | train_accuracy: 0.90946 | valid_logloss: 0.26632 | valid_accuracy: 0.90152 |  0:28:03s\n",
            "epoch 87 | loss: 0.25906 | train_logloss: 0.24945 | train_accuracy: 0.90568 | valid_logloss: 0.26731 | valid_accuracy: 0.89982 |  0:28:22s\n",
            "epoch 88 | loss: 0.24679 | train_logloss: 0.24082 | train_accuracy: 0.90822 | valid_logloss: 0.26475 | valid_accuracy: 0.90039 |  0:28:41s\n",
            "epoch 89 | loss: 0.24197 | train_logloss: 0.23288 | train_accuracy: 0.91062 | valid_logloss: 0.26262 | valid_accuracy: 0.90155 |  0:29:01s\n",
            "epoch 90 | loss: 0.23891 | train_logloss: 0.23311 | train_accuracy: 0.91078 | valid_logloss: 0.26463 | valid_accuracy: 0.90051 |  0:29:20s\n",
            "epoch 91 | loss: 0.23822 | train_logloss: 0.23192 | train_accuracy: 0.91107 | valid_logloss: 0.26533 | valid_accuracy: 0.90076 |  0:29:39s\n",
            "epoch 92 | loss: 0.23899 | train_logloss: 0.23283 | train_accuracy: 0.91125 | valid_logloss: 0.2655  | valid_accuracy: 0.90064 |  0:29:59s\n",
            "epoch 93 | loss: 0.23662 | train_logloss: 0.2292  | train_accuracy: 0.91135 | valid_logloss: 0.26543 | valid_accuracy: 0.90101 |  0:30:18s\n",
            "epoch 94 | loss: 0.23504 | train_logloss: 0.22874 | train_accuracy: 0.91242 | valid_logloss: 0.26546 | valid_accuracy: 0.90054 |  0:30:37s\n",
            "epoch 95 | loss: 0.23432 | train_logloss: 0.2269  | train_accuracy: 0.91257 | valid_logloss: 0.26588 | valid_accuracy: 0.89997 |  0:30:56s\n",
            "epoch 96 | loss: 0.23316 | train_logloss: 0.22796 | train_accuracy: 0.91209 | valid_logloss: 0.2679  | valid_accuracy: 0.90017 |  0:31:16s\n",
            "epoch 97 | loss: 0.23318 | train_logloss: 0.22693 | train_accuracy: 0.91292 | valid_logloss: 0.26469 | valid_accuracy: 0.90085 |  0:31:35s\n",
            "epoch 98 | loss: 0.23308 | train_logloss: 0.22573 | train_accuracy: 0.91324 | valid_logloss: 0.26729 | valid_accuracy: 0.90004 |  0:31:54s\n",
            "epoch 99 | loss: 0.23206 | train_logloss: 0.22433 | train_accuracy: 0.91332 | valid_logloss: 0.26853 | valid_accuracy: 0.90005 |  0:32:13s\n",
            "epoch 100| loss: 0.23161 | train_logloss: 0.22649 | train_accuracy: 0.91301 | valid_logloss: 0.26847 | valid_accuracy: 0.89949 |  0:32:33s\n",
            "epoch 101| loss: 0.23135 | train_logloss: 0.22432 | train_accuracy: 0.91408 | valid_logloss: 0.26945 | valid_accuracy: 0.90014 |  0:32:52s\n",
            "epoch 102| loss: 0.23085 | train_logloss: 0.2226  | train_accuracy: 0.91436 | valid_logloss: 0.26999 | valid_accuracy: 0.90036 |  0:33:11s\n",
            "epoch 103| loss: 0.2324  | train_logloss: 0.24624 | train_accuracy: 0.90666 | valid_logloss: 0.27862 | valid_accuracy: 0.89772 |  0:33:30s\n",
            "epoch 104| loss: 0.23666 | train_logloss: 0.22636 | train_accuracy: 0.91308 | valid_logloss: 0.27185 | valid_accuracy: 0.89912 |  0:33:49s\n",
            "epoch 105| loss: 0.23017 | train_logloss: 0.22057 | train_accuracy: 0.91559 | valid_logloss: 0.27196 | valid_accuracy: 0.9001  |  0:34:09s\n",
            "epoch 106| loss: 0.22978 | train_logloss: 0.22006 | train_accuracy: 0.91526 | valid_logloss: 0.26905 | valid_accuracy: 0.90067 |  0:34:28s\n",
            "epoch 107| loss: 0.22809 | train_logloss: 0.22132 | train_accuracy: 0.91523 | valid_logloss: 0.27546 | valid_accuracy: 0.89885 |  0:34:47s\n",
            "epoch 108| loss: 0.22764 | train_logloss: 0.22035 | train_accuracy: 0.91523 | valid_logloss: 0.27324 | valid_accuracy: 0.89933 |  0:35:07s\n",
            "epoch 109| loss: 0.22664 | train_logloss: 0.21828 | train_accuracy: 0.91581 | valid_logloss: 0.27535 | valid_accuracy: 0.89979 |  0:35:26s\n",
            "epoch 110| loss: 0.22689 | train_logloss: 0.22246 | train_accuracy: 0.91359 | valid_logloss: 0.27869 | valid_accuracy: 0.89792 |  0:35:46s\n",
            "epoch 111| loss: 0.22561 | train_logloss: 0.21975 | train_accuracy: 0.915   | valid_logloss: 0.27423 | valid_accuracy: 0.89925 |  0:36:05s\n",
            "epoch 112| loss: 0.2256  | train_logloss: 0.22066 | train_accuracy: 0.91564 | valid_logloss: 0.27397 | valid_accuracy: 0.90026 |  0:36:25s\n",
            "epoch 113| loss: 0.22525 | train_logloss: 0.21606 | train_accuracy: 0.91685 | valid_logloss: 0.27634 | valid_accuracy: 0.89879 |  0:36:45s\n",
            "epoch 114| loss: 0.22357 | train_logloss: 0.21604 | train_accuracy: 0.91649 | valid_logloss: 0.27748 | valid_accuracy: 0.89835 |  0:37:05s\n",
            "epoch 115| loss: 0.22239 | train_logloss: 0.21715 | train_accuracy: 0.91647 | valid_logloss: 0.27697 | valid_accuracy: 0.89851 |  0:37:25s\n",
            "epoch 116| loss: 0.22265 | train_logloss: 0.21589 | train_accuracy: 0.91678 | valid_logloss: 0.27751 | valid_accuracy: 0.89796 |  0:37:46s\n",
            "epoch 117| loss: 0.2212  | train_logloss: 0.21281 | train_accuracy: 0.91823 | valid_logloss: 0.27956 | valid_accuracy: 0.89887 |  0:38:06s\n",
            "epoch 118| loss: 0.22081 | train_logloss: 0.21169 | train_accuracy: 0.91834 | valid_logloss: 0.2828  | valid_accuracy: 0.89715 |  0:38:26s\n",
            "epoch 119| loss: 0.22103 | train_logloss: 0.21005 | train_accuracy: 0.91935 | valid_logloss: 0.27791 | valid_accuracy: 0.89873 |  0:38:45s\n",
            "epoch 120| loss: 0.2204  | train_logloss: 0.21064 | train_accuracy: 0.91891 | valid_logloss: 0.28232 | valid_accuracy: 0.89703 |  0:39:05s\n",
            "epoch 121| loss: 0.22109 | train_logloss: 0.21296 | train_accuracy: 0.91835 | valid_logloss: 0.27888 | valid_accuracy: 0.89816 |  0:39:25s\n",
            "epoch 122| loss: 0.21961 | train_logloss: 0.2117  | train_accuracy: 0.91843 | valid_logloss: 0.28213 | valid_accuracy: 0.89799 |  0:39:44s\n",
            "epoch 123| loss: 0.21909 | train_logloss: 0.21215 | train_accuracy: 0.91829 | valid_logloss: 0.27814 | valid_accuracy: 0.89875 |  0:40:04s\n",
            "epoch 124| loss: 0.21803 | train_logloss: 0.21027 | train_accuracy: 0.91881 | valid_logloss: 0.28323 | valid_accuracy: 0.89758 |  0:40:24s\n",
            "epoch 125| loss: 0.21693 | train_logloss: 0.2098  | train_accuracy: 0.91884 | valid_logloss: 0.28498 | valid_accuracy: 0.89793 |  0:40:43s\n",
            "epoch 126| loss: 0.216   | train_logloss: 0.20794 | train_accuracy: 0.92003 | valid_logloss: 0.28261 | valid_accuracy: 0.89794 |  0:41:02s\n",
            "epoch 127| loss: 0.21544 | train_logloss: 0.20676 | train_accuracy: 0.92053 | valid_logloss: 0.28642 | valid_accuracy: 0.89726 |  0:41:22s\n",
            "epoch 128| loss: 0.21685 | train_logloss: 0.20702 | train_accuracy: 0.92007 | valid_logloss: 0.28367 | valid_accuracy: 0.89651 |  0:41:43s\n",
            "\n",
            "Early stopping occurred at epoch 128 with best_epoch = 78 and best_valid_accuracy = 0.9023\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[422938  10318  46926]\n",
            " [ 13490 955292  31218]\n",
            " [ 28859  27173 376555]]\n",
            "Testing Score:  0.9023\n",
            "Confusion Matrix: \n",
            " [[425658  10373  44151]\n",
            " [ 14052 956071  29877]\n",
            " [ 32145  27139 373303]]\n",
            "Testing Score:  0.9016666666666666\n",
            "{'Rows': 300000, 'Nd': 64, 'Na': 64, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 300000, 'test': 300000, 'time_learn_tn': 2508.685421228409, 'time_tn': 29.672779321670532, 'accuracy_tn': 0.9023, 'time_learn_gb': 135.6886773109436, 'time_gb': 95.71342277526855, 'accuracy_gb': 0.9016666666666666}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "0yI1_hAVWghd",
        "outputId": "479858d1-7e23-4744-acfb-bdeb4795d6ee"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ... accuracy_gb time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...    0.883778      3.483706  16.722444\n",
              "1     9000    9000    9000  ...    0.883778      3.464101  17.349063\n",
              "2     9000    9000    9000  ...    0.883778      3.411332  16.978554\n",
              "3     9000    9000    9000  ...    0.883778      3.509229  17.331297\n",
              "4     9000    9000    9000  ...    0.883778      3.475400  17.451218\n",
              "5    30000   30000   30000  ...    0.891767      8.276289  27.093480\n",
              "6    30000   30000   30000  ...    0.891767      8.210687  27.343578\n",
              "7    30000   30000   30000  ...    0.891767      8.244918  28.102216\n",
              "8    30000   30000   30000  ...    0.891767      8.093423  27.197022\n",
              "9    30000   30000   30000  ...    0.891767      7.857594  27.867677\n",
              "10  300000  300000  300000  ...    0.901553    140.282187  95.826268\n",
              "11  300000  300000  300000  ...    0.901490    122.025770  81.670524\n",
              "12  300000  300000  300000  ...    0.901667    135.688677  95.713423\n",
              "\n",
              "[13 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbiQR28y-7ye"
      },
      "source": [
        "##3000000 rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu55irMfRcgQ"
      },
      "source": [
        "time_model(number_exp=14, \n",
        "     Rows=3000000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pywqp7G0WhEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6dbf38-83f1-4c5a-a6b8-bc5f4a1f966f"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885889</td>\n",
              "      <td>240.117316</td>\n",
              "      <td>77.182918</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>1.362470</td>\n",
              "      <td>2.113432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.922444</td>\n",
              "      <td>15321.927262</td>\n",
              "      <td>13.301398</td>\n",
              "      <td>0.922553</td>\n",
              "      <td>1216.291787</td>\n",
              "      <td>163.861900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rows    train     test  ...  accuracy_gb  time_learn_gb     time_gb\n",
              "0      9000     9000     9000  ...     0.883778       3.483706   16.722444\n",
              "1      9000     9000     9000  ...     0.883778       3.464101   17.349063\n",
              "2      9000     9000     9000  ...     0.883778       3.411332   16.978554\n",
              "3      9000     9000     9000  ...     0.883778       3.509229   17.331297\n",
              "4      9000     9000     9000  ...     0.883778       3.475400   17.451218\n",
              "5     30000    30000    30000  ...     0.891767       8.276289   27.093480\n",
              "6     30000    30000    30000  ...     0.891767       8.210687   27.343578\n",
              "7     30000    30000    30000  ...     0.891767       8.244918   28.102216\n",
              "8     30000    30000    30000  ...     0.891767       8.093423   27.197022\n",
              "9     30000    30000    30000  ...     0.891767       7.857594   27.867677\n",
              "10   300000   300000   300000  ...     0.901553     140.282187   95.826268\n",
              "11   300000   300000   300000  ...     0.901490     122.025770   81.670524\n",
              "12   300000   300000   300000  ...     0.901667     135.688677   95.713423\n",
              "13     9000     9000     9000  ...     0.883778       1.362470    2.113432\n",
              "14  3000000  1912767  1912769  ...     0.922553    1216.291787  163.861900\n",
              "\n",
              "[15 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TXq3EEIRcgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73b16b8-2a14-430f-b8ac-7925cab3a704"
      },
      "source": [
        "time_model(number_exp=15, \n",
        "     Rows=3000000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.34006 | train_logloss: 2.54978 | train_accuracy: 0.37824 | valid_logloss: 2.54953 | valid_accuracy: 0.37806 |  0:01:01s\n",
            "epoch 1  | loss: 0.26049 | train_logloss: 0.64599 | train_accuracy: 0.7374  | valid_logloss: 0.64489 | valid_accuracy: 0.73758 |  0:02:03s\n",
            "epoch 2  | loss: 0.25213 | train_logloss: 0.31135 | train_accuracy: 0.88725 | valid_logloss: 0.31095 | valid_accuracy: 0.88724 |  0:03:05s\n",
            "epoch 3  | loss: 0.24525 | train_logloss: 0.24858 | train_accuracy: 0.91084 | valid_logloss: 0.24871 | valid_accuracy: 0.91103 |  0:04:06s\n",
            "epoch 4  | loss: 0.24122 | train_logloss: 0.23694 | train_accuracy: 0.91453 | valid_logloss: 0.2372  | valid_accuracy: 0.91462 |  0:05:08s\n",
            "epoch 5  | loss: 0.23854 | train_logloss: 0.23283 | train_accuracy: 0.91661 | valid_logloss: 0.23323 | valid_accuracy: 0.91673 |  0:06:10s\n",
            "epoch 6  | loss: 0.23613 | train_logloss: 0.233   | train_accuracy: 0.91612 | valid_logloss: 0.23331 | valid_accuracy: 0.91625 |  0:07:12s\n",
            "epoch 7  | loss: 0.23466 | train_logloss: 0.22868 | train_accuracy: 0.91836 | valid_logloss: 0.22921 | valid_accuracy: 0.91846 |  0:08:14s\n",
            "epoch 8  | loss: 0.23834 | train_logloss: 0.23283 | train_accuracy: 0.91619 | valid_logloss: 0.23343 | valid_accuracy: 0.9161  |  0:09:16s\n",
            "epoch 9  | loss: 0.23335 | train_logloss: 0.22951 | train_accuracy: 0.91839 | valid_logloss: 0.23003 | valid_accuracy: 0.91846 |  0:10:18s\n",
            "epoch 10 | loss: 0.23079 | train_logloss: 0.22636 | train_accuracy: 0.91879 | valid_logloss: 0.22713 | valid_accuracy: 0.91882 |  0:11:19s\n",
            "epoch 11 | loss: 0.23009 | train_logloss: 0.2247  | train_accuracy: 0.91928 | valid_logloss: 0.22555 | valid_accuracy: 0.91929 |  0:12:21s\n",
            "epoch 12 | loss: 0.22909 | train_logloss: 0.22652 | train_accuracy: 0.91877 | valid_logloss: 0.22726 | valid_accuracy: 0.91879 |  0:13:23s\n",
            "epoch 13 | loss: 0.2287  | train_logloss: 0.2238  | train_accuracy: 0.91988 | valid_logloss: 0.2246  | valid_accuracy: 0.9199  |  0:14:24s\n",
            "epoch 14 | loss: 0.22773 | train_logloss: 0.22338 | train_accuracy: 0.9201  | valid_logloss: 0.22425 | valid_accuracy: 0.92006 |  0:15:26s\n",
            "epoch 15 | loss: 0.22721 | train_logloss: 0.22327 | train_accuracy: 0.92006 | valid_logloss: 0.22447 | valid_accuracy: 0.91983 |  0:16:28s\n",
            "epoch 16 | loss: 0.22595 | train_logloss: 0.2232  | train_accuracy: 0.92021 | valid_logloss: 0.22434 | valid_accuracy: 0.92008 |  0:17:30s\n",
            "epoch 17 | loss: 0.22555 | train_logloss: 0.22545 | train_accuracy: 0.91973 | valid_logloss: 0.22628 | valid_accuracy: 0.91953 |  0:18:32s\n",
            "epoch 18 | loss: 0.22557 | train_logloss: 0.22356 | train_accuracy: 0.91969 | valid_logloss: 0.22465 | valid_accuracy: 0.91943 |  0:19:33s\n",
            "epoch 19 | loss: 0.22586 | train_logloss: 0.22203 | train_accuracy: 0.92071 | valid_logloss: 0.22321 | valid_accuracy: 0.92054 |  0:20:35s\n",
            "epoch 20 | loss: 0.22892 | train_logloss: 0.22386 | train_accuracy: 0.92018 | valid_logloss: 0.22477 | valid_accuracy: 0.92014 |  0:21:37s\n",
            "epoch 21 | loss: 0.22563 | train_logloss: 0.22451 | train_accuracy: 0.91981 | valid_logloss: 0.22531 | valid_accuracy: 0.91969 |  0:22:39s\n",
            "epoch 22 | loss: 0.22581 | train_logloss: 0.2231  | train_accuracy: 0.92012 | valid_logloss: 0.22441 | valid_accuracy: 0.91988 |  0:23:40s\n",
            "epoch 23 | loss: 0.22421 | train_logloss: 0.22284 | train_accuracy: 0.9203  | valid_logloss: 0.22404 | valid_accuracy: 0.92002 |  0:24:42s\n",
            "epoch 24 | loss: 0.22388 | train_logloss: 0.22274 | train_accuracy: 0.91993 | valid_logloss: 0.22406 | valid_accuracy: 0.91964 |  0:25:44s\n",
            "epoch 25 | loss: 0.22407 | train_logloss: 0.22197 | train_accuracy: 0.92013 | valid_logloss: 0.22343 | valid_accuracy: 0.91966 |  0:26:46s\n",
            "epoch 26 | loss: 0.22305 | train_logloss: 0.22022 | train_accuracy: 0.9213  | valid_logloss: 0.22158 | valid_accuracy: 0.921   |  0:27:47s\n",
            "epoch 27 | loss: 0.22299 | train_logloss: 0.22453 | train_accuracy: 0.91998 | valid_logloss: 0.2257  | valid_accuracy: 0.91972 |  0:28:49s\n",
            "epoch 28 | loss: 0.22501 | train_logloss: 0.22225 | train_accuracy: 0.92058 | valid_logloss: 0.22325 | valid_accuracy: 0.9204  |  0:29:51s\n",
            "epoch 29 | loss: 0.22352 | train_logloss: 0.22193 | train_accuracy: 0.92079 | valid_logloss: 0.22328 | valid_accuracy: 0.92045 |  0:30:52s\n",
            "epoch 30 | loss: 0.22291 | train_logloss: 0.22147 | train_accuracy: 0.92097 | valid_logloss: 0.22275 | valid_accuracy: 0.92072 |  0:31:54s\n",
            "epoch 31 | loss: 0.22305 | train_logloss: 0.22112 | train_accuracy: 0.92098 | valid_logloss: 0.22231 | valid_accuracy: 0.92065 |  0:32:56s\n",
            "epoch 32 | loss: 0.22253 | train_logloss: 0.22032 | train_accuracy: 0.92134 | valid_logloss: 0.22189 | valid_accuracy: 0.92093 |  0:33:57s\n",
            "epoch 33 | loss: 0.22223 | train_logloss: 0.22021 | train_accuracy: 0.92135 | valid_logloss: 0.22166 | valid_accuracy: 0.92103 |  0:34:59s\n",
            "epoch 34 | loss: 0.22197 | train_logloss: 0.21882 | train_accuracy: 0.92162 | valid_logloss: 0.22037 | valid_accuracy: 0.92131 |  0:36:00s\n",
            "epoch 35 | loss: 0.22281 | train_logloss: 0.22223 | train_accuracy: 0.92053 | valid_logloss: 0.22369 | valid_accuracy: 0.92018 |  0:37:02s\n",
            "epoch 36 | loss: 0.22196 | train_logloss: 0.21939 | train_accuracy: 0.92187 | valid_logloss: 0.2209  | valid_accuracy: 0.92155 |  0:38:04s\n",
            "epoch 37 | loss: 0.2213  | train_logloss: 0.21831 | train_accuracy: 0.92184 | valid_logloss: 0.21981 | valid_accuracy: 0.92167 |  0:39:05s\n",
            "epoch 38 | loss: 0.22129 | train_logloss: 0.22256 | train_accuracy: 0.92006 | valid_logloss: 0.22424 | valid_accuracy: 0.91964 |  0:40:07s\n",
            "epoch 39 | loss: 0.22086 | train_logloss: 0.21766 | train_accuracy: 0.92227 | valid_logloss: 0.21935 | valid_accuracy: 0.92196 |  0:41:09s\n",
            "epoch 40 | loss: 0.22214 | train_logloss: 0.22054 | train_accuracy: 0.92085 | valid_logloss: 0.22192 | valid_accuracy: 0.92044 |  0:42:10s\n",
            "epoch 41 | loss: 0.22351 | train_logloss: 0.22226 | train_accuracy: 0.9206  | valid_logloss: 0.22404 | valid_accuracy: 0.92019 |  0:43:12s\n",
            "epoch 42 | loss: 0.22102 | train_logloss: 0.21862 | train_accuracy: 0.92153 | valid_logloss: 0.22032 | valid_accuracy: 0.92119 |  0:44:14s\n",
            "epoch 43 | loss: 0.22039 | train_logloss: 0.21796 | train_accuracy: 0.92194 | valid_logloss: 0.21974 | valid_accuracy: 0.92147 |  0:45:16s\n",
            "epoch 44 | loss: 0.2207  | train_logloss: 0.21775 | train_accuracy: 0.92201 | valid_logloss: 0.21942 | valid_accuracy: 0.92159 |  0:46:17s\n",
            "epoch 45 | loss: 0.22003 | train_logloss: 0.21733 | train_accuracy: 0.92242 | valid_logloss: 0.21921 | valid_accuracy: 0.92199 |  0:47:19s\n",
            "epoch 46 | loss: 0.22012 | train_logloss: 0.21861 | train_accuracy: 0.92169 | valid_logloss: 0.22043 | valid_accuracy: 0.92126 |  0:48:21s\n",
            "epoch 47 | loss: 0.22064 | train_logloss: 0.22409 | train_accuracy: 0.92034 | valid_logloss: 0.22543 | valid_accuracy: 0.91998 |  0:49:22s\n",
            "epoch 48 | loss: 0.22159 | train_logloss: 0.21728 | train_accuracy: 0.92247 | valid_logloss: 0.21931 | valid_accuracy: 0.9218  |  0:50:24s\n",
            "epoch 49 | loss: 0.22033 | train_logloss: 0.21877 | train_accuracy: 0.92189 | valid_logloss: 0.2209  | valid_accuracy: 0.92132 |  0:51:26s\n",
            "epoch 50 | loss: 0.22016 | train_logloss: 0.21912 | train_accuracy: 0.92121 | valid_logloss: 0.22073 | valid_accuracy: 0.92088 |  0:52:28s\n",
            "epoch 51 | loss: 0.22019 | train_logloss: 0.21841 | train_accuracy: 0.92206 | valid_logloss: 0.2204  | valid_accuracy: 0.92144 |  0:53:30s\n",
            "epoch 52 | loss: 0.22077 | train_logloss: 0.21652 | train_accuracy: 0.92252 | valid_logloss: 0.21849 | valid_accuracy: 0.92211 |  0:54:32s\n",
            "epoch 53 | loss: 0.21954 | train_logloss: 0.21685 | train_accuracy: 0.92262 | valid_logloss: 0.21892 | valid_accuracy: 0.92189 |  0:55:33s\n",
            "epoch 54 | loss: 0.21932 | train_logloss: 0.21854 | train_accuracy: 0.92188 | valid_logloss: 0.22062 | valid_accuracy: 0.92127 |  0:56:35s\n",
            "epoch 55 | loss: 0.22001 | train_logloss: 0.21783 | train_accuracy: 0.92202 | valid_logloss: 0.21994 | valid_accuracy: 0.9215  |  0:57:37s\n",
            "epoch 56 | loss: 0.22128 | train_logloss: 0.21852 | train_accuracy: 0.92177 | valid_logloss: 0.22038 | valid_accuracy: 0.92138 |  0:58:39s\n",
            "epoch 57 | loss: 0.22074 | train_logloss: 0.21686 | train_accuracy: 0.92239 | valid_logloss: 0.21863 | valid_accuracy: 0.92193 |  0:59:40s\n",
            "epoch 58 | loss: 0.21968 | train_logloss: 0.21805 | train_accuracy: 0.92184 | valid_logloss: 0.22001 | valid_accuracy: 0.92135 |  1:00:42s\n",
            "epoch 59 | loss: 0.2192  | train_logloss: 0.21624 | train_accuracy: 0.92282 | valid_logloss: 0.21846 | valid_accuracy: 0.92226 |  1:01:44s\n",
            "epoch 60 | loss: 0.21897 | train_logloss: 0.21571 | train_accuracy: 0.92267 | valid_logloss: 0.21799 | valid_accuracy: 0.92221 |  1:02:46s\n",
            "epoch 61 | loss: 0.21916 | train_logloss: 0.21694 | train_accuracy: 0.92208 | valid_logloss: 0.21921 | valid_accuracy: 0.92157 |  1:03:48s\n",
            "epoch 62 | loss: 0.21882 | train_logloss: 0.21753 | train_accuracy: 0.92194 | valid_logloss: 0.21966 | valid_accuracy: 0.9214  |  1:04:50s\n",
            "epoch 63 | loss: 0.21865 | train_logloss: 0.2162  | train_accuracy: 0.92277 | valid_logloss: 0.2187  | valid_accuracy: 0.92205 |  1:05:52s\n",
            "epoch 64 | loss: 0.21832 | train_logloss: 0.21509 | train_accuracy: 0.92299 | valid_logloss: 0.21753 | valid_accuracy: 0.92236 |  1:06:53s\n",
            "epoch 65 | loss: 0.21819 | train_logloss: 0.21572 | train_accuracy: 0.92262 | valid_logloss: 0.21811 | valid_accuracy: 0.92199 |  1:07:55s\n",
            "epoch 66 | loss: 0.21859 | train_logloss: 0.21568 | train_accuracy: 0.92296 | valid_logloss: 0.2185  | valid_accuracy: 0.92227 |  1:08:57s\n",
            "epoch 67 | loss: 0.21815 | train_logloss: 0.21703 | train_accuracy: 0.9225  | valid_logloss: 0.21939 | valid_accuracy: 0.92179 |  1:09:58s\n",
            "epoch 68 | loss: 0.21805 | train_logloss: 0.21566 | train_accuracy: 0.92291 | valid_logloss: 0.21835 | valid_accuracy: 0.92226 |  1:11:00s\n",
            "epoch 69 | loss: 0.21809 | train_logloss: 0.21521 | train_accuracy: 0.923   | valid_logloss: 0.21768 | valid_accuracy: 0.92234 |  1:12:02s\n",
            "epoch 70 | loss: 0.21817 | train_logloss: 0.2163  | train_accuracy: 0.92291 | valid_logloss: 0.21895 | valid_accuracy: 0.92228 |  1:13:04s\n",
            "epoch 71 | loss: 0.21828 | train_logloss: 0.21522 | train_accuracy: 0.9231  | valid_logloss: 0.21802 | valid_accuracy: 0.92238 |  1:14:05s\n",
            "epoch 72 | loss: 0.21835 | train_logloss: 0.21648 | train_accuracy: 0.92254 | valid_logloss: 0.21928 | valid_accuracy: 0.92169 |  1:15:07s\n",
            "epoch 73 | loss: 0.21809 | train_logloss: 0.21557 | train_accuracy: 0.92277 | valid_logloss: 0.21799 | valid_accuracy: 0.9221  |  1:16:09s\n",
            "epoch 74 | loss: 0.21807 | train_logloss: 0.21653 | train_accuracy: 0.92255 | valid_logloss: 0.21917 | valid_accuracy: 0.92167 |  1:17:11s\n",
            "epoch 75 | loss: 0.21837 | train_logloss: 0.21721 | train_accuracy: 0.92224 | valid_logloss: 0.21982 | valid_accuracy: 0.92164 |  1:18:12s\n",
            "epoch 76 | loss: 0.21809 | train_logloss: 0.21494 | train_accuracy: 0.92301 | valid_logloss: 0.218   | valid_accuracy: 0.92225 |  1:19:14s\n",
            "epoch 77 | loss: 0.2173  | train_logloss: 0.21456 | train_accuracy: 0.92321 | valid_logloss: 0.21772 | valid_accuracy: 0.9222  |  1:20:16s\n",
            "epoch 78 | loss: 0.21807 | train_logloss: 0.21702 | train_accuracy: 0.92229 | valid_logloss: 0.21963 | valid_accuracy: 0.92146 |  1:21:17s\n",
            "epoch 79 | loss: 0.2175  | train_logloss: 0.21584 | train_accuracy: 0.92277 | valid_logloss: 0.21873 | valid_accuracy: 0.92193 |  1:22:19s\n",
            "epoch 80 | loss: 0.21797 | train_logloss: 0.21763 | train_accuracy: 0.9223  | valid_logloss: 0.22044 | valid_accuracy: 0.92158 |  1:23:21s\n",
            "epoch 81 | loss: 0.21806 | train_logloss: 0.21659 | train_accuracy: 0.92264 | valid_logloss: 0.21925 | valid_accuracy: 0.92196 |  1:24:23s\n",
            "epoch 82 | loss: 0.21789 | train_logloss: 0.21612 | train_accuracy: 0.92254 | valid_logloss: 0.21907 | valid_accuracy: 0.92184 |  1:25:24s\n",
            "epoch 83 | loss: 0.21766 | train_logloss: 0.21571 | train_accuracy: 0.92278 | valid_logloss: 0.21849 | valid_accuracy: 0.92193 |  1:26:25s\n",
            "epoch 84 | loss: 0.21784 | train_logloss: 0.21381 | train_accuracy: 0.92351 | valid_logloss: 0.2168  | valid_accuracy: 0.92263 |  1:27:27s\n",
            "epoch 85 | loss: 0.2179  | train_logloss: 0.21462 | train_accuracy: 0.92325 | valid_logloss: 0.21754 | valid_accuracy: 0.92228 |  1:28:28s\n",
            "epoch 86 | loss: 0.21761 | train_logloss: 0.21726 | train_accuracy: 0.9225  | valid_logloss: 0.22017 | valid_accuracy: 0.92167 |  1:29:29s\n",
            "epoch 87 | loss: 0.21799 | train_logloss: 0.21536 | train_accuracy: 0.92296 | valid_logloss: 0.21816 | valid_accuracy: 0.92236 |  1:30:31s\n",
            "epoch 88 | loss: 0.21749 | train_logloss: 0.21477 | train_accuracy: 0.923   | valid_logloss: 0.21803 | valid_accuracy: 0.92226 |  1:31:32s\n",
            "epoch 89 | loss: 0.21704 | train_logloss: 0.21438 | train_accuracy: 0.92332 | valid_logloss: 0.21746 | valid_accuracy: 0.92243 |  1:32:33s\n",
            "epoch 90 | loss: 0.21708 | train_logloss: 0.21455 | train_accuracy: 0.92304 | valid_logloss: 0.21791 | valid_accuracy: 0.92218 |  1:33:34s\n",
            "epoch 91 | loss: 0.21688 | train_logloss: 0.21345 | train_accuracy: 0.92368 | valid_logloss: 0.21669 | valid_accuracy: 0.92278 |  1:34:35s\n",
            "epoch 92 | loss: 0.21708 | train_logloss: 0.21403 | train_accuracy: 0.9233  | valid_logloss: 0.21735 | valid_accuracy: 0.92245 |  1:35:37s\n",
            "epoch 93 | loss: 0.21734 | train_logloss: 0.21476 | train_accuracy: 0.92308 | valid_logloss: 0.2179  | valid_accuracy: 0.9223  |  1:36:39s\n",
            "epoch 94 | loss: 0.21695 | train_logloss: 0.2146  | train_accuracy: 0.92302 | valid_logloss: 0.21775 | valid_accuracy: 0.92206 |  1:37:40s\n",
            "epoch 95 | loss: 0.2165  | train_logloss: 0.21392 | train_accuracy: 0.92336 | valid_logloss: 0.21759 | valid_accuracy: 0.92242 |  1:38:42s\n",
            "epoch 96 | loss: 0.21866 | train_logloss: 0.21697 | train_accuracy: 0.92233 | valid_logloss: 0.21917 | valid_accuracy: 0.92174 |  1:39:43s\n",
            "epoch 97 | loss: 0.21873 | train_logloss: 0.21527 | train_accuracy: 0.9229  | valid_logloss: 0.21805 | valid_accuracy: 0.92212 |  1:40:45s\n",
            "epoch 98 | loss: 0.21804 | train_logloss: 0.21703 | train_accuracy: 0.92226 | valid_logloss: 0.21986 | valid_accuracy: 0.92138 |  1:41:46s\n",
            "epoch 99 | loss: 0.21993 | train_logloss: 0.21622 | train_accuracy: 0.92284 | valid_logloss: 0.21904 | valid_accuracy: 0.92197 |  1:42:48s\n",
            "epoch 100| loss: 0.21843 | train_logloss: 0.21525 | train_accuracy: 0.92301 | valid_logloss: 0.21801 | valid_accuracy: 0.92205 |  1:43:49s\n",
            "epoch 101| loss: 0.21819 | train_logloss: 0.21547 | train_accuracy: 0.92307 | valid_logloss: 0.21848 | valid_accuracy: 0.92223 |  1:44:51s\n",
            "epoch 102| loss: 0.218   | train_logloss: 0.2145  | train_accuracy: 0.92352 | valid_logloss: 0.21763 | valid_accuracy: 0.92269 |  1:45:52s\n",
            "epoch 103| loss: 0.21739 | train_logloss: 0.21609 | train_accuracy: 0.92292 | valid_logloss: 0.21906 | valid_accuracy: 0.92216 |  1:46:53s\n",
            "epoch 104| loss: 0.21671 | train_logloss: 0.21447 | train_accuracy: 0.92357 | valid_logloss: 0.21771 | valid_accuracy: 0.92252 |  1:47:55s\n",
            "epoch 105| loss: 0.217   | train_logloss: 0.21279 | train_accuracy: 0.92373 | valid_logloss: 0.21622 | valid_accuracy: 0.9228  |  1:48:56s\n",
            "epoch 106| loss: 0.21703 | train_logloss: 0.21579 | train_accuracy: 0.92289 | valid_logloss: 0.21883 | valid_accuracy: 0.92195 |  1:49:58s\n",
            "epoch 107| loss: 0.21663 | train_logloss: 0.21471 | train_accuracy: 0.92301 | valid_logloss: 0.21806 | valid_accuracy: 0.9221  |  1:50:59s\n",
            "epoch 108| loss: 0.21767 | train_logloss: 0.21553 | train_accuracy: 0.92283 | valid_logloss: 0.21875 | valid_accuracy: 0.92195 |  1:52:01s\n",
            "epoch 109| loss: 0.21626 | train_logloss: 0.21316 | train_accuracy: 0.92355 | valid_logloss: 0.21669 | valid_accuracy: 0.92249 |  1:53:02s\n",
            "epoch 110| loss: 0.21608 | train_logloss: 0.21285 | train_accuracy: 0.92373 | valid_logloss: 0.21651 | valid_accuracy: 0.92277 |  1:54:03s\n",
            "epoch 111| loss: 0.21589 | train_logloss: 0.21327 | train_accuracy: 0.92365 | valid_logloss: 0.21687 | valid_accuracy: 0.92258 |  1:55:05s\n",
            "epoch 112| loss: 0.21595 | train_logloss: 0.21485 | train_accuracy: 0.92326 | valid_logloss: 0.21868 | valid_accuracy: 0.92209 |  1:56:06s\n",
            "epoch 113| loss: 0.2179  | train_logloss: 0.21636 | train_accuracy: 0.92304 | valid_logloss: 0.21935 | valid_accuracy: 0.92215 |  1:57:08s\n",
            "epoch 114| loss: 0.22121 | train_logloss: 0.21883 | train_accuracy: 0.92189 | valid_logloss: 0.22082 | valid_accuracy: 0.92153 |  1:58:09s\n",
            "epoch 115| loss: 0.2194  | train_logloss: 0.21739 | train_accuracy: 0.92244 | valid_logloss: 0.21982 | valid_accuracy: 0.92177 |  1:59:10s\n",
            "epoch 116| loss: 0.2181  | train_logloss: 0.21633 | train_accuracy: 0.92254 | valid_logloss: 0.21929 | valid_accuracy: 0.92196 |  2:00:11s\n",
            "epoch 117| loss: 0.21894 | train_logloss: 0.21575 | train_accuracy: 0.92276 | valid_logloss: 0.21843 | valid_accuracy: 0.92223 |  2:01:13s\n",
            "epoch 118| loss: 0.21682 | train_logloss: 0.21404 | train_accuracy: 0.92352 | valid_logloss: 0.21723 | valid_accuracy: 0.92259 |  2:02:14s\n",
            "epoch 119| loss: 0.21678 | train_logloss: 0.2146  | train_accuracy: 0.92318 | valid_logloss: 0.21811 | valid_accuracy: 0.92226 |  2:03:16s\n",
            "epoch 120| loss: 0.21641 | train_logloss: 0.21429 | train_accuracy: 0.92358 | valid_logloss: 0.21769 | valid_accuracy: 0.9224  |  2:04:17s\n",
            "epoch 121| loss: 0.21688 | train_logloss: 0.2169  | train_accuracy: 0.92275 | valid_logloss: 0.21997 | valid_accuracy: 0.92195 |  2:05:19s\n",
            "epoch 122| loss: 0.21739 | train_logloss: 0.21431 | train_accuracy: 0.92343 | valid_logloss: 0.21786 | valid_accuracy: 0.92246 |  2:06:21s\n",
            "epoch 123| loss: 0.21728 | train_logloss: 0.21551 | train_accuracy: 0.92286 | valid_logloss: 0.2188  | valid_accuracy: 0.92204 |  2:07:22s\n",
            "epoch 124| loss: 0.21662 | train_logloss: 0.21533 | train_accuracy: 0.92296 | valid_logloss: 0.21881 | valid_accuracy: 0.92201 |  2:08:24s\n",
            "epoch 125| loss: 0.21957 | train_logloss: 0.21467 | train_accuracy: 0.92342 | valid_logloss: 0.21799 | valid_accuracy: 0.92232 |  2:09:26s\n",
            "epoch 126| loss: 0.21707 | train_logloss: 0.21356 | train_accuracy: 0.92352 | valid_logloss: 0.21704 | valid_accuracy: 0.9227  |  2:10:27s\n",
            "epoch 127| loss: 0.21659 | train_logloss: 0.21315 | train_accuracy: 0.92374 | valid_logloss: 0.21688 | valid_accuracy: 0.9227  |  2:11:29s\n",
            "epoch 128| loss: 0.21612 | train_logloss: 0.21414 | train_accuracy: 0.92336 | valid_logloss: 0.21777 | valid_accuracy: 0.92251 |  2:12:31s\n",
            "epoch 129| loss: 0.2164  | train_logloss: 0.21349 | train_accuracy: 0.92375 | valid_logloss: 0.21724 | valid_accuracy: 0.92258 |  2:13:32s\n",
            "epoch 130| loss: 0.21912 | train_logloss: 0.21742 | train_accuracy: 0.9224  | valid_logloss: 0.21985 | valid_accuracy: 0.92172 |  2:14:34s\n",
            "epoch 131| loss: 0.21779 | train_logloss: 0.21431 | train_accuracy: 0.92356 | valid_logloss: 0.21773 | valid_accuracy: 0.92266 |  2:15:36s\n",
            "epoch 132| loss: 0.21657 | train_logloss: 0.21315 | train_accuracy: 0.92388 | valid_logloss: 0.21676 | valid_accuracy: 0.92298 |  2:16:38s\n",
            "epoch 133| loss: 0.21625 | train_logloss: 0.2145  | train_accuracy: 0.92317 | valid_logloss: 0.21837 | valid_accuracy: 0.92228 |  2:17:39s\n",
            "epoch 134| loss: 0.21602 | train_logloss: 0.21515 | train_accuracy: 0.92308 | valid_logloss: 0.21857 | valid_accuracy: 0.92228 |  2:18:40s\n",
            "epoch 135| loss: 0.21679 | train_logloss: 0.21589 | train_accuracy: 0.92293 | valid_logloss: 0.21932 | valid_accuracy: 0.92192 |  2:19:42s\n",
            "epoch 136| loss: 0.21601 | train_logloss: 0.21277 | train_accuracy: 0.92379 | valid_logloss: 0.21664 | valid_accuracy: 0.92271 |  2:20:44s\n",
            "epoch 137| loss: 0.21596 | train_logloss: 0.21442 | train_accuracy: 0.92314 | valid_logloss: 0.21813 | valid_accuracy: 0.92213 |  2:21:46s\n",
            "epoch 138| loss: 0.21527 | train_logloss: 0.21254 | train_accuracy: 0.92392 | valid_logloss: 0.21646 | valid_accuracy: 0.923   |  2:22:48s\n",
            "epoch 139| loss: 0.21528 | train_logloss: 0.214   | train_accuracy: 0.92335 | valid_logloss: 0.21769 | valid_accuracy: 0.92233 |  2:23:49s\n",
            "epoch 140| loss: 0.2154  | train_logloss: 0.21386 | train_accuracy: 0.9236  | valid_logloss: 0.21776 | valid_accuracy: 0.92255 |  2:24:51s\n",
            "epoch 141| loss: 0.21528 | train_logloss: 0.2131  | train_accuracy: 0.92396 | valid_logloss: 0.21736 | valid_accuracy: 0.92277 |  2:25:52s\n",
            "epoch 142| loss: 0.21659 | train_logloss: 0.21348 | train_accuracy: 0.9236  | valid_logloss: 0.21745 | valid_accuracy: 0.9225  |  2:26:54s\n",
            "epoch 143| loss: 0.2155  | train_logloss: 0.21276 | train_accuracy: 0.92378 | valid_logloss: 0.217   | valid_accuracy: 0.92265 |  2:27:56s\n",
            "epoch 144| loss: 0.21537 | train_logloss: 0.21285 | train_accuracy: 0.92381 | valid_logloss: 0.21707 | valid_accuracy: 0.92261 |  2:28:57s\n",
            "epoch 145| loss: 0.21521 | train_logloss: 0.21152 | train_accuracy: 0.92431 | valid_logloss: 0.21586 | valid_accuracy: 0.92312 |  2:29:59s\n",
            "epoch 146| loss: 0.21487 | train_logloss: 0.21257 | train_accuracy: 0.92399 | valid_logloss: 0.21688 | valid_accuracy: 0.92277 |  2:31:00s\n",
            "epoch 147| loss: 0.21511 | train_logloss: 0.21279 | train_accuracy: 0.92374 | valid_logloss: 0.2175  | valid_accuracy: 0.92257 |  2:32:02s\n",
            "epoch 148| loss: 0.21498 | train_logloss: 0.21218 | train_accuracy: 0.9241  | valid_logloss: 0.21663 | valid_accuracy: 0.92288 |  2:33:03s\n",
            "epoch 149| loss: 0.21469 | train_logloss: 0.21192 | train_accuracy: 0.92409 | valid_logloss: 0.21654 | valid_accuracy: 0.92276 |  2:34:05s\n",
            "epoch 150| loss: 0.21464 | train_logloss: 0.21329 | train_accuracy: 0.92358 | valid_logloss: 0.2179  | valid_accuracy: 0.92229 |  2:35:07s\n",
            "epoch 151| loss: 0.21457 | train_logloss: 0.2124  | train_accuracy: 0.92395 | valid_logloss: 0.21738 | valid_accuracy: 0.92255 |  2:36:08s\n",
            "epoch 152| loss: 0.2152  | train_logloss: 0.21497 | train_accuracy: 0.92326 | valid_logloss: 0.21882 | valid_accuracy: 0.92206 |  2:37:10s\n",
            "epoch 153| loss: 0.22354 | train_logloss: 0.2153  | train_accuracy: 0.92321 | valid_logloss: 0.21803 | valid_accuracy: 0.92246 |  2:38:11s\n",
            "epoch 154| loss: 0.21774 | train_logloss: 0.2176  | train_accuracy: 0.92197 | valid_logloss: 0.22037 | valid_accuracy: 0.92122 |  2:39:13s\n",
            "epoch 155| loss: 0.21736 | train_logloss: 0.21421 | train_accuracy: 0.92356 | valid_logloss: 0.21776 | valid_accuracy: 0.92255 |  2:40:14s\n",
            "epoch 156| loss: 0.21639 | train_logloss: 0.2149  | train_accuracy: 0.92324 | valid_logloss: 0.21864 | valid_accuracy: 0.92221 |  2:41:16s\n",
            "epoch 157| loss: 0.21586 | train_logloss: 0.21424 | train_accuracy: 0.92352 | valid_logloss: 0.21783 | valid_accuracy: 0.92254 |  2:42:18s\n",
            "epoch 158| loss: 0.21578 | train_logloss: 0.21394 | train_accuracy: 0.92333 | valid_logloss: 0.21784 | valid_accuracy: 0.92232 |  2:43:20s\n",
            "epoch 159| loss: 0.21564 | train_logloss: 0.21435 | train_accuracy: 0.923   | valid_logloss: 0.2181  | valid_accuracy: 0.9219  |  2:44:21s\n",
            "epoch 160| loss: 0.21533 | train_logloss: 0.21283 | train_accuracy: 0.92369 | valid_logloss: 0.21722 | valid_accuracy: 0.92238 |  2:45:23s\n",
            "epoch 161| loss: 0.21534 | train_logloss: 0.21248 | train_accuracy: 0.92378 | valid_logloss: 0.21683 | valid_accuracy: 0.92248 |  2:46:25s\n",
            "epoch 162| loss: 0.21493 | train_logloss: 0.21268 | train_accuracy: 0.92364 | valid_logloss: 0.2173  | valid_accuracy: 0.92218 |  2:47:27s\n",
            "epoch 163| loss: 0.2156  | train_logloss: 0.21386 | train_accuracy: 0.92342 | valid_logloss: 0.21785 | valid_accuracy: 0.92246 |  2:48:29s\n",
            "epoch 164| loss: 0.21521 | train_logloss: 0.21335 | train_accuracy: 0.92374 | valid_logloss: 0.21778 | valid_accuracy: 0.9226  |  2:49:31s\n",
            "epoch 165| loss: 0.21512 | train_logloss: 0.21315 | train_accuracy: 0.92354 | valid_logloss: 0.2176  | valid_accuracy: 0.9223  |  2:50:32s\n",
            "epoch 166| loss: 0.2148  | train_logloss: 0.2122  | train_accuracy: 0.92392 | valid_logloss: 0.21636 | valid_accuracy: 0.92284 |  2:51:34s\n",
            "epoch 167| loss: 0.21478 | train_logloss: 0.21274 | train_accuracy: 0.9236  | valid_logloss: 0.21733 | valid_accuracy: 0.92224 |  2:52:36s\n",
            "epoch 168| loss: 0.21539 | train_logloss: 0.21307 | train_accuracy: 0.92379 | valid_logloss: 0.21722 | valid_accuracy: 0.9228  |  2:53:38s\n",
            "epoch 169| loss: 0.21499 | train_logloss: 0.21262 | train_accuracy: 0.9239  | valid_logloss: 0.21731 | valid_accuracy: 0.92253 |  2:54:39s\n",
            "epoch 170| loss: 0.21492 | train_logloss: 0.21245 | train_accuracy: 0.92405 | valid_logloss: 0.21696 | valid_accuracy: 0.92266 |  2:55:41s\n",
            "epoch 171| loss: 0.21475 | train_logloss: 0.21238 | train_accuracy: 0.9238  | valid_logloss: 0.21689 | valid_accuracy: 0.92258 |  2:56:43s\n",
            "epoch 172| loss: 0.2143  | train_logloss: 0.2139  | train_accuracy: 0.9234  | valid_logloss: 0.21879 | valid_accuracy: 0.92195 |  2:57:44s\n",
            "epoch 173| loss: 0.21524 | train_logloss: 0.21344 | train_accuracy: 0.92364 | valid_logloss: 0.21773 | valid_accuracy: 0.92224 |  2:58:46s\n",
            "epoch 174| loss: 0.21571 | train_logloss: 0.21312 | train_accuracy: 0.92377 | valid_logloss: 0.21697 | valid_accuracy: 0.92267 |  2:59:47s\n",
            "epoch 175| loss: 0.21495 | train_logloss: 0.2115  | train_accuracy: 0.92423 | valid_logloss: 0.21609 | valid_accuracy: 0.923   |  3:00:48s\n",
            "epoch 176| loss: 0.21464 | train_logloss: 0.21157 | train_accuracy: 0.92426 | valid_logloss: 0.21634 | valid_accuracy: 0.92291 |  3:01:50s\n",
            "epoch 177| loss: 0.21487 | train_logloss: 0.21229 | train_accuracy: 0.92378 | valid_logloss: 0.21692 | valid_accuracy: 0.92232 |  3:02:51s\n",
            "epoch 178| loss: 0.21451 | train_logloss: 0.21164 | train_accuracy: 0.92415 | valid_logloss: 0.21646 | valid_accuracy: 0.92278 |  3:03:53s\n",
            "epoch 179| loss: 0.21427 | train_logloss: 0.21429 | train_accuracy: 0.9231  | valid_logloss: 0.21917 | valid_accuracy: 0.92188 |  3:04:55s\n",
            "epoch 180| loss: 0.21475 | train_logloss: 0.2117  | train_accuracy: 0.92418 | valid_logloss: 0.21654 | valid_accuracy: 0.92283 |  3:05:56s\n",
            "epoch 181| loss: 0.21414 | train_logloss: 0.21243 | train_accuracy: 0.92403 | valid_logloss: 0.21731 | valid_accuracy: 0.92274 |  3:06:58s\n",
            "epoch 182| loss: 0.21484 | train_logloss: 0.21266 | train_accuracy: 0.92375 | valid_logloss: 0.21704 | valid_accuracy: 0.92263 |  3:07:59s\n",
            "epoch 183| loss: 0.21524 | train_logloss: 0.21212 | train_accuracy: 0.92396 | valid_logloss: 0.21694 | valid_accuracy: 0.92262 |  3:09:00s\n",
            "epoch 184| loss: 0.21455 | train_logloss: 0.21298 | train_accuracy: 0.92368 | valid_logloss: 0.21767 | valid_accuracy: 0.92243 |  3:10:01s\n",
            "epoch 185| loss: 0.2146  | train_logloss: 0.21451 | train_accuracy: 0.92328 | valid_logloss: 0.21882 | valid_accuracy: 0.9221  |  3:11:03s\n",
            "epoch 186| loss: 0.22182 | train_logloss: 0.22054 | train_accuracy: 0.92117 | valid_logloss: 0.22327 | valid_accuracy: 0.92033 |  3:12:04s\n",
            "epoch 187| loss: 0.21719 | train_logloss: 0.21304 | train_accuracy: 0.92346 | valid_logloss: 0.2172  | valid_accuracy: 0.92242 |  3:13:05s\n",
            "epoch 188| loss: 0.21536 | train_logloss: 0.21236 | train_accuracy: 0.92392 | valid_logloss: 0.21715 | valid_accuracy: 0.9225  |  3:14:07s\n",
            "epoch 189| loss: 0.21469 | train_logloss: 0.21329 | train_accuracy: 0.9236  | valid_logloss: 0.21805 | valid_accuracy: 0.92216 |  3:15:08s\n",
            "epoch 190| loss: 0.21509 | train_logloss: 0.21192 | train_accuracy: 0.92414 | valid_logloss: 0.21682 | valid_accuracy: 0.92271 |  3:16:09s\n",
            "epoch 191| loss: 0.21433 | train_logloss: 0.21217 | train_accuracy: 0.92409 | valid_logloss: 0.21693 | valid_accuracy: 0.92273 |  3:17:10s\n",
            "epoch 192| loss: 0.2148  | train_logloss: 0.21192 | train_accuracy: 0.92378 | valid_logloss: 0.217   | valid_accuracy: 0.92235 |  3:18:12s\n",
            "epoch 193| loss: 0.21443 | train_logloss: 0.2112  | train_accuracy: 0.9242  | valid_logloss: 0.21623 | valid_accuracy: 0.9228  |  3:19:13s\n",
            "epoch 194| loss: 0.21475 | train_logloss: 0.21334 | train_accuracy: 0.92363 | valid_logloss: 0.21798 | valid_accuracy: 0.92238 |  3:20:15s\n",
            "epoch 195| loss: 0.21548 | train_logloss: 0.21284 | train_accuracy: 0.92386 | valid_logloss: 0.21747 | valid_accuracy: 0.92253 |  3:21:16s\n",
            "\n",
            "Early stopping occurred at epoch 195 with best_epoch = 145 and best_valid_accuracy = 0.92312\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[427634  13145  39403]\n",
            " [  9587 969503  20910]\n",
            " [ 32339  31662 368586]]\n",
            "Testing Score:  0.9231240154979509\n",
            "Confusion Matrix: \n",
            " [[427208  13360  39614]\n",
            " [  9966 970013  20021]\n",
            " [ 33400  32597 366590]]\n",
            "Testing Score:  0.9221244175329065\n",
            "{'Rows': 3000000, 'Nd': 16, 'Na': 16, 'B': 16384, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 1912767, 'test': 1912769, 'time_learn_tn': 12098.683749437332, 'time_tn': 13.74125599861145, 'accuracy_tn': 0.9231240154979509, 'time_learn_gb': 937.4027698040009, 'time_gb': 120.16227746009827, 'accuracy_gb': 0.9221244175329065}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_vNM8tWh9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03b45e9-1ae0-4955-fb27-3c5d95a7380a"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885889</td>\n",
              "      <td>240.117316</td>\n",
              "      <td>77.182918</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>1.362470</td>\n",
              "      <td>2.113432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.922444</td>\n",
              "      <td>15321.927262</td>\n",
              "      <td>13.301398</td>\n",
              "      <td>0.922553</td>\n",
              "      <td>1216.291787</td>\n",
              "      <td>163.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.923124</td>\n",
              "      <td>12098.683749</td>\n",
              "      <td>13.741256</td>\n",
              "      <td>0.922124</td>\n",
              "      <td>937.402770</td>\n",
              "      <td>120.162277</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rows    train     test  ...  accuracy_gb  time_learn_gb     time_gb\n",
              "0      9000     9000     9000  ...     0.883778       3.483706   16.722444\n",
              "1      9000     9000     9000  ...     0.883778       3.464101   17.349063\n",
              "2      9000     9000     9000  ...     0.883778       3.411332   16.978554\n",
              "3      9000     9000     9000  ...     0.883778       3.509229   17.331297\n",
              "4      9000     9000     9000  ...     0.883778       3.475400   17.451218\n",
              "5     30000    30000    30000  ...     0.891767       8.276289   27.093480\n",
              "6     30000    30000    30000  ...     0.891767       8.210687   27.343578\n",
              "7     30000    30000    30000  ...     0.891767       8.244918   28.102216\n",
              "8     30000    30000    30000  ...     0.891767       8.093423   27.197022\n",
              "9     30000    30000    30000  ...     0.891767       7.857594   27.867677\n",
              "10   300000   300000   300000  ...     0.901553     140.282187   95.826268\n",
              "11   300000   300000   300000  ...     0.901490     122.025770   81.670524\n",
              "12   300000   300000   300000  ...     0.901667     135.688677   95.713423\n",
              "13     9000     9000     9000  ...     0.883778       1.362470    2.113432\n",
              "14  3000000  1912767  1912769  ...     0.922553    1216.291787  163.861900\n",
              "15  3000000  1912767  1912769  ...     0.922124     937.402770  120.162277\n",
              "\n",
              "[16 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctuss_gtRcgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a891ff6a-c069-4e99-ccb3-f4b2513c2769"
      },
      "source": [
        "time_model(number_exp=53, \n",
        "          Rows=3000000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=16384,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.37379 | train_logloss: 2.11995 | train_accuracy: 0.44448 | valid_logloss: 2.12328 | valid_accuracy: 0.44427 |  0:01:37s\n",
            "epoch 1  | loss: 0.27536 | train_logloss: 0.79771 | train_accuracy: 0.71381 | valid_logloss: 0.79647 | valid_accuracy: 0.71398 |  0:03:14s\n",
            "epoch 2  | loss: 0.2544  | train_logloss: 0.29855 | train_accuracy: 0.89117 | valid_logloss: 0.29791 | valid_accuracy: 0.89166 |  0:04:52s\n",
            "epoch 3  | loss: 0.25107 | train_logloss: 0.26659 | train_accuracy: 0.90728 | valid_logloss: 0.26622 | valid_accuracy: 0.9076  |  0:06:31s\n",
            "epoch 4  | loss: 0.24985 | train_logloss: 0.25267 | train_accuracy: 0.91046 | valid_logloss: 0.25263 | valid_accuracy: 0.91069 |  0:08:09s\n",
            "epoch 5  | loss: 0.24604 | train_logloss: 0.23961 | train_accuracy: 0.91447 | valid_logloss: 0.23959 | valid_accuracy: 0.91468 |  0:09:47s\n",
            "epoch 6  | loss: 0.2407  | train_logloss: 0.23748 | train_accuracy: 0.9154  | valid_logloss: 0.2377  | valid_accuracy: 0.9155  |  0:11:25s\n",
            "epoch 7  | loss: 0.24147 | train_logloss: 0.239   | train_accuracy: 0.91536 | valid_logloss: 0.23904 | valid_accuracy: 0.91538 |  0:13:03s\n",
            "epoch 8  | loss: 0.2449  | train_logloss: 0.24422 | train_accuracy: 0.91442 | valid_logloss: 0.24376 | valid_accuracy: 0.91452 |  0:14:41s\n",
            "epoch 9  | loss: 0.23962 | train_logloss: 0.23569 | train_accuracy: 0.91558 | valid_logloss: 0.23572 | valid_accuracy: 0.91583 |  0:16:20s\n",
            "epoch 10 | loss: 0.23678 | train_logloss: 0.23317 | train_accuracy: 0.91668 | valid_logloss: 0.23309 | valid_accuracy: 0.917   |  0:17:58s\n",
            "epoch 11 | loss: 0.23346 | train_logloss: 0.23423 | train_accuracy: 0.91691 | valid_logloss: 0.23448 | valid_accuracy: 0.91679 |  0:19:36s\n",
            "epoch 12 | loss: 0.23416 | train_logloss: 0.23291 | train_accuracy: 0.91726 | valid_logloss: 0.23322 | valid_accuracy: 0.91732 |  0:21:14s\n",
            "epoch 13 | loss: 0.23559 | train_logloss: 0.24142 | train_accuracy: 0.91488 | valid_logloss: 0.24159 | valid_accuracy: 0.91485 |  0:22:52s\n",
            "epoch 14 | loss: 0.23425 | train_logloss: 0.23274 | train_accuracy: 0.91679 | valid_logloss: 0.23281 | valid_accuracy: 0.91676 |  0:24:30s\n",
            "epoch 15 | loss: 0.23256 | train_logloss: 0.23197 | train_accuracy: 0.91704 | valid_logloss: 0.2324  | valid_accuracy: 0.91722 |  0:26:07s\n",
            "epoch 16 | loss: 0.23183 | train_logloss: 0.2273  | train_accuracy: 0.91843 | valid_logloss: 0.22774 | valid_accuracy: 0.91842 |  0:27:45s\n",
            "epoch 17 | loss: 0.22876 | train_logloss: 0.2254  | train_accuracy: 0.91942 | valid_logloss: 0.2261  | valid_accuracy: 0.9194  |  0:29:23s\n",
            "epoch 18 | loss: 0.22651 | train_logloss: 0.22309 | train_accuracy: 0.92016 | valid_logloss: 0.22398 | valid_accuracy: 0.92001 |  0:31:00s\n",
            "epoch 19 | loss: 0.22512 | train_logloss: 0.22315 | train_accuracy: 0.91994 | valid_logloss: 0.22408 | valid_accuracy: 0.91978 |  0:32:38s\n",
            "epoch 20 | loss: 0.22451 | train_logloss: 0.22303 | train_accuracy: 0.92026 | valid_logloss: 0.22393 | valid_accuracy: 0.92004 |  0:34:16s\n",
            "epoch 21 | loss: 0.22332 | train_logloss: 0.22061 | train_accuracy: 0.92099 | valid_logloss: 0.22184 | valid_accuracy: 0.92075 |  0:35:53s\n",
            "epoch 22 | loss: 0.22355 | train_logloss: 0.22189 | train_accuracy: 0.9204  | valid_logloss: 0.22313 | valid_accuracy: 0.9202  |  0:37:31s\n",
            "epoch 23 | loss: 0.22554 | train_logloss: 0.22297 | train_accuracy: 0.92022 | valid_logloss: 0.22395 | valid_accuracy: 0.92014 |  0:39:10s\n",
            "epoch 24 | loss: 0.22509 | train_logloss: 0.22169 | train_accuracy: 0.92062 | valid_logloss: 0.22266 | valid_accuracy: 0.92043 |  0:40:48s\n",
            "epoch 25 | loss: 0.22242 | train_logloss: 0.2221  | train_accuracy: 0.92064 | valid_logloss: 0.2234  | valid_accuracy: 0.9203  |  0:42:26s\n",
            "epoch 26 | loss: 0.22524 | train_logloss: 0.23    | train_accuracy: 0.91794 | valid_logloss: 0.23051 | valid_accuracy: 0.91783 |  0:44:04s\n",
            "epoch 27 | loss: 0.22546 | train_logloss: 0.2206  | train_accuracy: 0.9211  | valid_logloss: 0.22165 | valid_accuracy: 0.92099 |  0:45:41s\n",
            "epoch 28 | loss: 0.2221  | train_logloss: 0.22106 | train_accuracy: 0.9209  | valid_logloss: 0.22256 | valid_accuracy: 0.92051 |  0:47:19s\n",
            "epoch 29 | loss: 0.22108 | train_logloss: 0.21979 | train_accuracy: 0.92115 | valid_logloss: 0.22152 | valid_accuracy: 0.92068 |  0:48:57s\n",
            "epoch 30 | loss: 0.22122 | train_logloss: 0.22058 | train_accuracy: 0.92133 | valid_logloss: 0.22223 | valid_accuracy: 0.92098 |  0:50:35s\n",
            "epoch 31 | loss: 0.21984 | train_logloss: 0.21732 | train_accuracy: 0.92241 | valid_logloss: 0.21928 | valid_accuracy: 0.92188 |  0:52:11s\n",
            "epoch 32 | loss: 0.22348 | train_logloss: 0.21933 | train_accuracy: 0.92164 | valid_logloss: 0.22089 | valid_accuracy: 0.92119 |  0:53:46s\n",
            "epoch 33 | loss: 0.22034 | train_logloss: 0.21785 | train_accuracy: 0.92222 | valid_logloss: 0.21953 | valid_accuracy: 0.92174 |  0:55:21s\n",
            "epoch 34 | loss: 0.21952 | train_logloss: 0.21729 | train_accuracy: 0.92216 | valid_logloss: 0.21939 | valid_accuracy: 0.92165 |  0:56:57s\n",
            "epoch 35 | loss: 0.21861 | train_logloss: 0.21626 | train_accuracy: 0.92263 | valid_logloss: 0.21847 | valid_accuracy: 0.922   |  0:58:37s\n",
            "epoch 36 | loss: 0.21851 | train_logloss: 0.21642 | train_accuracy: 0.92263 | valid_logloss: 0.21866 | valid_accuracy: 0.92194 |  1:00:16s\n",
            "epoch 37 | loss: 0.21857 | train_logloss: 0.21771 | train_accuracy: 0.92204 | valid_logloss: 0.2198  | valid_accuracy: 0.92154 |  1:01:56s\n",
            "epoch 38 | loss: 0.21861 | train_logloss: 0.21676 | train_accuracy: 0.92243 | valid_logloss: 0.21911 | valid_accuracy: 0.92175 |  1:03:32s\n",
            "epoch 39 | loss: 0.21794 | train_logloss: 0.21738 | train_accuracy: 0.92262 | valid_logloss: 0.2199  | valid_accuracy: 0.92196 |  1:05:08s\n",
            "epoch 40 | loss: 0.21985 | train_logloss: 0.21873 | train_accuracy: 0.9223  | valid_logloss: 0.22072 | valid_accuracy: 0.92175 |  1:06:45s\n",
            "epoch 41 | loss: 0.21809 | train_logloss: 0.21742 | train_accuracy: 0.92183 | valid_logloss: 0.2201  | valid_accuracy: 0.92114 |  1:08:21s\n",
            "epoch 42 | loss: 0.21735 | train_logloss: 0.21633 | train_accuracy: 0.92283 | valid_logloss: 0.21896 | valid_accuracy: 0.92221 |  1:09:58s\n",
            "epoch 43 | loss: 0.21676 | train_logloss: 0.2145  | train_accuracy: 0.92331 | valid_logloss: 0.21747 | valid_accuracy: 0.92262 |  1:11:34s\n",
            "epoch 44 | loss: 0.21616 | train_logloss: 0.21497 | train_accuracy: 0.92313 | valid_logloss: 0.2179  | valid_accuracy: 0.92232 |  1:13:10s\n",
            "epoch 45 | loss: 0.21973 | train_logloss: 0.2152  | train_accuracy: 0.92315 | valid_logloss: 0.21777 | valid_accuracy: 0.92238 |  1:14:47s\n",
            "epoch 46 | loss: 0.21744 | train_logloss: 0.2163  | train_accuracy: 0.92236 | valid_logloss: 0.21917 | valid_accuracy: 0.92158 |  1:16:23s\n",
            "epoch 47 | loss: 0.22884 | train_logloss: 0.22562 | train_accuracy: 0.91996 | valid_logloss: 0.22682 | valid_accuracy: 0.91973 |  1:18:00s\n",
            "epoch 48 | loss: 0.22754 | train_logloss: 0.22036 | train_accuracy: 0.92156 | valid_logloss: 0.22176 | valid_accuracy: 0.92123 |  1:19:36s\n",
            "epoch 49 | loss: 0.22108 | train_logloss: 0.21892 | train_accuracy: 0.92201 | valid_logloss: 0.22076 | valid_accuracy: 0.92156 |  1:21:12s\n",
            "epoch 50 | loss: 0.21945 | train_logloss: 0.21841 | train_accuracy: 0.92189 | valid_logloss: 0.22021 | valid_accuracy: 0.92141 |  1:22:49s\n",
            "epoch 51 | loss: 0.21868 | train_logloss: 0.21652 | train_accuracy: 0.92284 | valid_logloss: 0.21881 | valid_accuracy: 0.92207 |  1:24:26s\n",
            "epoch 52 | loss: 0.21845 | train_logloss: 0.21688 | train_accuracy: 0.92234 | valid_logloss: 0.21937 | valid_accuracy: 0.92177 |  1:26:04s\n",
            "epoch 53 | loss: 0.21739 | train_logloss: 0.21643 | train_accuracy: 0.9226  | valid_logloss: 0.21912 | valid_accuracy: 0.92191 |  1:27:42s\n",
            "epoch 54 | loss: 0.21654 | train_logloss: 0.21403 | train_accuracy: 0.92333 | valid_logloss: 0.21705 | valid_accuracy: 0.92248 |  1:29:19s\n",
            "epoch 55 | loss: 0.21586 | train_logloss: 0.2141  | train_accuracy: 0.92363 | valid_logloss: 0.21732 | valid_accuracy: 0.9227  |  1:30:56s\n",
            "epoch 56 | loss: 0.21557 | train_logloss: 0.21421 | train_accuracy: 0.92322 | valid_logloss: 0.21769 | valid_accuracy: 0.9223  |  1:32:34s\n",
            "epoch 57 | loss: 0.21552 | train_logloss: 0.2134  | train_accuracy: 0.92363 | valid_logloss: 0.21717 | valid_accuracy: 0.92245 |  1:34:11s\n",
            "epoch 58 | loss: 0.21546 | train_logloss: 0.21355 | train_accuracy: 0.92355 | valid_logloss: 0.2172  | valid_accuracy: 0.92255 |  1:35:48s\n",
            "epoch 59 | loss: 0.21444 | train_logloss: 0.21517 | train_accuracy: 0.92242 | valid_logloss: 0.21905 | valid_accuracy: 0.92127 |  1:37:26s\n",
            "epoch 60 | loss: 0.21538 | train_logloss: 0.2133  | train_accuracy: 0.92391 | valid_logloss: 0.21715 | valid_accuracy: 0.9229  |  1:39:04s\n",
            "epoch 61 | loss: 0.21473 | train_logloss: 0.21324 | train_accuracy: 0.92358 | valid_logloss: 0.21727 | valid_accuracy: 0.92231 |  1:40:43s\n",
            "epoch 62 | loss: 0.21411 | train_logloss: 0.21187 | train_accuracy: 0.92403 | valid_logloss: 0.21627 | valid_accuracy: 0.92285 |  1:42:22s\n",
            "epoch 63 | loss: 0.21358 | train_logloss: 0.21144 | train_accuracy: 0.92425 | valid_logloss: 0.21587 | valid_accuracy: 0.92302 |  1:44:00s\n",
            "epoch 64 | loss: 0.21344 | train_logloss: 0.21087 | train_accuracy: 0.92455 | valid_logloss: 0.21586 | valid_accuracy: 0.92307 |  1:45:38s\n",
            "epoch 65 | loss: 0.21313 | train_logloss: 0.21079 | train_accuracy: 0.92462 | valid_logloss: 0.21637 | valid_accuracy: 0.92289 |  1:47:16s\n",
            "epoch 66 | loss: 0.21308 | train_logloss: 0.21129 | train_accuracy: 0.92419 | valid_logloss: 0.21659 | valid_accuracy: 0.92267 |  1:48:55s\n",
            "epoch 67 | loss: 0.21265 | train_logloss: 0.21315 | train_accuracy: 0.92365 | valid_logloss: 0.21834 | valid_accuracy: 0.92224 |  1:50:31s\n",
            "epoch 68 | loss: 0.21247 | train_logloss: 0.21023 | train_accuracy: 0.9247  | valid_logloss: 0.21606 | valid_accuracy: 0.92298 |  1:52:07s\n",
            "epoch 69 | loss: 0.21241 | train_logloss: 0.21045 | train_accuracy: 0.92464 | valid_logloss: 0.21614 | valid_accuracy: 0.92288 |  1:53:44s\n",
            "epoch 70 | loss: 0.21249 | train_logloss: 0.20948 | train_accuracy: 0.92491 | valid_logloss: 0.21566 | valid_accuracy: 0.92303 |  1:55:20s\n",
            "epoch 71 | loss: 0.21191 | train_logloss: 0.20997 | train_accuracy: 0.92491 | valid_logloss: 0.21612 | valid_accuracy: 0.92308 |  1:56:57s\n",
            "epoch 72 | loss: 0.21169 | train_logloss: 0.21085 | train_accuracy: 0.92447 | valid_logloss: 0.21673 | valid_accuracy: 0.92272 |  1:58:33s\n",
            "epoch 73 | loss: 0.21136 | train_logloss: 0.20888 | train_accuracy: 0.92511 | valid_logloss: 0.2161  | valid_accuracy: 0.92294 |  2:00:09s\n",
            "epoch 74 | loss: 0.21114 | train_logloss: 0.20891 | train_accuracy: 0.92512 | valid_logloss: 0.21611 | valid_accuracy: 0.92303 |  2:01:43s\n",
            "epoch 75 | loss: 0.21118 | train_logloss: 0.21049 | train_accuracy: 0.92434 | valid_logloss: 0.21791 | valid_accuracy: 0.92215 |  2:03:18s\n",
            "epoch 76 | loss: 0.21092 | train_logloss: 0.20956 | train_accuracy: 0.92482 | valid_logloss: 0.21698 | valid_accuracy: 0.92275 |  2:04:53s\n",
            "epoch 77 | loss: 0.2109  | train_logloss: 0.20872 | train_accuracy: 0.92524 | valid_logloss: 0.21599 | valid_accuracy: 0.92311 |  2:06:28s\n",
            "epoch 78 | loss: 0.21078 | train_logloss: 0.20887 | train_accuracy: 0.92515 | valid_logloss: 0.21682 | valid_accuracy: 0.92285 |  2:08:03s\n",
            "epoch 79 | loss: 0.2104  | train_logloss: 0.20905 | train_accuracy: 0.9248  | valid_logloss: 0.2176  | valid_accuracy: 0.92237 |  2:09:37s\n",
            "epoch 80 | loss: 0.21016 | train_logloss: 0.20805 | train_accuracy: 0.92544 | valid_logloss: 0.21676 | valid_accuracy: 0.923   |  2:11:11s\n",
            "epoch 81 | loss: 0.21009 | train_logloss: 0.20795 | train_accuracy: 0.92546 | valid_logloss: 0.21634 | valid_accuracy: 0.92301 |  2:12:45s\n",
            "epoch 82 | loss: 0.20966 | train_logloss: 0.20833 | train_accuracy: 0.92513 | valid_logloss: 0.21741 | valid_accuracy: 0.9225  |  2:14:20s\n",
            "epoch 83 | loss: 0.20985 | train_logloss: 0.20697 | train_accuracy: 0.92579 | valid_logloss: 0.21658 | valid_accuracy: 0.92328 |  2:15:54s\n",
            "epoch 84 | loss: 0.20945 | train_logloss: 0.20633 | train_accuracy: 0.92589 | valid_logloss: 0.21619 | valid_accuracy: 0.92303 |  2:17:29s\n",
            "epoch 85 | loss: 0.20918 | train_logloss: 0.20646 | train_accuracy: 0.92604 | valid_logloss: 0.21612 | valid_accuracy: 0.92315 |  2:19:03s\n",
            "epoch 86 | loss: 0.20899 | train_logloss: 0.206   | train_accuracy: 0.92607 | valid_logloss: 0.2164  | valid_accuracy: 0.92323 |  2:20:37s\n",
            "epoch 87 | loss: 0.20897 | train_logloss: 0.20624 | train_accuracy: 0.92605 | valid_logloss: 0.21722 | valid_accuracy: 0.92292 |  2:22:12s\n",
            "epoch 88 | loss: 0.20846 | train_logloss: 0.20804 | train_accuracy: 0.92527 | valid_logloss: 0.21812 | valid_accuracy: 0.92224 |  2:23:46s\n",
            "epoch 89 | loss: 0.20843 | train_logloss: 0.20681 | train_accuracy: 0.92568 | valid_logloss: 0.21757 | valid_accuracy: 0.92245 |  2:25:21s\n",
            "epoch 90 | loss: 0.20822 | train_logloss: 0.20508 | train_accuracy: 0.92642 | valid_logloss: 0.21675 | valid_accuracy: 0.92299 |  2:26:57s\n",
            "epoch 91 | loss: 0.20803 | train_logloss: 0.20573 | train_accuracy: 0.92624 | valid_logloss: 0.21712 | valid_accuracy: 0.923   |  2:28:32s\n",
            "epoch 92 | loss: 0.20796 | train_logloss: 0.20593 | train_accuracy: 0.92615 | valid_logloss: 0.21783 | valid_accuracy: 0.92286 |  2:30:07s\n",
            "epoch 93 | loss: 0.20758 | train_logloss: 0.20499 | train_accuracy: 0.92642 | valid_logloss: 0.21764 | valid_accuracy: 0.9228  |  2:31:41s\n",
            "epoch 94 | loss: 0.2074  | train_logloss: 0.20558 | train_accuracy: 0.92603 | valid_logloss: 0.21829 | valid_accuracy: 0.92255 |  2:33:17s\n",
            "epoch 95 | loss: 0.20701 | train_logloss: 0.20427 | train_accuracy: 0.92667 | valid_logloss: 0.21782 | valid_accuracy: 0.92253 |  2:34:52s\n",
            "epoch 96 | loss: 0.20691 | train_logloss: 0.20572 | train_accuracy: 0.92593 | valid_logloss: 0.21819 | valid_accuracy: 0.92229 |  2:36:27s\n",
            "epoch 97 | loss: 0.20681 | train_logloss: 0.20421 | train_accuracy: 0.92673 | valid_logloss: 0.21864 | valid_accuracy: 0.9228  |  2:38:04s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAzIb5zsWieR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb81c3dc-c685-4363-925c-ce8aed6f74fd"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>193.754094</td>\n",
              "      <td>55.350941</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.483706</td>\n",
              "      <td>16.722444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>222.945876</td>\n",
              "      <td>61.207308</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.464101</td>\n",
              "      <td>17.349063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885667</td>\n",
              "      <td>364.025402</td>\n",
              "      <td>90.066453</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.411332</td>\n",
              "      <td>16.978554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>249.533577</td>\n",
              "      <td>77.371469</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.509229</td>\n",
              "      <td>17.331297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888111</td>\n",
              "      <td>307.560998</td>\n",
              "      <td>90.364384</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>17.451218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894067</td>\n",
              "      <td>249.859268</td>\n",
              "      <td>20.273622</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.276289</td>\n",
              "      <td>27.093480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.892500</td>\n",
              "      <td>321.834888</td>\n",
              "      <td>22.630230</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.210687</td>\n",
              "      <td>27.343578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>518.142053</td>\n",
              "      <td>30.078117</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.244918</td>\n",
              "      <td>28.102216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894133</td>\n",
              "      <td>253.579474</td>\n",
              "      <td>26.655121</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>8.093423</td>\n",
              "      <td>27.197022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894433</td>\n",
              "      <td>539.947831</td>\n",
              "      <td>28.425750</td>\n",
              "      <td>0.891767</td>\n",
              "      <td>7.857594</td>\n",
              "      <td>27.867677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901943</td>\n",
              "      <td>3252.582673</td>\n",
              "      <td>19.763669</td>\n",
              "      <td>0.901553</td>\n",
              "      <td>140.282187</td>\n",
              "      <td>95.826268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901833</td>\n",
              "      <td>2499.872937</td>\n",
              "      <td>21.900165</td>\n",
              "      <td>0.901490</td>\n",
              "      <td>122.025770</td>\n",
              "      <td>81.670524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902300</td>\n",
              "      <td>2508.685421</td>\n",
              "      <td>29.672779</td>\n",
              "      <td>0.901667</td>\n",
              "      <td>135.688677</td>\n",
              "      <td>95.713423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.885889</td>\n",
              "      <td>240.117316</td>\n",
              "      <td>77.182918</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>1.362470</td>\n",
              "      <td>2.113432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.922444</td>\n",
              "      <td>15321.927262</td>\n",
              "      <td>13.301398</td>\n",
              "      <td>0.922553</td>\n",
              "      <td>1216.291787</td>\n",
              "      <td>163.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.923124</td>\n",
              "      <td>12098.683749</td>\n",
              "      <td>13.741256</td>\n",
              "      <td>0.922124</td>\n",
              "      <td>937.402770</td>\n",
              "      <td>120.162277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3000000</td>\n",
              "      <td>1912767</td>\n",
              "      <td>1912769</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>16384</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.923039</td>\n",
              "      <td>15052.750955</td>\n",
              "      <td>20.004159</td>\n",
              "      <td>0.922625</td>\n",
              "      <td>1470.582161</td>\n",
              "      <td>183.236929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rows    train     test  ...  accuracy_gb  time_learn_gb     time_gb\n",
              "0      9000     9000     9000  ...     0.883778       3.483706   16.722444\n",
              "1      9000     9000     9000  ...     0.883778       3.464101   17.349063\n",
              "2      9000     9000     9000  ...     0.883778       3.411332   16.978554\n",
              "3      9000     9000     9000  ...     0.883778       3.509229   17.331297\n",
              "4      9000     9000     9000  ...     0.883778       3.475400   17.451218\n",
              "5     30000    30000    30000  ...     0.891767       8.276289   27.093480\n",
              "6     30000    30000    30000  ...     0.891767       8.210687   27.343578\n",
              "7     30000    30000    30000  ...     0.891767       8.244918   28.102216\n",
              "8     30000    30000    30000  ...     0.891767       8.093423   27.197022\n",
              "9     30000    30000    30000  ...     0.891767       7.857594   27.867677\n",
              "10   300000   300000   300000  ...     0.901553     140.282187   95.826268\n",
              "11   300000   300000   300000  ...     0.901490     122.025770   81.670524\n",
              "12   300000   300000   300000  ...     0.901667     135.688677   95.713423\n",
              "13     9000     9000     9000  ...     0.883778       1.362470    2.113432\n",
              "14  3000000  1912767  1912769  ...     0.922553    1216.291787  163.861900\n",
              "15  3000000  1912767  1912769  ...     0.922124     937.402770  120.162277\n",
              "16  3000000  1912767  1912769  ...     0.922625    1470.582161  183.236929\n",
              "\n",
              "[17 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVyOw_8lRfHj"
      },
      "source": [
        "##Другие"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnB1167-DZ0"
      },
      "source": [
        "ones(number_exp=0, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WQUysI2_Qfu"
      },
      "source": [
        "ones(number_exp=1, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_4XW1cErKy"
      },
      "source": [
        "ones(number_exp=2, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7G1oUnsFPdU"
      },
      "source": [
        "ones(number_exp=3, \n",
        "     Rows=30000, \n",
        "     Nd=32,\tNa=32,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_UFQFX_FZMH"
      },
      "source": [
        "ones(number_exp=4, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGShfVf7z0Dd",
        "outputId": "24e826cf-d3f5-440b-82c5-a3fad26df3e5"
      },
      "source": [
        "!pip install memory_profiler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/fd/d92b3295657f8837e0177e7b48b32d6651436f0293af42b76d134c3bb489/memory_profiler-0.58.0.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-cp37-none-any.whl size=30180 sha256=5dff132182fdc6bba109ce582d8d376653122098650b1ef6a0ce91aeb3c522fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/e4/0b/aaab481fc5dd2a4ea59e78bc7231bb6aae7635ca7ee79f8ae5\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.58.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9pxDTlfHi-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e97186c-b6f4-4bc4-9889-3e2bcf881b23"
      },
      "source": [
        "memory(number_exp=5, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.97905 | train_logloss: 12.4491 | train_accuracy: 0.37373 | valid_logloss: 12.61953| valid_accuracy: 0.37013 |  0:00:02s\n",
            "epoch 1  | loss: 0.56978 | train_logloss: 14.36881| train_accuracy: 0.31703 | valid_logloss: 14.37557| valid_accuracy: 0.3213  |  0:00:04s\n",
            "epoch 2  | loss: 0.46949 | train_logloss: 4.74105 | train_accuracy: 0.52543 | valid_logloss: 4.76561 | valid_accuracy: 0.52323 |  0:00:06s\n",
            "epoch 3  | loss: 0.40363 | train_logloss: 5.61051 | train_accuracy: 0.30353 | valid_logloss: 5.61289 | valid_accuracy: 0.30657 |  0:00:08s\n",
            "epoch 4  | loss: 0.3827  | train_logloss: 2.38747 | train_accuracy: 0.30453 | valid_logloss: 2.3809  | valid_accuracy: 0.30113 |  0:00:11s\n",
            "epoch 5  | loss: 0.36719 | train_logloss: 1.9456  | train_accuracy: 0.32873 | valid_logloss: 1.95286 | valid_accuracy: 0.32877 |  0:00:13s\n",
            "epoch 6  | loss: 0.36546 | train_logloss: 1.8662  | train_accuracy: 0.36957 | valid_logloss: 1.87932 | valid_accuracy: 0.3725  |  0:00:15s\n",
            "epoch 7  | loss: 0.35385 | train_logloss: 1.85355 | train_accuracy: 0.43037 | valid_logloss: 1.86257 | valid_accuracy: 0.4297  |  0:00:17s\n",
            "epoch 8  | loss: 0.34529 | train_logloss: 1.92583 | train_accuracy: 0.26563 | valid_logloss: 1.93435 | valid_accuracy: 0.2617  |  0:00:19s\n",
            "epoch 9  | loss: 0.3531  | train_logloss: 1.54352 | train_accuracy: 0.6106  | valid_logloss: 1.54575 | valid_accuracy: 0.6044  |  0:00:21s\n",
            "epoch 10 | loss: 0.36885 | train_logloss: 1.36769 | train_accuracy: 0.47797 | valid_logloss: 1.37466 | valid_accuracy: 0.47293 |  0:00:24s\n",
            "epoch 11 | loss: 0.36291 | train_logloss: 0.96016 | train_accuracy: 0.63273 | valid_logloss: 0.98093 | valid_accuracy: 0.62423 |  0:00:26s\n",
            "epoch 12 | loss: 0.34936 | train_logloss: 0.73899 | train_accuracy: 0.7094  | valid_logloss: 0.75295 | valid_accuracy: 0.70293 |  0:00:28s\n",
            "epoch 13 | loss: 0.34543 | train_logloss: 0.60338 | train_accuracy: 0.80957 | valid_logloss: 0.61627 | valid_accuracy: 0.8034  |  0:00:30s\n",
            "epoch 14 | loss: 0.34373 | train_logloss: 0.6553  | train_accuracy: 0.77407 | valid_logloss: 0.66719 | valid_accuracy: 0.7696  |  0:00:32s\n",
            "epoch 15 | loss: 0.33511 | train_logloss: 0.55993 | train_accuracy: 0.79187 | valid_logloss: 0.57096 | valid_accuracy: 0.78863 |  0:00:35s\n",
            "epoch 16 | loss: 0.32642 | train_logloss: 0.53451 | train_accuracy: 0.80093 | valid_logloss: 0.54382 | valid_accuracy: 0.7994  |  0:00:37s\n",
            "epoch 17 | loss: 0.32882 | train_logloss: 0.46076 | train_accuracy: 0.82603 | valid_logloss: 0.47291 | valid_accuracy: 0.8215  |  0:00:39s\n",
            "epoch 18 | loss: 0.332   | train_logloss: 0.41772 | train_accuracy: 0.84733 | valid_logloss: 0.42707 | valid_accuracy: 0.84647 |  0:00:41s\n",
            "epoch 19 | loss: 0.33086 | train_logloss: 0.3881  | train_accuracy: 0.8589  | valid_logloss: 0.39446 | valid_accuracy: 0.8548  |  0:00:43s\n",
            "epoch 20 | loss: 0.32429 | train_logloss: 0.38051 | train_accuracy: 0.85453 | valid_logloss: 0.38792 | valid_accuracy: 0.8536  |  0:00:46s\n",
            "epoch 21 | loss: 0.32387 | train_logloss: 0.35743 | train_accuracy: 0.86827 | valid_logloss: 0.36519 | valid_accuracy: 0.8666  |  0:00:48s\n",
            "epoch 22 | loss: 0.3277  | train_logloss: 0.34388 | train_accuracy: 0.8761  | valid_logloss: 0.35529 | valid_accuracy: 0.87177 |  0:00:50s\n",
            "epoch 23 | loss: 0.32828 | train_logloss: 0.37716 | train_accuracy: 0.8667  | valid_logloss: 0.38501 | valid_accuracy: 0.86367 |  0:00:52s\n",
            "epoch 24 | loss: 0.33056 | train_logloss: 0.33555 | train_accuracy: 0.87477 | valid_logloss: 0.34323 | valid_accuracy: 0.87187 |  0:00:54s\n",
            "epoch 25 | loss: 0.3272  | train_logloss: 0.33469 | train_accuracy: 0.87443 | valid_logloss: 0.34333 | valid_accuracy: 0.8724  |  0:00:56s\n",
            "epoch 26 | loss: 0.32224 | train_logloss: 0.32578 | train_accuracy: 0.8778  | valid_logloss: 0.33451 | valid_accuracy: 0.8764  |  0:00:59s\n",
            "epoch 27 | loss: 0.32637 | train_logloss: 0.32538 | train_accuracy: 0.8784  | valid_logloss: 0.3362  | valid_accuracy: 0.87373 |  0:01:01s\n",
            "epoch 28 | loss: 0.32075 | train_logloss: 0.31854 | train_accuracy: 0.8792  | valid_logloss: 0.32856 | valid_accuracy: 0.87683 |  0:01:03s\n",
            "epoch 29 | loss: 0.31423 | train_logloss: 0.30562 | train_accuracy: 0.88577 | valid_logloss: 0.31782 | valid_accuracy: 0.8823  |  0:01:05s\n",
            "epoch 30 | loss: 0.32787 | train_logloss: 0.35006 | train_accuracy: 0.85887 | valid_logloss: 0.35993 | valid_accuracy: 0.8561  |  0:01:07s\n",
            "epoch 31 | loss: 0.32527 | train_logloss: 0.31964 | train_accuracy: 0.87607 | valid_logloss: 0.33061 | valid_accuracy: 0.8733  |  0:01:10s\n",
            "epoch 32 | loss: 0.31287 | train_logloss: 0.31665 | train_accuracy: 0.8785  | valid_logloss: 0.32985 | valid_accuracy: 0.8762  |  0:01:12s\n",
            "epoch 33 | loss: 0.3147  | train_logloss: 0.31728 | train_accuracy: 0.87887 | valid_logloss: 0.32836 | valid_accuracy: 0.87367 |  0:01:14s\n",
            "epoch 34 | loss: 0.30669 | train_logloss: 0.29686 | train_accuracy: 0.886   | valid_logloss: 0.31386 | valid_accuracy: 0.88143 |  0:01:16s\n",
            "epoch 35 | loss: 0.31008 | train_logloss: 0.30319 | train_accuracy: 0.88533 | valid_logloss: 0.31887 | valid_accuracy: 0.88037 |  0:01:18s\n",
            "epoch 36 | loss: 0.30761 | train_logloss: 0.29662 | train_accuracy: 0.88757 | valid_logloss: 0.31305 | valid_accuracy: 0.88097 |  0:01:21s\n",
            "epoch 37 | loss: 0.30404 | train_logloss: 0.30209 | train_accuracy: 0.88423 | valid_logloss: 0.31594 | valid_accuracy: 0.88173 |  0:01:23s\n",
            "epoch 38 | loss: 0.30785 | train_logloss: 0.3112  | train_accuracy: 0.88517 | valid_logloss: 0.32971 | valid_accuracy: 0.88077 |  0:01:25s\n",
            "epoch 39 | loss: 0.31085 | train_logloss: 0.30203 | train_accuracy: 0.88487 | valid_logloss: 0.3161  | valid_accuracy: 0.88007 |  0:01:27s\n",
            "epoch 40 | loss: 0.30089 | train_logloss: 0.29308 | train_accuracy: 0.8889  | valid_logloss: 0.30866 | valid_accuracy: 0.88373 |  0:01:29s\n",
            "epoch 41 | loss: 0.29935 | train_logloss: 0.29849 | train_accuracy: 0.88727 | valid_logloss: 0.31943 | valid_accuracy: 0.881   |  0:01:32s\n",
            "epoch 42 | loss: 0.29715 | train_logloss: 0.2925  | train_accuracy: 0.88943 | valid_logloss: 0.31371 | valid_accuracy: 0.88503 |  0:01:34s\n",
            "epoch 43 | loss: 0.29392 | train_logloss: 0.28365 | train_accuracy: 0.89127 | valid_logloss: 0.30535 | valid_accuracy: 0.8858  |  0:01:36s\n",
            "epoch 44 | loss: 0.28967 | train_logloss: 0.28508 | train_accuracy: 0.8905  | valid_logloss: 0.30791 | valid_accuracy: 0.88367 |  0:01:38s\n",
            "epoch 45 | loss: 0.29236 | train_logloss: 0.29584 | train_accuracy: 0.8876  | valid_logloss: 0.31726 | valid_accuracy: 0.88227 |  0:01:40s\n",
            "epoch 46 | loss: 0.29755 | train_logloss: 0.29315 | train_accuracy: 0.88827 | valid_logloss: 0.31389 | valid_accuracy: 0.88243 |  0:01:43s\n",
            "epoch 47 | loss: 0.29275 | train_logloss: 0.2859  | train_accuracy: 0.8922  | valid_logloss: 0.30784 | valid_accuracy: 0.88593 |  0:01:45s\n",
            "epoch 48 | loss: 0.28951 | train_logloss: 0.28184 | train_accuracy: 0.88977 | valid_logloss: 0.30245 | valid_accuracy: 0.8845  |  0:01:47s\n",
            "epoch 49 | loss: 0.28977 | train_logloss: 0.29449 | train_accuracy: 0.88533 | valid_logloss: 0.31502 | valid_accuracy: 0.881   |  0:01:49s\n",
            "epoch 50 | loss: 0.30073 | train_logloss: 0.31554 | train_accuracy: 0.8807  | valid_logloss: 0.33294 | valid_accuracy: 0.87637 |  0:01:51s\n",
            "epoch 51 | loss: 0.3108  | train_logloss: 0.30296 | train_accuracy: 0.88533 | valid_logloss: 0.32153 | valid_accuracy: 0.87937 |  0:01:54s\n",
            "epoch 52 | loss: 0.30582 | train_logloss: 0.30823 | train_accuracy: 0.8844  | valid_logloss: 0.32328 | valid_accuracy: 0.8809  |  0:01:56s\n",
            "epoch 53 | loss: 0.30711 | train_logloss: 0.29592 | train_accuracy: 0.88783 | valid_logloss: 0.31358 | valid_accuracy: 0.8834  |  0:01:58s\n",
            "epoch 54 | loss: 0.29994 | train_logloss: 0.29117 | train_accuracy: 0.8897  | valid_logloss: 0.30707 | valid_accuracy: 0.8864  |  0:02:00s\n",
            "epoch 55 | loss: 0.30592 | train_logloss: 0.30926 | train_accuracy: 0.8801  | valid_logloss: 0.32352 | valid_accuracy: 0.87327 |  0:02:03s\n",
            "epoch 56 | loss: 0.3032  | train_logloss: 0.29621 | train_accuracy: 0.88383 | valid_logloss: 0.31104 | valid_accuracy: 0.8807  |  0:02:05s\n",
            "epoch 57 | loss: 0.29733 | train_logloss: 0.28695 | train_accuracy: 0.88947 | valid_logloss: 0.30401 | valid_accuracy: 0.8836  |  0:02:07s\n",
            "epoch 58 | loss: 0.29487 | train_logloss: 0.28673 | train_accuracy: 0.88983 | valid_logloss: 0.30697 | valid_accuracy: 0.884   |  0:02:09s\n",
            "epoch 59 | loss: 0.28747 | train_logloss: 0.28264 | train_accuracy: 0.8901  | valid_logloss: 0.3038  | valid_accuracy: 0.8853  |  0:02:12s\n",
            "epoch 60 | loss: 0.2843  | train_logloss: 0.2783  | train_accuracy: 0.89277 | valid_logloss: 0.3015  | valid_accuracy: 0.887   |  0:02:14s\n",
            "epoch 61 | loss: 0.2869  | train_logloss: 0.28686 | train_accuracy: 0.8908  | valid_logloss: 0.30658 | valid_accuracy: 0.88613 |  0:02:16s\n",
            "epoch 62 | loss: 0.2877  | train_logloss: 0.27989 | train_accuracy: 0.8923  | valid_logloss: 0.30364 | valid_accuracy: 0.88587 |  0:02:18s\n",
            "epoch 63 | loss: 0.28477 | train_logloss: 0.27708 | train_accuracy: 0.89307 | valid_logloss: 0.30085 | valid_accuracy: 0.8863  |  0:02:21s\n",
            "epoch 64 | loss: 0.28547 | train_logloss: 0.28623 | train_accuracy: 0.89197 | valid_logloss: 0.30923 | valid_accuracy: 0.8857  |  0:02:23s\n",
            "epoch 65 | loss: 0.29204 | train_logloss: 0.28278 | train_accuracy: 0.89043 | valid_logloss: 0.30389 | valid_accuracy: 0.88547 |  0:02:25s\n",
            "epoch 66 | loss: 0.28597 | train_logloss: 0.27628 | train_accuracy: 0.89423 | valid_logloss: 0.29863 | valid_accuracy: 0.8876  |  0:02:27s\n",
            "epoch 67 | loss: 0.28349 | train_logloss: 0.27436 | train_accuracy: 0.89487 | valid_logloss: 0.29829 | valid_accuracy: 0.88917 |  0:02:30s\n",
            "epoch 68 | loss: 0.2798  | train_logloss: 0.27127 | train_accuracy: 0.89473 | valid_logloss: 0.29757 | valid_accuracy: 0.88847 |  0:02:32s\n",
            "epoch 69 | loss: 0.2833  | train_logloss: 0.27914 | train_accuracy: 0.89367 | valid_logloss: 0.30303 | valid_accuracy: 0.88787 |  0:02:34s\n",
            "epoch 70 | loss: 0.27968 | train_logloss: 0.27337 | train_accuracy: 0.896   | valid_logloss: 0.30016 | valid_accuracy: 0.88757 |  0:02:37s\n",
            "epoch 71 | loss: 0.27936 | train_logloss: 0.26939 | train_accuracy: 0.89603 | valid_logloss: 0.29306 | valid_accuracy: 0.89043 |  0:02:39s\n",
            "epoch 72 | loss: 0.27518 | train_logloss: 0.27008 | train_accuracy: 0.8957  | valid_logloss: 0.2966  | valid_accuracy: 0.88947 |  0:02:41s\n",
            "epoch 73 | loss: 0.28314 | train_logloss: 0.28498 | train_accuracy: 0.8893  | valid_logloss: 0.30627 | valid_accuracy: 0.88347 |  0:02:43s\n",
            "epoch 74 | loss: 0.28713 | train_logloss: 0.27665 | train_accuracy: 0.89463 | valid_logloss: 0.29996 | valid_accuracy: 0.8866  |  0:02:45s\n",
            "epoch 75 | loss: 0.27902 | train_logloss: 0.27327 | train_accuracy: 0.89563 | valid_logloss: 0.29478 | valid_accuracy: 0.88883 |  0:02:48s\n",
            "epoch 76 | loss: 0.27902 | train_logloss: 0.27501 | train_accuracy: 0.89663 | valid_logloss: 0.29729 | valid_accuracy: 0.88897 |  0:02:50s\n",
            "epoch 77 | loss: 0.27954 | train_logloss: 0.27598 | train_accuracy: 0.8948  | valid_logloss: 0.30121 | valid_accuracy: 0.88787 |  0:02:52s\n",
            "epoch 78 | loss: 0.27622 | train_logloss: 0.27014 | train_accuracy: 0.89677 | valid_logloss: 0.29308 | valid_accuracy: 0.88967 |  0:02:55s\n",
            "epoch 79 | loss: 0.27246 | train_logloss: 0.26798 | train_accuracy: 0.89783 | valid_logloss: 0.29556 | valid_accuracy: 0.8893  |  0:02:57s\n",
            "epoch 80 | loss: 0.27123 | train_logloss: 0.26514 | train_accuracy: 0.8988  | valid_logloss: 0.29273 | valid_accuracy: 0.89077 |  0:02:59s\n",
            "epoch 81 | loss: 0.27056 | train_logloss: 0.26272 | train_accuracy: 0.8967  | valid_logloss: 0.29323 | valid_accuracy: 0.8893  |  0:03:01s\n",
            "epoch 82 | loss: 0.27259 | train_logloss: 0.27664 | train_accuracy: 0.89473 | valid_logloss: 0.30252 | valid_accuracy: 0.8884  |  0:03:03s\n",
            "epoch 83 | loss: 0.27518 | train_logloss: 0.2741  | train_accuracy: 0.89667 | valid_logloss: 0.30608 | valid_accuracy: 0.8873  |  0:03:05s\n",
            "epoch 84 | loss: 0.27399 | train_logloss: 0.26742 | train_accuracy: 0.8969  | valid_logloss: 0.29482 | valid_accuracy: 0.8886  |  0:03:07s\n",
            "epoch 85 | loss: 0.27176 | train_logloss: 0.26468 | train_accuracy: 0.89863 | valid_logloss: 0.29395 | valid_accuracy: 0.8891  |  0:03:10s\n",
            "epoch 86 | loss: 0.27599 | train_logloss: 0.26946 | train_accuracy: 0.8966  | valid_logloss: 0.29539 | valid_accuracy: 0.88947 |  0:03:12s\n",
            "epoch 87 | loss: 0.27548 | train_logloss: 0.27249 | train_accuracy: 0.89597 | valid_logloss: 0.30289 | valid_accuracy: 0.88577 |  0:03:14s\n",
            "epoch 88 | loss: 0.27441 | train_logloss: 0.26797 | train_accuracy: 0.89893 | valid_logloss: 0.29994 | valid_accuracy: 0.88873 |  0:03:16s\n",
            "epoch 89 | loss: 0.27278 | train_logloss: 0.26455 | train_accuracy: 0.8981  | valid_logloss: 0.29379 | valid_accuracy: 0.89083 |  0:03:18s\n",
            "epoch 90 | loss: 0.28483 | train_logloss: 0.27475 | train_accuracy: 0.89593 | valid_logloss: 0.29878 | valid_accuracy: 0.8888  |  0:03:20s\n",
            "epoch 91 | loss: 0.2772  | train_logloss: 0.26589 | train_accuracy: 0.89763 | valid_logloss: 0.29391 | valid_accuracy: 0.89013 |  0:03:22s\n",
            "epoch 92 | loss: 0.27268 | train_logloss: 0.26492 | train_accuracy: 0.89967 | valid_logloss: 0.29426 | valid_accuracy: 0.89043 |  0:03:24s\n",
            "epoch 93 | loss: 0.27101 | train_logloss: 0.26461 | train_accuracy: 0.89853 | valid_logloss: 0.29421 | valid_accuracy: 0.8897  |  0:03:27s\n",
            "epoch 94 | loss: 0.26751 | train_logloss: 0.26042 | train_accuracy: 0.90063 | valid_logloss: 0.29237 | valid_accuracy: 0.89203 |  0:03:29s\n",
            "epoch 95 | loss: 0.268   | train_logloss: 0.26317 | train_accuracy: 0.89977 | valid_logloss: 0.2959  | valid_accuracy: 0.88863 |  0:03:31s\n",
            "epoch 96 | loss: 0.26938 | train_logloss: 0.25944 | train_accuracy: 0.90047 | valid_logloss: 0.29403 | valid_accuracy: 0.889   |  0:03:33s\n",
            "epoch 97 | loss: 0.2641  | train_logloss: 0.25672 | train_accuracy: 0.90157 | valid_logloss: 0.29248 | valid_accuracy: 0.89027 |  0:03:35s\n",
            "epoch 98 | loss: 0.26533 | train_logloss: 0.25793 | train_accuracy: 0.9005  | valid_logloss: 0.2938  | valid_accuracy: 0.89023 |  0:03:37s\n",
            "epoch 99 | loss: 0.26343 | train_logloss: 0.26057 | train_accuracy: 0.89953 | valid_logloss: 0.29541 | valid_accuracy: 0.8892  |  0:03:39s\n",
            "epoch 100| loss: 0.26208 | train_logloss: 0.25628 | train_accuracy: 0.90053 | valid_logloss: 0.29423 | valid_accuracy: 0.88953 |  0:03:42s\n",
            "epoch 101| loss: 0.26255 | train_logloss: 0.25914 | train_accuracy: 0.8999  | valid_logloss: 0.29751 | valid_accuracy: 0.88817 |  0:03:44s\n",
            "epoch 102| loss: 0.26052 | train_logloss: 0.25212 | train_accuracy: 0.90163 | valid_logloss: 0.29016 | valid_accuracy: 0.89113 |  0:03:46s\n",
            "epoch 103| loss: 0.25586 | train_logloss: 0.2526  | train_accuracy: 0.9027  | valid_logloss: 0.29393 | valid_accuracy: 0.89293 |  0:03:48s\n",
            "epoch 104| loss: 0.25851 | train_logloss: 0.25077 | train_accuracy: 0.90363 | valid_logloss: 0.29353 | valid_accuracy: 0.8915  |  0:03:50s\n",
            "epoch 105| loss: 0.25626 | train_logloss: 0.25198 | train_accuracy: 0.9026  | valid_logloss: 0.29673 | valid_accuracy: 0.89227 |  0:03:52s\n",
            "epoch 106| loss: 0.25923 | train_logloss: 0.25354 | train_accuracy: 0.90267 | valid_logloss: 0.29509 | valid_accuracy: 0.89123 |  0:03:54s\n",
            "epoch 107| loss: 0.25781 | train_logloss: 0.25163 | train_accuracy: 0.90283 | valid_logloss: 0.29753 | valid_accuracy: 0.89103 |  0:03:57s\n",
            "epoch 108| loss: 0.25661 | train_logloss: 0.2486  | train_accuracy: 0.90357 | valid_logloss: 0.29471 | valid_accuracy: 0.89163 |  0:03:59s\n",
            "epoch 109| loss: 0.25564 | train_logloss: 0.24838 | train_accuracy: 0.90517 | valid_logloss: 0.29885 | valid_accuracy: 0.88993 |  0:04:01s\n",
            "epoch 110| loss: 0.25444 | train_logloss: 0.24677 | train_accuracy: 0.9051  | valid_logloss: 0.29219 | valid_accuracy: 0.8928  |  0:04:03s\n",
            "epoch 111| loss: 0.25371 | train_logloss: 0.24358 | train_accuracy: 0.9057  | valid_logloss: 0.29406 | valid_accuracy: 0.8907  |  0:04:05s\n",
            "epoch 112| loss: 0.25121 | train_logloss: 0.24256 | train_accuracy: 0.90653 | valid_logloss: 0.29717 | valid_accuracy: 0.89203 |  0:04:07s\n",
            "epoch 113| loss: 0.25357 | train_logloss: 0.2452  | train_accuracy: 0.90473 | valid_logloss: 0.29589 | valid_accuracy: 0.89033 |  0:04:09s\n",
            "epoch 114| loss: 0.25284 | train_logloss: 0.24793 | train_accuracy: 0.9048  | valid_logloss: 0.30038 | valid_accuracy: 0.88933 |  0:04:12s\n",
            "epoch 115| loss: 0.25313 | train_logloss: 0.24265 | train_accuracy: 0.90623 | valid_logloss: 0.29613 | valid_accuracy: 0.89257 |  0:04:14s\n",
            "epoch 116| loss: 0.2508  | train_logloss: 0.24423 | train_accuracy: 0.90527 | valid_logloss: 0.29566 | valid_accuracy: 0.8905  |  0:04:16s\n",
            "epoch 117| loss: 0.2502  | train_logloss: 0.24441 | train_accuracy: 0.9058  | valid_logloss: 0.29898 | valid_accuracy: 0.89113 |  0:04:18s\n",
            "epoch 118| loss: 0.24916 | train_logloss: 0.24197 | train_accuracy: 0.90643 | valid_logloss: 0.29711 | valid_accuracy: 0.892   |  0:04:20s\n",
            "epoch 119| loss: 0.25319 | train_logloss: 0.25539 | train_accuracy: 0.90257 | valid_logloss: 0.30549 | valid_accuracy: 0.88883 |  0:04:22s\n",
            "epoch 120| loss: 0.26652 | train_logloss: 0.26247 | train_accuracy: 0.90117 | valid_logloss: 0.30016 | valid_accuracy: 0.89017 |  0:04:24s\n",
            "epoch 121| loss: 0.27249 | train_logloss: 0.26449 | train_accuracy: 0.89963 | valid_logloss: 0.29901 | valid_accuracy: 0.8886  |  0:04:27s\n",
            "epoch 122| loss: 0.27135 | train_logloss: 0.26047 | train_accuracy: 0.9007  | valid_logloss: 0.29678 | valid_accuracy: 0.88973 |  0:04:29s\n",
            "epoch 123| loss: 0.26678 | train_logloss: 0.25613 | train_accuracy: 0.90277 | valid_logloss: 0.29185 | valid_accuracy: 0.89137 |  0:04:31s\n",
            "epoch 124| loss: 0.26281 | train_logloss: 0.26444 | train_accuracy: 0.89693 | valid_logloss: 0.31038 | valid_accuracy: 0.88587 |  0:04:33s\n",
            "epoch 125| loss: 0.26687 | train_logloss: 0.26211 | train_accuracy: 0.8978  | valid_logloss: 0.2983  | valid_accuracy: 0.8894  |  0:04:35s\n",
            "epoch 126| loss: 0.26587 | train_logloss: 0.25389 | train_accuracy: 0.90173 | valid_logloss: 0.29404 | valid_accuracy: 0.89133 |  0:04:37s\n",
            "epoch 127| loss: 0.25967 | train_logloss: 0.25046 | train_accuracy: 0.90453 | valid_logloss: 0.29045 | valid_accuracy: 0.891   |  0:04:39s\n",
            "epoch 128| loss: 0.2578  | train_logloss: 0.24846 | train_accuracy: 0.9058  | valid_logloss: 0.29163 | valid_accuracy: 0.89243 |  0:04:42s\n",
            "epoch 129| loss: 0.25504 | train_logloss: 0.24565 | train_accuracy: 0.90423 | valid_logloss: 0.29582 | valid_accuracy: 0.8908  |  0:04:44s\n",
            "epoch 130| loss: 0.25352 | train_logloss: 0.24705 | train_accuracy: 0.9052  | valid_logloss: 0.29403 | valid_accuracy: 0.8902  |  0:04:46s\n",
            "epoch 131| loss: 0.25277 | train_logloss: 0.24267 | train_accuracy: 0.90613 | valid_logloss: 0.29331 | valid_accuracy: 0.89173 |  0:04:48s\n",
            "epoch 132| loss: 0.25263 | train_logloss: 0.24292 | train_accuracy: 0.90653 | valid_logloss: 0.29322 | valid_accuracy: 0.89063 |  0:04:50s\n",
            "epoch 133| loss: 0.24835 | train_logloss: 0.23953 | train_accuracy: 0.90613 | valid_logloss: 0.29604 | valid_accuracy: 0.8927  |  0:04:52s\n",
            "epoch 134| loss: 0.24808 | train_logloss: 0.24064 | train_accuracy: 0.90753 | valid_logloss: 0.29866 | valid_accuracy: 0.89197 |  0:04:54s\n",
            "epoch 135| loss: 0.24685 | train_logloss: 0.23822 | train_accuracy: 0.90803 | valid_logloss: 0.29688 | valid_accuracy: 0.89023 |  0:04:57s\n",
            "epoch 136| loss: 0.24936 | train_logloss: 0.23871 | train_accuracy: 0.9066  | valid_logloss: 0.29274 | valid_accuracy: 0.89133 |  0:04:59s\n",
            "epoch 137| loss: 0.25008 | train_logloss: 0.24246 | train_accuracy: 0.90587 | valid_logloss: 0.29693 | valid_accuracy: 0.89053 |  0:05:01s\n",
            "epoch 138| loss: 0.25028 | train_logloss: 0.24952 | train_accuracy: 0.90453 | valid_logloss: 0.30634 | valid_accuracy: 0.88817 |  0:05:03s\n",
            "epoch 139| loss: 0.25828 | train_logloss: 0.24596 | train_accuracy: 0.90467 | valid_logloss: 0.29939 | valid_accuracy: 0.8911  |  0:05:05s\n",
            "epoch 140| loss: 0.25418 | train_logloss: 0.24582 | train_accuracy: 0.90423 | valid_logloss: 0.2969  | valid_accuracy: 0.88917 |  0:05:07s\n",
            "epoch 141| loss: 0.25046 | train_logloss: 0.2442  | train_accuracy: 0.90697 | valid_logloss: 0.3023  | valid_accuracy: 0.88923 |  0:05:09s\n",
            "epoch 142| loss: 0.24713 | train_logloss: 0.23878 | train_accuracy: 0.9079  | valid_logloss: 0.29572 | valid_accuracy: 0.89083 |  0:05:12s\n",
            "epoch 143| loss: 0.24307 | train_logloss: 0.2354  | train_accuracy: 0.90817 | valid_logloss: 0.29799 | valid_accuracy: 0.89173 |  0:05:14s\n",
            "epoch 144| loss: 0.24463 | train_logloss: 0.23662 | train_accuracy: 0.9096  | valid_logloss: 0.29712 | valid_accuracy: 0.89083 |  0:05:16s\n",
            "epoch 145| loss: 0.24261 | train_logloss: 0.23206 | train_accuracy: 0.90947 | valid_logloss: 0.29989 | valid_accuracy: 0.89083 |  0:05:18s\n",
            "epoch 146| loss: 0.24088 | train_logloss: 0.2321  | train_accuracy: 0.91113 | valid_logloss: 0.29754 | valid_accuracy: 0.88967 |  0:05:20s\n",
            "epoch 147| loss: 0.24337 | train_logloss: 0.23131 | train_accuracy: 0.9105  | valid_logloss: 0.30043 | valid_accuracy: 0.8912  |  0:05:22s\n",
            "epoch 148| loss: 0.24063 | train_logloss: 0.23517 | train_accuracy: 0.90953 | valid_logloss: 0.30138 | valid_accuracy: 0.8907  |  0:05:24s\n",
            "epoch 149| loss: 0.24304 | train_logloss: 0.2372  | train_accuracy: 0.9094  | valid_logloss: 0.30225 | valid_accuracy: 0.88903 |  0:05:27s\n",
            "epoch 150| loss: 0.24496 | train_logloss: 0.2379  | train_accuracy: 0.9088  | valid_logloss: 0.30638 | valid_accuracy: 0.88997 |  0:05:29s\n",
            "epoch 151| loss: 0.24599 | train_logloss: 0.23585 | train_accuracy: 0.90893 | valid_logloss: 0.30486 | valid_accuracy: 0.88977 |  0:05:31s\n",
            "epoch 152| loss: 0.24439 | train_logloss: 0.23905 | train_accuracy: 0.90793 | valid_logloss: 0.30745 | valid_accuracy: 0.88843 |  0:05:33s\n",
            "epoch 153| loss: 0.24117 | train_logloss: 0.23141 | train_accuracy: 0.91043 | valid_logloss: 0.3028  | valid_accuracy: 0.88967 |  0:05:35s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 103 and best_valid_accuracy = 0.89293\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixw-bGGCzvyw",
        "outputId": "a271091e-e68a-47e0-dfbb-4f93da7c2b3a"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb5.pkl' 'tn5.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    253.5 MiB    253.5 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.1 MiB      5.6 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2319.0 MiB   2059.9 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkLeovCoHp2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055a52a6-99db-40da-87c5-e0244603c51a"
      },
      "source": [
        "memory(number_exp=6, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.69956 | train_logloss: 19.1368 | train_accuracy: 0.30157 | valid_logloss: 18.98184| valid_accuracy: 0.3066  |  0:00:01s\n",
            "epoch 1  | loss: 0.39224 | train_logloss: 13.7197 | train_accuracy: 0.3147  | valid_logloss: 13.76676| valid_accuracy: 0.31487 |  0:00:03s\n",
            "epoch 2  | loss: 0.35065 | train_logloss: 17.28233| train_accuracy: 0.31073 | valid_logloss: 17.28015| valid_accuracy: 0.31097 |  0:00:04s\n",
            "epoch 3  | loss: 0.33239 | train_logloss: 9.25345 | train_accuracy: 0.26283 | valid_logloss: 9.27368 | valid_accuracy: 0.26153 |  0:00:06s\n",
            "epoch 4  | loss: 0.32662 | train_logloss: 5.23646 | train_accuracy: 0.19393 | valid_logloss: 5.25429 | valid_accuracy: 0.19507 |  0:00:07s\n",
            "epoch 5  | loss: 0.31889 | train_logloss: 5.17342 | train_accuracy: 0.1914  | valid_logloss: 5.18116 | valid_accuracy: 0.19033 |  0:00:09s\n",
            "epoch 6  | loss: 0.31295 | train_logloss: 4.63485 | train_accuracy: 0.28783 | valid_logloss: 4.65883 | valid_accuracy: 0.2864  |  0:00:10s\n",
            "epoch 7  | loss: 0.30388 | train_logloss: 3.62504 | train_accuracy: 0.3366  | valid_logloss: 3.65505 | valid_accuracy: 0.33487 |  0:00:12s\n",
            "epoch 8  | loss: 0.30345 | train_logloss: 2.99254 | train_accuracy: 0.3589  | valid_logloss: 3.01201 | valid_accuracy: 0.3564  |  0:00:14s\n",
            "epoch 9  | loss: 0.30469 | train_logloss: 3.13761 | train_accuracy: 0.32433 | valid_logloss: 3.16707 | valid_accuracy: 0.31933 |  0:00:15s\n",
            "epoch 10 | loss: 0.30184 | train_logloss: 1.48885 | train_accuracy: 0.4621  | valid_logloss: 1.50825 | valid_accuracy: 0.45667 |  0:00:17s\n",
            "epoch 11 | loss: 0.29804 | train_logloss: 1.54527 | train_accuracy: 0.38867 | valid_logloss: 1.55547 | valid_accuracy: 0.38277 |  0:00:18s\n",
            "epoch 12 | loss: 0.29027 | train_logloss: 1.28611 | train_accuracy: 0.52847 | valid_logloss: 1.29732 | valid_accuracy: 0.52183 |  0:00:20s\n",
            "epoch 13 | loss: 0.28815 | train_logloss: 0.9834  | train_accuracy: 0.62217 | valid_logloss: 0.99384 | valid_accuracy: 0.61327 |  0:00:22s\n",
            "epoch 14 | loss: 0.29073 | train_logloss: 0.91848 | train_accuracy: 0.6366  | valid_logloss: 0.92275 | valid_accuracy: 0.63147 |  0:00:23s\n",
            "epoch 15 | loss: 0.28638 | train_logloss: 0.94037 | train_accuracy: 0.69493 | valid_logloss: 0.94953 | valid_accuracy: 0.69363 |  0:00:25s\n",
            "epoch 16 | loss: 0.2875  | train_logloss: 0.60466 | train_accuracy: 0.79803 | valid_logloss: 0.61054 | valid_accuracy: 0.79257 |  0:00:26s\n",
            "epoch 17 | loss: 0.28078 | train_logloss: 0.57784 | train_accuracy: 0.78457 | valid_logloss: 0.58723 | valid_accuracy: 0.7824  |  0:00:28s\n",
            "epoch 18 | loss: 0.2783  | train_logloss: 0.45591 | train_accuracy: 0.83937 | valid_logloss: 0.46561 | valid_accuracy: 0.83653 |  0:00:30s\n",
            "epoch 19 | loss: 0.28025 | train_logloss: 0.44233 | train_accuracy: 0.8412  | valid_logloss: 0.45462 | valid_accuracy: 0.83833 |  0:00:31s\n",
            "epoch 20 | loss: 0.2725  | train_logloss: 0.37264 | train_accuracy: 0.86767 | valid_logloss: 0.38941 | valid_accuracy: 0.8633  |  0:00:33s\n",
            "epoch 21 | loss: 0.26917 | train_logloss: 0.333   | train_accuracy: 0.87883 | valid_logloss: 0.35173 | valid_accuracy: 0.87663 |  0:00:34s\n",
            "epoch 22 | loss: 0.26927 | train_logloss: 0.32288 | train_accuracy: 0.88023 | valid_logloss: 0.34692 | valid_accuracy: 0.87437 |  0:00:36s\n",
            "epoch 23 | loss: 0.27185 | train_logloss: 0.31845 | train_accuracy: 0.8815  | valid_logloss: 0.34465 | valid_accuracy: 0.87733 |  0:00:38s\n",
            "epoch 24 | loss: 0.27367 | train_logloss: 0.31359 | train_accuracy: 0.88363 | valid_logloss: 0.34104 | valid_accuracy: 0.8765  |  0:00:39s\n",
            "epoch 25 | loss: 0.27909 | train_logloss: 0.30434 | train_accuracy: 0.8861  | valid_logloss: 0.32642 | valid_accuracy: 0.8821  |  0:00:41s\n",
            "epoch 26 | loss: 0.28364 | train_logloss: 0.29199 | train_accuracy: 0.88867 | valid_logloss: 0.31711 | valid_accuracy: 0.88207 |  0:00:42s\n",
            "epoch 27 | loss: 0.28466 | train_logloss: 0.28037 | train_accuracy: 0.89047 | valid_logloss: 0.30541 | valid_accuracy: 0.88623 |  0:00:44s\n",
            "epoch 28 | loss: 0.27769 | train_logloss: 0.26998 | train_accuracy: 0.8972  | valid_logloss: 0.29998 | valid_accuracy: 0.88757 |  0:00:46s\n",
            "epoch 29 | loss: 0.27103 | train_logloss: 0.26981 | train_accuracy: 0.89503 | valid_logloss: 0.30161 | valid_accuracy: 0.88623 |  0:00:47s\n",
            "epoch 30 | loss: 0.26906 | train_logloss: 0.27144 | train_accuracy: 0.8954  | valid_logloss: 0.3076  | valid_accuracy: 0.88533 |  0:00:49s\n",
            "epoch 31 | loss: 0.26877 | train_logloss: 0.27427 | train_accuracy: 0.896   | valid_logloss: 0.30713 | valid_accuracy: 0.88647 |  0:00:50s\n",
            "epoch 32 | loss: 0.27251 | train_logloss: 0.25733 | train_accuracy: 0.9011  | valid_logloss: 0.29773 | valid_accuracy: 0.89037 |  0:00:52s\n",
            "epoch 33 | loss: 0.26587 | train_logloss: 0.2604  | train_accuracy: 0.89937 | valid_logloss: 0.30247 | valid_accuracy: 0.88803 |  0:00:54s\n",
            "epoch 34 | loss: 0.2613  | train_logloss: 0.25367 | train_accuracy: 0.90137 | valid_logloss: 0.30591 | valid_accuracy: 0.88687 |  0:00:55s\n",
            "epoch 35 | loss: 0.26402 | train_logloss: 0.25528 | train_accuracy: 0.90063 | valid_logloss: 0.30259 | valid_accuracy: 0.88727 |  0:00:57s\n",
            "epoch 36 | loss: 0.26577 | train_logloss: 0.26107 | train_accuracy: 0.89813 | valid_logloss: 0.301   | valid_accuracy: 0.88733 |  0:00:58s\n",
            "epoch 37 | loss: 0.26369 | train_logloss: 0.26373 | train_accuracy: 0.89727 | valid_logloss: 0.31182 | valid_accuracy: 0.88463 |  0:01:00s\n",
            "epoch 38 | loss: 0.26477 | train_logloss: 0.2507  | train_accuracy: 0.90283 | valid_logloss: 0.29684 | valid_accuracy: 0.88893 |  0:01:02s\n",
            "epoch 39 | loss: 0.26347 | train_logloss: 0.25581 | train_accuracy: 0.90033 | valid_logloss: 0.30639 | valid_accuracy: 0.88617 |  0:01:03s\n",
            "epoch 40 | loss: 0.26119 | train_logloss: 0.24904 | train_accuracy: 0.904   | valid_logloss: 0.29753 | valid_accuracy: 0.88897 |  0:01:05s\n",
            "epoch 41 | loss: 0.25957 | train_logloss: 0.24666 | train_accuracy: 0.90407 | valid_logloss: 0.29711 | valid_accuracy: 0.88943 |  0:01:06s\n",
            "epoch 42 | loss: 0.25297 | train_logloss: 0.24978 | train_accuracy: 0.90277 | valid_logloss: 0.30109 | valid_accuracy: 0.88813 |  0:01:08s\n",
            "epoch 43 | loss: 0.25328 | train_logloss: 0.24026 | train_accuracy: 0.9057  | valid_logloss: 0.29791 | valid_accuracy: 0.88957 |  0:01:10s\n",
            "epoch 44 | loss: 0.25606 | train_logloss: 0.25127 | train_accuracy: 0.90307 | valid_logloss: 0.3076  | valid_accuracy: 0.88537 |  0:01:11s\n",
            "epoch 45 | loss: 0.25609 | train_logloss: 0.24682 | train_accuracy: 0.90383 | valid_logloss: 0.30369 | valid_accuracy: 0.8887  |  0:01:13s\n",
            "epoch 46 | loss: 0.25034 | train_logloss: 0.23985 | train_accuracy: 0.9069  | valid_logloss: 0.30083 | valid_accuracy: 0.89083 |  0:01:14s\n",
            "epoch 47 | loss: 0.24797 | train_logloss: 0.24048 | train_accuracy: 0.9074  | valid_logloss: 0.3006  | valid_accuracy: 0.88963 |  0:01:16s\n",
            "epoch 48 | loss: 0.25109 | train_logloss: 0.24268 | train_accuracy: 0.90657 | valid_logloss: 0.30486 | valid_accuracy: 0.88903 |  0:01:18s\n",
            "epoch 49 | loss: 0.25102 | train_logloss: 0.24309 | train_accuracy: 0.9064  | valid_logloss: 0.30837 | valid_accuracy: 0.88833 |  0:01:19s\n",
            "epoch 50 | loss: 0.25224 | train_logloss: 0.2485  | train_accuracy: 0.90407 | valid_logloss: 0.31019 | valid_accuracy: 0.88427 |  0:01:21s\n",
            "epoch 51 | loss: 0.25019 | train_logloss: 0.24065 | train_accuracy: 0.90533 | valid_logloss: 0.30463 | valid_accuracy: 0.8896  |  0:01:22s\n",
            "epoch 52 | loss: 0.24656 | train_logloss: 0.2307  | train_accuracy: 0.91097 | valid_logloss: 0.30465 | valid_accuracy: 0.8892  |  0:01:24s\n",
            "epoch 53 | loss: 0.24424 | train_logloss: 0.23967 | train_accuracy: 0.90627 | valid_logloss: 0.30471 | valid_accuracy: 0.89063 |  0:01:26s\n",
            "epoch 54 | loss: 0.2461  | train_logloss: 0.24324 | train_accuracy: 0.90643 | valid_logloss: 0.31383 | valid_accuracy: 0.88823 |  0:01:27s\n",
            "epoch 55 | loss: 0.24916 | train_logloss: 0.23904 | train_accuracy: 0.9081  | valid_logloss: 0.30828 | valid_accuracy: 0.88883 |  0:01:29s\n",
            "epoch 56 | loss: 0.24739 | train_logloss: 0.23448 | train_accuracy: 0.90887 | valid_logloss: 0.31015 | valid_accuracy: 0.88807 |  0:01:31s\n",
            "epoch 57 | loss: 0.24598 | train_logloss: 0.23369 | train_accuracy: 0.91047 | valid_logloss: 0.30944 | valid_accuracy: 0.88847 |  0:01:32s\n",
            "epoch 58 | loss: 0.24714 | train_logloss: 0.23352 | train_accuracy: 0.9085  | valid_logloss: 0.30644 | valid_accuracy: 0.8894  |  0:01:34s\n",
            "epoch 59 | loss: 0.24613 | train_logloss: 0.2373  | train_accuracy: 0.9079  | valid_logloss: 0.30617 | valid_accuracy: 0.88863 |  0:01:35s\n",
            "epoch 60 | loss: 0.24218 | train_logloss: 0.22919 | train_accuracy: 0.91163 | valid_logloss: 0.30369 | valid_accuracy: 0.89073 |  0:01:37s\n",
            "epoch 61 | loss: 0.24196 | train_logloss: 0.22888 | train_accuracy: 0.91317 | valid_logloss: 0.30795 | valid_accuracy: 0.88707 |  0:01:38s\n",
            "epoch 62 | loss: 0.24117 | train_logloss: 0.23111 | train_accuracy: 0.90977 | valid_logloss: 0.30653 | valid_accuracy: 0.88883 |  0:01:40s\n",
            "epoch 63 | loss: 0.24151 | train_logloss: 0.22783 | train_accuracy: 0.91127 | valid_logloss: 0.30817 | valid_accuracy: 0.88817 |  0:01:42s\n",
            "epoch 64 | loss: 0.24689 | train_logloss: 0.25441 | train_accuracy: 0.9024  | valid_logloss: 0.32106 | valid_accuracy: 0.88523 |  0:01:43s\n",
            "epoch 65 | loss: 0.26073 | train_logloss: 0.24474 | train_accuracy: 0.9034  | valid_logloss: 0.30644 | valid_accuracy: 0.89037 |  0:01:45s\n",
            "epoch 66 | loss: 0.25074 | train_logloss: 0.23878 | train_accuracy: 0.90703 | valid_logloss: 0.30699 | valid_accuracy: 0.8892  |  0:01:46s\n",
            "epoch 67 | loss: 0.24481 | train_logloss: 0.23022 | train_accuracy: 0.90943 | valid_logloss: 0.30348 | valid_accuracy: 0.88967 |  0:01:48s\n",
            "epoch 68 | loss: 0.24071 | train_logloss: 0.23372 | train_accuracy: 0.90873 | valid_logloss: 0.30591 | valid_accuracy: 0.88743 |  0:01:50s\n",
            "epoch 69 | loss: 0.24946 | train_logloss: 0.2421  | train_accuracy: 0.90687 | valid_logloss: 0.3072  | valid_accuracy: 0.88793 |  0:01:51s\n",
            "epoch 70 | loss: 0.24434 | train_logloss: 0.23373 | train_accuracy: 0.90917 | valid_logloss: 0.30339 | valid_accuracy: 0.88993 |  0:01:53s\n",
            "epoch 71 | loss: 0.24311 | train_logloss: 0.2337  | train_accuracy: 0.90937 | valid_logloss: 0.30298 | valid_accuracy: 0.89047 |  0:01:54s\n",
            "epoch 72 | loss: 0.25989 | train_logloss: 0.24889 | train_accuracy: 0.9047  | valid_logloss: 0.30539 | valid_accuracy: 0.88787 |  0:01:56s\n",
            "epoch 73 | loss: 0.25501 | train_logloss: 0.24494 | train_accuracy: 0.90443 | valid_logloss: 0.30845 | valid_accuracy: 0.88653 |  0:01:58s\n",
            "epoch 74 | loss: 0.24917 | train_logloss: 0.23304 | train_accuracy: 0.90927 | valid_logloss: 0.29757 | valid_accuracy: 0.89063 |  0:01:59s\n",
            "epoch 75 | loss: 0.24532 | train_logloss: 0.22756 | train_accuracy: 0.91087 | valid_logloss: 0.30487 | valid_accuracy: 0.88717 |  0:02:01s\n",
            "epoch 76 | loss: 0.23789 | train_logloss: 0.22774 | train_accuracy: 0.91123 | valid_logloss: 0.30376 | valid_accuracy: 0.8906  |  0:02:02s\n",
            "epoch 77 | loss: 0.23767 | train_logloss: 0.22236 | train_accuracy: 0.9126  | valid_logloss: 0.30741 | valid_accuracy: 0.89003 |  0:02:04s\n",
            "epoch 78 | loss: 0.2378  | train_logloss: 0.22221 | train_accuracy: 0.91203 | valid_logloss: 0.30335 | valid_accuracy: 0.89047 |  0:02:06s\n",
            "epoch 79 | loss: 0.23598 | train_logloss: 0.22302 | train_accuracy: 0.9126  | valid_logloss: 0.31282 | valid_accuracy: 0.8861  |  0:02:07s\n",
            "epoch 80 | loss: 0.2354  | train_logloss: 0.23323 | train_accuracy: 0.90743 | valid_logloss: 0.31787 | valid_accuracy: 0.88623 |  0:02:09s\n",
            "epoch 81 | loss: 0.23833 | train_logloss: 0.22512 | train_accuracy: 0.91183 | valid_logloss: 0.31225 | valid_accuracy: 0.8879  |  0:02:10s\n",
            "epoch 82 | loss: 0.23431 | train_logloss: 0.22375 | train_accuracy: 0.91313 | valid_logloss: 0.31459 | valid_accuracy: 0.88593 |  0:02:12s\n",
            "epoch 83 | loss: 0.23525 | train_logloss: 0.22461 | train_accuracy: 0.91177 | valid_logloss: 0.31115 | valid_accuracy: 0.88827 |  0:02:14s\n",
            "epoch 84 | loss: 0.23213 | train_logloss: 0.22219 | train_accuracy: 0.9139  | valid_logloss: 0.31074 | valid_accuracy: 0.887   |  0:02:15s\n",
            "epoch 85 | loss: 0.22845 | train_logloss: 0.21538 | train_accuracy: 0.91457 | valid_logloss: 0.30896 | valid_accuracy: 0.8894  |  0:02:17s\n",
            "epoch 86 | loss: 0.22596 | train_logloss: 0.20994 | train_accuracy: 0.91637 | valid_logloss: 0.31454 | valid_accuracy: 0.88933 |  0:02:19s\n",
            "epoch 87 | loss: 0.2222  | train_logloss: 0.20753 | train_accuracy: 0.9198  | valid_logloss: 0.31779 | valid_accuracy: 0.88723 |  0:02:20s\n",
            "epoch 88 | loss: 0.22213 | train_logloss: 0.21591 | train_accuracy: 0.91367 | valid_logloss: 0.319   | valid_accuracy: 0.88697 |  0:02:22s\n",
            "epoch 89 | loss: 0.2248  | train_logloss: 0.22132 | train_accuracy: 0.91223 | valid_logloss: 0.32317 | valid_accuracy: 0.88733 |  0:02:24s\n",
            "epoch 90 | loss: 0.23105 | train_logloss: 0.21612 | train_accuracy: 0.9153  | valid_logloss: 0.31653 | valid_accuracy: 0.88683 |  0:02:25s\n",
            "epoch 91 | loss: 0.22671 | train_logloss: 0.21115 | train_accuracy: 0.9172  | valid_logloss: 0.31153 | valid_accuracy: 0.88743 |  0:02:27s\n",
            "epoch 92 | loss: 0.22346 | train_logloss: 0.20663 | train_accuracy: 0.9188  | valid_logloss: 0.32332 | valid_accuracy: 0.88657 |  0:02:28s\n",
            "epoch 93 | loss: 0.21903 | train_logloss: 0.206   | train_accuracy: 0.9173  | valid_logloss: 0.32889 | valid_accuracy: 0.88733 |  0:02:30s\n",
            "epoch 94 | loss: 0.21815 | train_logloss: 0.20233 | train_accuracy: 0.9198  | valid_logloss: 0.32306 | valid_accuracy: 0.88823 |  0:02:32s\n",
            "epoch 95 | loss: 0.21441 | train_logloss: 0.20266 | train_accuracy: 0.9196  | valid_logloss: 0.32242 | valid_accuracy: 0.88853 |  0:02:33s\n",
            "epoch 96 | loss: 0.216   | train_logloss: 0.20427 | train_accuracy: 0.9182  | valid_logloss: 0.32594 | valid_accuracy: 0.88753 |  0:02:35s\n",
            "\n",
            "Early stopping occurred at epoch 96 with best_epoch = 46 and best_valid_accuracy = 0.89083\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtGVjktM0GXU",
        "outputId": "e6a8d8ab-592b-44db-b22e-311f8b2393c2"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb6.pkl' 'tn6.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    250.9 MiB    250.9 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    256.5 MiB      5.7 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2320.1 MiB   2063.6 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9w7lgB0IYng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c5117c-182c-4573-d443-c8813e333853"
      },
      "source": [
        "memory(number_exp=7, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.79359 | train_logloss: 15.44778| train_accuracy: 0.33477 | valid_logloss: 15.47836| valid_accuracy: 0.33477 |  0:00:01s\n",
            "epoch 1  | loss: 0.42554 | train_logloss: 22.44025| train_accuracy: 0.33337 | valid_logloss: 22.46656| valid_accuracy: 0.33317 |  0:00:03s\n",
            "epoch 2  | loss: 0.35965 | train_logloss: 12.26977| train_accuracy: 0.3281  | valid_logloss: 12.2782 | valid_accuracy: 0.3278  |  0:00:05s\n",
            "epoch 3  | loss: 0.34007 | train_logloss: 5.79972 | train_accuracy: 0.3622  | valid_logloss: 5.79605 | valid_accuracy: 0.36187 |  0:00:06s\n",
            "epoch 4  | loss: 0.32537 | train_logloss: 3.26044 | train_accuracy: 0.46783 | valid_logloss: 3.29715 | valid_accuracy: 0.4634  |  0:00:08s\n",
            "epoch 5  | loss: 0.31751 | train_logloss: 5.39595 | train_accuracy: 0.31773 | valid_logloss: 5.41002 | valid_accuracy: 0.31917 |  0:00:09s\n",
            "epoch 6  | loss: 0.31533 | train_logloss: 4.50698 | train_accuracy: 0.31837 | valid_logloss: 4.50554 | valid_accuracy: 0.3204  |  0:00:11s\n",
            "epoch 7  | loss: 0.30677 | train_logloss: 3.2863  | train_accuracy: 0.33573 | valid_logloss: 3.3033  | valid_accuracy: 0.3354  |  0:00:13s\n",
            "epoch 8  | loss: 0.30873 | train_logloss: 2.68492 | train_accuracy: 0.35813 | valid_logloss: 2.70148 | valid_accuracy: 0.35693 |  0:00:14s\n",
            "epoch 9  | loss: 0.30147 | train_logloss: 2.91223 | train_accuracy: 0.40167 | valid_logloss: 2.93773 | valid_accuracy: 0.39707 |  0:00:16s\n",
            "epoch 10 | loss: 0.29434 | train_logloss: 1.61735 | train_accuracy: 0.52577 | valid_logloss: 1.62845 | valid_accuracy: 0.52507 |  0:00:18s\n",
            "epoch 11 | loss: 0.29324 | train_logloss: 1.3859  | train_accuracy: 0.4636  | valid_logloss: 1.40105 | valid_accuracy: 0.4581  |  0:00:19s\n",
            "epoch 12 | loss: 0.29139 | train_logloss: 1.02658 | train_accuracy: 0.5322  | valid_logloss: 1.03835 | valid_accuracy: 0.5285  |  0:00:21s\n",
            "epoch 13 | loss: 0.29132 | train_logloss: 1.44733 | train_accuracy: 0.62137 | valid_logloss: 1.46033 | valid_accuracy: 0.61743 |  0:00:23s\n",
            "epoch 14 | loss: 0.28904 | train_logloss: 0.8384  | train_accuracy: 0.7256  | valid_logloss: 0.84476 | valid_accuracy: 0.72307 |  0:00:24s\n",
            "epoch 15 | loss: 0.28373 | train_logloss: 0.85121 | train_accuracy: 0.66403 | valid_logloss: 0.85675 | valid_accuracy: 0.65807 |  0:00:26s\n",
            "epoch 16 | loss: 0.2789  | train_logloss: 0.6688  | train_accuracy: 0.7656  | valid_logloss: 0.67791 | valid_accuracy: 0.75947 |  0:00:28s\n",
            "epoch 17 | loss: 0.28185 | train_logloss: 0.47702 | train_accuracy: 0.8336  | valid_logloss: 0.49217 | valid_accuracy: 0.82593 |  0:00:29s\n",
            "epoch 18 | loss: 0.27548 | train_logloss: 0.48007 | train_accuracy: 0.8245  | valid_logloss: 0.49615 | valid_accuracy: 0.81793 |  0:00:31s\n",
            "epoch 19 | loss: 0.27358 | train_logloss: 0.42041 | train_accuracy: 0.8531  | valid_logloss: 0.43445 | valid_accuracy: 0.84933 |  0:00:33s\n",
            "epoch 20 | loss: 0.26928 | train_logloss: 0.40095 | train_accuracy: 0.844   | valid_logloss: 0.4172  | valid_accuracy: 0.83973 |  0:00:34s\n",
            "epoch 21 | loss: 0.27007 | train_logloss: 0.36678 | train_accuracy: 0.86093 | valid_logloss: 0.3876  | valid_accuracy: 0.85657 |  0:00:36s\n",
            "epoch 22 | loss: 0.26547 | train_logloss: 0.32718 | train_accuracy: 0.8788  | valid_logloss: 0.35201 | valid_accuracy: 0.876   |  0:00:38s\n",
            "epoch 23 | loss: 0.26777 | train_logloss: 0.30843 | train_accuracy: 0.88503 | valid_logloss: 0.33385 | valid_accuracy: 0.87827 |  0:00:40s\n",
            "epoch 24 | loss: 0.26605 | train_logloss: 0.2887  | train_accuracy: 0.88963 | valid_logloss: 0.31845 | valid_accuracy: 0.88373 |  0:00:41s\n",
            "epoch 25 | loss: 0.26306 | train_logloss: 0.27829 | train_accuracy: 0.89303 | valid_logloss: 0.31631 | valid_accuracy: 0.88473 |  0:00:43s\n",
            "epoch 26 | loss: 0.26043 | train_logloss: 0.2755  | train_accuracy: 0.89407 | valid_logloss: 0.31676 | valid_accuracy: 0.8852  |  0:00:45s\n",
            "epoch 27 | loss: 0.26189 | train_logloss: 0.2585  | train_accuracy: 0.89967 | valid_logloss: 0.30405 | valid_accuracy: 0.8873  |  0:00:46s\n",
            "epoch 28 | loss: 0.26169 | train_logloss: 0.26381 | train_accuracy: 0.8972  | valid_logloss: 0.30764 | valid_accuracy: 0.88713 |  0:00:48s\n",
            "epoch 29 | loss: 0.26255 | train_logloss: 0.26428 | train_accuracy: 0.8991  | valid_logloss: 0.30946 | valid_accuracy: 0.88917 |  0:00:50s\n",
            "epoch 30 | loss: 0.26245 | train_logloss: 0.25599 | train_accuracy: 0.90183 | valid_logloss: 0.30148 | valid_accuracy: 0.89003 |  0:00:52s\n",
            "epoch 31 | loss: 0.25836 | train_logloss: 0.25031 | train_accuracy: 0.90307 | valid_logloss: 0.30817 | valid_accuracy: 0.8878  |  0:00:53s\n",
            "epoch 32 | loss: 0.25958 | train_logloss: 0.25021 | train_accuracy: 0.90253 | valid_logloss: 0.30012 | valid_accuracy: 0.88913 |  0:00:55s\n",
            "epoch 33 | loss: 0.25676 | train_logloss: 0.25046 | train_accuracy: 0.9012  | valid_logloss: 0.30374 | valid_accuracy: 0.8884  |  0:00:57s\n",
            "epoch 34 | loss: 0.25389 | train_logloss: 0.24458 | train_accuracy: 0.90557 | valid_logloss: 0.3027  | valid_accuracy: 0.88917 |  0:00:58s\n",
            "epoch 35 | loss: 0.25389 | train_logloss: 0.24661 | train_accuracy: 0.90397 | valid_logloss: 0.30201 | valid_accuracy: 0.88977 |  0:01:00s\n",
            "epoch 36 | loss: 0.25546 | train_logloss: 0.25023 | train_accuracy: 0.90377 | valid_logloss: 0.30482 | valid_accuracy: 0.88763 |  0:01:02s\n",
            "epoch 37 | loss: 0.26177 | train_logloss: 0.2526  | train_accuracy: 0.89953 | valid_logloss: 0.30358 | valid_accuracy: 0.88703 |  0:01:03s\n",
            "epoch 38 | loss: 0.25881 | train_logloss: 0.24518 | train_accuracy: 0.9052  | valid_logloss: 0.29718 | valid_accuracy: 0.89083 |  0:01:05s\n",
            "epoch 39 | loss: 0.25767 | train_logloss: 0.24799 | train_accuracy: 0.90333 | valid_logloss: 0.30333 | valid_accuracy: 0.8905  |  0:01:07s\n",
            "epoch 40 | loss: 0.25548 | train_logloss: 0.2453  | train_accuracy: 0.9029  | valid_logloss: 0.29998 | valid_accuracy: 0.8896  |  0:01:09s\n",
            "epoch 41 | loss: 0.25174 | train_logloss: 0.2406  | train_accuracy: 0.90517 | valid_logloss: 0.30059 | valid_accuracy: 0.88997 |  0:01:10s\n",
            "epoch 42 | loss: 0.25108 | train_logloss: 0.23984 | train_accuracy: 0.9066  | valid_logloss: 0.30205 | valid_accuracy: 0.89153 |  0:01:12s\n",
            "epoch 43 | loss: 0.24897 | train_logloss: 0.2385  | train_accuracy: 0.90817 | valid_logloss: 0.30428 | valid_accuracy: 0.8916  |  0:01:14s\n",
            "epoch 44 | loss: 0.24498 | train_logloss: 0.2337  | train_accuracy: 0.90923 | valid_logloss: 0.29929 | valid_accuracy: 0.8917  |  0:01:15s\n",
            "epoch 45 | loss: 0.24234 | train_logloss: 0.23411 | train_accuracy: 0.9097  | valid_logloss: 0.30068 | valid_accuracy: 0.8906  |  0:01:17s\n",
            "epoch 46 | loss: 0.24475 | train_logloss: 0.23368 | train_accuracy: 0.90753 | valid_logloss: 0.30605 | valid_accuracy: 0.8894  |  0:01:19s\n",
            "epoch 47 | loss: 0.24618 | train_logloss: 0.23277 | train_accuracy: 0.90883 | valid_logloss: 0.29665 | valid_accuracy: 0.89093 |  0:01:20s\n",
            "epoch 48 | loss: 0.24497 | train_logloss: 0.2379  | train_accuracy: 0.9079  | valid_logloss: 0.30145 | valid_accuracy: 0.89063 |  0:01:22s\n",
            "epoch 49 | loss: 0.24839 | train_logloss: 0.23948 | train_accuracy: 0.90613 | valid_logloss: 0.30505 | valid_accuracy: 0.88773 |  0:01:23s\n",
            "epoch 50 | loss: 0.2463  | train_logloss: 0.23881 | train_accuracy: 0.9067  | valid_logloss: 0.30594 | valid_accuracy: 0.89043 |  0:01:25s\n",
            "epoch 51 | loss: 0.24446 | train_logloss: 0.23717 | train_accuracy: 0.90827 | valid_logloss: 0.30703 | valid_accuracy: 0.88883 |  0:01:27s\n",
            "epoch 52 | loss: 0.24869 | train_logloss: 0.24203 | train_accuracy: 0.90557 | valid_logloss: 0.30739 | valid_accuracy: 0.89197 |  0:01:28s\n",
            "epoch 53 | loss: 0.25123 | train_logloss: 0.23404 | train_accuracy: 0.90973 | valid_logloss: 0.30154 | valid_accuracy: 0.892   |  0:01:30s\n",
            "epoch 54 | loss: 0.24388 | train_logloss: 0.23062 | train_accuracy: 0.91053 | valid_logloss: 0.30173 | valid_accuracy: 0.8923  |  0:01:31s\n",
            "epoch 55 | loss: 0.24478 | train_logloss: 0.23268 | train_accuracy: 0.9084  | valid_logloss: 0.30649 | valid_accuracy: 0.8893  |  0:01:33s\n",
            "epoch 56 | loss: 0.24706 | train_logloss: 0.24197 | train_accuracy: 0.9055  | valid_logloss: 0.31403 | valid_accuracy: 0.88713 |  0:01:34s\n",
            "epoch 57 | loss: 0.25383 | train_logloss: 0.24185 | train_accuracy: 0.90573 | valid_logloss: 0.30404 | valid_accuracy: 0.89    |  0:01:36s\n",
            "epoch 58 | loss: 0.24967 | train_logloss: 0.23638 | train_accuracy: 0.9076  | valid_logloss: 0.30362 | valid_accuracy: 0.89167 |  0:01:37s\n",
            "epoch 59 | loss: 0.24497 | train_logloss: 0.23835 | train_accuracy: 0.9068  | valid_logloss: 0.31302 | valid_accuracy: 0.88753 |  0:01:39s\n",
            "epoch 60 | loss: 0.24018 | train_logloss: 0.22958 | train_accuracy: 0.90843 | valid_logloss: 0.30496 | valid_accuracy: 0.8913  |  0:01:40s\n",
            "epoch 61 | loss: 0.23455 | train_logloss: 0.22123 | train_accuracy: 0.91313 | valid_logloss: 0.30849 | valid_accuracy: 0.8905  |  0:01:42s\n",
            "epoch 62 | loss: 0.2332  | train_logloss: 0.22367 | train_accuracy: 0.9133  | valid_logloss: 0.31857 | valid_accuracy: 0.89063 |  0:01:43s\n",
            "epoch 63 | loss: 0.23411 | train_logloss: 0.22364 | train_accuracy: 0.91067 | valid_logloss: 0.30946 | valid_accuracy: 0.89127 |  0:01:45s\n",
            "epoch 64 | loss: 0.238   | train_logloss: 0.22348 | train_accuracy: 0.91117 | valid_logloss: 0.31683 | valid_accuracy: 0.88853 |  0:01:46s\n",
            "epoch 65 | loss: 0.23534 | train_logloss: 0.228   | train_accuracy: 0.9105  | valid_logloss: 0.31744 | valid_accuracy: 0.8887  |  0:01:48s\n",
            "epoch 66 | loss: 0.23711 | train_logloss: 0.22069 | train_accuracy: 0.91253 | valid_logloss: 0.30627 | valid_accuracy: 0.89187 |  0:01:49s\n",
            "epoch 67 | loss: 0.23319 | train_logloss: 0.22268 | train_accuracy: 0.91117 | valid_logloss: 0.31411 | valid_accuracy: 0.8905  |  0:01:51s\n",
            "epoch 68 | loss: 0.23191 | train_logloss: 0.21853 | train_accuracy: 0.9141  | valid_logloss: 0.31164 | valid_accuracy: 0.8893  |  0:01:52s\n",
            "epoch 69 | loss: 0.23042 | train_logloss: 0.21363 | train_accuracy: 0.91447 | valid_logloss: 0.31137 | valid_accuracy: 0.891   |  0:01:54s\n",
            "epoch 70 | loss: 0.22882 | train_logloss: 0.21618 | train_accuracy: 0.91367 | valid_logloss: 0.31632 | valid_accuracy: 0.88883 |  0:01:55s\n",
            "epoch 71 | loss: 0.2233  | train_logloss: 0.21142 | train_accuracy: 0.91587 | valid_logloss: 0.32107 | valid_accuracy: 0.8904  |  0:01:57s\n",
            "epoch 72 | loss: 0.22758 | train_logloss: 0.2192  | train_accuracy: 0.91417 | valid_logloss: 0.31569 | valid_accuracy: 0.88727 |  0:01:58s\n",
            "epoch 73 | loss: 0.23087 | train_logloss: 0.21684 | train_accuracy: 0.91237 | valid_logloss: 0.31926 | valid_accuracy: 0.8883  |  0:02:00s\n",
            "epoch 74 | loss: 0.22484 | train_logloss: 0.2055  | train_accuracy: 0.91817 | valid_logloss: 0.31241 | valid_accuracy: 0.8923  |  0:02:01s\n",
            "epoch 75 | loss: 0.22223 | train_logloss: 0.21643 | train_accuracy: 0.91393 | valid_logloss: 0.31864 | valid_accuracy: 0.89073 |  0:02:02s\n",
            "epoch 76 | loss: 0.21979 | train_logloss: 0.2079  | train_accuracy: 0.91747 | valid_logloss: 0.32905 | valid_accuracy: 0.8891  |  0:02:04s\n",
            "epoch 77 | loss: 0.22384 | train_logloss: 0.2123  | train_accuracy: 0.9143  | valid_logloss: 0.33187 | valid_accuracy: 0.88557 |  0:02:05s\n",
            "epoch 78 | loss: 0.22349 | train_logloss: 0.21522 | train_accuracy: 0.91423 | valid_logloss: 0.32448 | valid_accuracy: 0.88837 |  0:02:07s\n",
            "epoch 79 | loss: 0.22886 | train_logloss: 0.21819 | train_accuracy: 0.91437 | valid_logloss: 0.31961 | valid_accuracy: 0.88913 |  0:02:08s\n",
            "epoch 80 | loss: 0.22519 | train_logloss: 0.20985 | train_accuracy: 0.91563 | valid_logloss: 0.3245  | valid_accuracy: 0.8905  |  0:02:10s\n",
            "epoch 81 | loss: 0.2175  | train_logloss: 0.20254 | train_accuracy: 0.9202  | valid_logloss: 0.32864 | valid_accuracy: 0.89027 |  0:02:11s\n",
            "epoch 82 | loss: 0.21537 | train_logloss: 0.20684 | train_accuracy: 0.9179  | valid_logloss: 0.33416 | valid_accuracy: 0.88527 |  0:02:13s\n",
            "epoch 83 | loss: 0.21131 | train_logloss: 0.1983  | train_accuracy: 0.92037 | valid_logloss: 0.33141 | valid_accuracy: 0.88913 |  0:02:14s\n",
            "epoch 84 | loss: 0.21523 | train_logloss: 0.20074 | train_accuracy: 0.91953 | valid_logloss: 0.32987 | valid_accuracy: 0.89027 |  0:02:16s\n",
            "epoch 85 | loss: 0.21012 | train_logloss: 0.19345 | train_accuracy: 0.92397 | valid_logloss: 0.33412 | valid_accuracy: 0.88713 |  0:02:17s\n",
            "epoch 86 | loss: 0.21073 | train_logloss: 0.20084 | train_accuracy: 0.92003 | valid_logloss: 0.3421  | valid_accuracy: 0.88423 |  0:02:19s\n",
            "epoch 87 | loss: 0.2128  | train_logloss: 0.19826 | train_accuracy: 0.92093 | valid_logloss: 0.33535 | valid_accuracy: 0.888   |  0:02:20s\n",
            "epoch 88 | loss: 0.212   | train_logloss: 0.21402 | train_accuracy: 0.91387 | valid_logloss: 0.3464  | valid_accuracy: 0.88597 |  0:02:22s\n",
            "epoch 89 | loss: 0.21115 | train_logloss: 0.20069 | train_accuracy: 0.91997 | valid_logloss: 0.33834 | valid_accuracy: 0.88657 |  0:02:23s\n",
            "epoch 90 | loss: 0.20698 | train_logloss: 0.19017 | train_accuracy: 0.92457 | valid_logloss: 0.34375 | valid_accuracy: 0.8875  |  0:02:25s\n",
            "epoch 91 | loss: 0.20605 | train_logloss: 0.19094 | train_accuracy: 0.9234  | valid_logloss: 0.34153 | valid_accuracy: 0.88613 |  0:02:26s\n",
            "epoch 92 | loss: 0.20664 | train_logloss: 0.19037 | train_accuracy: 0.92283 | valid_logloss: 0.336   | valid_accuracy: 0.88907 |  0:02:28s\n",
            "epoch 93 | loss: 0.20459 | train_logloss: 0.19482 | train_accuracy: 0.9224  | valid_logloss: 0.3491  | valid_accuracy: 0.8855  |  0:02:29s\n",
            "epoch 94 | loss: 0.20525 | train_logloss: 0.19032 | train_accuracy: 0.9235  | valid_logloss: 0.35517 | valid_accuracy: 0.8838  |  0:02:31s\n",
            "epoch 95 | loss: 0.20186 | train_logloss: 0.19056 | train_accuracy: 0.92453 | valid_logloss: 0.34896 | valid_accuracy: 0.8863  |  0:02:32s\n",
            "epoch 96 | loss: 0.20005 | train_logloss: 0.19149 | train_accuracy: 0.92213 | valid_logloss: 0.35078 | valid_accuracy: 0.8843  |  0:02:34s\n",
            "epoch 97 | loss: 0.20041 | train_logloss: 0.19384 | train_accuracy: 0.92383 | valid_logloss: 0.35476 | valid_accuracy: 0.88387 |  0:02:35s\n",
            "epoch 98 | loss: 0.21803 | train_logloss: 0.20454 | train_accuracy: 0.9192  | valid_logloss: 0.3369  | valid_accuracy: 0.88573 |  0:02:37s\n",
            "epoch 99 | loss: 0.21213 | train_logloss: 0.19357 | train_accuracy: 0.92327 | valid_logloss: 0.33846 | valid_accuracy: 0.88827 |  0:02:38s\n",
            "epoch 100| loss: 0.20318 | train_logloss: 0.18808 | train_accuracy: 0.9255  | valid_logloss: 0.3457  | valid_accuracy: 0.88627 |  0:02:40s\n",
            "epoch 101| loss: 0.21092 | train_logloss: 0.19344 | train_accuracy: 0.9223  | valid_logloss: 0.33589 | valid_accuracy: 0.8869  |  0:02:41s\n",
            "epoch 102| loss: 0.20221 | train_logloss: 0.1844  | train_accuracy: 0.9271  | valid_logloss: 0.34939 | valid_accuracy: 0.8845  |  0:02:43s\n",
            "epoch 103| loss: 0.19594 | train_logloss: 0.17713 | train_accuracy: 0.92877 | valid_logloss: 0.35972 | valid_accuracy: 0.8858  |  0:02:44s\n",
            "epoch 104| loss: 0.19272 | train_logloss: 0.17574 | train_accuracy: 0.92913 | valid_logloss: 0.3654  | valid_accuracy: 0.8827  |  0:02:46s\n",
            "\n",
            "Early stopping occurred at epoch 104 with best_epoch = 54 and best_valid_accuracy = 0.8923\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mQDonDa0Ksa",
        "outputId": "a472ea6d-9aba-4a35-a27f-f421bc3385c3"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb7.pkl' 'tn7.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    254.0 MiB    254.0 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.6 MiB      5.7 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2332.1 MiB   2072.5 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFy9iLyKIlVO"
      },
      "source": [
        "ones(number_exp=8, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YzTgGm7JB0s"
      },
      "source": [
        "ones(number_exp=9, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=7,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weawcLGoJaYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483cd3ed-f9cd-41ce-8cbe-1de7c5e91050"
      },
      "source": [
        "memory(number_exp=10, \n",
        "     Rows=9000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.04583 | train_logloss: 11.84129| train_accuracy: 0.354   | valid_logloss: 11.79447| valid_accuracy: 0.35944 |  0:00:01s\n",
            "epoch 1  | loss: 0.61206 | train_logloss: 8.09429 | train_accuracy: 0.61311 | valid_logloss: 7.91257 | valid_accuracy: 0.62033 |  0:00:03s\n",
            "epoch 2  | loss: 0.52071 | train_logloss: 20.06188| train_accuracy: 0.35133 | valid_logloss: 20.21294| valid_accuracy: 0.35156 |  0:00:05s\n",
            "epoch 3  | loss: 0.46929 | train_logloss: 8.4903  | train_accuracy: 0.22744 | valid_logloss: 8.40832 | valid_accuracy: 0.22922 |  0:00:07s\n",
            "epoch 4  | loss: 0.41851 | train_logloss: 1.92663 | train_accuracy: 0.55256 | valid_logloss: 1.9921  | valid_accuracy: 0.54578 |  0:00:09s\n",
            "epoch 5  | loss: 0.41071 | train_logloss: 1.43395 | train_accuracy: 0.52244 | valid_logloss: 1.43481 | valid_accuracy: 0.53011 |  0:00:11s\n",
            "epoch 6  | loss: 0.40321 | train_logloss: 1.31418 | train_accuracy: 0.51211 | valid_logloss: 1.28267 | valid_accuracy: 0.52656 |  0:00:13s\n",
            "epoch 7  | loss: 0.39996 | train_logloss: 1.47851 | train_accuracy: 0.57622 | valid_logloss: 1.48036 | valid_accuracy: 0.57933 |  0:00:15s\n",
            "epoch 8  | loss: 0.40898 | train_logloss: 0.98344 | train_accuracy: 0.64356 | valid_logloss: 0.97977 | valid_accuracy: 0.63956 |  0:00:17s\n",
            "epoch 9  | loss: 0.3975  | train_logloss: 0.98865 | train_accuracy: 0.64433 | valid_logloss: 0.97478 | valid_accuracy: 0.64611 |  0:00:19s\n",
            "epoch 10 | loss: 0.39431 | train_logloss: 0.63677 | train_accuracy: 0.74133 | valid_logloss: 0.6478  | valid_accuracy: 0.73833 |  0:00:21s\n",
            "epoch 11 | loss: 0.38798 | train_logloss: 0.91954 | train_accuracy: 0.63056 | valid_logloss: 0.92723 | valid_accuracy: 0.62789 |  0:00:22s\n",
            "epoch 12 | loss: 0.372   | train_logloss: 0.62025 | train_accuracy: 0.73389 | valid_logloss: 0.64695 | valid_accuracy: 0.72878 |  0:00:24s\n",
            "epoch 13 | loss: 0.36427 | train_logloss: 0.50053 | train_accuracy: 0.79667 | valid_logloss: 0.52873 | valid_accuracy: 0.78767 |  0:00:26s\n",
            "epoch 14 | loss: 0.36237 | train_logloss: 0.56726 | train_accuracy: 0.78744 | valid_logloss: 0.59053 | valid_accuracy: 0.78078 |  0:00:28s\n",
            "epoch 15 | loss: 0.36229 | train_logloss: 0.44811 | train_accuracy: 0.83144 | valid_logloss: 0.47408 | valid_accuracy: 0.82278 |  0:00:30s\n",
            "epoch 16 | loss: 0.36508 | train_logloss: 0.39371 | train_accuracy: 0.853   | valid_logloss: 0.42392 | valid_accuracy: 0.83978 |  0:00:32s\n",
            "epoch 17 | loss: 0.3555  | train_logloss: 0.38993 | train_accuracy: 0.851   | valid_logloss: 0.41895 | valid_accuracy: 0.843   |  0:00:34s\n",
            "epoch 18 | loss: 0.35264 | train_logloss: 0.36284 | train_accuracy: 0.86311 | valid_logloss: 0.3895  | valid_accuracy: 0.85233 |  0:00:37s\n",
            "epoch 19 | loss: 0.36107 | train_logloss: 0.34254 | train_accuracy: 0.86933 | valid_logloss: 0.37496 | valid_accuracy: 0.857   |  0:00:39s\n",
            "epoch 20 | loss: 0.36331 | train_logloss: 0.36227 | train_accuracy: 0.85978 | valid_logloss: 0.39719 | valid_accuracy: 0.85111 |  0:00:41s\n",
            "epoch 21 | loss: 0.36393 | train_logloss: 0.39241 | train_accuracy: 0.84911 | valid_logloss: 0.429   | valid_accuracy: 0.84389 |  0:00:43s\n",
            "epoch 22 | loss: 0.35307 | train_logloss: 0.36009 | train_accuracy: 0.86044 | valid_logloss: 0.39132 | valid_accuracy: 0.85378 |  0:00:45s\n",
            "epoch 23 | loss: 0.34196 | train_logloss: 0.33231 | train_accuracy: 0.87467 | valid_logloss: 0.36496 | valid_accuracy: 0.86733 |  0:00:47s\n",
            "epoch 24 | loss: 0.33722 | train_logloss: 0.3236  | train_accuracy: 0.87678 | valid_logloss: 0.36074 | valid_accuracy: 0.86989 |  0:00:49s\n",
            "epoch 25 | loss: 0.32656 | train_logloss: 0.31357 | train_accuracy: 0.877   | valid_logloss: 0.35068 | valid_accuracy: 0.87056 |  0:00:51s\n",
            "epoch 26 | loss: 0.32546 | train_logloss: 0.32161 | train_accuracy: 0.87689 | valid_logloss: 0.36071 | valid_accuracy: 0.86967 |  0:00:53s\n",
            "epoch 27 | loss: 0.32891 | train_logloss: 0.3132  | train_accuracy: 0.87833 | valid_logloss: 0.35176 | valid_accuracy: 0.86822 |  0:00:55s\n",
            "epoch 28 | loss: 0.32553 | train_logloss: 0.31257 | train_accuracy: 0.87844 | valid_logloss: 0.3569  | valid_accuracy: 0.86733 |  0:00:57s\n",
            "epoch 29 | loss: 0.32456 | train_logloss: 0.31595 | train_accuracy: 0.87911 | valid_logloss: 0.35905 | valid_accuracy: 0.86733 |  0:00:59s\n",
            "epoch 30 | loss: 0.32618 | train_logloss: 0.30924 | train_accuracy: 0.87922 | valid_logloss: 0.35473 | valid_accuracy: 0.86778 |  0:01:01s\n",
            "epoch 31 | loss: 0.31934 | train_logloss: 0.31365 | train_accuracy: 0.87844 | valid_logloss: 0.35525 | valid_accuracy: 0.86967 |  0:01:03s\n",
            "epoch 32 | loss: 0.32174 | train_logloss: 0.31301 | train_accuracy: 0.88133 | valid_logloss: 0.35463 | valid_accuracy: 0.86789 |  0:01:05s\n",
            "epoch 33 | loss: 0.3151  | train_logloss: 0.31321 | train_accuracy: 0.87922 | valid_logloss: 0.36038 | valid_accuracy: 0.86933 |  0:01:07s\n",
            "epoch 34 | loss: 0.31774 | train_logloss: 0.30344 | train_accuracy: 0.88256 | valid_logloss: 0.34917 | valid_accuracy: 0.87178 |  0:01:09s\n",
            "epoch 35 | loss: 0.31183 | train_logloss: 0.29325 | train_accuracy: 0.88533 | valid_logloss: 0.34891 | valid_accuracy: 0.87056 |  0:01:11s\n",
            "epoch 36 | loss: 0.31406 | train_logloss: 0.3     | train_accuracy: 0.88033 | valid_logloss: 0.34982 | valid_accuracy: 0.87111 |  0:01:13s\n",
            "epoch 37 | loss: 0.31161 | train_logloss: 0.30615 | train_accuracy: 0.87811 | valid_logloss: 0.3573  | valid_accuracy: 0.86544 |  0:01:15s\n",
            "epoch 38 | loss: 0.30173 | train_logloss: 0.29461 | train_accuracy: 0.88022 | valid_logloss: 0.35389 | valid_accuracy: 0.86822 |  0:01:17s\n",
            "epoch 39 | loss: 0.30822 | train_logloss: 0.29926 | train_accuracy: 0.886   | valid_logloss: 0.35045 | valid_accuracy: 0.87422 |  0:01:19s\n",
            "epoch 40 | loss: 0.30894 | train_logloss: 0.29511 | train_accuracy: 0.88411 | valid_logloss: 0.3453  | valid_accuracy: 0.87222 |  0:01:21s\n",
            "epoch 41 | loss: 0.3166  | train_logloss: 0.2906  | train_accuracy: 0.88689 | valid_logloss: 0.33706 | valid_accuracy: 0.87578 |  0:01:23s\n",
            "epoch 42 | loss: 0.31188 | train_logloss: 0.28577 | train_accuracy: 0.88556 | valid_logloss: 0.3363  | valid_accuracy: 0.87589 |  0:01:25s\n",
            "epoch 43 | loss: 0.30563 | train_logloss: 0.29005 | train_accuracy: 0.88722 | valid_logloss: 0.3456  | valid_accuracy: 0.87422 |  0:01:27s\n",
            "epoch 44 | loss: 0.305   | train_logloss: 0.29736 | train_accuracy: 0.88522 | valid_logloss: 0.35129 | valid_accuracy: 0.87444 |  0:01:29s\n",
            "epoch 45 | loss: 0.30928 | train_logloss: 0.29258 | train_accuracy: 0.88789 | valid_logloss: 0.34715 | valid_accuracy: 0.87422 |  0:01:31s\n",
            "epoch 46 | loss: 0.3009  | train_logloss: 0.28914 | train_accuracy: 0.88756 | valid_logloss: 0.34172 | valid_accuracy: 0.87444 |  0:01:33s\n",
            "epoch 47 | loss: 0.31248 | train_logloss: 0.3048  | train_accuracy: 0.87978 | valid_logloss: 0.34667 | valid_accuracy: 0.87267 |  0:01:35s\n",
            "epoch 48 | loss: 0.31687 | train_logloss: 0.29751 | train_accuracy: 0.88522 | valid_logloss: 0.34183 | valid_accuracy: 0.87667 |  0:01:37s\n",
            "epoch 49 | loss: 0.31895 | train_logloss: 0.30424 | train_accuracy: 0.88567 | valid_logloss: 0.35005 | valid_accuracy: 0.87644 |  0:01:39s\n",
            "epoch 50 | loss: 0.31699 | train_logloss: 0.30241 | train_accuracy: 0.88367 | valid_logloss: 0.34736 | valid_accuracy: 0.875   |  0:01:41s\n",
            "epoch 51 | loss: 0.30978 | train_logloss: 0.29737 | train_accuracy: 0.887   | valid_logloss: 0.34197 | valid_accuracy: 0.88111 |  0:01:43s\n",
            "epoch 52 | loss: 0.29914 | train_logloss: 0.28576 | train_accuracy: 0.887   | valid_logloss: 0.33582 | valid_accuracy: 0.87956 |  0:01:45s\n",
            "epoch 53 | loss: 0.30161 | train_logloss: 0.28188 | train_accuracy: 0.88956 | valid_logloss: 0.33304 | valid_accuracy: 0.87744 |  0:01:47s\n",
            "epoch 54 | loss: 0.29153 | train_logloss: 0.28149 | train_accuracy: 0.888   | valid_logloss: 0.34981 | valid_accuracy: 0.87622 |  0:01:49s\n",
            "epoch 55 | loss: 0.28735 | train_logloss: 0.28768 | train_accuracy: 0.88822 | valid_logloss: 0.33971 | valid_accuracy: 0.87822 |  0:01:51s\n",
            "epoch 56 | loss: 0.30547 | train_logloss: 0.31377 | train_accuracy: 0.87733 | valid_logloss: 0.36815 | valid_accuracy: 0.867   |  0:01:53s\n",
            "epoch 57 | loss: 0.32209 | train_logloss: 0.29942 | train_accuracy: 0.88689 | valid_logloss: 0.34779 | valid_accuracy: 0.87622 |  0:01:55s\n",
            "epoch 58 | loss: 0.31295 | train_logloss: 0.29109 | train_accuracy: 0.887   | valid_logloss: 0.34116 | valid_accuracy: 0.878   |  0:01:57s\n",
            "epoch 59 | loss: 0.3123  | train_logloss: 0.31084 | train_accuracy: 0.88    | valid_logloss: 0.35884 | valid_accuracy: 0.87    |  0:01:59s\n",
            "epoch 60 | loss: 0.30589 | train_logloss: 0.2953  | train_accuracy: 0.88933 | valid_logloss: 0.3526  | valid_accuracy: 0.87489 |  0:02:01s\n",
            "epoch 61 | loss: 0.30539 | train_logloss: 0.29593 | train_accuracy: 0.88667 | valid_logloss: 0.34411 | valid_accuracy: 0.87911 |  0:02:03s\n",
            "epoch 62 | loss: 0.30813 | train_logloss: 0.29116 | train_accuracy: 0.88689 | valid_logloss: 0.34127 | valid_accuracy: 0.87344 |  0:02:04s\n",
            "epoch 63 | loss: 0.30151 | train_logloss: 0.28024 | train_accuracy: 0.894   | valid_logloss: 0.34282 | valid_accuracy: 0.87256 |  0:02:06s\n",
            "epoch 64 | loss: 0.29391 | train_logloss: 0.28435 | train_accuracy: 0.89344 | valid_logloss: 0.34465 | valid_accuracy: 0.87867 |  0:02:08s\n",
            "epoch 65 | loss: 0.29133 | train_logloss: 0.27433 | train_accuracy: 0.89133 | valid_logloss: 0.34109 | valid_accuracy: 0.87744 |  0:02:10s\n",
            "epoch 66 | loss: 0.29248 | train_logloss: 0.28419 | train_accuracy: 0.89044 | valid_logloss: 0.34152 | valid_accuracy: 0.87644 |  0:02:12s\n",
            "epoch 67 | loss: 0.30378 | train_logloss: 0.27152 | train_accuracy: 0.89744 | valid_logloss: 0.33502 | valid_accuracy: 0.87744 |  0:02:14s\n",
            "epoch 68 | loss: 0.29001 | train_logloss: 0.28401 | train_accuracy: 0.88878 | valid_logloss: 0.33691 | valid_accuracy: 0.87667 |  0:02:16s\n",
            "epoch 69 | loss: 0.30214 | train_logloss: 0.28237 | train_accuracy: 0.893   | valid_logloss: 0.33563 | valid_accuracy: 0.88067 |  0:02:18s\n",
            "epoch 70 | loss: 0.29559 | train_logloss: 0.28766 | train_accuracy: 0.88978 | valid_logloss: 0.33814 | valid_accuracy: 0.87844 |  0:02:19s\n",
            "epoch 71 | loss: 0.29486 | train_logloss: 0.27996 | train_accuracy: 0.89267 | valid_logloss: 0.33691 | valid_accuracy: 0.88022 |  0:02:21s\n",
            "epoch 72 | loss: 0.29686 | train_logloss: 0.28444 | train_accuracy: 0.89144 | valid_logloss: 0.3439  | valid_accuracy: 0.88111 |  0:02:23s\n",
            "epoch 73 | loss: 0.29667 | train_logloss: 0.28432 | train_accuracy: 0.89078 | valid_logloss: 0.33367 | valid_accuracy: 0.88033 |  0:02:25s\n",
            "epoch 74 | loss: 0.29816 | train_logloss: 0.28671 | train_accuracy: 0.89344 | valid_logloss: 0.33212 | valid_accuracy: 0.88011 |  0:02:27s\n",
            "epoch 75 | loss: 0.28517 | train_logloss: 0.28279 | train_accuracy: 0.89211 | valid_logloss: 0.34299 | valid_accuracy: 0.87689 |  0:02:29s\n",
            "epoch 76 | loss: 0.29627 | train_logloss: 0.30279 | train_accuracy: 0.88678 | valid_logloss: 0.35479 | valid_accuracy: 0.87078 |  0:02:31s\n",
            "epoch 77 | loss: 0.30051 | train_logloss: 0.27891 | train_accuracy: 0.89367 | valid_logloss: 0.3298  | valid_accuracy: 0.88089 |  0:02:33s\n",
            "epoch 78 | loss: 0.29409 | train_logloss: 0.28235 | train_accuracy: 0.89111 | valid_logloss: 0.34331 | valid_accuracy: 0.87878 |  0:02:34s\n",
            "epoch 79 | loss: 0.29114 | train_logloss: 0.27611 | train_accuracy: 0.89367 | valid_logloss: 0.33935 | valid_accuracy: 0.87778 |  0:02:36s\n",
            "epoch 80 | loss: 0.28235 | train_logloss: 0.27857 | train_accuracy: 0.89444 | valid_logloss: 0.33218 | valid_accuracy: 0.88189 |  0:02:38s\n",
            "epoch 81 | loss: 0.29212 | train_logloss: 0.27559 | train_accuracy: 0.89567 | valid_logloss: 0.32543 | valid_accuracy: 0.88122 |  0:02:40s\n",
            "epoch 82 | loss: 0.2869  | train_logloss: 0.27514 | train_accuracy: 0.89289 | valid_logloss: 0.33018 | valid_accuracy: 0.88256 |  0:02:42s\n",
            "epoch 83 | loss: 0.30314 | train_logloss: 0.30337 | train_accuracy: 0.88378 | valid_logloss: 0.3519  | valid_accuracy: 0.87256 |  0:02:44s\n",
            "epoch 84 | loss: 0.31862 | train_logloss: 0.31835 | train_accuracy: 0.88244 | valid_logloss: 0.37024 | valid_accuracy: 0.86878 |  0:02:46s\n",
            "epoch 85 | loss: 0.32648 | train_logloss: 0.33073 | train_accuracy: 0.87811 | valid_logloss: 0.36936 | valid_accuracy: 0.86389 |  0:02:48s\n",
            "epoch 86 | loss: 0.30808 | train_logloss: 0.28556 | train_accuracy: 0.88911 | valid_logloss: 0.33835 | valid_accuracy: 0.87844 |  0:02:50s\n",
            "epoch 87 | loss: 0.3006  | train_logloss: 0.28685 | train_accuracy: 0.89289 | valid_logloss: 0.33998 | valid_accuracy: 0.87733 |  0:02:51s\n",
            "epoch 88 | loss: 0.29333 | train_logloss: 0.28205 | train_accuracy: 0.89367 | valid_logloss: 0.33313 | valid_accuracy: 0.883   |  0:02:53s\n",
            "epoch 89 | loss: 0.29516 | train_logloss: 0.28179 | train_accuracy: 0.891   | valid_logloss: 0.33127 | valid_accuracy: 0.87822 |  0:02:55s\n",
            "epoch 90 | loss: 0.29318 | train_logloss: 0.28324 | train_accuracy: 0.894   | valid_logloss: 0.34019 | valid_accuracy: 0.88256 |  0:02:57s\n",
            "epoch 91 | loss: 0.28459 | train_logloss: 0.27575 | train_accuracy: 0.89422 | valid_logloss: 0.3335  | valid_accuracy: 0.88044 |  0:02:59s\n",
            "epoch 92 | loss: 0.28386 | train_logloss: 0.26321 | train_accuracy: 0.898   | valid_logloss: 0.33195 | valid_accuracy: 0.88133 |  0:03:01s\n",
            "epoch 93 | loss: 0.29131 | train_logloss: 0.2772  | train_accuracy: 0.89389 | valid_logloss: 0.32916 | valid_accuracy: 0.88033 |  0:03:03s\n",
            "epoch 94 | loss: 0.28853 | train_logloss: 0.27393 | train_accuracy: 0.89822 | valid_logloss: 0.33653 | valid_accuracy: 0.87844 |  0:03:04s\n",
            "epoch 95 | loss: 0.27955 | train_logloss: 0.27655 | train_accuracy: 0.89467 | valid_logloss: 0.34995 | valid_accuracy: 0.87744 |  0:03:06s\n",
            "epoch 96 | loss: 0.28521 | train_logloss: 0.27151 | train_accuracy: 0.89467 | valid_logloss: 0.34204 | valid_accuracy: 0.88322 |  0:03:08s\n",
            "epoch 97 | loss: 0.284   | train_logloss: 0.26164 | train_accuracy: 0.89878 | valid_logloss: 0.33414 | valid_accuracy: 0.882   |  0:03:10s\n",
            "epoch 98 | loss: 0.28391 | train_logloss: 0.27207 | train_accuracy: 0.89644 | valid_logloss: 0.34125 | valid_accuracy: 0.882   |  0:03:12s\n",
            "epoch 99 | loss: 0.27974 | train_logloss: 0.28001 | train_accuracy: 0.89444 | valid_logloss: 0.3419  | valid_accuracy: 0.87978 |  0:03:14s\n",
            "epoch 100| loss: 0.27955 | train_logloss: 0.25986 | train_accuracy: 0.89967 | valid_logloss: 0.33039 | valid_accuracy: 0.88222 |  0:03:16s\n",
            "epoch 101| loss: 0.28688 | train_logloss: 0.28066 | train_accuracy: 0.89289 | valid_logloss: 0.34935 | valid_accuracy: 0.87556 |  0:03:18s\n",
            "epoch 102| loss: 0.29271 | train_logloss: 0.28329 | train_accuracy: 0.88867 | valid_logloss: 0.33729 | valid_accuracy: 0.87644 |  0:03:20s\n",
            "epoch 103| loss: 0.28457 | train_logloss: 0.26907 | train_accuracy: 0.89622 | valid_logloss: 0.32813 | valid_accuracy: 0.88644 |  0:03:22s\n",
            "epoch 104| loss: 0.28505 | train_logloss: 0.26933 | train_accuracy: 0.89444 | valid_logloss: 0.33263 | valid_accuracy: 0.88367 |  0:03:23s\n",
            "epoch 105| loss: 0.28174 | train_logloss: 0.26248 | train_accuracy: 0.897   | valid_logloss: 0.34026 | valid_accuracy: 0.87978 |  0:03:25s\n",
            "epoch 106| loss: 0.27335 | train_logloss: 0.26777 | train_accuracy: 0.89422 | valid_logloss: 0.34763 | valid_accuracy: 0.87911 |  0:03:27s\n",
            "epoch 107| loss: 0.27255 | train_logloss: 0.26114 | train_accuracy: 0.89944 | valid_logloss: 0.33569 | valid_accuracy: 0.88267 |  0:03:29s\n",
            "epoch 108| loss: 0.2705  | train_logloss: 0.26947 | train_accuracy: 0.90022 | valid_logloss: 0.35037 | valid_accuracy: 0.88144 |  0:03:31s\n",
            "epoch 109| loss: 0.26956 | train_logloss: 0.25215 | train_accuracy: 0.90289 | valid_logloss: 0.33183 | valid_accuracy: 0.886   |  0:03:33s\n",
            "epoch 110| loss: 0.26734 | train_logloss: 0.25267 | train_accuracy: 0.90322 | valid_logloss: 0.32737 | valid_accuracy: 0.88511 |  0:03:35s\n",
            "epoch 111| loss: 0.26771 | train_logloss: 0.24955 | train_accuracy: 0.90356 | valid_logloss: 0.33858 | valid_accuracy: 0.88289 |  0:03:37s\n",
            "epoch 112| loss: 0.2688  | train_logloss: 0.25822 | train_accuracy: 0.90089 | valid_logloss: 0.34265 | valid_accuracy: 0.88044 |  0:03:38s\n",
            "epoch 113| loss: 0.27534 | train_logloss: 0.26633 | train_accuracy: 0.89767 | valid_logloss: 0.34334 | valid_accuracy: 0.883   |  0:03:40s\n",
            "epoch 114| loss: 0.27351 | train_logloss: 0.2644  | train_accuracy: 0.89933 | valid_logloss: 0.33465 | valid_accuracy: 0.88144 |  0:03:42s\n",
            "epoch 115| loss: 0.26735 | train_logloss: 0.24537 | train_accuracy: 0.905   | valid_logloss: 0.32395 | valid_accuracy: 0.88611 |  0:03:44s\n",
            "epoch 116| loss: 0.26285 | train_logloss: 0.25443 | train_accuracy: 0.89978 | valid_logloss: 0.33432 | valid_accuracy: 0.884   |  0:03:46s\n",
            "epoch 117| loss: 0.26907 | train_logloss: 0.25134 | train_accuracy: 0.90444 | valid_logloss: 0.33676 | valid_accuracy: 0.88267 |  0:03:48s\n",
            "epoch 118| loss: 0.26441 | train_logloss: 0.2428  | train_accuracy: 0.908   | valid_logloss: 0.33447 | valid_accuracy: 0.88467 |  0:03:50s\n",
            "epoch 119| loss: 0.25948 | train_logloss: 0.24019 | train_accuracy: 0.90756 | valid_logloss: 0.32966 | valid_accuracy: 0.88256 |  0:03:52s\n",
            "epoch 120| loss: 0.25961 | train_logloss: 0.23962 | train_accuracy: 0.90578 | valid_logloss: 0.3311  | valid_accuracy: 0.88489 |  0:03:53s\n",
            "epoch 121| loss: 0.26105 | train_logloss: 0.25395 | train_accuracy: 0.90411 | valid_logloss: 0.34879 | valid_accuracy: 0.88078 |  0:03:55s\n",
            "epoch 122| loss: 0.27014 | train_logloss: 0.25432 | train_accuracy: 0.90322 | valid_logloss: 0.34525 | valid_accuracy: 0.88256 |  0:03:57s\n",
            "epoch 123| loss: 0.26347 | train_logloss: 0.24508 | train_accuracy: 0.90689 | valid_logloss: 0.33624 | valid_accuracy: 0.88222 |  0:03:59s\n",
            "epoch 124| loss: 0.26257 | train_logloss: 0.24241 | train_accuracy: 0.90533 | valid_logloss: 0.33986 | valid_accuracy: 0.883   |  0:04:01s\n",
            "epoch 125| loss: 0.25643 | train_logloss: 0.23501 | train_accuracy: 0.90744 | valid_logloss: 0.33605 | valid_accuracy: 0.88111 |  0:04:03s\n",
            "epoch 126| loss: 0.25646 | train_logloss: 0.24082 | train_accuracy: 0.90689 | valid_logloss: 0.33861 | valid_accuracy: 0.88356 |  0:04:05s\n",
            "epoch 127| loss: 0.25602 | train_logloss: 0.24345 | train_accuracy: 0.90644 | valid_logloss: 0.34043 | valid_accuracy: 0.88122 |  0:04:07s\n",
            "epoch 128| loss: 0.25964 | train_logloss: 0.25109 | train_accuracy: 0.90622 | valid_logloss: 0.35544 | valid_accuracy: 0.881   |  0:04:09s\n",
            "epoch 129| loss: 0.25402 | train_logloss: 0.2426  | train_accuracy: 0.90433 | valid_logloss: 0.35536 | valid_accuracy: 0.87856 |  0:04:11s\n",
            "epoch 130| loss: 0.25075 | train_logloss: 0.24075 | train_accuracy: 0.90767 | valid_logloss: 0.3424  | valid_accuracy: 0.88056 |  0:04:13s\n",
            "epoch 131| loss: 0.2495  | train_logloss: 0.23724 | train_accuracy: 0.90689 | valid_logloss: 0.34599 | valid_accuracy: 0.885   |  0:04:15s\n",
            "epoch 132| loss: 0.25845 | train_logloss: 0.23544 | train_accuracy: 0.90867 | valid_logloss: 0.34175 | valid_accuracy: 0.87978 |  0:04:17s\n",
            "epoch 133| loss: 0.25481 | train_logloss: 0.24521 | train_accuracy: 0.90311 | valid_logloss: 0.35676 | valid_accuracy: 0.88167 |  0:04:19s\n",
            "epoch 134| loss: 0.25068 | train_logloss: 0.24251 | train_accuracy: 0.90678 | valid_logloss: 0.35733 | valid_accuracy: 0.88011 |  0:04:21s\n",
            "epoch 135| loss: 0.26222 | train_logloss: 0.2419  | train_accuracy: 0.90689 | valid_logloss: 0.35108 | valid_accuracy: 0.87733 |  0:04:23s\n",
            "epoch 136| loss: 0.25261 | train_logloss: 0.2274  | train_accuracy: 0.91278 | valid_logloss: 0.34505 | valid_accuracy: 0.88489 |  0:04:25s\n",
            "epoch 137| loss: 0.24834 | train_logloss: 0.24453 | train_accuracy: 0.90511 | valid_logloss: 0.35112 | valid_accuracy: 0.88133 |  0:04:27s\n",
            "epoch 138| loss: 0.24984 | train_logloss: 0.25387 | train_accuracy: 0.89789 | valid_logloss: 0.35961 | valid_accuracy: 0.87656 |  0:04:29s\n",
            "epoch 139| loss: 0.2591  | train_logloss: 0.23466 | train_accuracy: 0.91    | valid_logloss: 0.34212 | valid_accuracy: 0.88522 |  0:04:31s\n",
            "epoch 140| loss: 0.25594 | train_logloss: 0.23933 | train_accuracy: 0.90756 | valid_logloss: 0.34009 | valid_accuracy: 0.88178 |  0:04:33s\n",
            "epoch 141| loss: 0.25499 | train_logloss: 0.2342  | train_accuracy: 0.90922 | valid_logloss: 0.35274 | valid_accuracy: 0.88211 |  0:04:35s\n",
            "epoch 142| loss: 0.25713 | train_logloss: 0.23157 | train_accuracy: 0.90744 | valid_logloss: 0.33433 | valid_accuracy: 0.88156 |  0:04:37s\n",
            "epoch 143| loss: 0.24526 | train_logloss: 0.23555 | train_accuracy: 0.91078 | valid_logloss: 0.35055 | valid_accuracy: 0.88211 |  0:04:39s\n",
            "epoch 144| loss: 0.25204 | train_logloss: 0.23181 | train_accuracy: 0.90878 | valid_logloss: 0.35111 | valid_accuracy: 0.87911 |  0:04:41s\n",
            "epoch 145| loss: 0.24588 | train_logloss: 0.21977 | train_accuracy: 0.912   | valid_logloss: 0.34798 | valid_accuracy: 0.88211 |  0:04:43s\n",
            "epoch 146| loss: 0.24258 | train_logloss: 0.23671 | train_accuracy: 0.90711 | valid_logloss: 0.34667 | valid_accuracy: 0.88289 |  0:04:45s\n",
            "epoch 147| loss: 0.25519 | train_logloss: 0.23663 | train_accuracy: 0.90833 | valid_logloss: 0.34883 | valid_accuracy: 0.88244 |  0:04:47s\n",
            "epoch 148| loss: 0.24743 | train_logloss: 0.22495 | train_accuracy: 0.91289 | valid_logloss: 0.34854 | valid_accuracy: 0.88233 |  0:04:49s\n",
            "epoch 149| loss: 0.23847 | train_logloss: 0.21895 | train_accuracy: 0.91456 | valid_logloss: 0.35002 | valid_accuracy: 0.88433 |  0:04:51s\n",
            "epoch 150| loss: 0.2383  | train_logloss: 0.22346 | train_accuracy: 0.91033 | valid_logloss: 0.36177 | valid_accuracy: 0.87778 |  0:04:53s\n",
            "epoch 151| loss: 0.236   | train_logloss: 0.22308 | train_accuracy: 0.91322 | valid_logloss: 0.35694 | valid_accuracy: 0.88111 |  0:04:55s\n",
            "epoch 152| loss: 0.23722 | train_logloss: 0.22578 | train_accuracy: 0.912   | valid_logloss: 0.37041 | valid_accuracy: 0.88244 |  0:04:57s\n",
            "epoch 153| loss: 0.24339 | train_logloss: 0.23504 | train_accuracy: 0.90967 | valid_logloss: 0.34664 | valid_accuracy: 0.87944 |  0:04:59s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 103 and best_valid_accuracy = 0.88644\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORW5YG_knt2j",
        "outputId": "d121ee93-d861-4188-f1aa-1c0b9717d81e"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb10.pkl' 'tn10.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    255.4 MiB    255.4 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.8 MiB      4.4 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2319.0 MiB   2059.3 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhZ__ntF_hOu"
      },
      "source": [
        "  number_exp=11\n",
        "  Rows=30000\n",
        "  Nd=64\n",
        "  Na=64\t\n",
        "  B=512\n",
        "  BV=128\n",
        "  mB=0.7\t\n",
        "  λsparse=0.001\n",
        "  Nsteps=5\n",
        "  γ=1.5 \n",
        "  learning_rate=0.02\n",
        "  decay_rate=0.95\n",
        "  decay_iterations=200\t\n",
        "  shared=2\n",
        "  decision=2\n",
        "  mask_type='entmax'\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('number: '+str(11)+', ')\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  #X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  #X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  #X_valid      = np.concatenate((X1_test[2*count : 3*count], X2_test[2*count : 3*count], X3_test[2*count : 3*count]))\n",
        "  #y_valid      = np.concatenate((y1_test[2*count : 3*count], y2_test[2*count : 3*count], y3_test[2*count : 3*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  #X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=20,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'colsample_bytree': 0.6437405148446416,\n",
        "    'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 100,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_test_norm, y_test)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #Accuracy\n",
        "  acc, err = bootstrap_accuracy(tn, X_test, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc TN: '+str(acc)+'+-'+str(err)+', ')\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(tn, 'TN', Rows)\n",
        "  feature_acc(gb, 'GB', Rows)\n",
        "\n",
        "  #save model\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.txt')\n",
        "  tn.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdmZ-Md1uTFA"
      },
      "source": [
        "ones(number_exp=12, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='sparsemax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldqGo0H1-Gq"
      },
      "source": [
        "ones(number_exp=13, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeKGjsIi6NLC",
        "outputId": "abe30477-390f-46c7-a3ac-cb43fd4f0389"
      },
      "source": [
        "memory(number_exp=14, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.66566 | train_logloss: 14.13323| train_accuracy: 0.25928 | valid_logloss: 14.09257| valid_accuracy: 0.26009 |  0:00:11s\n",
            "epoch 1  | loss: 0.36818 | train_logloss: 8.37912 | train_accuracy: 0.3296  | valid_logloss: 8.38603 | valid_accuracy: 0.3297  |  0:00:22s\n",
            "epoch 2  | loss: 0.33743 | train_logloss: 2.09891 | train_accuracy: 0.33258 | valid_logloss: 2.09696 | valid_accuracy: 0.33267 |  0:00:34s\n",
            "epoch 3  | loss: 0.32329 | train_logloss: 1.56248 | train_accuracy: 0.33715 | valid_logloss: 1.5629  | valid_accuracy: 0.33734 |  0:00:45s\n",
            "epoch 4  | loss: 0.31404 | train_logloss: 3.23258 | train_accuracy: 0.31048 | valid_logloss: 3.2338  | valid_accuracy: 0.31048 |  0:00:57s\n",
            "epoch 5  | loss: 0.31225 | train_logloss: 2.10267 | train_accuracy: 0.38407 | valid_logloss: 2.10322 | valid_accuracy: 0.38382 |  0:01:08s\n",
            "epoch 6  | loss: 0.30942 | train_logloss: 1.47126 | train_accuracy: 0.34909 | valid_logloss: 1.4706  | valid_accuracy: 0.34812 |  0:01:20s\n",
            "epoch 7  | loss: 0.30211 | train_logloss: 1.22033 | train_accuracy: 0.41751 | valid_logloss: 1.22159 | valid_accuracy: 0.41664 |  0:01:31s\n",
            "epoch 8  | loss: 0.2998  | train_logloss: 1.08499 | train_accuracy: 0.46878 | valid_logloss: 1.08626 | valid_accuracy: 0.46655 |  0:01:43s\n",
            "epoch 9  | loss: 0.29796 | train_logloss: 1.05283 | train_accuracy: 0.52471 | valid_logloss: 1.05274 | valid_accuracy: 0.52406 |  0:01:54s\n",
            "epoch 10 | loss: 0.29522 | train_logloss: 0.88857 | train_accuracy: 0.58912 | valid_logloss: 0.88921 | valid_accuracy: 0.58893 |  0:02:05s\n",
            "epoch 11 | loss: 0.29085 | train_logloss: 0.70195 | train_accuracy: 0.69263 | valid_logloss: 0.70209 | valid_accuracy: 0.69258 |  0:02:17s\n",
            "epoch 12 | loss: 0.28768 | train_logloss: 0.59451 | train_accuracy: 0.76096 | valid_logloss: 0.59428 | valid_accuracy: 0.76161 |  0:02:28s\n",
            "epoch 13 | loss: 0.28968 | train_logloss: 0.49959 | train_accuracy: 0.80687 | valid_logloss: 0.50039 | valid_accuracy: 0.80681 |  0:02:40s\n",
            "epoch 14 | loss: 0.2869  | train_logloss: 0.42977 | train_accuracy: 0.84025 | valid_logloss: 0.43068 | valid_accuracy: 0.83958 |  0:02:51s\n",
            "epoch 15 | loss: 0.28329 | train_logloss: 0.38883 | train_accuracy: 0.85345 | valid_logloss: 0.39009 | valid_accuracy: 0.85306 |  0:03:03s\n",
            "epoch 16 | loss: 0.28058 | train_logloss: 0.33939 | train_accuracy: 0.87467 | valid_logloss: 0.34185 | valid_accuracy: 0.87454 |  0:03:14s\n",
            "epoch 17 | loss: 0.27929 | train_logloss: 0.32776 | train_accuracy: 0.87709 | valid_logloss: 0.3305  | valid_accuracy: 0.87641 |  0:03:26s\n",
            "epoch 18 | loss: 0.2793  | train_logloss: 0.30942 | train_accuracy: 0.8855  | valid_logloss: 0.31225 | valid_accuracy: 0.88522 |  0:03:37s\n",
            "epoch 19 | loss: 0.27865 | train_logloss: 0.2969  | train_accuracy: 0.88825 | valid_logloss: 0.30169 | valid_accuracy: 0.88756 |  0:03:48s\n",
            "epoch 20 | loss: 0.27624 | train_logloss: 0.28523 | train_accuracy: 0.89088 | valid_logloss: 0.28996 | valid_accuracy: 0.89002 |  0:04:00s\n",
            "epoch 21 | loss: 0.27554 | train_logloss: 0.28031 | train_accuracy: 0.8945  | valid_logloss: 0.28505 | valid_accuracy: 0.89358 |  0:04:11s\n",
            "epoch 22 | loss: 0.27511 | train_logloss: 0.28056 | train_accuracy: 0.89312 | valid_logloss: 0.28555 | valid_accuracy: 0.89186 |  0:04:22s\n",
            "epoch 23 | loss: 0.27669 | train_logloss: 0.27827 | train_accuracy: 0.89474 | valid_logloss: 0.28374 | valid_accuracy: 0.89384 |  0:04:34s\n",
            "epoch 24 | loss: 0.27651 | train_logloss: 0.27391 | train_accuracy: 0.89593 | valid_logloss: 0.27989 | valid_accuracy: 0.89454 |  0:04:45s\n",
            "epoch 25 | loss: 0.27597 | train_logloss: 0.27251 | train_accuracy: 0.89625 | valid_logloss: 0.27838 | valid_accuracy: 0.895   |  0:04:57s\n",
            "epoch 26 | loss: 0.27287 | train_logloss: 0.2673  | train_accuracy: 0.89859 | valid_logloss: 0.27425 | valid_accuracy: 0.89643 |  0:05:08s\n",
            "epoch 27 | loss: 0.27168 | train_logloss: 0.26818 | train_accuracy: 0.89822 | valid_logloss: 0.27493 | valid_accuracy: 0.89635 |  0:05:19s\n",
            "epoch 28 | loss: 0.27077 | train_logloss: 0.26704 | train_accuracy: 0.89876 | valid_logloss: 0.27445 | valid_accuracy: 0.89716 |  0:05:31s\n",
            "epoch 29 | loss: 0.27043 | train_logloss: 0.26679 | train_accuracy: 0.89859 | valid_logloss: 0.27285 | valid_accuracy: 0.89676 |  0:05:42s\n",
            "epoch 30 | loss: 0.27014 | train_logloss: 0.27427 | train_accuracy: 0.89506 | valid_logloss: 0.28082 | valid_accuracy: 0.8938  |  0:05:54s\n",
            "epoch 31 | loss: 0.27055 | train_logloss: 0.26624 | train_accuracy: 0.89829 | valid_logloss: 0.27346 | valid_accuracy: 0.89646 |  0:06:05s\n",
            "epoch 32 | loss: 0.26992 | train_logloss: 0.26485 | train_accuracy: 0.89891 | valid_logloss: 0.27246 | valid_accuracy: 0.89743 |  0:06:16s\n",
            "epoch 33 | loss: 0.26744 | train_logloss: 0.26478 | train_accuracy: 0.8992  | valid_logloss: 0.27229 | valid_accuracy: 0.89752 |  0:06:28s\n",
            "epoch 34 | loss: 0.26733 | train_logloss: 0.26383 | train_accuracy: 0.89953 | valid_logloss: 0.27106 | valid_accuracy: 0.89761 |  0:06:39s\n",
            "epoch 35 | loss: 0.26726 | train_logloss: 0.26247 | train_accuracy: 0.89928 | valid_logloss: 0.27058 | valid_accuracy: 0.89765 |  0:06:50s\n",
            "epoch 36 | loss: 0.26813 | train_logloss: 0.2639  | train_accuracy: 0.90003 | valid_logloss: 0.27056 | valid_accuracy: 0.89845 |  0:07:02s\n",
            "epoch 37 | loss: 0.26647 | train_logloss: 0.26172 | train_accuracy: 0.8994  | valid_logloss: 0.26946 | valid_accuracy: 0.89769 |  0:07:13s\n",
            "epoch 38 | loss: 0.26538 | train_logloss: 0.2645  | train_accuracy: 0.89894 | valid_logloss: 0.27262 | valid_accuracy: 0.89682 |  0:07:24s\n",
            "epoch 39 | loss: 0.26572 | train_logloss: 0.26109 | train_accuracy: 0.9005  | valid_logloss: 0.26919 | valid_accuracy: 0.89836 |  0:07:36s\n",
            "epoch 40 | loss: 0.26511 | train_logloss: 0.25939 | train_accuracy: 0.90065 | valid_logloss: 0.26782 | valid_accuracy: 0.89857 |  0:07:47s\n",
            "epoch 41 | loss: 0.26527 | train_logloss: 0.26251 | train_accuracy: 0.89953 | valid_logloss: 0.27027 | valid_accuracy: 0.89798 |  0:07:58s\n",
            "epoch 42 | loss: 0.26714 | train_logloss: 0.2625  | train_accuracy: 0.89985 | valid_logloss: 0.26996 | valid_accuracy: 0.8983  |  0:08:10s\n",
            "epoch 43 | loss: 0.2646  | train_logloss: 0.25885 | train_accuracy: 0.90108 | valid_logloss: 0.26706 | valid_accuracy: 0.89902 |  0:08:21s\n",
            "epoch 44 | loss: 0.26369 | train_logloss: 0.26049 | train_accuracy: 0.90093 | valid_logloss: 0.26912 | valid_accuracy: 0.89866 |  0:08:32s\n",
            "epoch 45 | loss: 0.26507 | train_logloss: 0.25996 | train_accuracy: 0.90071 | valid_logloss: 0.26769 | valid_accuracy: 0.8986  |  0:08:43s\n",
            "epoch 46 | loss: 0.26383 | train_logloss: 0.26037 | train_accuracy: 0.90066 | valid_logloss: 0.26919 | valid_accuracy: 0.89904 |  0:08:55s\n",
            "epoch 47 | loss: 0.26211 | train_logloss: 0.25739 | train_accuracy: 0.90132 | valid_logloss: 0.26644 | valid_accuracy: 0.89949 |  0:09:06s\n",
            "epoch 48 | loss: 0.26254 | train_logloss: 0.26186 | train_accuracy: 0.89979 | valid_logloss: 0.26971 | valid_accuracy: 0.89821 |  0:09:17s\n",
            "epoch 49 | loss: 0.26686 | train_logloss: 0.26045 | train_accuracy: 0.90064 | valid_logloss: 0.2686  | valid_accuracy: 0.89941 |  0:09:29s\n",
            "epoch 50 | loss: 0.26423 | train_logloss: 0.26047 | train_accuracy: 0.90065 | valid_logloss: 0.26865 | valid_accuracy: 0.89882 |  0:09:40s\n",
            "epoch 51 | loss: 0.26331 | train_logloss: 0.26172 | train_accuracy: 0.89962 | valid_logloss: 0.27016 | valid_accuracy: 0.89762 |  0:09:51s\n",
            "epoch 52 | loss: 0.26286 | train_logloss: 0.25828 | train_accuracy: 0.90082 | valid_logloss: 0.26672 | valid_accuracy: 0.89901 |  0:10:03s\n",
            "epoch 53 | loss: 0.26181 | train_logloss: 0.25587 | train_accuracy: 0.90218 | valid_logloss: 0.26509 | valid_accuracy: 0.89994 |  0:10:14s\n",
            "epoch 54 | loss: 0.2621  | train_logloss: 0.25897 | train_accuracy: 0.90006 | valid_logloss: 0.26878 | valid_accuracy: 0.89819 |  0:10:25s\n",
            "epoch 55 | loss: 0.26162 | train_logloss: 0.2599  | train_accuracy: 0.9009  | valid_logloss: 0.26962 | valid_accuracy: 0.89869 |  0:10:36s\n",
            "epoch 56 | loss: 0.26153 | train_logloss: 0.25649 | train_accuracy: 0.90141 | valid_logloss: 0.26635 | valid_accuracy: 0.89923 |  0:10:48s\n",
            "epoch 57 | loss: 0.26037 | train_logloss: 0.25846 | train_accuracy: 0.90132 | valid_logloss: 0.26778 | valid_accuracy: 0.89898 |  0:10:59s\n",
            "epoch 58 | loss: 0.26086 | train_logloss: 0.25756 | train_accuracy: 0.90112 | valid_logloss: 0.26664 | valid_accuracy: 0.89922 |  0:11:10s\n",
            "epoch 59 | loss: 0.2609  | train_logloss: 0.25586 | train_accuracy: 0.90232 | valid_logloss: 0.26558 | valid_accuracy: 0.90019 |  0:11:22s\n",
            "epoch 60 | loss: 0.26057 | train_logloss: 0.25647 | train_accuracy: 0.90162 | valid_logloss: 0.26647 | valid_accuracy: 0.89919 |  0:11:33s\n",
            "epoch 61 | loss: 0.25985 | train_logloss: 0.25818 | train_accuracy: 0.90157 | valid_logloss: 0.26832 | valid_accuracy: 0.8993  |  0:11:44s\n",
            "epoch 62 | loss: 0.25905 | train_logloss: 0.25693 | train_accuracy: 0.90146 | valid_logloss: 0.26802 | valid_accuracy: 0.8989  |  0:11:55s\n",
            "epoch 63 | loss: 0.25927 | train_logloss: 0.25466 | train_accuracy: 0.9025  | valid_logloss: 0.26558 | valid_accuracy: 0.89942 |  0:12:07s\n",
            "epoch 64 | loss: 0.25934 | train_logloss: 0.25803 | train_accuracy: 0.90114 | valid_logloss: 0.26753 | valid_accuracy: 0.89926 |  0:12:18s\n",
            "epoch 65 | loss: 0.26013 | train_logloss: 0.25655 | train_accuracy: 0.90162 | valid_logloss: 0.26678 | valid_accuracy: 0.89914 |  0:12:29s\n",
            "epoch 66 | loss: 0.2596  | train_logloss: 0.2544  | train_accuracy: 0.90276 | valid_logloss: 0.26422 | valid_accuracy: 0.90034 |  0:12:41s\n",
            "epoch 67 | loss: 0.25984 | train_logloss: 0.258   | train_accuracy: 0.90066 | valid_logloss: 0.26811 | valid_accuracy: 0.89802 |  0:12:52s\n",
            "epoch 68 | loss: 0.26044 | train_logloss: 0.25585 | train_accuracy: 0.90279 | valid_logloss: 0.26513 | valid_accuracy: 0.90016 |  0:13:03s\n",
            "epoch 69 | loss: 0.25826 | train_logloss: 0.25275 | train_accuracy: 0.90311 | valid_logloss: 0.26344 | valid_accuracy: 0.9004  |  0:13:15s\n",
            "epoch 70 | loss: 0.25767 | train_logloss: 0.25293 | train_accuracy: 0.90321 | valid_logloss: 0.26386 | valid_accuracy: 0.90001 |  0:13:26s\n",
            "epoch 71 | loss: 0.25782 | train_logloss: 0.25362 | train_accuracy: 0.90288 | valid_logloss: 0.26475 | valid_accuracy: 0.90018 |  0:13:37s\n",
            "epoch 72 | loss: 0.25879 | train_logloss: 0.25545 | train_accuracy: 0.90253 | valid_logloss: 0.26713 | valid_accuracy: 0.89943 |  0:13:48s\n",
            "epoch 73 | loss: 0.25862 | train_logloss: 0.25424 | train_accuracy: 0.90253 | valid_logloss: 0.26491 | valid_accuracy: 0.89935 |  0:13:59s\n",
            "epoch 74 | loss: 0.2581  | train_logloss: 0.25312 | train_accuracy: 0.90314 | valid_logloss: 0.26393 | valid_accuracy: 0.90036 |  0:14:10s\n",
            "epoch 75 | loss: 0.25755 | train_logloss: 0.25326 | train_accuracy: 0.90315 | valid_logloss: 0.26419 | valid_accuracy: 0.90024 |  0:14:21s\n",
            "epoch 76 | loss: 0.25727 | train_logloss: 0.25184 | train_accuracy: 0.90364 | valid_logloss: 0.2643  | valid_accuracy: 0.90052 |  0:14:32s\n",
            "epoch 77 | loss: 0.25696 | train_logloss: 0.25152 | train_accuracy: 0.90344 | valid_logloss: 0.26387 | valid_accuracy: 0.89993 |  0:14:43s\n",
            "epoch 78 | loss: 0.25562 | train_logloss: 0.25424 | train_accuracy: 0.90202 | valid_logloss: 0.26686 | valid_accuracy: 0.89931 |  0:14:54s\n",
            "epoch 79 | loss: 0.25546 | train_logloss: 0.25228 | train_accuracy: 0.90373 | valid_logloss: 0.26436 | valid_accuracy: 0.90031 |  0:15:05s\n",
            "epoch 80 | loss: 0.25612 | train_logloss: 0.25286 | train_accuracy: 0.90305 | valid_logloss: 0.26597 | valid_accuracy: 0.89972 |  0:15:16s\n",
            "epoch 81 | loss: 0.25607 | train_logloss: 0.25056 | train_accuracy: 0.9041  | valid_logloss: 0.26317 | valid_accuracy: 0.90089 |  0:15:27s\n",
            "epoch 82 | loss: 0.25593 | train_logloss: 0.25314 | train_accuracy: 0.90293 | valid_logloss: 0.26524 | valid_accuracy: 0.9002  |  0:15:38s\n",
            "epoch 83 | loss: 0.25552 | train_logloss: 0.25469 | train_accuracy: 0.90257 | valid_logloss: 0.26643 | valid_accuracy: 0.89947 |  0:15:49s\n",
            "epoch 84 | loss: 0.25623 | train_logloss: 0.25314 | train_accuracy: 0.90305 | valid_logloss: 0.26657 | valid_accuracy: 0.89939 |  0:16:00s\n",
            "epoch 85 | loss: 0.25554 | train_logloss: 0.25304 | train_accuracy: 0.90292 | valid_logloss: 0.26491 | valid_accuracy: 0.90019 |  0:16:12s\n",
            "epoch 86 | loss: 0.2555  | train_logloss: 0.25135 | train_accuracy: 0.90394 | valid_logloss: 0.26311 | valid_accuracy: 0.90126 |  0:16:23s\n",
            "epoch 87 | loss: 0.2567  | train_logloss: 0.25361 | train_accuracy: 0.9035  | valid_logloss: 0.26433 | valid_accuracy: 0.90043 |  0:16:35s\n",
            "epoch 88 | loss: 0.2566  | train_logloss: 0.25368 | train_accuracy: 0.90231 | valid_logloss: 0.26552 | valid_accuracy: 0.89972 |  0:16:46s\n",
            "epoch 89 | loss: 0.25696 | train_logloss: 0.25594 | train_accuracy: 0.90191 | valid_logloss: 0.26772 | valid_accuracy: 0.89906 |  0:16:58s\n",
            "epoch 90 | loss: 0.27703 | train_logloss: 0.27336 | train_accuracy: 0.89726 | valid_logloss: 0.27959 | valid_accuracy: 0.89565 |  0:17:09s\n",
            "epoch 91 | loss: 0.27068 | train_logloss: 0.26674 | train_accuracy: 0.89891 | valid_logloss: 0.27437 | valid_accuracy: 0.8967  |  0:17:21s\n",
            "epoch 92 | loss: 0.26573 | train_logloss: 0.25971 | train_accuracy: 0.90117 | valid_logloss: 0.26796 | valid_accuracy: 0.89968 |  0:17:33s\n",
            "epoch 93 | loss: 0.26383 | train_logloss: 0.26012 | train_accuracy: 0.90059 | valid_logloss: 0.26913 | valid_accuracy: 0.89862 |  0:17:44s\n",
            "epoch 94 | loss: 0.26279 | train_logloss: 0.25818 | train_accuracy: 0.90131 | valid_logloss: 0.26643 | valid_accuracy: 0.89963 |  0:17:56s\n",
            "epoch 95 | loss: 0.26038 | train_logloss: 0.256   | train_accuracy: 0.90236 | valid_logloss: 0.26547 | valid_accuracy: 0.89957 |  0:18:08s\n",
            "epoch 96 | loss: 0.2589  | train_logloss: 0.25381 | train_accuracy: 0.9028  | valid_logloss: 0.26405 | valid_accuracy: 0.90025 |  0:18:20s\n",
            "epoch 97 | loss: 0.25806 | train_logloss: 0.25508 | train_accuracy: 0.90263 | valid_logloss: 0.26524 | valid_accuracy: 0.90025 |  0:18:32s\n",
            "epoch 98 | loss: 0.25966 | train_logloss: 0.25551 | train_accuracy: 0.90238 | valid_logloss: 0.2658  | valid_accuracy: 0.90017 |  0:18:45s\n",
            "epoch 99 | loss: 0.25884 | train_logloss: 0.25461 | train_accuracy: 0.90274 | valid_logloss: 0.26507 | valid_accuracy: 0.89992 |  0:18:57s\n",
            "epoch 100| loss: 0.25716 | train_logloss: 0.25274 | train_accuracy: 0.9027  | valid_logloss: 0.26405 | valid_accuracy: 0.90004 |  0:19:08s\n",
            "epoch 101| loss: 0.25644 | train_logloss: 0.25114 | train_accuracy: 0.90383 | valid_logloss: 0.26285 | valid_accuracy: 0.90108 |  0:19:19s\n",
            "epoch 102| loss: 0.25594 | train_logloss: 0.25317 | train_accuracy: 0.90322 | valid_logloss: 0.26493 | valid_accuracy: 0.89996 |  0:19:31s\n",
            "epoch 103| loss: 0.25676 | train_logloss: 0.25321 | train_accuracy: 0.90216 | valid_logloss: 0.26385 | valid_accuracy: 0.89989 |  0:19:42s\n",
            "epoch 104| loss: 0.2559  | train_logloss: 0.25276 | train_accuracy: 0.90338 | valid_logloss: 0.26468 | valid_accuracy: 0.90035 |  0:19:53s\n",
            "epoch 105| loss: 0.25575 | train_logloss: 0.25024 | train_accuracy: 0.90434 | valid_logloss: 0.26217 | valid_accuracy: 0.90129 |  0:20:04s\n",
            "epoch 106| loss: 0.25434 | train_logloss: 0.24991 | train_accuracy: 0.90403 | valid_logloss: 0.26285 | valid_accuracy: 0.90099 |  0:20:15s\n",
            "epoch 107| loss: 0.25405 | train_logloss: 0.25339 | train_accuracy: 0.90258 | valid_logloss: 0.2658  | valid_accuracy: 0.8993  |  0:20:27s\n",
            "epoch 108| loss: 0.25506 | train_logloss: 0.25137 | train_accuracy: 0.90371 | valid_logloss: 0.2632  | valid_accuracy: 0.90093 |  0:20:38s\n",
            "epoch 109| loss: 0.25468 | train_logloss: 0.25106 | train_accuracy: 0.90347 | valid_logloss: 0.26259 | valid_accuracy: 0.90079 |  0:20:49s\n",
            "epoch 110| loss: 0.2549  | train_logloss: 0.25114 | train_accuracy: 0.90361 | valid_logloss: 0.26393 | valid_accuracy: 0.90041 |  0:21:00s\n",
            "epoch 111| loss: 0.25413 | train_logloss: 0.25166 | train_accuracy: 0.90317 | valid_logloss: 0.26406 | valid_accuracy: 0.9003  |  0:21:11s\n",
            "epoch 112| loss: 0.25467 | train_logloss: 0.25078 | train_accuracy: 0.90344 | valid_logloss: 0.26432 | valid_accuracy: 0.90016 |  0:21:23s\n",
            "epoch 113| loss: 0.25549 | train_logloss: 0.25556 | train_accuracy: 0.90206 | valid_logloss: 0.2679  | valid_accuracy: 0.89929 |  0:21:35s\n",
            "epoch 114| loss: 0.25599 | train_logloss: 0.25127 | train_accuracy: 0.90385 | valid_logloss: 0.26347 | valid_accuracy: 0.90105 |  0:21:47s\n",
            "epoch 115| loss: 0.25742 | train_logloss: 0.25339 | train_accuracy: 0.90309 | valid_logloss: 0.26503 | valid_accuracy: 0.90035 |  0:21:58s\n",
            "epoch 116| loss: 0.257   | train_logloss: 0.25644 | train_accuracy: 0.90175 | valid_logloss: 0.26723 | valid_accuracy: 0.89934 |  0:22:09s\n",
            "epoch 117| loss: 0.25794 | train_logloss: 0.25348 | train_accuracy: 0.90243 | valid_logloss: 0.26401 | valid_accuracy: 0.89998 |  0:22:21s\n",
            "epoch 118| loss: 0.25599 | train_logloss: 0.25083 | train_accuracy: 0.90354 | valid_logloss: 0.26262 | valid_accuracy: 0.90094 |  0:22:33s\n",
            "epoch 119| loss: 0.25802 | train_logloss: 0.25362 | train_accuracy: 0.90305 | valid_logloss: 0.26457 | valid_accuracy: 0.9004  |  0:22:44s\n",
            "epoch 120| loss: 0.25667 | train_logloss: 0.25272 | train_accuracy: 0.90372 | valid_logloss: 0.26426 | valid_accuracy: 0.9007  |  0:22:56s\n",
            "epoch 121| loss: 0.25553 | train_logloss: 0.25379 | train_accuracy: 0.90167 | valid_logloss: 0.26567 | valid_accuracy: 0.89864 |  0:23:08s\n",
            "epoch 122| loss: 0.25519 | train_logloss: 0.25122 | train_accuracy: 0.90377 | valid_logloss: 0.26443 | valid_accuracy: 0.90065 |  0:23:20s\n",
            "epoch 123| loss: 0.25347 | train_logloss: 0.2494  | train_accuracy: 0.90429 | valid_logloss: 0.26298 | valid_accuracy: 0.90072 |  0:23:32s\n",
            "epoch 124| loss: 0.2535  | train_logloss: 0.25147 | train_accuracy: 0.90387 | valid_logloss: 0.26418 | valid_accuracy: 0.90055 |  0:23:44s\n",
            "epoch 125| loss: 0.25417 | train_logloss: 0.24925 | train_accuracy: 0.904   | valid_logloss: 0.26228 | valid_accuracy: 0.9009  |  0:23:56s\n",
            "epoch 126| loss: 0.25389 | train_logloss: 0.25192 | train_accuracy: 0.90344 | valid_logloss: 0.26564 | valid_accuracy: 0.90015 |  0:24:08s\n",
            "epoch 127| loss: 0.25379 | train_logloss: 0.25011 | train_accuracy: 0.90398 | valid_logloss: 0.2635  | valid_accuracy: 0.90066 |  0:24:19s\n",
            "epoch 128| loss: 0.25312 | train_logloss: 0.24935 | train_accuracy: 0.90414 | valid_logloss: 0.26333 | valid_accuracy: 0.90038 |  0:24:31s\n",
            "epoch 129| loss: 0.25358 | train_logloss: 0.25138 | train_accuracy: 0.90322 | valid_logloss: 0.26544 | valid_accuracy: 0.90002 |  0:24:42s\n",
            "epoch 130| loss: 0.25389 | train_logloss: 0.25061 | train_accuracy: 0.9038  | valid_logloss: 0.26381 | valid_accuracy: 0.90015 |  0:24:53s\n",
            "epoch 131| loss: 0.25263 | train_logloss: 0.24931 | train_accuracy: 0.90411 | valid_logloss: 0.26298 | valid_accuracy: 0.9007  |  0:25:04s\n",
            "epoch 132| loss: 0.25265 | train_logloss: 0.24895 | train_accuracy: 0.90481 | valid_logloss: 0.2635  | valid_accuracy: 0.901   |  0:25:15s\n",
            "epoch 133| loss: 0.25251 | train_logloss: 0.24987 | train_accuracy: 0.90334 | valid_logloss: 0.26444 | valid_accuracy: 0.90043 |  0:25:26s\n",
            "epoch 134| loss: 0.25262 | train_logloss: 0.25133 | train_accuracy: 0.90346 | valid_logloss: 0.2644  | valid_accuracy: 0.90007 |  0:25:38s\n",
            "epoch 135| loss: 0.25251 | train_logloss: 0.24967 | train_accuracy: 0.90411 | valid_logloss: 0.26392 | valid_accuracy: 0.90078 |  0:25:49s\n",
            "epoch 136| loss: 0.25666 | train_logloss: 0.25893 | train_accuracy: 0.90118 | valid_logloss: 0.27088 | valid_accuracy: 0.89846 |  0:26:00s\n",
            "epoch 137| loss: 0.26248 | train_logloss: 0.25791 | train_accuracy: 0.90059 | valid_logloss: 0.26846 | valid_accuracy: 0.89808 |  0:26:11s\n",
            "epoch 138| loss: 0.25861 | train_logloss: 0.25422 | train_accuracy: 0.90269 | valid_logloss: 0.26546 | valid_accuracy: 0.89989 |  0:26:22s\n",
            "epoch 139| loss: 0.25576 | train_logloss: 0.25205 | train_accuracy: 0.90319 | valid_logloss: 0.26397 | valid_accuracy: 0.90043 |  0:26:34s\n",
            "epoch 140| loss: 0.25703 | train_logloss: 0.25289 | train_accuracy: 0.90273 | valid_logloss: 0.26519 | valid_accuracy: 0.90058 |  0:26:46s\n",
            "epoch 141| loss: 0.25575 | train_logloss: 0.25239 | train_accuracy: 0.90307 | valid_logloss: 0.26494 | valid_accuracy: 0.90036 |  0:26:59s\n",
            "epoch 142| loss: 0.25456 | train_logloss: 0.25011 | train_accuracy: 0.90371 | valid_logloss: 0.26305 | valid_accuracy: 0.90055 |  0:27:11s\n",
            "epoch 143| loss: 0.25398 | train_logloss: 0.25209 | train_accuracy: 0.90286 | valid_logloss: 0.26477 | valid_accuracy: 0.89962 |  0:27:22s\n",
            "epoch 144| loss: 0.25292 | train_logloss: 0.24739 | train_accuracy: 0.90455 | valid_logloss: 0.26177 | valid_accuracy: 0.90126 |  0:27:34s\n",
            "epoch 145| loss: 0.25309 | train_logloss: 0.25141 | train_accuracy: 0.90397 | valid_logloss: 0.26475 | valid_accuracy: 0.9002  |  0:27:45s\n",
            "epoch 146| loss: 0.25476 | train_logloss: 0.25135 | train_accuracy: 0.90341 | valid_logloss: 0.26501 | valid_accuracy: 0.90038 |  0:27:56s\n",
            "epoch 147| loss: 0.2539  | train_logloss: 0.25145 | train_accuracy: 0.90313 | valid_logloss: 0.26452 | valid_accuracy: 0.89993 |  0:28:07s\n",
            "epoch 148| loss: 0.25198 | train_logloss: 0.24924 | train_accuracy: 0.90449 | valid_logloss: 0.26376 | valid_accuracy: 0.90091 |  0:28:18s\n",
            "epoch 149| loss: 0.25206 | train_logloss: 0.24796 | train_accuracy: 0.90456 | valid_logloss: 0.2626  | valid_accuracy: 0.90112 |  0:28:30s\n",
            "epoch 150| loss: 0.25148 | train_logloss: 0.24877 | train_accuracy: 0.90429 | valid_logloss: 0.26367 | valid_accuracy: 0.90109 |  0:28:41s\n",
            "epoch 151| loss: 0.25196 | train_logloss: 0.24653 | train_accuracy: 0.90487 | valid_logloss: 0.2617  | valid_accuracy: 0.90143 |  0:28:52s\n",
            "epoch 152| loss: 0.25141 | train_logloss: 0.24678 | train_accuracy: 0.90477 | valid_logloss: 0.26193 | valid_accuracy: 0.90132 |  0:29:03s\n",
            "epoch 153| loss: 0.25305 | train_logloss: 0.24925 | train_accuracy: 0.90392 | valid_logloss: 0.26426 | valid_accuracy: 0.90055 |  0:29:14s\n",
            "epoch 154| loss: 0.25281 | train_logloss: 0.24877 | train_accuracy: 0.90434 | valid_logloss: 0.26406 | valid_accuracy: 0.90048 |  0:29:25s\n",
            "epoch 155| loss: 0.25146 | train_logloss: 0.24606 | train_accuracy: 0.90514 | valid_logloss: 0.26165 | valid_accuracy: 0.90144 |  0:29:36s\n",
            "epoch 156| loss: 0.2509  | train_logloss: 0.24699 | train_accuracy: 0.90508 | valid_logloss: 0.2623  | valid_accuracy: 0.90167 |  0:29:47s\n",
            "epoch 157| loss: 0.25089 | train_logloss: 0.24948 | train_accuracy: 0.90431 | valid_logloss: 0.26432 | valid_accuracy: 0.90047 |  0:29:58s\n",
            "epoch 158| loss: 0.25544 | train_logloss: 0.25424 | train_accuracy: 0.90263 | valid_logloss: 0.26634 | valid_accuracy: 0.89928 |  0:30:09s\n",
            "epoch 159| loss: 0.25513 | train_logloss: 0.25163 | train_accuracy: 0.90376 | valid_logloss: 0.26394 | valid_accuracy: 0.90066 |  0:30:20s\n",
            "epoch 160| loss: 0.2539  | train_logloss: 0.2486  | train_accuracy: 0.90465 | valid_logloss: 0.26194 | valid_accuracy: 0.90135 |  0:30:32s\n",
            "epoch 161| loss: 0.25286 | train_logloss: 0.24771 | train_accuracy: 0.9051  | valid_logloss: 0.26149 | valid_accuracy: 0.90158 |  0:30:43s\n",
            "epoch 162| loss: 0.25267 | train_logloss: 0.24865 | train_accuracy: 0.90502 | valid_logloss: 0.2623  | valid_accuracy: 0.90109 |  0:30:54s\n",
            "epoch 163| loss: 0.25102 | train_logloss: 0.24682 | train_accuracy: 0.90546 | valid_logloss: 0.2615  | valid_accuracy: 0.90126 |  0:31:06s\n",
            "epoch 164| loss: 0.25037 | train_logloss: 0.24821 | train_accuracy: 0.9046  | valid_logloss: 0.26352 | valid_accuracy: 0.90061 |  0:31:17s\n",
            "epoch 165| loss: 0.25283 | train_logloss: 0.24738 | train_accuracy: 0.905   | valid_logloss: 0.26229 | valid_accuracy: 0.9015  |  0:31:28s\n",
            "epoch 166| loss: 0.2509  | train_logloss: 0.24509 | train_accuracy: 0.90542 | valid_logloss: 0.26108 | valid_accuracy: 0.90143 |  0:31:39s\n",
            "epoch 167| loss: 0.25034 | train_logloss: 0.24725 | train_accuracy: 0.90486 | valid_logloss: 0.26292 | valid_accuracy: 0.90122 |  0:31:51s\n",
            "epoch 168| loss: 0.24998 | train_logloss: 0.24874 | train_accuracy: 0.90471 | valid_logloss: 0.26362 | valid_accuracy: 0.9009  |  0:32:02s\n",
            "epoch 169| loss: 0.24898 | train_logloss: 0.24742 | train_accuracy: 0.90455 | valid_logloss: 0.26412 | valid_accuracy: 0.90054 |  0:32:13s\n",
            "epoch 170| loss: 0.24934 | train_logloss: 0.24789 | train_accuracy: 0.90525 | valid_logloss: 0.26428 | valid_accuracy: 0.90082 |  0:32:24s\n",
            "epoch 171| loss: 0.24865 | train_logloss: 0.24504 | train_accuracy: 0.90505 | valid_logloss: 0.26278 | valid_accuracy: 0.9008  |  0:32:36s\n",
            "epoch 172| loss: 0.24946 | train_logloss: 0.2449  | train_accuracy: 0.90599 | valid_logloss: 0.26169 | valid_accuracy: 0.90141 |  0:32:47s\n",
            "epoch 173| loss: 0.24873 | train_logloss: 0.24473 | train_accuracy: 0.9062  | valid_logloss: 0.26136 | valid_accuracy: 0.90171 |  0:32:58s\n",
            "epoch 174| loss: 0.24887 | train_logloss: 0.24457 | train_accuracy: 0.90614 | valid_logloss: 0.26358 | valid_accuracy: 0.90136 |  0:33:10s\n",
            "epoch 175| loss: 0.25    | train_logloss: 0.24649 | train_accuracy: 0.90566 | valid_logloss: 0.26385 | valid_accuracy: 0.90097 |  0:33:21s\n",
            "epoch 176| loss: 0.2489  | train_logloss: 0.24331 | train_accuracy: 0.9066  | valid_logloss: 0.26208 | valid_accuracy: 0.90177 |  0:33:32s\n",
            "epoch 177| loss: 0.24827 | train_logloss: 0.243   | train_accuracy: 0.90658 | valid_logloss: 0.26179 | valid_accuracy: 0.90157 |  0:33:44s\n",
            "epoch 178| loss: 0.24844 | train_logloss: 0.24527 | train_accuracy: 0.9055  | valid_logloss: 0.26372 | valid_accuracy: 0.90127 |  0:33:55s\n",
            "epoch 179| loss: 0.24775 | train_logloss: 0.24662 | train_accuracy: 0.90503 | valid_logloss: 0.26466 | valid_accuracy: 0.89989 |  0:34:06s\n",
            "epoch 180| loss: 0.24815 | train_logloss: 0.24364 | train_accuracy: 0.90648 | valid_logloss: 0.26351 | valid_accuracy: 0.90114 |  0:34:18s\n",
            "epoch 181| loss: 0.24803 | train_logloss: 0.24515 | train_accuracy: 0.90626 | valid_logloss: 0.26423 | valid_accuracy: 0.90146 |  0:34:29s\n",
            "epoch 182| loss: 0.24949 | train_logloss: 0.24487 | train_accuracy: 0.90611 | valid_logloss: 0.26449 | valid_accuracy: 0.90127 |  0:34:41s\n",
            "epoch 183| loss: 0.24858 | train_logloss: 0.24383 | train_accuracy: 0.90657 | valid_logloss: 0.26249 | valid_accuracy: 0.9018  |  0:34:52s\n",
            "epoch 184| loss: 0.24727 | train_logloss: 0.24237 | train_accuracy: 0.90714 | valid_logloss: 0.26254 | valid_accuracy: 0.90148 |  0:35:03s\n",
            "epoch 185| loss: 0.24714 | train_logloss: 0.24294 | train_accuracy: 0.90692 | valid_logloss: 0.26255 | valid_accuracy: 0.90151 |  0:35:15s\n",
            "epoch 186| loss: 0.24691 | train_logloss: 0.24146 | train_accuracy: 0.90734 | valid_logloss: 0.26171 | valid_accuracy: 0.90199 |  0:35:26s\n",
            "epoch 187| loss: 0.2477  | train_logloss: 0.24425 | train_accuracy: 0.90575 | valid_logloss: 0.2643  | valid_accuracy: 0.90103 |  0:35:38s\n",
            "epoch 188| loss: 0.24713 | train_logloss: 0.24148 | train_accuracy: 0.9068  | valid_logloss: 0.26285 | valid_accuracy: 0.90144 |  0:35:49s\n",
            "epoch 189| loss: 0.24731 | train_logloss: 0.24263 | train_accuracy: 0.90638 | valid_logloss: 0.26292 | valid_accuracy: 0.90142 |  0:36:01s\n",
            "epoch 190| loss: 0.24751 | train_logloss: 0.24048 | train_accuracy: 0.90727 | valid_logloss: 0.26091 | valid_accuracy: 0.90197 |  0:36:12s\n",
            "epoch 191| loss: 0.24657 | train_logloss: 0.24333 | train_accuracy: 0.90642 | valid_logloss: 0.26332 | valid_accuracy: 0.90068 |  0:36:23s\n",
            "epoch 192| loss: 0.24673 | train_logloss: 0.24182 | train_accuracy: 0.90695 | valid_logloss: 0.26299 | valid_accuracy: 0.90167 |  0:36:35s\n",
            "epoch 193| loss: 0.24709 | train_logloss: 0.24166 | train_accuracy: 0.90731 | valid_logloss: 0.26174 | valid_accuracy: 0.90157 |  0:36:46s\n",
            "epoch 194| loss: 0.24614 | train_logloss: 0.24382 | train_accuracy: 0.90578 | valid_logloss: 0.26529 | valid_accuracy: 0.90041 |  0:36:57s\n",
            "epoch 195| loss: 0.24669 | train_logloss: 0.24245 | train_accuracy: 0.90704 | valid_logloss: 0.26231 | valid_accuracy: 0.9018  |  0:37:09s\n",
            "epoch 196| loss: 0.24587 | train_logloss: 0.24059 | train_accuracy: 0.90708 | valid_logloss: 0.2627  | valid_accuracy: 0.90125 |  0:37:20s\n",
            "epoch 197| loss: 0.24602 | train_logloss: 0.2454  | train_accuracy: 0.90524 | valid_logloss: 0.26642 | valid_accuracy: 0.89995 |  0:37:31s\n",
            "epoch 198| loss: 0.24651 | train_logloss: 0.24191 | train_accuracy: 0.90688 | valid_logloss: 0.26322 | valid_accuracy: 0.90151 |  0:37:43s\n",
            "epoch 199| loss: 0.24753 | train_logloss: 0.24371 | train_accuracy: 0.90655 | valid_logloss: 0.26388 | valid_accuracy: 0.90122 |  0:37:54s\n",
            "epoch 200| loss: 0.24816 | train_logloss: 0.24283 | train_accuracy: 0.90664 | valid_logloss: 0.26379 | valid_accuracy: 0.90128 |  0:38:06s\n",
            "epoch 201| loss: 0.24649 | train_logloss: 0.24133 | train_accuracy: 0.90705 | valid_logloss: 0.26255 | valid_accuracy: 0.90173 |  0:38:17s\n",
            "epoch 202| loss: 0.24519 | train_logloss: 0.24329 | train_accuracy: 0.90589 | valid_logloss: 0.26443 | valid_accuracy: 0.90046 |  0:38:29s\n",
            "epoch 203| loss: 0.24519 | train_logloss: 0.24109 | train_accuracy: 0.90739 | valid_logloss: 0.26395 | valid_accuracy: 0.90131 |  0:38:40s\n",
            "epoch 204| loss: 0.2459  | train_logloss: 0.24188 | train_accuracy: 0.90716 | valid_logloss: 0.26393 | valid_accuracy: 0.90155 |  0:38:52s\n",
            "epoch 205| loss: 0.24737 | train_logloss: 0.24354 | train_accuracy: 0.90603 | valid_logloss: 0.26494 | valid_accuracy: 0.90048 |  0:39:03s\n",
            "epoch 206| loss: 0.24635 | train_logloss: 0.24166 | train_accuracy: 0.90719 | valid_logloss: 0.26403 | valid_accuracy: 0.90137 |  0:39:15s\n",
            "epoch 207| loss: 0.24606 | train_logloss: 0.23987 | train_accuracy: 0.90734 | valid_logloss: 0.26274 | valid_accuracy: 0.90116 |  0:39:26s\n",
            "epoch 208| loss: 0.24491 | train_logloss: 0.23854 | train_accuracy: 0.90799 | valid_logloss: 0.26169 | valid_accuracy: 0.90202 |  0:39:37s\n",
            "epoch 209| loss: 0.24534 | train_logloss: 0.24124 | train_accuracy: 0.90699 | valid_logloss: 0.26269 | valid_accuracy: 0.90116 |  0:39:49s\n",
            "epoch 210| loss: 0.24705 | train_logloss: 0.24248 | train_accuracy: 0.90658 | valid_logloss: 0.26477 | valid_accuracy: 0.90018 |  0:40:00s\n",
            "epoch 211| loss: 0.2454  | train_logloss: 0.24105 | train_accuracy: 0.90722 | valid_logloss: 0.26333 | valid_accuracy: 0.90149 |  0:40:12s\n",
            "epoch 212| loss: 0.24534 | train_logloss: 0.24175 | train_accuracy: 0.90689 | valid_logloss: 0.2638  | valid_accuracy: 0.90112 |  0:40:23s\n",
            "epoch 213| loss: 0.24627 | train_logloss: 0.24224 | train_accuracy: 0.9062  | valid_logloss: 0.26482 | valid_accuracy: 0.9002  |  0:40:35s\n",
            "epoch 214| loss: 0.24663 | train_logloss: 0.24356 | train_accuracy: 0.9065  | valid_logloss: 0.26573 | valid_accuracy: 0.90035 |  0:40:46s\n",
            "epoch 215| loss: 0.24629 | train_logloss: 0.24076 | train_accuracy: 0.90744 | valid_logloss: 0.26206 | valid_accuracy: 0.9018  |  0:40:57s\n",
            "epoch 216| loss: 0.24873 | train_logloss: 0.24456 | train_accuracy: 0.90666 | valid_logloss: 0.26376 | valid_accuracy: 0.90114 |  0:41:09s\n",
            "epoch 217| loss: 0.25825 | train_logloss: 0.26122 | train_accuracy: 0.90082 | valid_logloss: 0.27325 | valid_accuracy: 0.89796 |  0:41:20s\n",
            "epoch 218| loss: 0.26291 | train_logloss: 0.25637 | train_accuracy: 0.90194 | valid_logloss: 0.26856 | valid_accuracy: 0.89922 |  0:41:32s\n",
            "epoch 219| loss: 0.26005 | train_logloss: 0.25271 | train_accuracy: 0.90292 | valid_logloss: 0.26583 | valid_accuracy: 0.8996  |  0:41:43s\n",
            "epoch 220| loss: 0.25779 | train_logloss: 0.25475 | train_accuracy: 0.9027  | valid_logloss: 0.26782 | valid_accuracy: 0.89962 |  0:41:54s\n",
            "epoch 221| loss: 0.25583 | train_logloss: 0.2502  | train_accuracy: 0.90369 | valid_logloss: 0.26556 | valid_accuracy: 0.90007 |  0:42:06s\n",
            "epoch 222| loss: 0.25284 | train_logloss: 0.24668 | train_accuracy: 0.90504 | valid_logloss: 0.2625  | valid_accuracy: 0.901   |  0:42:18s\n",
            "epoch 223| loss: 0.25181 | train_logloss: 0.24652 | train_accuracy: 0.90547 | valid_logloss: 0.26412 | valid_accuracy: 0.90077 |  0:42:29s\n",
            "epoch 224| loss: 0.25099 | train_logloss: 0.24655 | train_accuracy: 0.90514 | valid_logloss: 0.26446 | valid_accuracy: 0.90106 |  0:42:41s\n",
            "epoch 225| loss: 0.25145 | train_logloss: 0.24948 | train_accuracy: 0.90418 | valid_logloss: 0.26646 | valid_accuracy: 0.90004 |  0:42:52s\n",
            "epoch 226| loss: 0.25076 | train_logloss: 0.24716 | train_accuracy: 0.90476 | valid_logloss: 0.26518 | valid_accuracy: 0.90023 |  0:43:04s\n",
            "epoch 227| loss: 0.25052 | train_logloss: 0.2458  | train_accuracy: 0.90557 | valid_logloss: 0.26432 | valid_accuracy: 0.9006  |  0:43:15s\n",
            "epoch 228| loss: 0.25146 | train_logloss: 0.24769 | train_accuracy: 0.90529 | valid_logloss: 0.26592 | valid_accuracy: 0.90109 |  0:43:26s\n",
            "epoch 229| loss: 0.25172 | train_logloss: 0.25014 | train_accuracy: 0.90395 | valid_logloss: 0.26697 | valid_accuracy: 0.89974 |  0:43:38s\n",
            "epoch 230| loss: 0.25152 | train_logloss: 0.24753 | train_accuracy: 0.90485 | valid_logloss: 0.26505 | valid_accuracy: 0.90028 |  0:43:49s\n",
            "epoch 231| loss: 0.24992 | train_logloss: 0.24422 | train_accuracy: 0.90636 | valid_logloss: 0.26285 | valid_accuracy: 0.90108 |  0:44:01s\n",
            "epoch 232| loss: 0.2491  | train_logloss: 0.24471 | train_accuracy: 0.906   | valid_logloss: 0.26343 | valid_accuracy: 0.90137 |  0:44:12s\n",
            "epoch 233| loss: 0.24813 | train_logloss: 0.24511 | train_accuracy: 0.90613 | valid_logloss: 0.26381 | valid_accuracy: 0.90099 |  0:44:24s\n",
            "epoch 234| loss: 0.2521  | train_logloss: 0.25234 | train_accuracy: 0.90331 | valid_logloss: 0.26792 | valid_accuracy: 0.89984 |  0:44:35s\n",
            "epoch 235| loss: 0.25618 | train_logloss: 0.24888 | train_accuracy: 0.90437 | valid_logloss: 0.2652  | valid_accuracy: 0.90044 |  0:44:47s\n",
            "epoch 236| loss: 0.25214 | train_logloss: 0.24733 | train_accuracy: 0.90501 | valid_logloss: 0.26419 | valid_accuracy: 0.90105 |  0:44:58s\n",
            "epoch 237| loss: 0.2515  | train_logloss: 0.24552 | train_accuracy: 0.90598 | valid_logloss: 0.26294 | valid_accuracy: 0.90129 |  0:45:10s\n",
            "epoch 238| loss: 0.24943 | train_logloss: 0.2451  | train_accuracy: 0.90574 | valid_logloss: 0.26449 | valid_accuracy: 0.90038 |  0:45:21s\n",
            "epoch 239| loss: 0.24908 | train_logloss: 0.24413 | train_accuracy: 0.90637 | valid_logloss: 0.26333 | valid_accuracy: 0.9017  |  0:45:33s\n",
            "epoch 240| loss: 0.24757 | train_logloss: 0.24346 | train_accuracy: 0.9062  | valid_logloss: 0.26373 | valid_accuracy: 0.90078 |  0:45:44s\n",
            "epoch 241| loss: 0.24722 | train_logloss: 0.24419 | train_accuracy: 0.90624 | valid_logloss: 0.26334 | valid_accuracy: 0.90111 |  0:45:56s\n",
            "epoch 242| loss: 0.24809 | train_logloss: 0.24366 | train_accuracy: 0.90634 | valid_logloss: 0.26369 | valid_accuracy: 0.90086 |  0:46:08s\n",
            "epoch 243| loss: 0.24774 | train_logloss: 0.24581 | train_accuracy: 0.9059  | valid_logloss: 0.26459 | valid_accuracy: 0.90053 |  0:46:20s\n",
            "epoch 244| loss: 0.24665 | train_logloss: 0.24454 | train_accuracy: 0.90517 | valid_logloss: 0.26581 | valid_accuracy: 0.89989 |  0:46:32s\n",
            "epoch 245| loss: 0.24652 | train_logloss: 0.2415  | train_accuracy: 0.90705 | valid_logloss: 0.26276 | valid_accuracy: 0.90117 |  0:46:44s\n",
            "epoch 246| loss: 0.24677 | train_logloss: 0.24184 | train_accuracy: 0.90733 | valid_logloss: 0.26344 | valid_accuracy: 0.90123 |  0:46:56s\n",
            "epoch 247| loss: 0.24698 | train_logloss: 0.24455 | train_accuracy: 0.90622 | valid_logloss: 0.26537 | valid_accuracy: 0.90081 |  0:47:08s\n",
            "epoch 248| loss: 0.24776 | train_logloss: 0.24249 | train_accuracy: 0.90654 | valid_logloss: 0.26427 | valid_accuracy: 0.90129 |  0:47:19s\n",
            "epoch 249| loss: 0.24711 | train_logloss: 0.24274 | train_accuracy: 0.90648 | valid_logloss: 0.26435 | valid_accuracy: 0.90069 |  0:47:31s\n",
            "epoch 250| loss: 0.24619 | train_logloss: 0.24102 | train_accuracy: 0.90715 | valid_logloss: 0.26307 | valid_accuracy: 0.90101 |  0:47:42s\n",
            "epoch 251| loss: 0.24619 | train_logloss: 0.24185 | train_accuracy: 0.90708 | valid_logloss: 0.26425 | valid_accuracy: 0.90119 |  0:47:54s\n",
            "epoch 252| loss: 0.24588 | train_logloss: 0.24105 | train_accuracy: 0.90679 | valid_logloss: 0.26459 | valid_accuracy: 0.90083 |  0:48:06s\n",
            "epoch 253| loss: 0.24616 | train_logloss: 0.24037 | train_accuracy: 0.9074  | valid_logloss: 0.26315 | valid_accuracy: 0.90126 |  0:48:17s\n",
            "epoch 254| loss: 0.24526 | train_logloss: 0.24154 | train_accuracy: 0.90716 | valid_logloss: 0.26419 | valid_accuracy: 0.90132 |  0:48:29s\n",
            "epoch 255| loss: 0.24616 | train_logloss: 0.24145 | train_accuracy: 0.90697 | valid_logloss: 0.26531 | valid_accuracy: 0.90063 |  0:48:41s\n",
            "epoch 256| loss: 0.24571 | train_logloss: 0.24154 | train_accuracy: 0.90714 | valid_logloss: 0.26481 | valid_accuracy: 0.90105 |  0:48:52s\n",
            "epoch 257| loss: 0.24535 | train_logloss: 0.24082 | train_accuracy: 0.90714 | valid_logloss: 0.26459 | valid_accuracy: 0.90043 |  0:49:04s\n",
            "epoch 258| loss: 0.24562 | train_logloss: 0.24162 | train_accuracy: 0.90718 | valid_logloss: 0.26527 | valid_accuracy: 0.9011  |  0:49:15s\n",
            "\n",
            "Early stopping occurred at epoch 258 with best_epoch = 208 and best_valid_accuracy = 0.90202\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-mfBn97n9Jq",
        "outputId": "41d99273-6c9d-4cce-da97-8278bace67dd"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb14.pkl' 'tn14.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    252.0 MiB    252.0 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    270.2 MiB     18.2 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2315.6 MiB   2045.4 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoG5VV95VlNY"
      },
      "source": [
        "ones(number_exp=15, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=16384,\tBV=1024,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltTzu3W1fva9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5a09cb-efcc-4155-c351-df14247544ca"
      },
      "source": [
        "memory(number_exp=20, \n",
        "     Rows=3000000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=30000,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.38582 | train_logloss: 5.25475 | train_accuracy: 0.13576 | valid_logloss: 4.21574 | valid_accuracy: 0.17169 |  0:01:05s\n",
            "epoch 1  | loss: 0.28353 | train_logloss: 1.27363 | train_accuracy: 0.30494 | valid_logloss: 1.20286 | valid_accuracy: 0.34235 |  0:02:10s\n",
            "epoch 2  | loss: 0.27088 | train_logloss: 0.72128 | train_accuracy: 0.72223 | valid_logloss: 0.58551 | valid_accuracy: 0.80174 |  0:03:14s\n",
            "epoch 3  | loss: 0.26378 | train_logloss: 0.38992 | train_accuracy: 0.85821 | valid_logloss: 0.32406 | valid_accuracy: 0.89515 |  0:04:19s\n",
            "epoch 4  | loss: 0.26047 | train_logloss: 0.28149 | train_accuracy: 0.89758 | valid_logloss: 0.24074 | valid_accuracy: 0.9191  |  0:05:24s\n",
            "epoch 5  | loss: 0.25833 | train_logloss: 0.26003 | train_accuracy: 0.905   | valid_logloss: 0.22064 | valid_accuracy: 0.92386 |  0:06:29s\n",
            "epoch 6  | loss: 0.25563 | train_logloss: 0.25203 | train_accuracy: 0.90783 | valid_logloss: 0.2188  | valid_accuracy: 0.92543 |  0:07:34s\n",
            "epoch 7  | loss: 0.25257 | train_logloss: 0.24885 | train_accuracy: 0.90921 | valid_logloss: 0.21741 | valid_accuracy: 0.92515 |  0:08:39s\n",
            "epoch 8  | loss: 0.24991 | train_logloss: 0.24599 | train_accuracy: 0.90989 | valid_logloss: 0.21593 | valid_accuracy: 0.92594 |  0:09:44s\n",
            "epoch 9  | loss: 0.24836 | train_logloss: 0.24645 | train_accuracy: 0.90969 | valid_logloss: 0.21619 | valid_accuracy: 0.92581 |  0:10:49s\n",
            "epoch 10 | loss: 0.24775 | train_logloss: 0.24681 | train_accuracy: 0.90873 | valid_logloss: 0.21975 | valid_accuracy: 0.92433 |  0:11:54s\n",
            "epoch 11 | loss: 0.24803 | train_logloss: 0.24327 | train_accuracy: 0.91071 | valid_logloss: 0.2127  | valid_accuracy: 0.92655 |  0:12:59s\n",
            "epoch 12 | loss: 0.24735 | train_logloss: 0.24531 | train_accuracy: 0.91018 | valid_logloss: 0.21809 | valid_accuracy: 0.9256  |  0:14:04s\n",
            "epoch 13 | loss: 0.24528 | train_logloss: 0.24114 | train_accuracy: 0.91173 | valid_logloss: 0.21532 | valid_accuracy: 0.92646 |  0:15:09s\n",
            "epoch 14 | loss: 0.24422 | train_logloss: 0.24019 | train_accuracy: 0.91209 | valid_logloss: 0.21086 | valid_accuracy: 0.92773 |  0:16:14s\n",
            "epoch 15 | loss: 0.24362 | train_logloss: 0.24089 | train_accuracy: 0.91174 | valid_logloss: 0.20943 | valid_accuracy: 0.92739 |  0:17:19s\n",
            "epoch 16 | loss: 0.24274 | train_logloss: 0.23909 | train_accuracy: 0.91234 | valid_logloss: 0.21266 | valid_accuracy: 0.9274  |  0:18:24s\n",
            "epoch 17 | loss: 0.24257 | train_logloss: 0.23809 | train_accuracy: 0.91247 | valid_logloss: 0.20961 | valid_accuracy: 0.92756 |  0:19:29s\n",
            "epoch 18 | loss: 0.24195 | train_logloss: 0.23779 | train_accuracy: 0.9127  | valid_logloss: 0.20695 | valid_accuracy: 0.92818 |  0:20:34s\n",
            "epoch 19 | loss: 0.24097 | train_logloss: 0.23681 | train_accuracy: 0.91325 | valid_logloss: 0.20893 | valid_accuracy: 0.92833 |  0:21:39s\n",
            "epoch 20 | loss: 0.24019 | train_logloss: 0.23625 | train_accuracy: 0.91345 | valid_logloss: 0.20909 | valid_accuracy: 0.92857 |  0:22:44s\n",
            "epoch 21 | loss: 0.24022 | train_logloss: 0.23686 | train_accuracy: 0.91319 | valid_logloss: 0.20958 | valid_accuracy: 0.92833 |  0:23:49s\n",
            "epoch 22 | loss: 0.23947 | train_logloss: 0.23716 | train_accuracy: 0.91309 | valid_logloss: 0.20681 | valid_accuracy: 0.9288  |  0:24:53s\n",
            "epoch 23 | loss: 0.23952 | train_logloss: 0.23627 | train_accuracy: 0.91336 | valid_logloss: 0.20817 | valid_accuracy: 0.92835 |  0:25:58s\n",
            "epoch 24 | loss: 0.2391  | train_logloss: 0.23675 | train_accuracy: 0.91294 | valid_logloss: 0.20947 | valid_accuracy: 0.92745 |  0:27:03s\n",
            "epoch 25 | loss: 0.23838 | train_logloss: 0.23544 | train_accuracy: 0.9139  | valid_logloss: 0.20488 | valid_accuracy: 0.92907 |  0:28:07s\n",
            "epoch 26 | loss: 0.23808 | train_logloss: 0.23614 | train_accuracy: 0.91349 | valid_logloss: 0.20997 | valid_accuracy: 0.92766 |  0:29:12s\n",
            "epoch 27 | loss: 0.23933 | train_logloss: 0.23756 | train_accuracy: 0.91296 | valid_logloss: 0.20789 | valid_accuracy: 0.92885 |  0:30:16s\n",
            "epoch 28 | loss: 0.23874 | train_logloss: 0.23567 | train_accuracy: 0.91356 | valid_logloss: 0.21002 | valid_accuracy: 0.92801 |  0:31:21s\n",
            "epoch 29 | loss: 0.2382  | train_logloss: 0.23522 | train_accuracy: 0.91363 | valid_logloss: 0.20886 | valid_accuracy: 0.92831 |  0:32:26s\n",
            "epoch 30 | loss: 0.23763 | train_logloss: 0.23656 | train_accuracy: 0.91333 | valid_logloss: 0.215   | valid_accuracy: 0.92663 |  0:33:30s\n",
            "epoch 31 | loss: 0.23741 | train_logloss: 0.23391 | train_accuracy: 0.9143  | valid_logloss: 0.20716 | valid_accuracy: 0.92897 |  0:34:35s\n",
            "epoch 32 | loss: 0.23668 | train_logloss: 0.23424 | train_accuracy: 0.91408 | valid_logloss: 0.20501 | valid_accuracy: 0.92919 |  0:35:40s\n",
            "epoch 33 | loss: 0.23698 | train_logloss: 0.23423 | train_accuracy: 0.91397 | valid_logloss: 0.2096  | valid_accuracy: 0.9284  |  0:36:44s\n",
            "epoch 34 | loss: 0.23654 | train_logloss: 0.23466 | train_accuracy: 0.9139  | valid_logloss: 0.20508 | valid_accuracy: 0.92918 |  0:37:49s\n",
            "epoch 35 | loss: 0.23634 | train_logloss: 0.23611 | train_accuracy: 0.91353 | valid_logloss: 0.21317 | valid_accuracy: 0.92692 |  0:38:54s\n",
            "epoch 36 | loss: 0.23579 | train_logloss: 0.23322 | train_accuracy: 0.91454 | valid_logloss: 0.20382 | valid_accuracy: 0.92946 |  0:39:59s\n",
            "epoch 37 | loss: 0.23609 | train_logloss: 0.23467 | train_accuracy: 0.91392 | valid_logloss: 0.20462 | valid_accuracy: 0.9295  |  0:41:03s\n",
            "epoch 38 | loss: 0.23695 | train_logloss: 0.23511 | train_accuracy: 0.91388 | valid_logloss: 0.2042  | valid_accuracy: 0.92992 |  0:42:08s\n",
            "epoch 39 | loss: 0.23674 | train_logloss: 0.23279 | train_accuracy: 0.91449 | valid_logloss: 0.20615 | valid_accuracy: 0.92936 |  0:43:13s\n",
            "epoch 40 | loss: 0.23574 | train_logloss: 0.23203 | train_accuracy: 0.91478 | valid_logloss: 0.20728 | valid_accuracy: 0.92856 |  0:44:18s\n",
            "epoch 41 | loss: 0.23523 | train_logloss: 0.23319 | train_accuracy: 0.91412 | valid_logloss: 0.20292 | valid_accuracy: 0.92962 |  0:45:22s\n",
            "epoch 42 | loss: 0.23549 | train_logloss: 0.23415 | train_accuracy: 0.91427 | valid_logloss: 0.20968 | valid_accuracy: 0.92841 |  0:46:27s\n",
            "epoch 43 | loss: 0.23508 | train_logloss: 0.23288 | train_accuracy: 0.91459 | valid_logloss: 0.20471 | valid_accuracy: 0.92928 |  0:47:32s\n",
            "epoch 44 | loss: 0.23539 | train_logloss: 0.23226 | train_accuracy: 0.91488 | valid_logloss: 0.20687 | valid_accuracy: 0.92919 |  0:48:37s\n",
            "epoch 45 | loss: 0.23473 | train_logloss: 0.2329  | train_accuracy: 0.91455 | valid_logloss: 0.20543 | valid_accuracy: 0.92955 |  0:49:42s\n",
            "epoch 46 | loss: 0.2347  | train_logloss: 0.23307 | train_accuracy: 0.914   | valid_logloss: 0.20572 | valid_accuracy: 0.92889 |  0:50:46s\n",
            "epoch 47 | loss: 0.23524 | train_logloss: 0.23211 | train_accuracy: 0.91478 | valid_logloss: 0.20598 | valid_accuracy: 0.92901 |  0:51:51s\n",
            "epoch 48 | loss: 0.23507 | train_logloss: 0.23147 | train_accuracy: 0.91509 | valid_logloss: 0.2025  | valid_accuracy: 0.93013 |  0:52:56s\n",
            "epoch 49 | loss: 0.23479 | train_logloss: 0.2341  | train_accuracy: 0.91412 | valid_logloss: 0.20845 | valid_accuracy: 0.92857 |  0:54:01s\n",
            "epoch 50 | loss: 0.23455 | train_logloss: 0.23398 | train_accuracy: 0.91425 | valid_logloss: 0.21211 | valid_accuracy: 0.92822 |  0:55:06s\n",
            "epoch 51 | loss: 0.23463 | train_logloss: 0.23277 | train_accuracy: 0.91444 | valid_logloss: 0.20422 | valid_accuracy: 0.92947 |  0:56:11s\n",
            "epoch 52 | loss: 0.23427 | train_logloss: 0.23116 | train_accuracy: 0.91503 | valid_logloss: 0.20399 | valid_accuracy: 0.92987 |  0:57:16s\n",
            "epoch 53 | loss: 0.23397 | train_logloss: 0.23197 | train_accuracy: 0.91505 | valid_logloss: 0.20701 | valid_accuracy: 0.92948 |  0:58:22s\n",
            "epoch 54 | loss: 0.23395 | train_logloss: 0.23078 | train_accuracy: 0.91523 | valid_logloss: 0.20646 | valid_accuracy: 0.92906 |  0:59:28s\n",
            "epoch 55 | loss: 0.23414 | train_logloss: 0.23136 | train_accuracy: 0.91495 | valid_logloss: 0.20371 | valid_accuracy: 0.92989 |  1:00:33s\n",
            "epoch 56 | loss: 0.23378 | train_logloss: 0.23131 | train_accuracy: 0.91507 | valid_logloss: 0.20146 | valid_accuracy: 0.93036 |  1:01:37s\n",
            "epoch 57 | loss: 0.23339 | train_logloss: 0.23186 | train_accuracy: 0.91491 | valid_logloss: 0.20479 | valid_accuracy: 0.92975 |  1:02:42s\n",
            "epoch 58 | loss: 0.23378 | train_logloss: 0.23174 | train_accuracy: 0.91503 | valid_logloss: 0.2038  | valid_accuracy: 0.92953 |  1:03:47s\n",
            "epoch 59 | loss: 0.23384 | train_logloss: 0.23282 | train_accuracy: 0.91453 | valid_logloss: 0.2082  | valid_accuracy: 0.92825 |  1:04:54s\n",
            "epoch 60 | loss: 0.23403 | train_logloss: 0.23282 | train_accuracy: 0.91444 | valid_logloss: 0.20834 | valid_accuracy: 0.92857 |  1:05:59s\n",
            "epoch 61 | loss: 0.23411 | train_logloss: 0.23253 | train_accuracy: 0.91484 | valid_logloss: 0.20964 | valid_accuracy: 0.92879 |  1:07:05s\n",
            "epoch 62 | loss: 0.23443 | train_logloss: 0.23151 | train_accuracy: 0.91505 | valid_logloss: 0.2096  | valid_accuracy: 0.92827 |  1:08:11s\n",
            "epoch 63 | loss: 0.23395 | train_logloss: 0.23242 | train_accuracy: 0.91468 | valid_logloss: 0.20181 | valid_accuracy: 0.93036 |  1:09:15s\n",
            "epoch 64 | loss: 0.23364 | train_logloss: 0.23193 | train_accuracy: 0.91483 | valid_logloss: 0.20702 | valid_accuracy: 0.9288  |  1:10:20s\n",
            "epoch 65 | loss: 0.23346 | train_logloss: 0.23106 | train_accuracy: 0.91508 | valid_logloss: 0.20725 | valid_accuracy: 0.92901 |  1:11:25s\n",
            "epoch 66 | loss: 0.23325 | train_logloss: 0.23125 | train_accuracy: 0.91494 | valid_logloss: 0.20854 | valid_accuracy: 0.92797 |  1:12:29s\n",
            "epoch 67 | loss: 0.233   | train_logloss: 0.23004 | train_accuracy: 0.91551 | valid_logloss: 0.20637 | valid_accuracy: 0.92956 |  1:13:34s\n",
            "epoch 68 | loss: 0.23304 | train_logloss: 0.23041 | train_accuracy: 0.91545 | valid_logloss: 0.20366 | valid_accuracy: 0.92971 |  1:14:38s\n",
            "epoch 69 | loss: 0.23274 | train_logloss: 0.22987 | train_accuracy: 0.9155  | valid_logloss: 0.2069  | valid_accuracy: 0.92896 |  1:15:43s\n",
            "epoch 70 | loss: 0.23282 | train_logloss: 0.22967 | train_accuracy: 0.91581 | valid_logloss: 0.20366 | valid_accuracy: 0.93025 |  1:16:47s\n",
            "epoch 71 | loss: 0.23267 | train_logloss: 0.23028 | train_accuracy: 0.91552 | valid_logloss: 0.20341 | valid_accuracy: 0.92997 |  1:17:53s\n",
            "epoch 72 | loss: 0.23301 | train_logloss: 0.22999 | train_accuracy: 0.91541 | valid_logloss: 0.20418 | valid_accuracy: 0.92995 |  1:18:58s\n",
            "epoch 73 | loss: 0.2343  | train_logloss: 0.23094 | train_accuracy: 0.91536 | valid_logloss: 0.20774 | valid_accuracy: 0.92917 |  1:20:03s\n",
            "epoch 74 | loss: 0.23314 | train_logloss: 0.23003 | train_accuracy: 0.91558 | valid_logloss: 0.2057  | valid_accuracy: 0.9296  |  1:21:08s\n",
            "epoch 75 | loss: 0.2324  | train_logloss: 0.2294  | train_accuracy: 0.91597 | valid_logloss: 0.20209 | valid_accuracy: 0.93024 |  1:22:13s\n",
            "epoch 76 | loss: 0.2325  | train_logloss: 0.22993 | train_accuracy: 0.9155  | valid_logloss: 0.20331 | valid_accuracy: 0.92992 |  1:23:18s\n",
            "epoch 77 | loss: 0.23262 | train_logloss: 0.22956 | train_accuracy: 0.91589 | valid_logloss: 0.20502 | valid_accuracy: 0.92975 |  1:24:23s\n",
            "epoch 78 | loss: 0.23221 | train_logloss: 0.23063 | train_accuracy: 0.91521 | valid_logloss: 0.2046  | valid_accuracy: 0.92963 |  1:25:28s\n",
            "epoch 79 | loss: 0.23207 | train_logloss: 0.23255 | train_accuracy: 0.91497 | valid_logloss: 0.20801 | valid_accuracy: 0.92858 |  1:26:32s\n",
            "epoch 80 | loss: 0.23433 | train_logloss: 0.23014 | train_accuracy: 0.91563 | valid_logloss: 0.20427 | valid_accuracy: 0.92943 |  1:27:37s\n",
            "epoch 81 | loss: 0.23284 | train_logloss: 0.23019 | train_accuracy: 0.91572 | valid_logloss: 0.20784 | valid_accuracy: 0.92923 |  1:28:42s\n",
            "epoch 82 | loss: 0.23233 | train_logloss: 0.22949 | train_accuracy: 0.91564 | valid_logloss: 0.2041  | valid_accuracy: 0.92977 |  1:29:47s\n",
            "epoch 83 | loss: 0.23195 | train_logloss: 0.22984 | train_accuracy: 0.91546 | valid_logloss: 0.20612 | valid_accuracy: 0.92916 |  1:30:51s\n",
            "epoch 84 | loss: 0.23248 | train_logloss: 0.23085 | train_accuracy: 0.91503 | valid_logloss: 0.20512 | valid_accuracy: 0.92949 |  1:31:56s\n",
            "epoch 85 | loss: 0.2323  | train_logloss: 0.22984 | train_accuracy: 0.91565 | valid_logloss: 0.20778 | valid_accuracy: 0.92898 |  1:33:01s\n",
            "epoch 86 | loss: 0.23187 | train_logloss: 0.22975 | train_accuracy: 0.91577 | valid_logloss: 0.20137 | valid_accuracy: 0.93049 |  1:34:05s\n",
            "epoch 87 | loss: 0.23184 | train_logloss: 0.22995 | train_accuracy: 0.9154  | valid_logloss: 0.2036  | valid_accuracy: 0.92994 |  1:35:10s\n",
            "epoch 88 | loss: 0.23284 | train_logloss: 0.23087 | train_accuracy: 0.91546 | valid_logloss: 0.2056  | valid_accuracy: 0.9299  |  1:36:15s\n",
            "epoch 89 | loss: 0.23283 | train_logloss: 0.23001 | train_accuracy: 0.91559 | valid_logloss: 0.20621 | valid_accuracy: 0.92957 |  1:37:19s\n",
            "epoch 90 | loss: 0.23252 | train_logloss: 0.22928 | train_accuracy: 0.91573 | valid_logloss: 0.20321 | valid_accuracy: 0.93    |  1:38:25s\n",
            "epoch 91 | loss: 0.23259 | train_logloss: 0.23124 | train_accuracy: 0.91512 | valid_logloss: 0.20596 | valid_accuracy: 0.92935 |  1:39:31s\n",
            "epoch 92 | loss: 0.23238 | train_logloss: 0.2288  | train_accuracy: 0.91599 | valid_logloss: 0.20346 | valid_accuracy: 0.93024 |  1:40:36s\n",
            "epoch 93 | loss: 0.23172 | train_logloss: 0.22895 | train_accuracy: 0.91584 | valid_logloss: 0.2041  | valid_accuracy: 0.92997 |  1:41:41s\n",
            "epoch 94 | loss: 0.2319  | train_logloss: 0.22852 | train_accuracy: 0.91614 | valid_logloss: 0.20397 | valid_accuracy: 0.93007 |  1:42:46s\n",
            "epoch 95 | loss: 0.23187 | train_logloss: 0.22931 | train_accuracy: 0.91578 | valid_logloss: 0.20727 | valid_accuracy: 0.92899 |  1:43:51s\n",
            "epoch 96 | loss: 0.23169 | train_logloss: 0.22959 | train_accuracy: 0.91553 | valid_logloss: 0.20335 | valid_accuracy: 0.9302  |  1:44:56s\n",
            "epoch 97 | loss: 0.2315  | train_logloss: 0.22888 | train_accuracy: 0.9158  | valid_logloss: 0.20357 | valid_accuracy: 0.92982 |  1:46:01s\n",
            "epoch 98 | loss: 0.23142 | train_logloss: 0.22892 | train_accuracy: 0.91595 | valid_logloss: 0.20593 | valid_accuracy: 0.92947 |  1:47:06s\n",
            "epoch 99 | loss: 0.23109 | train_logloss: 0.22832 | train_accuracy: 0.91637 | valid_logloss: 0.20207 | valid_accuracy: 0.93038 |  1:48:11s\n",
            "epoch 100| loss: 0.23159 | train_logloss: 0.22996 | train_accuracy: 0.91551 | valid_logloss: 0.20294 | valid_accuracy: 0.93035 |  1:49:16s\n",
            "epoch 101| loss: 0.23441 | train_logloss: 0.23169 | train_accuracy: 0.91483 | valid_logloss: 0.20661 | valid_accuracy: 0.92932 |  1:50:20s\n",
            "epoch 102| loss: 0.23303 | train_logloss: 0.23931 | train_accuracy: 0.91268 | valid_logloss: 0.21168 | valid_accuracy: 0.9276  |  1:51:25s\n",
            "epoch 103| loss: 0.23643 | train_logloss: 0.23198 | train_accuracy: 0.91489 | valid_logloss: 0.20615 | valid_accuracy: 0.92939 |  1:52:30s\n",
            "epoch 104| loss: 0.23371 | train_logloss: 0.23007 | train_accuracy: 0.91573 | valid_logloss: 0.20334 | valid_accuracy: 0.92987 |  1:53:35s\n",
            "epoch 105| loss: 0.23323 | train_logloss: 0.23024 | train_accuracy: 0.91544 | valid_logloss: 0.20491 | valid_accuracy: 0.92952 |  1:54:40s\n",
            "epoch 106| loss: 0.23238 | train_logloss: 0.229   | train_accuracy: 0.91595 | valid_logloss: 0.20254 | valid_accuracy: 0.93055 |  1:55:45s\n",
            "epoch 107| loss: 0.23264 | train_logloss: 0.22957 | train_accuracy: 0.91566 | valid_logloss: 0.20449 | valid_accuracy: 0.92964 |  1:56:49s\n",
            "epoch 108| loss: 0.23203 | train_logloss: 0.22972 | train_accuracy: 0.91572 | valid_logloss: 0.20464 | valid_accuracy: 0.92974 |  1:57:55s\n",
            "epoch 109| loss: 0.23161 | train_logloss: 0.22869 | train_accuracy: 0.91608 | valid_logloss: 0.20318 | valid_accuracy: 0.93042 |  1:58:59s\n",
            "epoch 110| loss: 0.23132 | train_logloss: 0.22865 | train_accuracy: 0.91615 | valid_logloss: 0.20235 | valid_accuracy: 0.93054 |  2:00:04s\n",
            "epoch 111| loss: 0.23113 | train_logloss: 0.22855 | train_accuracy: 0.91622 | valid_logloss: 0.20352 | valid_accuracy: 0.93005 |  2:01:09s\n",
            "epoch 112| loss: 0.23153 | train_logloss: 0.22849 | train_accuracy: 0.91604 | valid_logloss: 0.20241 | valid_accuracy: 0.93014 |  2:02:14s\n",
            "epoch 113| loss: 0.23119 | train_logloss: 0.22826 | train_accuracy: 0.91609 | valid_logloss: 0.20532 | valid_accuracy: 0.92956 |  2:03:18s\n",
            "epoch 114| loss: 0.23148 | train_logloss: 0.22917 | train_accuracy: 0.91603 | valid_logloss: 0.20377 | valid_accuracy: 0.93007 |  2:04:23s\n",
            "epoch 115| loss: 0.23119 | train_logloss: 0.22949 | train_accuracy: 0.91546 | valid_logloss: 0.20413 | valid_accuracy: 0.92967 |  2:05:28s\n",
            "epoch 116| loss: 0.23099 | train_logloss: 0.22861 | train_accuracy: 0.91607 | valid_logloss: 0.20378 | valid_accuracy: 0.93037 |  2:06:32s\n",
            "epoch 117| loss: 0.23062 | train_logloss: 0.22821 | train_accuracy: 0.91616 | valid_logloss: 0.20131 | valid_accuracy: 0.93078 |  2:07:37s\n",
            "epoch 118| loss: 0.23089 | train_logloss: 0.22872 | train_accuracy: 0.91592 | valid_logloss: 0.20199 | valid_accuracy: 0.93027 |  2:08:42s\n",
            "epoch 119| loss: 0.23052 | train_logloss: 0.22819 | train_accuracy: 0.91617 | valid_logloss: 0.20306 | valid_accuracy: 0.92999 |  2:09:46s\n",
            "epoch 120| loss: 0.23075 | train_logloss: 0.22832 | train_accuracy: 0.91616 | valid_logloss: 0.20577 | valid_accuracy: 0.92921 |  2:10:50s\n",
            "epoch 121| loss: 0.23323 | train_logloss: 0.23028 | train_accuracy: 0.91537 | valid_logloss: 0.20696 | valid_accuracy: 0.92925 |  2:11:54s\n",
            "epoch 122| loss: 0.23225 | train_logloss: 0.22857 | train_accuracy: 0.91605 | valid_logloss: 0.20231 | valid_accuracy: 0.93018 |  2:12:59s\n",
            "epoch 123| loss: 0.23123 | train_logloss: 0.22876 | train_accuracy: 0.91599 | valid_logloss: 0.206   | valid_accuracy: 0.92964 |  2:14:03s\n",
            "epoch 124| loss: 0.23153 | train_logloss: 0.22931 | train_accuracy: 0.91578 | valid_logloss: 0.20635 | valid_accuracy: 0.92958 |  2:15:07s\n",
            "epoch 125| loss: 0.23121 | train_logloss: 0.22874 | train_accuracy: 0.91599 | valid_logloss: 0.20621 | valid_accuracy: 0.92972 |  2:16:11s\n",
            "epoch 126| loss: 0.23109 | train_logloss: 0.22952 | train_accuracy: 0.91566 | valid_logloss: 0.20679 | valid_accuracy: 0.92888 |  2:17:16s\n",
            "epoch 127| loss: 0.23067 | train_logloss: 0.22835 | train_accuracy: 0.91627 | valid_logloss: 0.20126 | valid_accuracy: 0.93085 |  2:18:20s\n",
            "epoch 128| loss: 0.23099 | train_logloss: 0.22778 | train_accuracy: 0.91624 | valid_logloss: 0.20141 | valid_accuracy: 0.93071 |  2:19:24s\n",
            "epoch 129| loss: 0.23056 | train_logloss: 0.22791 | train_accuracy: 0.91632 | valid_logloss: 0.20328 | valid_accuracy: 0.9302  |  2:20:29s\n",
            "epoch 130| loss: 0.23039 | train_logloss: 0.2289  | train_accuracy: 0.91599 | valid_logloss: 0.2081  | valid_accuracy: 0.92925 |  2:21:33s\n",
            "epoch 131| loss: 0.23047 | train_logloss: 0.22824 | train_accuracy: 0.91616 | valid_logloss: 0.20254 | valid_accuracy: 0.93034 |  2:22:37s\n",
            "epoch 132| loss: 0.23167 | train_logloss: 0.23089 | train_accuracy: 0.91529 | valid_logloss: 0.20266 | valid_accuracy: 0.93022 |  2:23:42s\n",
            "epoch 133| loss: 0.23128 | train_logloss: 0.22805 | train_accuracy: 0.91583 | valid_logloss: 0.20418 | valid_accuracy: 0.92985 |  2:24:46s\n",
            "epoch 134| loss: 0.23074 | train_logloss: 0.22767 | train_accuracy: 0.9163  | valid_logloss: 0.20411 | valid_accuracy: 0.92968 |  2:25:51s\n",
            "epoch 135| loss: 0.23062 | train_logloss: 0.22792 | train_accuracy: 0.91628 | valid_logloss: 0.20474 | valid_accuracy: 0.93014 |  2:26:55s\n",
            "epoch 136| loss: 0.23018 | train_logloss: 0.22839 | train_accuracy: 0.9162  | valid_logloss: 0.20275 | valid_accuracy: 0.9304  |  2:27:59s\n",
            "epoch 137| loss: 0.22999 | train_logloss: 0.22763 | train_accuracy: 0.91652 | valid_logloss: 0.20244 | valid_accuracy: 0.93057 |  2:29:04s\n",
            "epoch 138| loss: 0.23005 | train_logloss: 0.22773 | train_accuracy: 0.91646 | valid_logloss: 0.20697 | valid_accuracy: 0.92957 |  2:30:08s\n",
            "epoch 139| loss: 0.23002 | train_logloss: 0.22854 | train_accuracy: 0.91614 | valid_logloss: 0.203   | valid_accuracy: 0.93008 |  2:31:12s\n",
            "epoch 140| loss: 0.23046 | train_logloss: 0.22759 | train_accuracy: 0.91638 | valid_logloss: 0.20576 | valid_accuracy: 0.92949 |  2:32:16s\n",
            "epoch 141| loss: 0.23037 | train_logloss: 0.22825 | train_accuracy: 0.91609 | valid_logloss: 0.20332 | valid_accuracy: 0.9299  |  2:33:21s\n",
            "epoch 142| loss: 0.23063 | train_logloss: 0.22784 | train_accuracy: 0.91636 | valid_logloss: 0.20744 | valid_accuracy: 0.92938 |  2:34:25s\n",
            "epoch 143| loss: 0.23023 | train_logloss: 0.22716 | train_accuracy: 0.9166  | valid_logloss: 0.20237 | valid_accuracy: 0.93038 |  2:35:30s\n",
            "epoch 144| loss: 0.23016 | train_logloss: 0.22648 | train_accuracy: 0.91674 | valid_logloss: 0.20285 | valid_accuracy: 0.93019 |  2:36:34s\n",
            "epoch 145| loss: 0.2297  | train_logloss: 0.22681 | train_accuracy: 0.91662 | valid_logloss: 0.20274 | valid_accuracy: 0.93003 |  2:37:38s\n",
            "epoch 146| loss: 0.2299  | train_logloss: 0.22727 | train_accuracy: 0.91651 | valid_logloss: 0.20361 | valid_accuracy: 0.93007 |  2:38:42s\n",
            "epoch 147| loss: 0.2296  | train_logloss: 0.22677 | train_accuracy: 0.91677 | valid_logloss: 0.20242 | valid_accuracy: 0.93054 |  2:39:47s\n",
            "epoch 148| loss: 0.22973 | train_logloss: 0.22673 | train_accuracy: 0.91666 | valid_logloss: 0.20213 | valid_accuracy: 0.93064 |  2:40:51s\n",
            "epoch 149| loss: 0.23021 | train_logloss: 0.22761 | train_accuracy: 0.91646 | valid_logloss: 0.20827 | valid_accuracy: 0.92924 |  2:41:55s\n",
            "epoch 150| loss: 0.22971 | train_logloss: 0.22734 | train_accuracy: 0.91638 | valid_logloss: 0.20268 | valid_accuracy: 0.93027 |  2:42:59s\n",
            "epoch 151| loss: 0.22936 | train_logloss: 0.22719 | train_accuracy: 0.91659 | valid_logloss: 0.20631 | valid_accuracy: 0.92939 |  2:44:04s\n",
            "epoch 152| loss: 0.22969 | train_logloss: 0.22911 | train_accuracy: 0.91588 | valid_logloss: 0.2111  | valid_accuracy: 0.92839 |  2:45:08s\n",
            "epoch 153| loss: 0.22988 | train_logloss: 0.22771 | train_accuracy: 0.91635 | valid_logloss: 0.20172 | valid_accuracy: 0.93064 |  2:46:12s\n",
            "epoch 154| loss: 0.22989 | train_logloss: 0.22761 | train_accuracy: 0.91624 | valid_logloss: 0.20418 | valid_accuracy: 0.92958 |  2:47:17s\n",
            "epoch 155| loss: 0.22981 | train_logloss: 0.22752 | train_accuracy: 0.91641 | valid_logloss: 0.20632 | valid_accuracy: 0.92911 |  2:48:21s\n",
            "epoch 156| loss: 0.22969 | train_logloss: 0.22788 | train_accuracy: 0.91621 | valid_logloss: 0.20314 | valid_accuracy: 0.92987 |  2:49:25s\n",
            "epoch 157| loss: 0.22962 | train_logloss: 0.22647 | train_accuracy: 0.91688 | valid_logloss: 0.20214 | valid_accuracy: 0.93055 |  2:50:31s\n",
            "epoch 158| loss: 0.2298  | train_logloss: 0.22668 | train_accuracy: 0.91669 | valid_logloss: 0.20202 | valid_accuracy: 0.93054 |  2:51:36s\n",
            "epoch 159| loss: 0.22952 | train_logloss: 0.22642 | train_accuracy: 0.91672 | valid_logloss: 0.20192 | valid_accuracy: 0.93037 |  2:52:40s\n",
            "epoch 160| loss: 0.22946 | train_logloss: 0.22787 | train_accuracy: 0.9162  | valid_logloss: 0.20374 | valid_accuracy: 0.92983 |  2:53:44s\n",
            "epoch 161| loss: 0.22941 | train_logloss: 0.22728 | train_accuracy: 0.91651 | valid_logloss: 0.20555 | valid_accuracy: 0.9298  |  2:54:48s\n",
            "epoch 162| loss: 0.23003 | train_logloss: 0.22797 | train_accuracy: 0.91632 | valid_logloss: 0.20092 | valid_accuracy: 0.93088 |  2:55:53s\n",
            "epoch 163| loss: 0.22967 | train_logloss: 0.22849 | train_accuracy: 0.91598 | valid_logloss: 0.20133 | valid_accuracy: 0.93019 |  2:56:57s\n",
            "epoch 164| loss: 0.22956 | train_logloss: 0.22678 | train_accuracy: 0.91676 | valid_logloss: 0.20297 | valid_accuracy: 0.93059 |  2:58:01s\n",
            "epoch 165| loss: 0.22968 | train_logloss: 0.22834 | train_accuracy: 0.91627 | valid_logloss: 0.20061 | valid_accuracy: 0.93063 |  2:59:06s\n",
            "epoch 166| loss: 0.22975 | train_logloss: 0.22771 | train_accuracy: 0.91635 | valid_logloss: 0.20531 | valid_accuracy: 0.92966 |  3:00:10s\n",
            "epoch 167| loss: 0.2294  | train_logloss: 0.22626 | train_accuracy: 0.9169  | valid_logloss: 0.2025  | valid_accuracy: 0.93049 |  3:01:14s\n",
            "epoch 168| loss: 0.22933 | train_logloss: 0.22805 | train_accuracy: 0.91623 | valid_logloss: 0.20687 | valid_accuracy: 0.92943 |  3:02:18s\n",
            "epoch 169| loss: 0.22952 | train_logloss: 0.22637 | train_accuracy: 0.91684 | valid_logloss: 0.20446 | valid_accuracy: 0.92999 |  3:03:23s\n",
            "epoch 170| loss: 0.22929 | train_logloss: 0.22853 | train_accuracy: 0.91605 | valid_logloss: 0.20912 | valid_accuracy: 0.92879 |  3:04:27s\n",
            "epoch 171| loss: 0.22979 | train_logloss: 0.23046 | train_accuracy: 0.91574 | valid_logloss: 0.20384 | valid_accuracy: 0.9299  |  3:05:31s\n",
            "epoch 172| loss: 0.23089 | train_logloss: 0.22699 | train_accuracy: 0.91665 | valid_logloss: 0.2047  | valid_accuracy: 0.93023 |  3:06:36s\n",
            "epoch 173| loss: 0.2309  | train_logloss: 0.23009 | train_accuracy: 0.91554 | valid_logloss: 0.20589 | valid_accuracy: 0.92942 |  3:07:40s\n",
            "epoch 174| loss: 0.23055 | train_logloss: 0.22733 | train_accuracy: 0.91649 | valid_logloss: 0.20301 | valid_accuracy: 0.93036 |  3:08:45s\n",
            "epoch 175| loss: 0.23019 | train_logloss: 0.2275  | train_accuracy: 0.91651 | valid_logloss: 0.20341 | valid_accuracy: 0.93016 |  3:09:49s\n",
            "epoch 176| loss: 0.22978 | train_logloss: 0.22657 | train_accuracy: 0.91665 | valid_logloss: 0.20192 | valid_accuracy: 0.93042 |  3:10:54s\n",
            "epoch 177| loss: 0.22924 | train_logloss: 0.22827 | train_accuracy: 0.91625 | valid_logloss: 0.20479 | valid_accuracy: 0.93003 |  3:11:58s\n",
            "epoch 178| loss: 0.22938 | train_logloss: 0.22676 | train_accuracy: 0.91655 | valid_logloss: 0.20124 | valid_accuracy: 0.93089 |  3:13:03s\n",
            "epoch 179| loss: 0.22914 | train_logloss: 0.22668 | train_accuracy: 0.91684 | valid_logloss: 0.20174 | valid_accuracy: 0.93057 |  3:14:08s\n",
            "epoch 180| loss: 0.22889 | train_logloss: 0.22677 | train_accuracy: 0.91686 | valid_logloss: 0.20366 | valid_accuracy: 0.93009 |  3:15:13s\n",
            "epoch 181| loss: 0.22933 | train_logloss: 0.22638 | train_accuracy: 0.91698 | valid_logloss: 0.20244 | valid_accuracy: 0.93032 |  3:16:17s\n",
            "epoch 182| loss: 0.22892 | train_logloss: 0.22667 | train_accuracy: 0.91666 | valid_logloss: 0.20162 | valid_accuracy: 0.93063 |  3:17:22s\n",
            "epoch 183| loss: 0.22879 | train_logloss: 0.22661 | train_accuracy: 0.9169  | valid_logloss: 0.20462 | valid_accuracy: 0.92987 |  3:18:26s\n",
            "epoch 184| loss: 0.22855 | train_logloss: 0.22606 | train_accuracy: 0.91694 | valid_logloss: 0.2047  | valid_accuracy: 0.92978 |  3:19:31s\n",
            "epoch 185| loss: 0.22899 | train_logloss: 0.22776 | train_accuracy: 0.91609 | valid_logloss: 0.20603 | valid_accuracy: 0.92925 |  3:20:35s\n",
            "epoch 186| loss: 0.22903 | train_logloss: 0.22663 | train_accuracy: 0.91677 | valid_logloss: 0.20372 | valid_accuracy: 0.9302  |  3:21:40s\n",
            "epoch 187| loss: 0.22938 | train_logloss: 0.22671 | train_accuracy: 0.9167  | valid_logloss: 0.20481 | valid_accuracy: 0.92987 |  3:22:44s\n",
            "epoch 188| loss: 0.2291  | train_logloss: 0.22603 | train_accuracy: 0.91718 | valid_logloss: 0.20094 | valid_accuracy: 0.93103 |  3:23:49s\n",
            "epoch 189| loss: 0.22859 | train_logloss: 0.22509 | train_accuracy: 0.91734 | valid_logloss: 0.20255 | valid_accuracy: 0.93053 |  3:24:53s\n",
            "epoch 190| loss: 0.22862 | train_logloss: 0.22635 | train_accuracy: 0.91702 | valid_logloss: 0.20306 | valid_accuracy: 0.93026 |  3:25:57s\n",
            "epoch 191| loss: 0.22877 | train_logloss: 0.22575 | train_accuracy: 0.91714 | valid_logloss: 0.20251 | valid_accuracy: 0.93047 |  3:27:02s\n",
            "epoch 192| loss: 0.22876 | train_logloss: 0.22611 | train_accuracy: 0.91682 | valid_logloss: 0.20077 | valid_accuracy: 0.9307  |  3:28:06s\n",
            "epoch 193| loss: 0.22863 | train_logloss: 0.22626 | train_accuracy: 0.91694 | valid_logloss: 0.20282 | valid_accuracy: 0.93052 |  3:29:11s\n",
            "epoch 194| loss: 0.22882 | train_logloss: 0.22629 | train_accuracy: 0.91709 | valid_logloss: 0.20425 | valid_accuracy: 0.93013 |  3:30:16s\n",
            "epoch 195| loss: 0.2287  | train_logloss: 0.22736 | train_accuracy: 0.91666 | valid_logloss: 0.19866 | valid_accuracy: 0.93129 |  3:31:20s\n",
            "epoch 196| loss: 0.22843 | train_logloss: 0.22657 | train_accuracy: 0.91691 | valid_logloss: 0.20379 | valid_accuracy: 0.93017 |  3:32:25s\n",
            "epoch 197| loss: 0.2282  | train_logloss: 0.22675 | train_accuracy: 0.91674 | valid_logloss: 0.2061  | valid_accuracy: 0.92929 |  3:33:29s\n",
            "epoch 198| loss: 0.22877 | train_logloss: 0.22873 | train_accuracy: 0.91609 | valid_logloss: 0.20491 | valid_accuracy: 0.92927 |  3:34:34s\n",
            "epoch 199| loss: 0.22968 | train_logloss: 0.2298  | train_accuracy: 0.91589 | valid_logloss: 0.20402 | valid_accuracy: 0.92971 |  3:35:38s\n",
            "epoch 200| loss: 0.23055 | train_logloss: 0.22893 | train_accuracy: 0.91588 | valid_logloss: 0.20555 | valid_accuracy: 0.92956 |  3:36:43s\n",
            "epoch 201| loss: 0.22996 | train_logloss: 0.22696 | train_accuracy: 0.91664 | valid_logloss: 0.20218 | valid_accuracy: 0.9303  |  3:37:47s\n",
            "epoch 202| loss: 0.22944 | train_logloss: 0.22609 | train_accuracy: 0.91697 | valid_logloss: 0.20478 | valid_accuracy: 0.92977 |  3:38:52s\n",
            "epoch 203| loss: 0.22883 | train_logloss: 0.22604 | train_accuracy: 0.91703 | valid_logloss: 0.2038  | valid_accuracy: 0.93026 |  3:39:57s\n",
            "epoch 204| loss: 0.22873 | train_logloss: 0.22627 | train_accuracy: 0.91679 | valid_logloss: 0.20504 | valid_accuracy: 0.92963 |  3:41:01s\n",
            "epoch 205| loss: 0.22863 | train_logloss: 0.22532 | train_accuracy: 0.91714 | valid_logloss: 0.20386 | valid_accuracy: 0.9302  |  3:42:06s\n",
            "epoch 206| loss: 0.22849 | train_logloss: 0.22664 | train_accuracy: 0.91678 | valid_logloss: 0.20313 | valid_accuracy: 0.92993 |  3:43:10s\n",
            "epoch 207| loss: 0.22819 | train_logloss: 0.22586 | train_accuracy: 0.91688 | valid_logloss: 0.20014 | valid_accuracy: 0.931   |  3:44:15s\n",
            "epoch 208| loss: 0.2281  | train_logloss: 0.22669 | train_accuracy: 0.91691 | valid_logloss: 0.20659 | valid_accuracy: 0.92984 |  3:45:20s\n",
            "epoch 209| loss: 0.22846 | train_logloss: 0.22641 | train_accuracy: 0.9168  | valid_logloss: 0.20226 | valid_accuracy: 0.93064 |  3:46:24s\n",
            "epoch 210| loss: 0.23013 | train_logloss: 0.22728 | train_accuracy: 0.9167  | valid_logloss: 0.20521 | valid_accuracy: 0.92972 |  3:47:29s\n",
            "epoch 211| loss: 0.22958 | train_logloss: 0.22696 | train_accuracy: 0.91659 | valid_logloss: 0.20136 | valid_accuracy: 0.9307  |  3:48:33s\n",
            "epoch 212| loss: 0.22898 | train_logloss: 0.22662 | train_accuracy: 0.91677 | valid_logloss: 0.20222 | valid_accuracy: 0.93036 |  3:49:38s\n",
            "epoch 213| loss: 0.2288  | train_logloss: 0.22605 | train_accuracy: 0.91689 | valid_logloss: 0.20666 | valid_accuracy: 0.92916 |  3:50:43s\n",
            "epoch 214| loss: 0.22847 | train_logloss: 0.2266  | train_accuracy: 0.9169  | valid_logloss: 0.20024 | valid_accuracy: 0.93105 |  3:51:47s\n",
            "epoch 215| loss: 0.22856 | train_logloss: 0.22663 | train_accuracy: 0.91679 | valid_logloss: 0.20541 | valid_accuracy: 0.92954 |  3:52:52s\n",
            "epoch 216| loss: 0.22854 | train_logloss: 0.22804 | train_accuracy: 0.91605 | valid_logloss: 0.20748 | valid_accuracy: 0.9288  |  3:53:56s\n",
            "epoch 217| loss: 0.22892 | train_logloss: 0.22604 | train_accuracy: 0.91703 | valid_logloss: 0.2019  | valid_accuracy: 0.9306  |  3:55:01s\n",
            "epoch 218| loss: 0.2289  | train_logloss: 0.22612 | train_accuracy: 0.91691 | valid_logloss: 0.20239 | valid_accuracy: 0.93062 |  3:56:06s\n",
            "epoch 219| loss: 0.22839 | train_logloss: 0.22642 | train_accuracy: 0.91694 | valid_logloss: 0.20635 | valid_accuracy: 0.92977 |  3:57:10s\n",
            "epoch 220| loss: 0.22822 | train_logloss: 0.22534 | train_accuracy: 0.91726 | valid_logloss: 0.2046  | valid_accuracy: 0.93013 |  3:58:15s\n",
            "epoch 221| loss: 0.22839 | train_logloss: 0.2281  | train_accuracy: 0.91619 | valid_logloss: 0.20156 | valid_accuracy: 0.93076 |  3:59:20s\n",
            "epoch 222| loss: 0.22832 | train_logloss: 0.22553 | train_accuracy: 0.91715 | valid_logloss: 0.20442 | valid_accuracy: 0.92993 |  4:00:25s\n",
            "epoch 223| loss: 0.22806 | train_logloss: 0.22594 | train_accuracy: 0.917   | valid_logloss: 0.20396 | valid_accuracy: 0.93008 |  4:01:29s\n",
            "epoch 224| loss: 0.22831 | train_logloss: 0.22645 | train_accuracy: 0.91697 | valid_logloss: 0.20724 | valid_accuracy: 0.92972 |  4:02:34s\n",
            "epoch 225| loss: 0.22821 | train_logloss: 0.22563 | train_accuracy: 0.91712 | valid_logloss: 0.20145 | valid_accuracy: 0.93067 |  4:03:39s\n",
            "epoch 226| loss: 0.22793 | train_logloss: 0.22656 | train_accuracy: 0.9172  | valid_logloss: 0.21066 | valid_accuracy: 0.92912 |  4:04:43s\n",
            "epoch 227| loss: 0.22817 | train_logloss: 0.22531 | train_accuracy: 0.91721 | valid_logloss: 0.20462 | valid_accuracy: 0.93035 |  4:05:48s\n",
            "epoch 228| loss: 0.22822 | train_logloss: 0.22603 | train_accuracy: 0.91699 | valid_logloss: 0.20051 | valid_accuracy: 0.9312  |  4:06:53s\n",
            "epoch 229| loss: 0.22897 | train_logloss: 0.22799 | train_accuracy: 0.91658 | valid_logloss: 0.20614 | valid_accuracy: 0.92976 |  4:07:58s\n",
            "epoch 230| loss: 0.22897 | train_logloss: 0.22562 | train_accuracy: 0.91721 | valid_logloss: 0.20336 | valid_accuracy: 0.93033 |  4:09:02s\n",
            "epoch 231| loss: 0.22857 | train_logloss: 0.2262  | train_accuracy: 0.91691 | valid_logloss: 0.20205 | valid_accuracy: 0.93044 |  4:10:07s\n",
            "epoch 232| loss: 0.22847 | train_logloss: 0.22525 | train_accuracy: 0.91723 | valid_logloss: 0.20518 | valid_accuracy: 0.92975 |  4:11:12s\n",
            "epoch 233| loss: 0.2281  | train_logloss: 0.22673 | train_accuracy: 0.91689 | valid_logloss: 0.20675 | valid_accuracy: 0.92937 |  4:12:16s\n",
            "epoch 234| loss: 0.2282  | train_logloss: 0.22611 | train_accuracy: 0.91694 | valid_logloss: 0.20524 | valid_accuracy: 0.92988 |  4:13:21s\n",
            "epoch 235| loss: 0.22806 | train_logloss: 0.22557 | train_accuracy: 0.9171  | valid_logloss: 0.20479 | valid_accuracy: 0.92989 |  4:14:26s\n",
            "epoch 236| loss: 0.22846 | train_logloss: 0.22495 | train_accuracy: 0.91742 | valid_logloss: 0.20549 | valid_accuracy: 0.92976 |  4:15:30s\n",
            "epoch 237| loss: 0.22818 | train_logloss: 0.22523 | train_accuracy: 0.91735 | valid_logloss: 0.20589 | valid_accuracy: 0.92996 |  4:16:35s\n",
            "epoch 238| loss: 0.23119 | train_logloss: 0.23507 | train_accuracy: 0.91429 | valid_logloss: 0.21152 | valid_accuracy: 0.92808 |  4:17:40s\n",
            "epoch 239| loss: 0.23325 | train_logloss: 0.22849 | train_accuracy: 0.91623 | valid_logloss: 0.20296 | valid_accuracy: 0.93016 |  4:18:45s\n",
            "epoch 240| loss: 0.23048 | train_logloss: 0.22762 | train_accuracy: 0.91654 | valid_logloss: 0.20325 | valid_accuracy: 0.93022 |  4:19:49s\n",
            "epoch 241| loss: 0.22964 | train_logloss: 0.22786 | train_accuracy: 0.91638 | valid_logloss: 0.20474 | valid_accuracy: 0.92972 |  4:20:54s\n",
            "epoch 242| loss: 0.2291  | train_logloss: 0.22682 | train_accuracy: 0.91691 | valid_logloss: 0.20802 | valid_accuracy: 0.92971 |  4:21:59s\n",
            "epoch 243| loss: 0.22888 | train_logloss: 0.22577 | train_accuracy: 0.91714 | valid_logloss: 0.2039  | valid_accuracy: 0.93033 |  4:23:04s\n",
            "epoch 244| loss: 0.22853 | train_logloss: 0.22576 | train_accuracy: 0.9169  | valid_logloss: 0.20362 | valid_accuracy: 0.93003 |  4:24:08s\n",
            "epoch 245| loss: 0.2284  | train_logloss: 0.22543 | train_accuracy: 0.91706 | valid_logloss: 0.20345 | valid_accuracy: 0.92998 |  4:25:13s\n",
            "\n",
            "Early stopping occurred at epoch 245 with best_epoch = 195 and best_valid_accuracy = 0.93129\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea_KEeoBoFtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142ee504-ffb7-47ec-c667-19f38bdd0023"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb20.pkl' 'tn20.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    257.2 MiB    257.2 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    302.7 MiB     45.5 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2342.0 MiB   2039.3 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RezXgsvA_0Te"
      },
      "source": [
        "ones(number_exp=16, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkvhQ62bxm6d"
      },
      "source": [
        "ones(number_exp=17, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.9,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRBx3JDt-Veh",
        "outputId": "68751604-cf81-4204-d315-2d0aeb61cfe6"
      },
      "source": [
        "ones(number_exp=18, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.0001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.0721  | train_logloss: 20.48473| train_accuracy: 0.3116  | valid_logloss: 20.52434| valid_accuracy: 0.31487 |  0:00:02s\n",
            "epoch 1  | loss: 0.62702 | train_logloss: 7.26988 | train_accuracy: 0.28423 | valid_logloss: 7.35665 | valid_accuracy: 0.28297 |  0:00:04s\n",
            "epoch 2  | loss: 0.4926  | train_logloss: 4.78337 | train_accuracy: 0.3647  | valid_logloss: 4.88082 | valid_accuracy: 0.36347 |  0:00:06s\n",
            "epoch 3  | loss: 0.44313 | train_logloss: 3.25561 | train_accuracy: 0.29043 | valid_logloss: 3.28029 | valid_accuracy: 0.29257 |  0:00:08s\n",
            "epoch 4  | loss: 0.4382  | train_logloss: 5.81552 | train_accuracy: 0.35523 | valid_logloss: 5.85558 | valid_accuracy: 0.35263 |  0:00:10s\n",
            "epoch 5  | loss: 0.42124 | train_logloss: 3.80157 | train_accuracy: 0.3198  | valid_logloss: 3.82596 | valid_accuracy: 0.32247 |  0:00:12s\n",
            "epoch 6  | loss: 0.40068 | train_logloss: 2.142   | train_accuracy: 0.3114  | valid_logloss: 2.15032 | valid_accuracy: 0.31263 |  0:00:14s\n",
            "epoch 7  | loss: 0.39334 | train_logloss: 1.34959 | train_accuracy: 0.4055  | valid_logloss: 1.36478 | valid_accuracy: 0.40283 |  0:00:16s\n",
            "epoch 8  | loss: 0.38421 | train_logloss: 2.04112 | train_accuracy: 0.2778  | valid_logloss: 2.0746  | valid_accuracy: 0.27353 |  0:00:18s\n",
            "epoch 9  | loss: 0.36923 | train_logloss: 1.39782 | train_accuracy: 0.4167  | valid_logloss: 1.41947 | valid_accuracy: 0.40707 |  0:00:20s\n",
            "epoch 10 | loss: 0.35991 | train_logloss: 0.93261 | train_accuracy: 0.6835  | valid_logloss: 0.94757 | valid_accuracy: 0.67823 |  0:00:22s\n",
            "epoch 11 | loss: 0.36143 | train_logloss: 0.92531 | train_accuracy: 0.67113 | valid_logloss: 0.94228 | valid_accuracy: 0.66313 |  0:00:24s\n",
            "epoch 12 | loss: 0.36208 | train_logloss: 0.75854 | train_accuracy: 0.63137 | valid_logloss: 0.77162 | valid_accuracy: 0.62067 |  0:00:26s\n",
            "epoch 13 | loss: 0.35072 | train_logloss: 0.65422 | train_accuracy: 0.72803 | valid_logloss: 0.66262 | valid_accuracy: 0.72177 |  0:00:28s\n",
            "epoch 14 | loss: 0.34479 | train_logloss: 0.64139 | train_accuracy: 0.77137 | valid_logloss: 0.65007 | valid_accuracy: 0.76677 |  0:00:30s\n",
            "epoch 15 | loss: 0.33878 | train_logloss: 0.54607 | train_accuracy: 0.8126  | valid_logloss: 0.55488 | valid_accuracy: 0.80833 |  0:00:33s\n",
            "epoch 16 | loss: 0.33383 | train_logloss: 0.49983 | train_accuracy: 0.8259  | valid_logloss: 0.50804 | valid_accuracy: 0.82147 |  0:00:35s\n",
            "epoch 17 | loss: 0.33658 | train_logloss: 0.39857 | train_accuracy: 0.85717 | valid_logloss: 0.40424 | valid_accuracy: 0.8565  |  0:00:37s\n",
            "epoch 18 | loss: 0.34441 | train_logloss: 0.39084 | train_accuracy: 0.8579  | valid_logloss: 0.39665 | valid_accuracy: 0.8559  |  0:00:39s\n",
            "epoch 19 | loss: 0.33534 | train_logloss: 0.39496 | train_accuracy: 0.86057 | valid_logloss: 0.39884 | valid_accuracy: 0.8598  |  0:00:41s\n",
            "epoch 20 | loss: 0.34243 | train_logloss: 0.37095 | train_accuracy: 0.86187 | valid_logloss: 0.37499 | valid_accuracy: 0.85943 |  0:00:43s\n",
            "epoch 21 | loss: 0.35844 | train_logloss: 0.36074 | train_accuracy: 0.8628  | valid_logloss: 0.36884 | valid_accuracy: 0.861   |  0:00:45s\n",
            "epoch 22 | loss: 0.34144 | train_logloss: 0.35265 | train_accuracy: 0.8701  | valid_logloss: 0.36195 | valid_accuracy: 0.86793 |  0:00:47s\n",
            "epoch 23 | loss: 0.33422 | train_logloss: 0.34395 | train_accuracy: 0.8682  | valid_logloss: 0.35279 | valid_accuracy: 0.86477 |  0:00:49s\n",
            "epoch 24 | loss: 0.32953 | train_logloss: 0.3424  | train_accuracy: 0.87227 | valid_logloss: 0.35381 | valid_accuracy: 0.86943 |  0:00:51s\n",
            "epoch 25 | loss: 0.33326 | train_logloss: 0.34047 | train_accuracy: 0.871   | valid_logloss: 0.35438 | valid_accuracy: 0.86863 |  0:00:53s\n",
            "epoch 26 | loss: 0.32762 | train_logloss: 0.33234 | train_accuracy: 0.87097 | valid_logloss: 0.34278 | valid_accuracy: 0.86967 |  0:00:55s\n",
            "epoch 27 | loss: 0.32341 | train_logloss: 0.32039 | train_accuracy: 0.87537 | valid_logloss: 0.33346 | valid_accuracy: 0.8721  |  0:00:57s\n",
            "epoch 28 | loss: 0.32219 | train_logloss: 0.32146 | train_accuracy: 0.87773 | valid_logloss: 0.33367 | valid_accuracy: 0.87467 |  0:00:59s\n",
            "epoch 29 | loss: 0.32317 | train_logloss: 0.31209 | train_accuracy: 0.8778  | valid_logloss: 0.32736 | valid_accuracy: 0.87483 |  0:01:02s\n",
            "epoch 30 | loss: 0.31925 | train_logloss: 0.31908 | train_accuracy: 0.87643 | valid_logloss: 0.33484 | valid_accuracy: 0.8716  |  0:01:04s\n",
            "epoch 31 | loss: 0.31853 | train_logloss: 0.31679 | train_accuracy: 0.87993 | valid_logloss: 0.33483 | valid_accuracy: 0.8722  |  0:01:06s\n",
            "epoch 32 | loss: 0.32118 | train_logloss: 0.32433 | train_accuracy: 0.87767 | valid_logloss: 0.33863 | valid_accuracy: 0.8734  |  0:01:08s\n",
            "epoch 33 | loss: 0.31992 | train_logloss: 0.31434 | train_accuracy: 0.88153 | valid_logloss: 0.32799 | valid_accuracy: 0.874   |  0:01:10s\n",
            "epoch 34 | loss: 0.3181  | train_logloss: 0.30971 | train_accuracy: 0.88043 | valid_logloss: 0.32648 | valid_accuracy: 0.8765  |  0:01:12s\n",
            "epoch 35 | loss: 0.31753 | train_logloss: 0.31938 | train_accuracy: 0.8773  | valid_logloss: 0.33488 | valid_accuracy: 0.87233 |  0:01:14s\n",
            "epoch 36 | loss: 0.32233 | train_logloss: 0.31166 | train_accuracy: 0.8807  | valid_logloss: 0.32691 | valid_accuracy: 0.8747  |  0:01:16s\n",
            "epoch 37 | loss: 0.31679 | train_logloss: 0.30835 | train_accuracy: 0.88147 | valid_logloss: 0.3208  | valid_accuracy: 0.87877 |  0:01:18s\n",
            "epoch 38 | loss: 0.31735 | train_logloss: 0.32477 | train_accuracy: 0.87737 | valid_logloss: 0.33791 | valid_accuracy: 0.87397 |  0:01:20s\n",
            "epoch 39 | loss: 0.3302  | train_logloss: 0.3245  | train_accuracy: 0.8773  | valid_logloss: 0.33604 | valid_accuracy: 0.873   |  0:01:22s\n",
            "epoch 40 | loss: 0.32972 | train_logloss: 0.3186  | train_accuracy: 0.8783  | valid_logloss: 0.32822 | valid_accuracy: 0.87497 |  0:01:24s\n",
            "epoch 41 | loss: 0.33287 | train_logloss: 0.33    | train_accuracy: 0.87767 | valid_logloss: 0.33803 | valid_accuracy: 0.87453 |  0:01:27s\n",
            "epoch 42 | loss: 0.3288  | train_logloss: 0.31949 | train_accuracy: 0.87983 | valid_logloss: 0.32887 | valid_accuracy: 0.87647 |  0:01:29s\n",
            "epoch 43 | loss: 0.33366 | train_logloss: 0.36999 | train_accuracy: 0.8547  | valid_logloss: 0.38024 | valid_accuracy: 0.8549  |  0:01:31s\n",
            "epoch 44 | loss: 0.36122 | train_logloss: 0.35163 | train_accuracy: 0.8677  | valid_logloss: 0.36271 | valid_accuracy: 0.86447 |  0:01:33s\n",
            "epoch 45 | loss: 0.36099 | train_logloss: 0.35869 | train_accuracy: 0.8667  | valid_logloss: 0.36364 | valid_accuracy: 0.8671  |  0:01:35s\n",
            "epoch 46 | loss: 0.36181 | train_logloss: 0.35149 | train_accuracy: 0.86917 | valid_logloss: 0.36073 | valid_accuracy: 0.86683 |  0:01:37s\n",
            "epoch 47 | loss: 0.34776 | train_logloss: 0.34358 | train_accuracy: 0.87183 | valid_logloss: 0.35279 | valid_accuracy: 0.8703  |  0:01:39s\n",
            "epoch 48 | loss: 0.34301 | train_logloss: 0.34459 | train_accuracy: 0.87073 | valid_logloss: 0.35203 | valid_accuracy: 0.8676  |  0:01:41s\n",
            "epoch 49 | loss: 0.34192 | train_logloss: 0.33294 | train_accuracy: 0.87473 | valid_logloss: 0.33943 | valid_accuracy: 0.8718  |  0:01:43s\n",
            "epoch 50 | loss: 0.34117 | train_logloss: 0.34001 | train_accuracy: 0.8708  | valid_logloss: 0.34509 | valid_accuracy: 0.86953 |  0:01:45s\n",
            "epoch 51 | loss: 0.33987 | train_logloss: 0.34442 | train_accuracy: 0.8704  | valid_logloss: 0.35111 | valid_accuracy: 0.86927 |  0:01:47s\n",
            "epoch 52 | loss: 0.34994 | train_logloss: 0.34002 | train_accuracy: 0.8726  | valid_logloss: 0.34634 | valid_accuracy: 0.87077 |  0:01:49s\n",
            "epoch 53 | loss: 0.3369  | train_logloss: 0.32543 | train_accuracy: 0.87647 | valid_logloss: 0.33239 | valid_accuracy: 0.87377 |  0:01:51s\n",
            "epoch 54 | loss: 0.33345 | train_logloss: 0.34132 | train_accuracy: 0.87113 | valid_logloss: 0.34995 | valid_accuracy: 0.86843 |  0:01:53s\n",
            "epoch 55 | loss: 0.33686 | train_logloss: 0.3314  | train_accuracy: 0.87517 | valid_logloss: 0.34065 | valid_accuracy: 0.8706  |  0:01:56s\n",
            "epoch 56 | loss: 0.32852 | train_logloss: 0.32283 | train_accuracy: 0.8765  | valid_logloss: 0.33137 | valid_accuracy: 0.87413 |  0:01:58s\n",
            "epoch 57 | loss: 0.32582 | train_logloss: 0.31916 | train_accuracy: 0.87847 | valid_logloss: 0.3293  | valid_accuracy: 0.87507 |  0:02:00s\n",
            "epoch 58 | loss: 0.32467 | train_logloss: 0.33451 | train_accuracy: 0.87117 | valid_logloss: 0.34933 | valid_accuracy: 0.86673 |  0:02:02s\n",
            "epoch 59 | loss: 0.33102 | train_logloss: 0.32348 | train_accuracy: 0.87717 | valid_logloss: 0.33785 | valid_accuracy: 0.873   |  0:02:04s\n",
            "epoch 60 | loss: 0.32731 | train_logloss: 0.32795 | train_accuracy: 0.87493 | valid_logloss: 0.33929 | valid_accuracy: 0.8715  |  0:02:06s\n",
            "epoch 61 | loss: 0.33617 | train_logloss: 0.32658 | train_accuracy: 0.877   | valid_logloss: 0.33882 | valid_accuracy: 0.87263 |  0:02:08s\n",
            "epoch 62 | loss: 0.32664 | train_logloss: 0.31847 | train_accuracy: 0.87857 | valid_logloss: 0.33201 | valid_accuracy: 0.87457 |  0:02:10s\n",
            "epoch 63 | loss: 0.32063 | train_logloss: 0.31143 | train_accuracy: 0.8811  | valid_logloss: 0.32449 | valid_accuracy: 0.87733 |  0:02:12s\n",
            "epoch 64 | loss: 0.32056 | train_logloss: 0.30953 | train_accuracy: 0.88197 | valid_logloss: 0.32281 | valid_accuracy: 0.8778  |  0:02:14s\n",
            "epoch 65 | loss: 0.31719 | train_logloss: 0.31554 | train_accuracy: 0.87957 | valid_logloss: 0.32795 | valid_accuracy: 0.87697 |  0:02:16s\n",
            "epoch 66 | loss: 0.32752 | train_logloss: 0.36298 | train_accuracy: 0.86153 | valid_logloss: 0.36992 | valid_accuracy: 0.86037 |  0:02:18s\n",
            "epoch 67 | loss: 0.34035 | train_logloss: 0.32633 | train_accuracy: 0.8742  | valid_logloss: 0.33685 | valid_accuracy: 0.87483 |  0:02:20s\n",
            "epoch 68 | loss: 0.32298 | train_logloss: 0.32032 | train_accuracy: 0.87837 | valid_logloss: 0.33164 | valid_accuracy: 0.8775  |  0:02:22s\n",
            "epoch 69 | loss: 0.32361 | train_logloss: 0.31667 | train_accuracy: 0.88057 | valid_logloss: 0.32589 | valid_accuracy: 0.87807 |  0:02:24s\n",
            "epoch 70 | loss: 0.31694 | train_logloss: 0.30773 | train_accuracy: 0.88093 | valid_logloss: 0.31684 | valid_accuracy: 0.88077 |  0:02:26s\n",
            "epoch 71 | loss: 0.31268 | train_logloss: 0.30908 | train_accuracy: 0.8823  | valid_logloss: 0.31715 | valid_accuracy: 0.8799  |  0:02:28s\n",
            "epoch 72 | loss: 0.31259 | train_logloss: 0.30612 | train_accuracy: 0.88053 | valid_logloss: 0.31917 | valid_accuracy: 0.87793 |  0:02:30s\n",
            "epoch 73 | loss: 0.30778 | train_logloss: 0.3011  | train_accuracy: 0.88453 | valid_logloss: 0.31364 | valid_accuracy: 0.8811  |  0:02:32s\n",
            "epoch 74 | loss: 0.30228 | train_logloss: 0.29713 | train_accuracy: 0.88537 | valid_logloss: 0.30912 | valid_accuracy: 0.88193 |  0:02:34s\n",
            "epoch 75 | loss: 0.30166 | train_logloss: 0.29897 | train_accuracy: 0.8848  | valid_logloss: 0.3109  | valid_accuracy: 0.88263 |  0:02:36s\n",
            "epoch 76 | loss: 0.30514 | train_logloss: 0.30238 | train_accuracy: 0.88397 | valid_logloss: 0.31618 | valid_accuracy: 0.88063 |  0:02:38s\n",
            "epoch 77 | loss: 0.30687 | train_logloss: 0.30146 | train_accuracy: 0.88403 | valid_logloss: 0.31432 | valid_accuracy: 0.87963 |  0:02:41s\n",
            "epoch 78 | loss: 0.30118 | train_logloss: 0.294   | train_accuracy: 0.88757 | valid_logloss: 0.30804 | valid_accuracy: 0.88373 |  0:02:42s\n",
            "epoch 79 | loss: 0.30063 | train_logloss: 0.29697 | train_accuracy: 0.88603 | valid_logloss: 0.31109 | valid_accuracy: 0.88293 |  0:02:45s\n",
            "epoch 80 | loss: 0.29783 | train_logloss: 0.29716 | train_accuracy: 0.8841  | valid_logloss: 0.31014 | valid_accuracy: 0.8827  |  0:02:47s\n",
            "epoch 81 | loss: 0.29859 | train_logloss: 0.29616 | train_accuracy: 0.88483 | valid_logloss: 0.31123 | valid_accuracy: 0.88183 |  0:02:49s\n",
            "epoch 82 | loss: 0.29977 | train_logloss: 0.29965 | train_accuracy: 0.88437 | valid_logloss: 0.31709 | valid_accuracy: 0.88147 |  0:02:51s\n",
            "epoch 83 | loss: 0.30154 | train_logloss: 0.2972  | train_accuracy: 0.88537 | valid_logloss: 0.31163 | valid_accuracy: 0.88233 |  0:02:53s\n",
            "epoch 84 | loss: 0.30465 | train_logloss: 0.29776 | train_accuracy: 0.88693 | valid_logloss: 0.31187 | valid_accuracy: 0.8831  |  0:02:55s\n",
            "epoch 85 | loss: 0.29823 | train_logloss: 0.30347 | train_accuracy: 0.8849  | valid_logloss: 0.31903 | valid_accuracy: 0.88023 |  0:02:57s\n",
            "epoch 86 | loss: 0.31783 | train_logloss: 0.31415 | train_accuracy: 0.8813  | valid_logloss: 0.32569 | valid_accuracy: 0.87683 |  0:02:59s\n",
            "epoch 87 | loss: 0.3147  | train_logloss: 0.31408 | train_accuracy: 0.878   | valid_logloss: 0.32479 | valid_accuracy: 0.87543 |  0:03:01s\n",
            "epoch 88 | loss: 0.30986 | train_logloss: 0.30321 | train_accuracy: 0.88353 | valid_logloss: 0.31575 | valid_accuracy: 0.87987 |  0:03:03s\n",
            "epoch 89 | loss: 0.30653 | train_logloss: 0.29653 | train_accuracy: 0.8853  | valid_logloss: 0.3098  | valid_accuracy: 0.88097 |  0:03:05s\n",
            "epoch 90 | loss: 0.29841 | train_logloss: 0.30086 | train_accuracy: 0.8864  | valid_logloss: 0.31523 | valid_accuracy: 0.8836  |  0:03:07s\n",
            "epoch 91 | loss: 0.30355 | train_logloss: 0.29664 | train_accuracy: 0.88593 | valid_logloss: 0.31054 | valid_accuracy: 0.88127 |  0:03:09s\n",
            "epoch 92 | loss: 0.29839 | train_logloss: 0.29472 | train_accuracy: 0.88503 | valid_logloss: 0.30962 | valid_accuracy: 0.88027 |  0:03:11s\n",
            "epoch 93 | loss: 0.29636 | train_logloss: 0.29552 | train_accuracy: 0.88707 | valid_logloss: 0.31089 | valid_accuracy: 0.88197 |  0:03:13s\n",
            "epoch 94 | loss: 0.29494 | train_logloss: 0.28877 | train_accuracy: 0.88867 | valid_logloss: 0.30618 | valid_accuracy: 0.88393 |  0:03:15s\n",
            "epoch 95 | loss: 0.29462 | train_logloss: 0.29225 | train_accuracy: 0.88807 | valid_logloss: 0.3087  | valid_accuracy: 0.88513 |  0:03:17s\n",
            "epoch 96 | loss: 0.29659 | train_logloss: 0.29097 | train_accuracy: 0.88827 | valid_logloss: 0.30521 | valid_accuracy: 0.885   |  0:03:19s\n",
            "epoch 97 | loss: 0.2951  | train_logloss: 0.28936 | train_accuracy: 0.8885  | valid_logloss: 0.30325 | valid_accuracy: 0.8861  |  0:03:21s\n",
            "epoch 98 | loss: 0.2923  | train_logloss: 0.28761 | train_accuracy: 0.88833 | valid_logloss: 0.30374 | valid_accuracy: 0.88543 |  0:03:23s\n",
            "epoch 99 | loss: 0.29853 | train_logloss: 0.29645 | train_accuracy: 0.88343 | valid_logloss: 0.30816 | valid_accuracy: 0.8824  |  0:03:25s\n",
            "epoch 100| loss: 0.29969 | train_logloss: 0.28781 | train_accuracy: 0.88757 | valid_logloss: 0.30166 | valid_accuracy: 0.8847  |  0:03:27s\n",
            "epoch 101| loss: 0.29417 | train_logloss: 0.29253 | train_accuracy: 0.88803 | valid_logloss: 0.30483 | valid_accuracy: 0.8845  |  0:03:29s\n",
            "epoch 102| loss: 0.2961  | train_logloss: 0.2885  | train_accuracy: 0.88847 | valid_logloss: 0.30063 | valid_accuracy: 0.88543 |  0:03:31s\n",
            "epoch 103| loss: 0.2927  | train_logloss: 0.28873 | train_accuracy: 0.88853 | valid_logloss: 0.30264 | valid_accuracy: 0.88657 |  0:03:33s\n",
            "epoch 104| loss: 0.28977 | train_logloss: 0.28468 | train_accuracy: 0.88973 | valid_logloss: 0.30263 | valid_accuracy: 0.88433 |  0:03:35s\n",
            "epoch 105| loss: 0.28876 | train_logloss: 0.28477 | train_accuracy: 0.88967 | valid_logloss: 0.30109 | valid_accuracy: 0.88523 |  0:03:37s\n",
            "epoch 106| loss: 0.28888 | train_logloss: 0.28333 | train_accuracy: 0.89023 | valid_logloss: 0.30089 | valid_accuracy: 0.8855  |  0:03:39s\n",
            "epoch 107| loss: 0.28863 | train_logloss: 0.28617 | train_accuracy: 0.8891  | valid_logloss: 0.30199 | valid_accuracy: 0.8842  |  0:03:41s\n",
            "epoch 108| loss: 0.29166 | train_logloss: 0.28963 | train_accuracy: 0.89037 | valid_logloss: 0.31013 | valid_accuracy: 0.88483 |  0:03:43s\n",
            "epoch 109| loss: 0.29072 | train_logloss: 0.28823 | train_accuracy: 0.89087 | valid_logloss: 0.30807 | valid_accuracy: 0.886   |  0:03:45s\n",
            "epoch 110| loss: 0.2899  | train_logloss: 0.29326 | train_accuracy: 0.88747 | valid_logloss: 0.3089  | valid_accuracy: 0.88283 |  0:03:47s\n",
            "epoch 111| loss: 0.29537 | train_logloss: 0.28919 | train_accuracy: 0.89003 | valid_logloss: 0.30315 | valid_accuracy: 0.88627 |  0:03:49s\n",
            "epoch 112| loss: 0.29401 | train_logloss: 0.29495 | train_accuracy: 0.88617 | valid_logloss: 0.30834 | valid_accuracy: 0.88313 |  0:03:51s\n",
            "epoch 113| loss: 0.2947  | train_logloss: 0.28594 | train_accuracy: 0.89043 | valid_logloss: 0.29893 | valid_accuracy: 0.8872  |  0:03:53s\n",
            "epoch 114| loss: 0.29162 | train_logloss: 0.2856  | train_accuracy: 0.88983 | valid_logloss: 0.30138 | valid_accuracy: 0.88673 |  0:03:55s\n",
            "epoch 115| loss: 0.29783 | train_logloss: 0.29854 | train_accuracy: 0.88473 | valid_logloss: 0.31239 | valid_accuracy: 0.88163 |  0:03:57s\n",
            "epoch 116| loss: 0.30822 | train_logloss: 0.30237 | train_accuracy: 0.88513 | valid_logloss: 0.31138 | valid_accuracy: 0.88223 |  0:03:59s\n",
            "epoch 117| loss: 0.30385 | train_logloss: 0.29642 | train_accuracy: 0.88597 | valid_logloss: 0.31085 | valid_accuracy: 0.8825  |  0:04:01s\n",
            "epoch 118| loss: 0.29666 | train_logloss: 0.29031 | train_accuracy: 0.88833 | valid_logloss: 0.3052  | valid_accuracy: 0.884   |  0:04:03s\n",
            "epoch 119| loss: 0.29436 | train_logloss: 0.28863 | train_accuracy: 0.89103 | valid_logloss: 0.30378 | valid_accuracy: 0.88607 |  0:04:05s\n",
            "epoch 120| loss: 0.29266 | train_logloss: 0.28664 | train_accuracy: 0.8898  | valid_logloss: 0.30107 | valid_accuracy: 0.88673 |  0:04:07s\n",
            "epoch 121| loss: 0.29158 | train_logloss: 0.2887  | train_accuracy: 0.88823 | valid_logloss: 0.30416 | valid_accuracy: 0.88443 |  0:04:09s\n",
            "epoch 122| loss: 0.28949 | train_logloss: 0.28323 | train_accuracy: 0.8927  | valid_logloss: 0.30021 | valid_accuracy: 0.88617 |  0:04:11s\n",
            "epoch 123| loss: 0.29078 | train_logloss: 0.28858 | train_accuracy: 0.89043 | valid_logloss: 0.30491 | valid_accuracy: 0.88483 |  0:04:13s\n",
            "epoch 124| loss: 0.28991 | train_logloss: 0.30816 | train_accuracy: 0.88403 | valid_logloss: 0.32276 | valid_accuracy: 0.88    |  0:04:15s\n",
            "epoch 125| loss: 0.29713 | train_logloss: 0.28751 | train_accuracy: 0.88977 | valid_logloss: 0.30317 | valid_accuracy: 0.8841  |  0:04:17s\n",
            "epoch 126| loss: 0.2915  | train_logloss: 0.28604 | train_accuracy: 0.8908  | valid_logloss: 0.30158 | valid_accuracy: 0.88463 |  0:04:19s\n",
            "epoch 127| loss: 0.2886  | train_logloss: 0.28589 | train_accuracy: 0.88953 | valid_logloss: 0.30317 | valid_accuracy: 0.88403 |  0:04:21s\n",
            "epoch 128| loss: 0.28848 | train_logloss: 0.2851  | train_accuracy: 0.89143 | valid_logloss: 0.30311 | valid_accuracy: 0.8843  |  0:04:23s\n",
            "epoch 129| loss: 0.2903  | train_logloss: 0.28506 | train_accuracy: 0.88873 | valid_logloss: 0.30325 | valid_accuracy: 0.88343 |  0:04:25s\n",
            "epoch 130| loss: 0.2894  | train_logloss: 0.28965 | train_accuracy: 0.89057 | valid_logloss: 0.30727 | valid_accuracy: 0.8842  |  0:04:27s\n",
            "epoch 131| loss: 0.28762 | train_logloss: 0.28461 | train_accuracy: 0.89223 | valid_logloss: 0.30297 | valid_accuracy: 0.8849  |  0:04:29s\n",
            "epoch 132| loss: 0.285   | train_logloss: 0.27961 | train_accuracy: 0.89263 | valid_logloss: 0.29898 | valid_accuracy: 0.88577 |  0:04:31s\n",
            "epoch 133| loss: 0.2856  | train_logloss: 0.28006 | train_accuracy: 0.89123 | valid_logloss: 0.29784 | valid_accuracy: 0.88593 |  0:04:33s\n",
            "epoch 134| loss: 0.28631 | train_logloss: 0.27796 | train_accuracy: 0.8936  | valid_logloss: 0.29582 | valid_accuracy: 0.88547 |  0:04:35s\n",
            "epoch 135| loss: 0.28502 | train_logloss: 0.28171 | train_accuracy: 0.89147 | valid_logloss: 0.30053 | valid_accuracy: 0.8871  |  0:04:37s\n",
            "epoch 136| loss: 0.28233 | train_logloss: 0.28323 | train_accuracy: 0.8912  | valid_logloss: 0.29975 | valid_accuracy: 0.88393 |  0:04:39s\n",
            "epoch 137| loss: 0.28947 | train_logloss: 0.28355 | train_accuracy: 0.89027 | valid_logloss: 0.30281 | valid_accuracy: 0.88617 |  0:04:41s\n",
            "epoch 138| loss: 0.29231 | train_logloss: 0.29651 | train_accuracy: 0.88693 | valid_logloss: 0.31217 | valid_accuracy: 0.8844  |  0:04:43s\n",
            "epoch 139| loss: 0.29671 | train_logloss: 0.29588 | train_accuracy: 0.88577 | valid_logloss: 0.31043 | valid_accuracy: 0.88387 |  0:04:45s\n",
            "epoch 140| loss: 0.29229 | train_logloss: 0.28555 | train_accuracy: 0.88843 | valid_logloss: 0.30152 | valid_accuracy: 0.8868  |  0:04:47s\n",
            "epoch 141| loss: 0.28667 | train_logloss: 0.28253 | train_accuracy: 0.8922  | valid_logloss: 0.30055 | valid_accuracy: 0.88567 |  0:04:49s\n",
            "epoch 142| loss: 0.2838  | train_logloss: 0.2798  | train_accuracy: 0.89207 | valid_logloss: 0.29761 | valid_accuracy: 0.88663 |  0:04:51s\n",
            "epoch 143| loss: 0.28263 | train_logloss: 0.27713 | train_accuracy: 0.8931  | valid_logloss: 0.29762 | valid_accuracy: 0.88773 |  0:04:53s\n",
            "epoch 144| loss: 0.29312 | train_logloss: 0.29438 | train_accuracy: 0.88807 | valid_logloss: 0.30979 | valid_accuracy: 0.88223 |  0:04:55s\n",
            "epoch 145| loss: 0.29435 | train_logloss: 0.29418 | train_accuracy: 0.88613 | valid_logloss: 0.31456 | valid_accuracy: 0.87967 |  0:04:57s\n",
            "epoch 146| loss: 0.3019  | train_logloss: 0.29062 | train_accuracy: 0.88933 | valid_logloss: 0.30791 | valid_accuracy: 0.8841  |  0:04:59s\n",
            "epoch 147| loss: 0.29219 | train_logloss: 0.28273 | train_accuracy: 0.89143 | valid_logloss: 0.30019 | valid_accuracy: 0.88597 |  0:05:01s\n",
            "epoch 148| loss: 0.28887 | train_logloss: 0.27913 | train_accuracy: 0.8936  | valid_logloss: 0.29889 | valid_accuracy: 0.88717 |  0:05:03s\n",
            "epoch 149| loss: 0.28628 | train_logloss: 0.2815  | train_accuracy: 0.8928  | valid_logloss: 0.30365 | valid_accuracy: 0.88627 |  0:05:05s\n",
            "epoch 150| loss: 0.29148 | train_logloss: 0.29523 | train_accuracy: 0.8854  | valid_logloss: 0.31148 | valid_accuracy: 0.88047 |  0:05:07s\n",
            "epoch 151| loss: 0.29156 | train_logloss: 0.28685 | train_accuracy: 0.88903 | valid_logloss: 0.30559 | valid_accuracy: 0.88477 |  0:05:09s\n",
            "epoch 152| loss: 0.28895 | train_logloss: 0.28363 | train_accuracy: 0.8912  | valid_logloss: 0.30186 | valid_accuracy: 0.88377 |  0:05:11s\n",
            "epoch 153| loss: 0.28358 | train_logloss: 0.27792 | train_accuracy: 0.89273 | valid_logloss: 0.29818 | valid_accuracy: 0.8862  |  0:05:13s\n",
            "epoch 154| loss: 0.28502 | train_logloss: 0.28191 | train_accuracy: 0.89177 | valid_logloss: 0.30061 | valid_accuracy: 0.88553 |  0:05:15s\n",
            "epoch 155| loss: 0.2873  | train_logloss: 0.28285 | train_accuracy: 0.89017 | valid_logloss: 0.30119 | valid_accuracy: 0.886   |  0:05:17s\n",
            "epoch 156| loss: 0.2916  | train_logloss: 0.28573 | train_accuracy: 0.89197 | valid_logloss: 0.30249 | valid_accuracy: 0.886   |  0:05:19s\n",
            "epoch 157| loss: 0.29098 | train_logloss: 0.2845  | train_accuracy: 0.89053 | valid_logloss: 0.30209 | valid_accuracy: 0.8862  |  0:05:21s\n",
            "epoch 158| loss: 0.28928 | train_logloss: 0.28452 | train_accuracy: 0.8921  | valid_logloss: 0.30025 | valid_accuracy: 0.8885  |  0:05:23s\n",
            "epoch 159| loss: 0.28896 | train_logloss: 0.28423 | train_accuracy: 0.89053 | valid_logloss: 0.30158 | valid_accuracy: 0.8856  |  0:05:25s\n",
            "epoch 160| loss: 0.28667 | train_logloss: 0.27949 | train_accuracy: 0.89277 | valid_logloss: 0.29901 | valid_accuracy: 0.88813 |  0:05:27s\n",
            "epoch 161| loss: 0.28757 | train_logloss: 0.2874  | train_accuracy: 0.88827 | valid_logloss: 0.30536 | valid_accuracy: 0.88547 |  0:05:29s\n",
            "epoch 162| loss: 0.29313 | train_logloss: 0.29471 | train_accuracy: 0.8865  | valid_logloss: 0.31204 | valid_accuracy: 0.88117 |  0:05:31s\n",
            "epoch 163| loss: 0.29558 | train_logloss: 0.28574 | train_accuracy: 0.89093 | valid_logloss: 0.30555 | valid_accuracy: 0.8851  |  0:05:33s\n",
            "epoch 164| loss: 0.28722 | train_logloss: 0.27993 | train_accuracy: 0.892   | valid_logloss: 0.30142 | valid_accuracy: 0.88607 |  0:05:35s\n",
            "epoch 165| loss: 0.28414 | train_logloss: 0.27833 | train_accuracy: 0.8926  | valid_logloss: 0.29729 | valid_accuracy: 0.88927 |  0:05:37s\n",
            "epoch 166| loss: 0.28333 | train_logloss: 0.27734 | train_accuracy: 0.89387 | valid_logloss: 0.30068 | valid_accuracy: 0.88723 |  0:05:39s\n",
            "epoch 167| loss: 0.28187 | train_logloss: 0.2782  | train_accuracy: 0.89287 | valid_logloss: 0.30147 | valid_accuracy: 0.88587 |  0:05:41s\n",
            "epoch 168| loss: 0.28429 | train_logloss: 0.28651 | train_accuracy: 0.89063 | valid_logloss: 0.30885 | valid_accuracy: 0.88433 |  0:05:43s\n",
            "epoch 169| loss: 0.29058 | train_logloss: 0.2813  | train_accuracy: 0.8926  | valid_logloss: 0.30253 | valid_accuracy: 0.8862  |  0:05:45s\n",
            "epoch 170| loss: 0.28234 | train_logloss: 0.27975 | train_accuracy: 0.8912  | valid_logloss: 0.29938 | valid_accuracy: 0.8866  |  0:05:47s\n",
            "epoch 171| loss: 0.28526 | train_logloss: 0.28394 | train_accuracy: 0.89347 | valid_logloss: 0.29788 | valid_accuracy: 0.88793 |  0:05:49s\n",
            "epoch 172| loss: 0.2855  | train_logloss: 0.28728 | train_accuracy: 0.8917  | valid_logloss: 0.30292 | valid_accuracy: 0.8862  |  0:05:51s\n",
            "epoch 173| loss: 0.28274 | train_logloss: 0.27901 | train_accuracy: 0.8943  | valid_logloss: 0.29868 | valid_accuracy: 0.8879  |  0:05:53s\n",
            "epoch 174| loss: 0.28248 | train_logloss: 0.27409 | train_accuracy: 0.89443 | valid_logloss: 0.29338 | valid_accuracy: 0.8893  |  0:05:55s\n",
            "epoch 175| loss: 0.28451 | train_logloss: 0.28231 | train_accuracy: 0.89313 | valid_logloss: 0.30192 | valid_accuracy: 0.88703 |  0:05:57s\n",
            "epoch 176| loss: 0.28208 | train_logloss: 0.27588 | train_accuracy: 0.89387 | valid_logloss: 0.29491 | valid_accuracy: 0.88873 |  0:05:59s\n",
            "epoch 177| loss: 0.28024 | train_logloss: 0.27356 | train_accuracy: 0.89443 | valid_logloss: 0.29585 | valid_accuracy: 0.8881  |  0:06:01s\n",
            "epoch 178| loss: 0.27697 | train_logloss: 0.27657 | train_accuracy: 0.89183 | valid_logloss: 0.29854 | valid_accuracy: 0.88857 |  0:06:03s\n",
            "epoch 179| loss: 0.27632 | train_logloss: 0.27165 | train_accuracy: 0.89497 | valid_logloss: 0.29381 | valid_accuracy: 0.8889  |  0:06:05s\n",
            "epoch 180| loss: 0.27514 | train_logloss: 0.27138 | train_accuracy: 0.89543 | valid_logloss: 0.29585 | valid_accuracy: 0.88807 |  0:06:07s\n",
            "epoch 181| loss: 0.27933 | train_logloss: 0.28085 | train_accuracy: 0.8904  | valid_logloss: 0.30109 | valid_accuracy: 0.88693 |  0:06:09s\n",
            "epoch 182| loss: 0.28048 | train_logloss: 0.2739  | train_accuracy: 0.89457 | valid_logloss: 0.29348 | valid_accuracy: 0.8901  |  0:06:11s\n",
            "epoch 183| loss: 0.27598 | train_logloss: 0.27392 | train_accuracy: 0.89427 | valid_logloss: 0.29632 | valid_accuracy: 0.88907 |  0:06:13s\n",
            "epoch 184| loss: 0.27808 | train_logloss: 0.27132 | train_accuracy: 0.89533 | valid_logloss: 0.29501 | valid_accuracy: 0.8883  |  0:06:15s\n",
            "epoch 185| loss: 0.27451 | train_logloss: 0.27087 | train_accuracy: 0.8971  | valid_logloss: 0.29424 | valid_accuracy: 0.89063 |  0:06:17s\n",
            "epoch 186| loss: 0.27738 | train_logloss: 0.26597 | train_accuracy: 0.8973  | valid_logloss: 0.29087 | valid_accuracy: 0.89043 |  0:06:19s\n",
            "epoch 187| loss: 0.27284 | train_logloss: 0.26481 | train_accuracy: 0.89733 | valid_logloss: 0.29195 | valid_accuracy: 0.8901  |  0:06:21s\n",
            "epoch 188| loss: 0.27198 | train_logloss: 0.26528 | train_accuracy: 0.89777 | valid_logloss: 0.29029 | valid_accuracy: 0.89147 |  0:06:23s\n",
            "epoch 189| loss: 0.26994 | train_logloss: 0.27022 | train_accuracy: 0.8957  | valid_logloss: 0.29573 | valid_accuracy: 0.8893  |  0:06:25s\n",
            "epoch 190| loss: 0.27016 | train_logloss: 0.26559 | train_accuracy: 0.8973  | valid_logloss: 0.29195 | valid_accuracy: 0.89007 |  0:06:27s\n",
            "epoch 191| loss: 0.27049 | train_logloss: 0.26866 | train_accuracy: 0.89663 | valid_logloss: 0.29771 | valid_accuracy: 0.88677 |  0:06:29s\n",
            "epoch 192| loss: 0.27184 | train_logloss: 0.27726 | train_accuracy: 0.89497 | valid_logloss: 0.3085  | valid_accuracy: 0.88393 |  0:06:31s\n",
            "epoch 193| loss: 0.28232 | train_logloss: 0.27191 | train_accuracy: 0.8955  | valid_logloss: 0.29736 | valid_accuracy: 0.88873 |  0:06:33s\n",
            "epoch 194| loss: 0.27718 | train_logloss: 0.26705 | train_accuracy: 0.89727 | valid_logloss: 0.29394 | valid_accuracy: 0.88967 |  0:06:35s\n",
            "epoch 195| loss: 0.2862  | train_logloss: 0.32688 | train_accuracy: 0.88123 | valid_logloss: 0.33832 | valid_accuracy: 0.87627 |  0:06:37s\n",
            "epoch 196| loss: 0.30374 | train_logloss: 0.28732 | train_accuracy: 0.89123 | valid_logloss: 0.30627 | valid_accuracy: 0.88523 |  0:06:39s\n",
            "epoch 197| loss: 0.28761 | train_logloss: 0.28678 | train_accuracy: 0.89087 | valid_logloss: 0.30569 | valid_accuracy: 0.8858  |  0:06:41s\n",
            "epoch 198| loss: 0.28241 | train_logloss: 0.27526 | train_accuracy: 0.8935  | valid_logloss: 0.2954  | valid_accuracy: 0.88847 |  0:06:43s\n",
            "epoch 199| loss: 0.28075 | train_logloss: 0.2743  | train_accuracy: 0.89553 | valid_logloss: 0.29774 | valid_accuracy: 0.88843 |  0:06:45s\n",
            "epoch 200| loss: 0.2769  | train_logloss: 0.27187 | train_accuracy: 0.8954  | valid_logloss: 0.29708 | valid_accuracy: 0.88913 |  0:06:47s\n",
            "epoch 201| loss: 0.27565 | train_logloss: 0.27795 | train_accuracy: 0.8929  | valid_logloss: 0.30414 | valid_accuracy: 0.88657 |  0:06:49s\n",
            "epoch 202| loss: 0.28035 | train_logloss: 0.27685 | train_accuracy: 0.8936  | valid_logloss: 0.29509 | valid_accuracy: 0.8874  |  0:06:51s\n",
            "epoch 203| loss: 0.27828 | train_logloss: 0.26967 | train_accuracy: 0.89667 | valid_logloss: 0.293   | valid_accuracy: 0.88973 |  0:06:53s\n",
            "epoch 204| loss: 0.27842 | train_logloss: 0.27066 | train_accuracy: 0.8955  | valid_logloss: 0.29366 | valid_accuracy: 0.8893  |  0:06:55s\n",
            "epoch 205| loss: 0.27532 | train_logloss: 0.269   | train_accuracy: 0.89553 | valid_logloss: 0.29426 | valid_accuracy: 0.88893 |  0:06:57s\n",
            "epoch 206| loss: 0.2722  | train_logloss: 0.27194 | train_accuracy: 0.89317 | valid_logloss: 0.29736 | valid_accuracy: 0.88687 |  0:06:59s\n",
            "epoch 207| loss: 0.27339 | train_logloss: 0.26651 | train_accuracy: 0.89797 | valid_logloss: 0.29347 | valid_accuracy: 0.8882  |  0:07:01s\n",
            "epoch 208| loss: 0.27339 | train_logloss: 0.26685 | train_accuracy: 0.8977  | valid_logloss: 0.29235 | valid_accuracy: 0.89    |  0:07:03s\n",
            "epoch 209| loss: 0.27029 | train_logloss: 0.2641  | train_accuracy: 0.89917 | valid_logloss: 0.29264 | valid_accuracy: 0.88963 |  0:07:05s\n",
            "epoch 210| loss: 0.26937 | train_logloss: 0.26392 | train_accuracy: 0.89783 | valid_logloss: 0.29663 | valid_accuracy: 0.8887  |  0:07:07s\n",
            "epoch 211| loss: 0.26854 | train_logloss: 0.26281 | train_accuracy: 0.89843 | valid_logloss: 0.29376 | valid_accuracy: 0.88967 |  0:07:09s\n",
            "epoch 212| loss: 0.27452 | train_logloss: 0.27711 | train_accuracy: 0.89377 | valid_logloss: 0.30342 | valid_accuracy: 0.88673 |  0:07:11s\n",
            "epoch 213| loss: 0.27671 | train_logloss: 0.26866 | train_accuracy: 0.89647 | valid_logloss: 0.29479 | valid_accuracy: 0.8882  |  0:07:13s\n",
            "epoch 214| loss: 0.27195 | train_logloss: 0.26481 | train_accuracy: 0.8975  | valid_logloss: 0.29176 | valid_accuracy: 0.88987 |  0:07:16s\n",
            "epoch 215| loss: 0.26989 | train_logloss: 0.26513 | train_accuracy: 0.8972  | valid_logloss: 0.29397 | valid_accuracy: 0.89093 |  0:07:17s\n",
            "epoch 216| loss: 0.27027 | train_logloss: 0.26362 | train_accuracy: 0.898   | valid_logloss: 0.29274 | valid_accuracy: 0.88963 |  0:07:20s\n",
            "epoch 217| loss: 0.26623 | train_logloss: 0.26164 | train_accuracy: 0.9001  | valid_logloss: 0.29202 | valid_accuracy: 0.89037 |  0:07:22s\n",
            "epoch 218| loss: 0.26571 | train_logloss: 0.26241 | train_accuracy: 0.8995  | valid_logloss: 0.29477 | valid_accuracy: 0.8896  |  0:07:24s\n",
            "epoch 219| loss: 0.26664 | train_logloss: 0.25976 | train_accuracy: 0.8995  | valid_logloss: 0.29294 | valid_accuracy: 0.88937 |  0:07:26s\n",
            "epoch 220| loss: 0.26711 | train_logloss: 0.26537 | train_accuracy: 0.89807 | valid_logloss: 0.29732 | valid_accuracy: 0.88917 |  0:07:28s\n",
            "epoch 221| loss: 0.26658 | train_logloss: 0.25995 | train_accuracy: 0.89917 | valid_logloss: 0.2916  | valid_accuracy: 0.89007 |  0:07:30s\n",
            "epoch 222| loss: 0.2673  | train_logloss: 0.26162 | train_accuracy: 0.89893 | valid_logloss: 0.2925  | valid_accuracy: 0.8905  |  0:07:32s\n",
            "epoch 223| loss: 0.26618 | train_logloss: 0.26324 | train_accuracy: 0.89887 | valid_logloss: 0.29778 | valid_accuracy: 0.88977 |  0:07:34s\n",
            "epoch 224| loss: 0.2652  | train_logloss: 0.26261 | train_accuracy: 0.8977  | valid_logloss: 0.29519 | valid_accuracy: 0.89053 |  0:07:36s\n",
            "epoch 225| loss: 0.26423 | train_logloss: 0.25686 | train_accuracy: 0.9009  | valid_logloss: 0.2933  | valid_accuracy: 0.8913  |  0:07:38s\n",
            "epoch 226| loss: 0.26273 | train_logloss: 0.25545 | train_accuracy: 0.90063 | valid_logloss: 0.29231 | valid_accuracy: 0.89043 |  0:07:40s\n",
            "epoch 227| loss: 0.26255 | train_logloss: 0.25584 | train_accuracy: 0.9005  | valid_logloss: 0.29217 | valid_accuracy: 0.89143 |  0:07:42s\n",
            "epoch 228| loss: 0.25956 | train_logloss: 0.25575 | train_accuracy: 0.9006  | valid_logloss: 0.28865 | valid_accuracy: 0.89107 |  0:07:44s\n",
            "epoch 229| loss: 0.25952 | train_logloss: 0.25504 | train_accuracy: 0.90107 | valid_logloss: 0.29357 | valid_accuracy: 0.89147 |  0:07:46s\n",
            "epoch 230| loss: 0.27308 | train_logloss: 0.27744 | train_accuracy: 0.8939  | valid_logloss: 0.30525 | valid_accuracy: 0.88583 |  0:07:48s\n",
            "epoch 231| loss: 0.2756  | train_logloss: 0.2654  | train_accuracy: 0.8977  | valid_logloss: 0.2954  | valid_accuracy: 0.8893  |  0:07:50s\n",
            "epoch 232| loss: 0.27109 | train_logloss: 0.26601 | train_accuracy: 0.89613 | valid_logloss: 0.2968  | valid_accuracy: 0.88777 |  0:07:52s\n",
            "epoch 233| loss: 0.26694 | train_logloss: 0.26158 | train_accuracy: 0.89923 | valid_logloss: 0.29497 | valid_accuracy: 0.89023 |  0:07:54s\n",
            "epoch 234| loss: 0.26703 | train_logloss: 0.26414 | train_accuracy: 0.89757 | valid_logloss: 0.29627 | valid_accuracy: 0.88967 |  0:07:56s\n",
            "epoch 235| loss: 0.26891 | train_logloss: 0.26247 | train_accuracy: 0.89753 | valid_logloss: 0.29674 | valid_accuracy: 0.88933 |  0:07:58s\n",
            "epoch 236| loss: 0.2648  | train_logloss: 0.25947 | train_accuracy: 0.89883 | valid_logloss: 0.2955  | valid_accuracy: 0.89013 |  0:08:00s\n",
            "epoch 237| loss: 0.26411 | train_logloss: 0.25461 | train_accuracy: 0.9011  | valid_logloss: 0.29302 | valid_accuracy: 0.892   |  0:08:02s\n",
            "epoch 238| loss: 0.26003 | train_logloss: 0.25766 | train_accuracy: 0.89867 | valid_logloss: 0.29744 | valid_accuracy: 0.88993 |  0:08:04s\n",
            "epoch 239| loss: 0.2601  | train_logloss: 0.25471 | train_accuracy: 0.9016  | valid_logloss: 0.29461 | valid_accuracy: 0.89117 |  0:08:06s\n",
            "epoch 240| loss: 0.26437 | train_logloss: 0.26948 | train_accuracy: 0.89597 | valid_logloss: 0.30102 | valid_accuracy: 0.8889  |  0:08:08s\n",
            "epoch 241| loss: 0.27138 | train_logloss: 0.2643  | train_accuracy: 0.8975  | valid_logloss: 0.2999  | valid_accuracy: 0.8884  |  0:08:10s\n",
            "epoch 242| loss: 0.26557 | train_logloss: 0.25649 | train_accuracy: 0.90027 | valid_logloss: 0.29486 | valid_accuracy: 0.8906  |  0:08:12s\n",
            "epoch 243| loss: 0.26536 | train_logloss: 0.25769 | train_accuracy: 0.89917 | valid_logloss: 0.29773 | valid_accuracy: 0.89033 |  0:08:14s\n",
            "epoch 244| loss: 0.26142 | train_logloss: 0.25467 | train_accuracy: 0.9007  | valid_logloss: 0.29352 | valid_accuracy: 0.8916  |  0:08:16s\n",
            "epoch 245| loss: 0.25737 | train_logloss: 0.25418 | train_accuracy: 0.90203 | valid_logloss: 0.29455 | valid_accuracy: 0.89157 |  0:08:18s\n",
            "epoch 246| loss: 0.25858 | train_logloss: 0.25286 | train_accuracy: 0.9009  | valid_logloss: 0.2943  | valid_accuracy: 0.89083 |  0:08:20s\n",
            "epoch 247| loss: 0.25952 | train_logloss: 0.25374 | train_accuracy: 0.90157 | valid_logloss: 0.29939 | valid_accuracy: 0.8915  |  0:08:22s\n",
            "epoch 248| loss: 0.25929 | train_logloss: 0.25598 | train_accuracy: 0.90067 | valid_logloss: 0.3003  | valid_accuracy: 0.88927 |  0:08:24s\n",
            "epoch 249| loss: 0.26206 | train_logloss: 0.25927 | train_accuracy: 0.89893 | valid_logloss: 0.30242 | valid_accuracy: 0.88867 |  0:08:26s\n",
            "epoch 250| loss: 0.25907 | train_logloss: 0.25544 | train_accuracy: 0.90027 | valid_logloss: 0.30002 | valid_accuracy: 0.88907 |  0:08:28s\n",
            "epoch 251| loss: 0.26633 | train_logloss: 0.27912 | train_accuracy: 0.89017 | valid_logloss: 0.31513 | valid_accuracy: 0.88313 |  0:08:30s\n",
            "epoch 252| loss: 0.28278 | train_logloss: 0.274   | train_accuracy: 0.89303 | valid_logloss: 0.30406 | valid_accuracy: 0.88817 |  0:08:32s\n",
            "epoch 253| loss: 0.28233 | train_logloss: 0.27218 | train_accuracy: 0.8955  | valid_logloss: 0.30228 | valid_accuracy: 0.88747 |  0:08:34s\n",
            "epoch 254| loss: 0.28067 | train_logloss: 0.27421 | train_accuracy: 0.89563 | valid_logloss: 0.29829 | valid_accuracy: 0.88707 |  0:08:36s\n",
            "epoch 255| loss: 0.27548 | train_logloss: 0.26786 | train_accuracy: 0.8971  | valid_logloss: 0.29646 | valid_accuracy: 0.88763 |  0:08:38s\n",
            "epoch 256| loss: 0.27237 | train_logloss: 0.26337 | train_accuracy: 0.8992  | valid_logloss: 0.2942  | valid_accuracy: 0.8889  |  0:08:40s\n",
            "epoch 257| loss: 0.26585 | train_logloss: 0.25792 | train_accuracy: 0.9008  | valid_logloss: 0.29311 | valid_accuracy: 0.89023 |  0:08:42s\n",
            "epoch 258| loss: 0.27488 | train_logloss: 0.28174 | train_accuracy: 0.8917  | valid_logloss: 0.30313 | valid_accuracy: 0.88617 |  0:08:44s\n",
            "epoch 259| loss: 0.27983 | train_logloss: 0.27409 | train_accuracy: 0.89557 | valid_logloss: 0.3007  | valid_accuracy: 0.88927 |  0:08:46s\n",
            "epoch 260| loss: 0.27451 | train_logloss: 0.27247 | train_accuracy: 0.89607 | valid_logloss: 0.29911 | valid_accuracy: 0.88793 |  0:08:48s\n",
            "epoch 261| loss: 0.27055 | train_logloss: 0.25954 | train_accuracy: 0.89933 | valid_logloss: 0.28966 | valid_accuracy: 0.8903  |  0:08:50s\n",
            "epoch 262| loss: 0.26646 | train_logloss: 0.26116 | train_accuracy: 0.8992  | valid_logloss: 0.29387 | valid_accuracy: 0.8899  |  0:08:52s\n",
            "epoch 263| loss: 0.26555 | train_logloss: 0.26226 | train_accuracy: 0.8992  | valid_logloss: 0.2944  | valid_accuracy: 0.89    |  0:08:54s\n",
            "epoch 264| loss: 0.26853 | train_logloss: 0.27316 | train_accuracy: 0.8965  | valid_logloss: 0.30094 | valid_accuracy: 0.8901  |  0:08:56s\n",
            "epoch 265| loss: 0.27209 | train_logloss: 0.26304 | train_accuracy: 0.8982  | valid_logloss: 0.29688 | valid_accuracy: 0.88857 |  0:08:58s\n",
            "epoch 266| loss: 0.26892 | train_logloss: 0.25988 | train_accuracy: 0.90077 | valid_logloss: 0.29097 | valid_accuracy: 0.89077 |  0:09:00s\n",
            "epoch 267| loss: 0.26533 | train_logloss: 0.25597 | train_accuracy: 0.90103 | valid_logloss: 0.29024 | valid_accuracy: 0.8907  |  0:09:02s\n",
            "epoch 268| loss: 0.2619  | train_logloss: 0.26325 | train_accuracy: 0.8982  | valid_logloss: 0.29668 | valid_accuracy: 0.88817 |  0:09:04s\n",
            "epoch 269| loss: 0.26179 | train_logloss: 0.2524  | train_accuracy: 0.9017  | valid_logloss: 0.28855 | valid_accuracy: 0.89113 |  0:09:06s\n",
            "epoch 270| loss: 0.25819 | train_logloss: 0.25315 | train_accuracy: 0.9014  | valid_logloss: 0.29278 | valid_accuracy: 0.89097 |  0:09:08s\n",
            "epoch 271| loss: 0.25938 | train_logloss: 0.25187 | train_accuracy: 0.9022  | valid_logloss: 0.29391 | valid_accuracy: 0.88997 |  0:09:10s\n",
            "epoch 272| loss: 0.2604  | train_logloss: 0.25152 | train_accuracy: 0.90283 | valid_logloss: 0.29234 | valid_accuracy: 0.89007 |  0:09:12s\n",
            "epoch 273| loss: 0.25584 | train_logloss: 0.25066 | train_accuracy: 0.9026  | valid_logloss: 0.28853 | valid_accuracy: 0.89167 |  0:09:14s\n",
            "epoch 274| loss: 0.25739 | train_logloss: 0.25001 | train_accuracy: 0.903   | valid_logloss: 0.2938  | valid_accuracy: 0.88987 |  0:09:16s\n",
            "epoch 275| loss: 0.25625 | train_logloss: 0.26212 | train_accuracy: 0.89917 | valid_logloss: 0.30536 | valid_accuracy: 0.88603 |  0:09:18s\n",
            "epoch 276| loss: 0.26663 | train_logloss: 0.26069 | train_accuracy: 0.8976  | valid_logloss: 0.30017 | valid_accuracy: 0.88707 |  0:09:20s\n",
            "epoch 277| loss: 0.26376 | train_logloss: 0.25722 | train_accuracy: 0.8994  | valid_logloss: 0.29478 | valid_accuracy: 0.88853 |  0:09:22s\n",
            "epoch 278| loss: 0.25891 | train_logloss: 0.25476 | train_accuracy: 0.90173 | valid_logloss: 0.29633 | valid_accuracy: 0.8892  |  0:09:24s\n",
            "epoch 279| loss: 0.25522 | train_logloss: 0.24765 | train_accuracy: 0.90377 | valid_logloss: 0.29184 | valid_accuracy: 0.88967 |  0:09:26s\n",
            "epoch 280| loss: 0.26783 | train_logloss: 0.27098 | train_accuracy: 0.8977  | valid_logloss: 0.30806 | valid_accuracy: 0.88457 |  0:09:28s\n",
            "epoch 281| loss: 0.2784  | train_logloss: 0.2828  | train_accuracy: 0.89387 | valid_logloss: 0.3106  | valid_accuracy: 0.8829  |  0:09:30s\n",
            "epoch 282| loss: 0.2904  | train_logloss: 0.27913 | train_accuracy: 0.895   | valid_logloss: 0.30259 | valid_accuracy: 0.88593 |  0:09:32s\n",
            "epoch 283| loss: 0.27844 | train_logloss: 0.27004 | train_accuracy: 0.8978  | valid_logloss: 0.29571 | valid_accuracy: 0.88807 |  0:09:34s\n",
            "epoch 284| loss: 0.27037 | train_logloss: 0.26244 | train_accuracy: 0.90007 | valid_logloss: 0.29262 | valid_accuracy: 0.88967 |  0:09:36s\n",
            "epoch 285| loss: 0.2664  | train_logloss: 0.25863 | train_accuracy: 0.90033 | valid_logloss: 0.2929  | valid_accuracy: 0.8898  |  0:09:38s\n",
            "epoch 286| loss: 0.26153 | train_logloss: 0.25482 | train_accuracy: 0.9013  | valid_logloss: 0.28829 | valid_accuracy: 0.8906  |  0:09:40s\n",
            "epoch 287| loss: 0.25943 | train_logloss: 0.25334 | train_accuracy: 0.90313 | valid_logloss: 0.2894  | valid_accuracy: 0.89153 |  0:09:42s\n",
            "\n",
            "Early stopping occurred at epoch 287 with best_epoch = 237 and best_valid_accuracy = 0.892\n",
            "Best weights from best epoch are automatically used!\n",
            "TN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz8TXSqYuxWr"
      },
      "source": [
        "  c1 = 9000\n",
        "  data_split = data_preparation(X, y, c=c1//3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = c1//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'colsample_bytree': 0.6437405148446416,\n",
        "    'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 10000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  print(t)\n",
        "\n",
        "  data_split = data_preparation(X, y, c=30000//3)\n",
        "\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = 30000//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #Accuracy\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('GB '+str(c1)+'   ')\n",
        "\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(gb, 'GB', Rows)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb_size'+str(c1)+'.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}