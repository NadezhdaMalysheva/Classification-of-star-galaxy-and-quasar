{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Эксперименты с гиперпараметрами",
      "provenance": [],
      "collapsed_sections": [
        "m0kPZJ5yOVbo",
        "N-hlXdD5xbL8",
        "pB1sVwYkEq9Z"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadezhdaMalysheva/projects/blob/main/Hyperparameters_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXFJs19Ks2y",
        "outputId": "852844e1-49ba-492f-ac1c-ebdf5d83149e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0kPZJ5yOVbo"
      },
      "source": [
        "# Подгружаем необходимые библиотеки "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtFLkOBsdnJR",
        "outputId": "0f08ef4f-3096-4f6c-f520-7811aa43b944"
      },
      "source": [
        "pip install pytorch-tabnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (0.22.2.post1)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLtYHFqwmod"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2lA0dgTwmib",
        "outputId": "e9367941-366f-40fa-ee8f-993031492334"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.offline as py\n",
        "color = sns.color_palette()\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.tools as tls\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVxxcXZGwmdb"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "from hyperopt import hp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
        "\n",
        "from time import time\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuBGRaaiDoJ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hlXdD5xbL8"
      },
      "source": [
        "#Загружаем данные, параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16jPxCCCBTwd"
      },
      "source": [
        "target = 'class'\n",
        "\n",
        "add_columns = ['subClass', 'objID', 'z', 'zErr', 'ra', 'dec']\n",
        "\n",
        "photo_columns = ['psfMag_u',\t'psfMag_g',\t'psfMag_r',\t'psfMag_i',\t'psfMag_z',\n",
        "                 'cModelMag_u',\t'cModelMag_g',\t'cModelMag_r',\t'cModelMag_i',\t'cModelMag_z']\n",
        "\n",
        "feature_columns = (\n",
        "    photo_columns + add_columns + [target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIwVGqf2C1KA"
      },
      "source": [
        "#df = joblib.load('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/df_agg.pkl')#pd.read_csv('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/df_agg.csv')\n",
        "#agr_feature = [x for x in df.columns if x not in feature_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yer4R5LMyP0m"
      },
      "source": [
        "def data_preparation(X, y, c=10000, test_size = 0.8):\n",
        "\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(X[y==1], y[y==1], test_size=test_size, random_state = 43)\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(X[y==2], y[y==2], test_size=test_size, random_state = 43)\n",
        "    X3_train, X3_test, y3_train, y3_test = train_test_split(X[y==3], y[y==3], test_size=test_size, random_state = 43)\n",
        "    \n",
        "    count = c\n",
        "    count1 = c\n",
        "\n",
        "    X_train, X_test = np.concatenate((X1_train[:count], X2_train[:count], X3_train[:count])), np.concatenate((X1_test[:count1], X2_test[:count1], X3_test[:count1]))\n",
        "    y_train, y_test = np.concatenate((y1_train[:count], y2_train[:count], y3_train[:count])), np.concatenate((y1_test[:count1], y2_test[:count1], y3_test[:count1]))\n",
        "    \n",
        "\n",
        "    return [X_train, X_test, y_train, y_test, X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test, X3_train, X3_test, y3_train, y3_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQX2MYFQ5Ebl"
      },
      "source": [
        "def scor(y_test, y_pred):\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "lgb_reg_params = {\n",
        "    'min_child_samples':hp.randint('min_child_samples', 50)+1,\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.9),\n",
        "    'num_leaves' :      hp.randint('num_leaves', 100)+10,\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 0.001, 0.99),\n",
        "    'n_estimators':     1000\n",
        "}\n",
        "lgb_fit_params = {\n",
        "    'early_stopping_rounds': 50,\n",
        "    'verbose': False\n",
        "}\n",
        "lgb_para = dict()\n",
        "lgb_para['reg_params'] = lgb_reg_params\n",
        "lgb_para['fit_params'] = lgb_fit_params\n",
        "lgb_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "\n",
        "rf_reg_params = {\n",
        "    'min_samples_leaf': hp.randint('min_samples_leaf', 20)+1,\n",
        "    'min_samples_split':hp.uniform('min_samples_split', 0.001, 0.1),\n",
        "    #'max_features':     hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
        "    #'learning_rate':    hp.uniform('learning_rate', 0.001, 0.1),\n",
        "    'n_estimators':     hp.randint('n_estimators', 800)+100\n",
        "}\n",
        "rf_fit_params = {\n",
        "}\n",
        "rf_para = dict()\n",
        "rf_para['reg_params'] = rf_reg_params\n",
        "rf_para['fit_params'] = rf_fit_params\n",
        "rf_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "tabnet_reg_params = {\n",
        "    'n_d' :              8,\n",
        "    'n_a' :              8,\n",
        "    'n_shared':          hp.randint('n_shared', 3)+1,\n",
        "    'n_independent':     hp.randint('n_independent', 3)+1,\n",
        "    'n_steps' :          hp.randint('n_steps', 3)+3,\n",
        "    'gamma' :            hp.uniform('gamma', 1.0, 3.0),\n",
        "    'lambda_sparse' :    hp.uniform('lambda_sparse', 0.0, 0.01),\n",
        "    'optimizer_params' : dict(lr=2e-2),\n",
        "    'scheduler_params' : {\"step_size\":200, \"gamma\":0.95},\n",
        "    'scheduler_fn' :     torch.optim.lr_scheduler.StepLR,\n",
        "    'mask_type' :       'entmax'\n",
        "}\n",
        "\n",
        "tabnet_fit_params = {\n",
        "    'max_epochs' : 2000, \n",
        "    'patience' : 20,\n",
        "    'batch_size' : 1024,\n",
        "    'virtual_batch_size' : 128,\n",
        "    'num_workers' : 0,\n",
        "    'weights' : 1,\n",
        "    'drop_last' : False,\n",
        "    #'from_unsupervised' : unsupervised_model\n",
        "}\n",
        "tabnet_para = dict()\n",
        "tabnet_para['reg_params'] = tabnet_reg_params\n",
        "tabnet_para['fit_params'] = tabnet_fit_params\n",
        "tabnet_para['score'] = lambda y, pred: -accuracy_score(y, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWMNPpeokzaC"
      },
      "source": [
        "class HPOpt(object):\n",
        "\n",
        "    def __init__(self, X, y, cv=3):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.cv = cv\n",
        "        #print('init')\n",
        "\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        #print('in process')\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            #print('try')\n",
        "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
        "            #print('i can')\n",
        "        except Exception as e:\n",
        "            print({'status': STATUS_FAIL,\n",
        "                    'exception': str(e)})\n",
        "            return {'status': STATUS_FAIL,\n",
        "                    'exception': str(e)}\n",
        "        return result, trials\n",
        "\n",
        "    def rf_reg(self, para):\n",
        "        reg = RandomForestClassifier(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def lgb_reg(self, para):\n",
        "        reg = lgb.LGBMClassifier(**para['reg_params'])\n",
        "        if self.cv>1:\n",
        "          return self.train_cv_gb(reg, para)\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def tabnet_reg(self, para):\n",
        "        #print('class')\n",
        "        reg = TabNetClassifier(**para['reg_params'])\n",
        "        #print('end class')\n",
        "        if self.cv>1:\n",
        "          return self.train_cv_tn(reg, para)\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "\n",
        "    def train_reg(self, reg, para):\n",
        "        if len(para['fit_params'])>0:\n",
        "            #print('start')\n",
        "            reg.fit(self.X, self.y,\n",
        "                  eval_set=[(self.X, self.y), (self.X, self.y)],\n",
        "                  **para['fit_params'])\n",
        "        else:\n",
        "            reg.fit(self.X, self.y)\n",
        "        pred = reg.predict(self.X)\n",
        "        loss = para['score'](self.y, pred)\n",
        "        return {'loss': loss, 'status': STATUS_OK}\n",
        "\n",
        "    def train_cv_tn(self, reg, para):\n",
        "        kf = KFold(n_splits=self.cv, shuffle=False)\n",
        "        loss = 0 \n",
        "        for train, test in kf.split(self.X):\n",
        "            #print('start', type(train[0]), type(test[0]), type(self.y[0]), type(self.X))\n",
        "            if len(para['fit_params'])>0:\n",
        "                reg = TabNetClassifier(**para['reg_params'])\n",
        "                reg.fit(self.X[train], self.y[train],\n",
        "                      eval_set=[(self.X[train], self.y[train]), (self.X[test], self.y[test])],\n",
        "                      **para['fit_params'])\n",
        "            else:\n",
        "                reg.fit(self.X[train], self.y[train])\n",
        "            #print('pred')\n",
        "            pred = reg.predict(self.X[test])\n",
        "            score = para['score'](self.y[test], pred)\n",
        "            loss += score\n",
        "\n",
        "        loss=loss/self.cv\n",
        "        return {'loss': loss, 'status': STATUS_OK}\n",
        "\n",
        "    def train_cv_gb(self, reg, para):\n",
        "        kf = KFold(n_splits=self.cv, shuffle=False)\n",
        "        loss = 0 \n",
        "        for train, test in kf.split(self.X):\n",
        "            #print('start', type(train[0]), type(test[0]), type(self.y[0]), type(self.X))\n",
        "            if len(para['fit_params'])>0:\n",
        "                reg = lgb.LGBMClassifier(**para['reg_params'])\n",
        "                reg.fit(self.X[train], self.y[train],\n",
        "                      eval_set=[(self.X[train], self.y[train]), (self.X[test], self.y[test])],\n",
        "                      **para['fit_params'])\n",
        "            else:\n",
        "                reg.fit(self.X[train], self.y[train])\n",
        "            #print('pred')\n",
        "            pred = reg.predict(self.X[test])\n",
        "            score = para['score'](self.y[test], pred)\n",
        "            loss += score\n",
        "\n",
        "        loss=loss/self.cv\n",
        "        return {'loss': loss, 'status': STATUS_OK}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Krr4en2uzv-"
      },
      "source": [
        "target = 'class'\n",
        "\n",
        "add_columns = ['subClass', 'objID', 'z', 'zErr', 'ra', 'dec']\n",
        "\n",
        "photo_columns = ['psfMag_u',\t'psfMag_g',\t'psfMag_r',\t'psfMag_i',\t'psfMag_z',\n",
        "                 'cModelMag_u',\t'cModelMag_g',\t'cModelMag_r',\t'cModelMag_i',\t'cModelMag_z']\n",
        "\n",
        "feature_columns = (\n",
        "    photo_columns + add_columns + [target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeLm9XEC7z88"
      },
      "source": [
        "X, y  = joblib.load('/content/drive/MyDrive/Научная работа/Спецсем/TabNetModel/X_y.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NezzZ9yr4mvm",
        "outputId": "4e1af415-2340-42cd-f979-2a6df37b1a81"
      },
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize = (24, 6))\n",
        "sns.histplot(X[y==1][:, 7], ax = ax1, bins = 50, color = 'g', element=\"step\", binrange=(10, 26))\n",
        "sns.histplot(X[y==2][:, 7], ax = ax2, bins = 50, color = 'r', element=\"step\", binrange=(10, 26))\n",
        "sns.histplot(X[y==3][:, 7], ax = ax3, bins = 50, color = 'b', element=\"step\", binrange=(10, 26))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f348ffef850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Senx9ogAuulc"
      },
      "source": [
        "# Общая функция"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eonNui1w1Cfx"
      },
      "source": [
        "#dftn_time = pd.DataFrame(columns = ['Rows', 'train', 'test', 'Nd',\t'Na',\t'B',\t'BV',\t'mB',\t'λsparse',\t'Nsteps',\t'γ',\t'learning rate',\t'decay rate',\t'decay iterations', 'shared', \t'decision',\t'mask_type',\t'accuracy_tn', 'time_learn_tn',\t'time_tn',\t'accuracy_gb', 'time_learn_gb',\t'time_gb'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JY_V9ybJ9_o"
      },
      "source": [
        "dftn_time = pd.read_csv('/content/drive/MyDrive/Научная работа/Data/hyper/cpu_time.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "2rGmf-Eg3_ht",
        "outputId": "52337819-9643-4287-a761-40d270da9141"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894833</td>\n",
              "      <td>814.815927</td>\n",
              "      <td>103.367213</td>\n",
              "      <td>0.893500</td>\n",
              "      <td>1.618983e+09</td>\n",
              "      <td>20.529149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894367</td>\n",
              "      <td>5522.418471</td>\n",
              "      <td>288.120832</td>\n",
              "      <td>0.894933</td>\n",
              "      <td>1.618989e+09</td>\n",
              "      <td>22.549996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901847</td>\n",
              "      <td>5962.013143</td>\n",
              "      <td>69.414092</td>\n",
              "      <td>0.902523</td>\n",
              "      <td>1.618997e+09</td>\n",
              "      <td>62.969158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ...  accuracy_gb  time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...     0.886444   1.618973e+09  16.887252\n",
              "1     9000    9000    9000  ...     0.887333   1.618974e+09  13.399141\n",
              "2     9000    9000    9000  ...     0.886222   1.618975e+09  18.229882\n",
              "3     9000    9000    9000  ...     0.888667   1.618976e+09  19.224410\n",
              "4     9000    9000    9000  ...     0.886000   1.618978e+09  15.819996\n",
              "5    30000   30000   30000  ...     0.893733   1.618979e+09  25.906476\n",
              "6    30000   30000   30000  ...     0.893633   1.618980e+09  21.026869\n",
              "7    30000   30000   30000  ...     0.894200   1.618982e+09  20.734107\n",
              "8    30000   30000   30000  ...     0.893500   1.618983e+09  20.529149\n",
              "9    30000   30000   30000  ...     0.894933   1.618989e+09  22.549996\n",
              "10  300000  300000  300000  ...     0.902523   1.618997e+09  62.969158\n",
              "\n",
              "[11 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB1sVwYkEq9Z"
      },
      "source": [
        "##Старые"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqcSoieSu6bG"
      },
      "source": [
        "with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'w') as f:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvnRCetO2baB"
      },
      "source": [
        "def bootstrap_accuracy(model, X_test, y_test):\n",
        "  from sklearn.utils import resample\n",
        "  from matplotlib import pyplot\n",
        "  values = np.concatenate((X_test, y_test.reshape((len(y_test), 1))), axis=1)\n",
        "  n_iterations = 100\n",
        "  n_size = int(len(y_test) * 0.50)\n",
        "  stats = list()\n",
        "  for i in range(n_iterations):\n",
        "    test = resample(values, n_samples=n_size)\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    score = accuracy_score(test[:,-1], predictions)\n",
        "    stats.append(score)\n",
        "  #pyplot.hist(stats, range=(0.872, 0.9))\n",
        "  #pyplot.show()\n",
        "  alpha = 0.97\n",
        "  p = ((1.0-alpha)/2.0) * 100\n",
        "  lower = max(0.0, np.percentile(stats, p))\n",
        "  p = (alpha+((1.0-alpha)/2.0)) * 100\n",
        "  upper = min(1.0, np.percentile(stats, p))\n",
        "  main = np.mean(stats)\n",
        "  #print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
        "  return main, np.max([main-lower, upper-main])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ysnlzZFAZ8T"
      },
      "source": [
        "def bootstrap(X_test, y_test, X_valid, y_valid):\n",
        "  from sklearn.utils import resample\n",
        "  from matplotlib import pyplot\n",
        "  values = np.concatenate((X_test, y_test.reshape((len(y_test), 1))), axis=1)\n",
        "  n_iterations = 100\n",
        "  n_size = int(len(y_test) * 0.50)\n",
        "  stats = list()\n",
        "  for i in range(n_iterations):\n",
        "    train_ind = np.random.randint(0, len(values), n_size)\n",
        "    test_ind = np.setdiff1d(range(len(values)), train_ind)\n",
        "    train = values[train_ind]\n",
        "    test = values[test_ind]\n",
        "    # fit model\n",
        "    model = lgb.LGBMClassifier()\n",
        "    model.fit(train[:,:-1], train[:,-1], eval_set=[(train[:,:-1], train[:,-1]), (X_valid, y_valid)], **lgb_fit_params)\n",
        "    # evaluate model\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    test = resample(values, n_samples=n_size)\n",
        "    predictions = model.predict(test[:,:-1])\n",
        "    score = accuracy_score(test[:,-1], predictions)\n",
        "    stats.append(score)\n",
        "  alpha = 0.97\n",
        "  p = ((1.0-alpha)/2.0) * 100\n",
        "  lower = max(0.0, np.percentile(stats, p))\n",
        "  p = (alpha+((1.0-alpha)/2.0)) * 100\n",
        "  upper = min(1.0, np.percentile(stats, p))\n",
        "  main = np.mean(stats)\n",
        "  #print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
        "  return main, np.max([main-lower, upper-main])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0pUcz3TxaRJ"
      },
      "source": [
        "def feature_acc(model, str_m, Rows):\n",
        "  print(str_m)\n",
        "\n",
        "  feature_imp=pd.DataFrame((zip(model.feature_importances_, photo_columns+agr_feature)), columns=['Model','Feature'])\n",
        "  t = 3\n",
        "  feature = feature_imp.sort_values(by='Model', ascending=False).iloc[:t]['Feature'].values\n",
        "  X = df[feature].values\n",
        "\n",
        "  data_split = data_preparation(X, y, Rows, 0.8)\n",
        "  count = Rows//3\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_valid   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  y_valid   = np.concatenate((y1_test[count : 2*count], y2_test[count : 2*count], y3_test[count : 2*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "  acc, err = bootstrap(X_test_norm, y_test, X_valid_norm, y_valid)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc_feature '+str_m+': '+str(acc)+'+-'+str(err)+', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUaTn8q9wItC"
      },
      "source": [
        "def ones(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type):\n",
        "  \n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('number: '+str(number_exp)+', ')\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=50\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 1000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #Accuracy\n",
        "  acc, err = bootstrap_accuracy(tn, X_test, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc TN: '+str(acc)+'+-'+str(err)+', ')\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(tn, 'TN', 9000)\n",
        "  feature_acc(gb, 'GB', 9000)\n",
        "\n",
        "  #save model\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.txt')\n",
        "  tn.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNijQemux8TZ"
      },
      "source": [
        "def memory(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type):\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.5)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  #X_valid      = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  #y_valid      = np.concatenate((y1_test[count : 2*count], y2_test[count : 2*count], y3_test[count : 2*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  #X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=50\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 1000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_test_norm, y_test)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #save model\n",
        "\n",
        "  joblib.dump(gb, '/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.pkl')\n",
        "  joblib.dump(tn, '/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp)+'.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoqLuZDE2wh"
      },
      "source": [
        "##Новая"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei75tuaUSggZ"
      },
      "source": [
        "X_main_test = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/X_main_test.pkl')\n",
        "y_main_test = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/y_main_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApXy6-E1kyv"
      },
      "source": [
        "def time_model(number_exp, Rows, Nd,\tNa,\tB,\tBV,\tmB,\tλsparse,\tNsteps,\tγ, learning_rate,\tdecay_rate,\tdecay_iterations,\tshared, decision, mask_type, gb_do=False):\n",
        "\n",
        "  global dftn_time\n",
        "  global X_main_test \n",
        "  global y_main_test\n",
        "\n",
        "  data = {'Rows':Rows, 'Nd':Nd,\t'Na':Na,\t'B':B,\t'BV':BV,\t'mB':mB,\t'λsparse':λsparse,\t'Nsteps':Nsteps,\t'γ':γ,\t'learning rate':learning_rate,\t'decay rate':decay_rate,\t'decay iterations':decay_iterations,\t'shared':shared, \t'decision':decision,\t'mask_type':mask_type}\n",
        "  'accuracy_tn', 'time_learn_tn',\t'time_tn',\t'accuracy_gb', 'time_learn_tn',\t'time_gb'\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.5)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  data['train'] = len(y_train)\n",
        "  data['test'] = len(y_test)\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_valid = X_test\n",
        "  y_valid = y_test\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_main_test_norm = robust.transform(X_main_test)\n",
        "  X_valid_norm = X_test_norm\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "\n",
        "  patience=30\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=patience,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  data['time_learn_tn'] = t\n",
        "\n",
        "  t = time()\n",
        "  pred = tn.predict(X_main_test)\n",
        "  t = time() - t\n",
        "  data['time_tn'] = t\n",
        "  print('Confusion Matrix: \\n', confusion_matrix(y_main_test, pred))\n",
        "  acc = accuracy_score(y_test, tn.predict(X_test))\n",
        "  data['accuracy_tn'] = acc\n",
        "  print('Testing Score: ', acc)\n",
        "\n",
        "  joblib.dump(tn, '/content/drive/MyDrive/Научная работа/Data/hyper/53/cpu_tn'+str(number_exp)+'.pkl')\n",
        "\n",
        "\n",
        "  if gb_do:\n",
        "    obj = HPOpt(X_train_norm, y_train, cv=2)\n",
        "    lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=20)\n",
        "    print(lgb_opt)\n",
        "    gb = lgb.LGBMClassifier( \n",
        "                              **{'colsample_bytree': lgb_opt[0]['colsample_bytree'],\n",
        "                                'min_child_samples': lgb_opt[0]['min_child_samples']+1,\n",
        "                                'min_child_weight': lgb_opt[0]['min_child_weight'],\n",
        "                                'num_leaves': lgb_opt[0]['num_leaves']+10,\n",
        "                                'n_estimators': 1000\n",
        "                                }\n",
        "                            )\n",
        "    t = time()\n",
        "    gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "    print(time()-t)\n",
        "    data['time_learn_gb'] = t\n",
        "\n",
        "    t = time()\n",
        "    pred = gb.predict(X_main_test_norm)\n",
        "    t = time() - t\n",
        "    data['time_gb'] = t\n",
        "    print('Confusion Matrix: \\n', confusion_matrix(y_main_test, pred))\n",
        "    acc = accuracy_score(y_test, gb.predict(X_test_norm))\n",
        "    data['accuracy_gb'] = acc\n",
        "    print('Testing Score: ', acc)\n",
        "    joblib.dump(gb, '/content/drive/MyDrive/Научная работа/Data/hyper/53/cpu_gb'+str(number_exp)+'.pkl')\n",
        "    gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/53/cpu_gb'+str(number_exp)+'txt')\n",
        "  else:\n",
        "    data['time_learn_gb'] = 0\n",
        "    data['time_gb'] = 0\n",
        "    data['accuracy_gb'] = 0 \n",
        "\n",
        "  #save model\n",
        "  print(data)\n",
        "  dftn_time = dftn_time.append(data, ignore_index=True)\n",
        "  dftn_time.to_csv('/content/drive/MyDrive/Научная работа/Data/hyper/cpu_time.csv', index=False)\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGuS7NKi9x6T"
      },
      "source": [
        "#Запускаем тесты по времени (в данном эксперименте не так важна точность, она будет проверяться дальше)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VemFf9oi99hV"
      },
      "source": [
        "##9000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29n-76Co_nSM",
        "outputId": "220e477e-2d07-4f0b-e98c-8800997cb0be"
      },
      "source": [
        "time_model(number_exp=1.1, \n",
        "     Rows=9000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.98487 | train_logloss: 23.02678| train_accuracy: 0.333   | valid_logloss: 23.01576| valid_accuracy: 0.33333 |  0:00:01s\n",
            "epoch 1  | loss: 0.64644 | train_logloss: 22.94983| train_accuracy: 0.33333 | valid_logloss: 22.94916| valid_accuracy: 0.33356 |  0:00:02s\n",
            "epoch 2  | loss: 0.51131 | train_logloss: 21.75227| train_accuracy: 0.35089 | valid_logloss: 21.69792| valid_accuracy: 0.35222 |  0:00:03s\n",
            "epoch 3  | loss: 0.44587 | train_logloss: 13.19595| train_accuracy: 0.229   | valid_logloss: 13.19804| valid_accuracy: 0.23022 |  0:00:05s\n",
            "epoch 4  | loss: 0.40863 | train_logloss: 10.27826| train_accuracy: 0.34133 | valid_logloss: 10.274  | valid_accuracy: 0.33867 |  0:00:06s\n",
            "epoch 5  | loss: 0.38708 | train_logloss: 13.25457| train_accuracy: 0.29178 | valid_logloss: 13.46107| valid_accuracy: 0.28989 |  0:00:08s\n",
            "epoch 6  | loss: 0.37337 | train_logloss: 5.89163 | train_accuracy: 0.40178 | valid_logloss: 5.88058 | valid_accuracy: 0.40189 |  0:00:09s\n",
            "epoch 7  | loss: 0.37066 | train_logloss: 5.9149  | train_accuracy: 0.37167 | valid_logloss: 5.85554 | valid_accuracy: 0.37333 |  0:00:10s\n",
            "epoch 8  | loss: 0.36132 | train_logloss: 4.88096 | train_accuracy: 0.33844 | valid_logloss: 4.90496 | valid_accuracy: 0.33911 |  0:00:12s\n",
            "epoch 9  | loss: 0.36169 | train_logloss: 3.44384 | train_accuracy: 0.37822 | valid_logloss: 3.50964 | valid_accuracy: 0.37122 |  0:00:13s\n",
            "epoch 10 | loss: 0.35698 | train_logloss: 7.08071 | train_accuracy: 0.26133 | valid_logloss: 7.11232 | valid_accuracy: 0.25422 |  0:00:14s\n",
            "epoch 11 | loss: 0.34768 | train_logloss: 8.67834 | train_accuracy: 0.25711 | valid_logloss: 8.67159 | valid_accuracy: 0.253   |  0:00:16s\n",
            "epoch 12 | loss: 0.34214 | train_logloss: 6.79849 | train_accuracy: 0.31289 | valid_logloss: 6.75472 | valid_accuracy: 0.31511 |  0:00:17s\n",
            "epoch 13 | loss: 0.33458 | train_logloss: 3.94578 | train_accuracy: 0.34667 | valid_logloss: 3.90013 | valid_accuracy: 0.34956 |  0:00:18s\n",
            "epoch 14 | loss: 0.33265 | train_logloss: 4.28656 | train_accuracy: 0.34989 | valid_logloss: 4.28329 | valid_accuracy: 0.35056 |  0:00:20s\n",
            "epoch 15 | loss: 0.3278  | train_logloss: 4.47457 | train_accuracy: 0.34844 | valid_logloss: 4.48497 | valid_accuracy: 0.35044 |  0:00:21s\n",
            "epoch 16 | loss: 0.32347 | train_logloss: 5.13783 | train_accuracy: 0.35367 | valid_logloss: 5.14079 | valid_accuracy: 0.35122 |  0:00:22s\n",
            "epoch 17 | loss: 0.32038 | train_logloss: 4.85198 | train_accuracy: 0.35333 | valid_logloss: 4.84559 | valid_accuracy: 0.35267 |  0:00:24s\n",
            "epoch 18 | loss: 0.31711 | train_logloss: 3.7205  | train_accuracy: 0.384   | valid_logloss: 3.67008 | valid_accuracy: 0.38222 |  0:00:25s\n",
            "epoch 19 | loss: 0.31404 | train_logloss: 3.47123 | train_accuracy: 0.38522 | valid_logloss: 3.47964 | valid_accuracy: 0.38233 |  0:00:26s\n",
            "epoch 20 | loss: 0.31575 | train_logloss: 4.02864 | train_accuracy: 0.37589 | valid_logloss: 4.08734 | valid_accuracy: 0.37044 |  0:00:28s\n",
            "epoch 21 | loss: 0.30925 | train_logloss: 2.62777 | train_accuracy: 0.47311 | valid_logloss: 2.60762 | valid_accuracy: 0.47    |  0:00:29s\n",
            "epoch 22 | loss: 0.31689 | train_logloss: 2.98434 | train_accuracy: 0.12444 | valid_logloss: 2.96575 | valid_accuracy: 0.13144 |  0:00:31s\n",
            "epoch 23 | loss: 0.30408 | train_logloss: 2.98829 | train_accuracy: 0.17567 | valid_logloss: 2.97924 | valid_accuracy: 0.17711 |  0:00:32s\n",
            "epoch 24 | loss: 0.30244 | train_logloss: 2.96633 | train_accuracy: 0.185   | valid_logloss: 2.9457  | valid_accuracy: 0.18567 |  0:00:33s\n",
            "epoch 25 | loss: 0.29655 | train_logloss: 2.72677 | train_accuracy: 0.22011 | valid_logloss: 2.70151 | valid_accuracy: 0.22844 |  0:00:35s\n",
            "epoch 26 | loss: 0.30009 | train_logloss: 3.05815 | train_accuracy: 0.30467 | valid_logloss: 3.03422 | valid_accuracy: 0.30778 |  0:00:36s\n",
            "epoch 27 | loss: 0.29287 | train_logloss: 2.92926 | train_accuracy: 0.306   | valid_logloss: 2.90395 | valid_accuracy: 0.30889 |  0:00:37s\n",
            "epoch 28 | loss: 0.30033 | train_logloss: 2.74342 | train_accuracy: 0.29389 | valid_logloss: 2.71636 | valid_accuracy: 0.29489 |  0:00:39s\n",
            "epoch 29 | loss: 0.28686 | train_logloss: 2.62468 | train_accuracy: 0.21922 | valid_logloss: 2.61519 | valid_accuracy: 0.22133 |  0:00:40s\n",
            "epoch 30 | loss: 0.29115 | train_logloss: 2.28721 | train_accuracy: 0.31467 | valid_logloss: 2.28179 | valid_accuracy: 0.31478 |  0:00:41s\n",
            "epoch 31 | loss: 0.28612 | train_logloss: 2.18465 | train_accuracy: 0.32211 | valid_logloss: 2.15982 | valid_accuracy: 0.32467 |  0:00:43s\n",
            "epoch 32 | loss: 0.28992 | train_logloss: 1.79208 | train_accuracy: 0.34356 | valid_logloss: 1.75649 | valid_accuracy: 0.35389 |  0:00:44s\n",
            "epoch 33 | loss: 0.28916 | train_logloss: 1.73955 | train_accuracy: 0.40889 | valid_logloss: 1.72641 | valid_accuracy: 0.40978 |  0:00:46s\n",
            "epoch 34 | loss: 0.28656 | train_logloss: 1.82882 | train_accuracy: 0.35611 | valid_logloss: 1.82204 | valid_accuracy: 0.36167 |  0:00:47s\n",
            "epoch 35 | loss: 0.28301 | train_logloss: 1.86045 | train_accuracy: 0.37733 | valid_logloss: 1.86277 | valid_accuracy: 0.37233 |  0:00:49s\n",
            "epoch 36 | loss: 0.29061 | train_logloss: 1.51495 | train_accuracy: 0.46467 | valid_logloss: 1.53995 | valid_accuracy: 0.45644 |  0:00:50s\n",
            "epoch 37 | loss: 0.28352 | train_logloss: 1.60146 | train_accuracy: 0.40533 | valid_logloss: 1.62821 | valid_accuracy: 0.39489 |  0:00:51s\n",
            "epoch 38 | loss: 0.27727 | train_logloss: 1.64157 | train_accuracy: 0.42656 | valid_logloss: 1.64623 | valid_accuracy: 0.41822 |  0:00:53s\n",
            "epoch 39 | loss: 0.28054 | train_logloss: 1.42269 | train_accuracy: 0.50956 | valid_logloss: 1.439   | valid_accuracy: 0.50844 |  0:00:54s\n",
            "epoch 40 | loss: 0.28104 | train_logloss: 1.55483 | train_accuracy: 0.498   | valid_logloss: 1.56195 | valid_accuracy: 0.49389 |  0:00:56s\n",
            "epoch 41 | loss: 0.27713 | train_logloss: 1.53053 | train_accuracy: 0.45889 | valid_logloss: 1.53197 | valid_accuracy: 0.45467 |  0:00:57s\n",
            "epoch 42 | loss: 0.28002 | train_logloss: 1.20346 | train_accuracy: 0.51556 | valid_logloss: 1.21053 | valid_accuracy: 0.51544 |  0:00:58s\n",
            "epoch 43 | loss: 0.27718 | train_logloss: 1.2142  | train_accuracy: 0.53067 | valid_logloss: 1.23306 | valid_accuracy: 0.52511 |  0:01:00s\n",
            "epoch 44 | loss: 0.2868  | train_logloss: 1.07442 | train_accuracy: 0.60189 | valid_logloss: 1.06642 | valid_accuracy: 0.60356 |  0:01:01s\n",
            "epoch 45 | loss: 0.28889 | train_logloss: 1.03377 | train_accuracy: 0.61022 | valid_logloss: 1.02232 | valid_accuracy: 0.61344 |  0:01:03s\n",
            "epoch 46 | loss: 0.28148 | train_logloss: 0.96293 | train_accuracy: 0.63733 | valid_logloss: 0.94818 | valid_accuracy: 0.64389 |  0:01:04s\n",
            "epoch 47 | loss: 0.27576 | train_logloss: 0.87577 | train_accuracy: 0.65933 | valid_logloss: 0.86421 | valid_accuracy: 0.66267 |  0:01:05s\n",
            "epoch 48 | loss: 0.27127 | train_logloss: 0.80861 | train_accuracy: 0.69856 | valid_logloss: 0.80106 | valid_accuracy: 0.70089 |  0:01:07s\n",
            "epoch 49 | loss: 0.27187 | train_logloss: 0.83375 | train_accuracy: 0.68556 | valid_logloss: 0.82747 | valid_accuracy: 0.68633 |  0:01:08s\n",
            "epoch 50 | loss: 0.26604 | train_logloss: 0.79684 | train_accuracy: 0.70689 | valid_logloss: 0.78661 | valid_accuracy: 0.71078 |  0:01:10s\n",
            "epoch 51 | loss: 0.26251 | train_logloss: 0.61862 | train_accuracy: 0.77889 | valid_logloss: 0.61917 | valid_accuracy: 0.78289 |  0:01:11s\n",
            "epoch 52 | loss: 0.26268 | train_logloss: 0.63332 | train_accuracy: 0.76411 | valid_logloss: 0.64045 | valid_accuracy: 0.76844 |  0:01:13s\n",
            "epoch 53 | loss: 0.26271 | train_logloss: 0.57636 | train_accuracy: 0.80211 | valid_logloss: 0.57995 | valid_accuracy: 0.80278 |  0:01:14s\n",
            "epoch 54 | loss: 0.26679 | train_logloss: 0.57512 | train_accuracy: 0.80233 | valid_logloss: 0.59426 | valid_accuracy: 0.80344 |  0:01:15s\n",
            "epoch 55 | loss: 0.25834 | train_logloss: 0.52385 | train_accuracy: 0.81122 | valid_logloss: 0.54043 | valid_accuracy: 0.81278 |  0:01:17s\n",
            "epoch 56 | loss: 0.26733 | train_logloss: 0.52567 | train_accuracy: 0.80333 | valid_logloss: 0.54478 | valid_accuracy: 0.80156 |  0:01:18s\n",
            "epoch 57 | loss: 0.28084 | train_logloss: 0.53176 | train_accuracy: 0.797   | valid_logloss: 0.56049 | valid_accuracy: 0.801   |  0:01:20s\n",
            "epoch 58 | loss: 0.27508 | train_logloss: 0.50529 | train_accuracy: 0.81678 | valid_logloss: 0.52773 | valid_accuracy: 0.81522 |  0:01:21s\n",
            "epoch 59 | loss: 0.28137 | train_logloss: 0.46481 | train_accuracy: 0.83333 | valid_logloss: 0.47871 | valid_accuracy: 0.831   |  0:01:23s\n",
            "epoch 60 | loss: 0.27472 | train_logloss: 0.50845 | train_accuracy: 0.81    | valid_logloss: 0.53605 | valid_accuracy: 0.80822 |  0:01:24s\n",
            "epoch 61 | loss: 0.27488 | train_logloss: 0.42533 | train_accuracy: 0.84144 | valid_logloss: 0.45519 | valid_accuracy: 0.83533 |  0:01:25s\n",
            "epoch 62 | loss: 0.26656 | train_logloss: 0.42277 | train_accuracy: 0.83411 | valid_logloss: 0.45716 | valid_accuracy: 0.82744 |  0:01:27s\n",
            "epoch 63 | loss: 0.26313 | train_logloss: 0.36926 | train_accuracy: 0.85767 | valid_logloss: 0.40373 | valid_accuracy: 0.85178 |  0:01:28s\n",
            "epoch 64 | loss: 0.26351 | train_logloss: 0.40314 | train_accuracy: 0.85033 | valid_logloss: 0.4335  | valid_accuracy: 0.84289 |  0:01:30s\n",
            "epoch 65 | loss: 0.26175 | train_logloss: 0.36819 | train_accuracy: 0.86544 | valid_logloss: 0.40215 | valid_accuracy: 0.85956 |  0:01:31s\n",
            "epoch 66 | loss: 0.26578 | train_logloss: 0.37679 | train_accuracy: 0.85956 | valid_logloss: 0.41303 | valid_accuracy: 0.85267 |  0:01:32s\n",
            "epoch 67 | loss: 0.26882 | train_logloss: 0.36345 | train_accuracy: 0.85867 | valid_logloss: 0.40546 | valid_accuracy: 0.85067 |  0:01:34s\n",
            "epoch 68 | loss: 0.27062 | train_logloss: 0.35781 | train_accuracy: 0.86489 | valid_logloss: 0.40201 | valid_accuracy: 0.85944 |  0:01:35s\n",
            "epoch 69 | loss: 0.26583 | train_logloss: 0.36098 | train_accuracy: 0.86733 | valid_logloss: 0.40949 | valid_accuracy: 0.85833 |  0:01:37s\n",
            "epoch 70 | loss: 0.26467 | train_logloss: 0.35125 | train_accuracy: 0.86856 | valid_logloss: 0.40609 | valid_accuracy: 0.85967 |  0:01:38s\n",
            "epoch 71 | loss: 0.26044 | train_logloss: 0.31013 | train_accuracy: 0.87989 | valid_logloss: 0.37098 | valid_accuracy: 0.869   |  0:01:39s\n",
            "epoch 72 | loss: 0.25745 | train_logloss: 0.3186  | train_accuracy: 0.87678 | valid_logloss: 0.38595 | valid_accuracy: 0.86    |  0:01:41s\n",
            "epoch 73 | loss: 0.26452 | train_logloss: 0.31853 | train_accuracy: 0.87367 | valid_logloss: 0.38691 | valid_accuracy: 0.86489 |  0:01:42s\n",
            "epoch 74 | loss: 0.26261 | train_logloss: 0.33042 | train_accuracy: 0.87722 | valid_logloss: 0.40146 | valid_accuracy: 0.86256 |  0:01:44s\n",
            "epoch 75 | loss: 0.26263 | train_logloss: 0.30529 | train_accuracy: 0.88722 | valid_logloss: 0.38195 | valid_accuracy: 0.87178 |  0:01:45s\n",
            "epoch 76 | loss: 0.25854 | train_logloss: 0.29849 | train_accuracy: 0.88344 | valid_logloss: 0.37967 | valid_accuracy: 0.86833 |  0:01:47s\n",
            "epoch 77 | loss: 0.25685 | train_logloss: 0.29513 | train_accuracy: 0.88889 | valid_logloss: 0.38547 | valid_accuracy: 0.87056 |  0:01:48s\n",
            "epoch 78 | loss: 0.27281 | train_logloss: 0.30642 | train_accuracy: 0.88089 | valid_logloss: 0.3761  | valid_accuracy: 0.86956 |  0:01:49s\n",
            "epoch 79 | loss: 0.28946 | train_logloss: 0.31909 | train_accuracy: 0.87689 | valid_logloss: 0.38039 | valid_accuracy: 0.867   |  0:01:51s\n",
            "epoch 80 | loss: 0.28675 | train_logloss: 0.30377 | train_accuracy: 0.88656 | valid_logloss: 0.36408 | valid_accuracy: 0.878   |  0:01:52s\n",
            "epoch 81 | loss: 0.27745 | train_logloss: 0.30191 | train_accuracy: 0.88644 | valid_logloss: 0.3757  | valid_accuracy: 0.87378 |  0:01:54s\n",
            "epoch 82 | loss: 0.26005 | train_logloss: 0.27396 | train_accuracy: 0.89511 | valid_logloss: 0.35967 | valid_accuracy: 0.87833 |  0:01:55s\n",
            "epoch 83 | loss: 0.26722 | train_logloss: 0.29456 | train_accuracy: 0.886   | valid_logloss: 0.36553 | valid_accuracy: 0.87511 |  0:01:57s\n",
            "epoch 84 | loss: 0.26443 | train_logloss: 0.27751 | train_accuracy: 0.89256 | valid_logloss: 0.35068 | valid_accuracy: 0.87967 |  0:01:58s\n",
            "epoch 85 | loss: 0.26677 | train_logloss: 0.29373 | train_accuracy: 0.88989 | valid_logloss: 0.36406 | valid_accuracy: 0.87867 |  0:01:59s\n",
            "epoch 86 | loss: 0.27055 | train_logloss: 0.27612 | train_accuracy: 0.89389 | valid_logloss: 0.3538  | valid_accuracy: 0.87733 |  0:02:01s\n",
            "epoch 87 | loss: 0.26447 | train_logloss: 0.27696 | train_accuracy: 0.89222 | valid_logloss: 0.35486 | valid_accuracy: 0.87644 |  0:02:02s\n",
            "epoch 88 | loss: 0.25943 | train_logloss: 0.27116 | train_accuracy: 0.89822 | valid_logloss: 0.35543 | valid_accuracy: 0.87867 |  0:02:04s\n",
            "epoch 89 | loss: 0.26726 | train_logloss: 0.263   | train_accuracy: 0.899   | valid_logloss: 0.35547 | valid_accuracy: 0.88156 |  0:02:05s\n",
            "epoch 90 | loss: 0.26103 | train_logloss: 0.26174 | train_accuracy: 0.89922 | valid_logloss: 0.34996 | valid_accuracy: 0.88067 |  0:02:06s\n",
            "epoch 91 | loss: 0.25629 | train_logloss: 0.25257 | train_accuracy: 0.902   | valid_logloss: 0.34758 | valid_accuracy: 0.88156 |  0:02:08s\n",
            "epoch 92 | loss: 0.25225 | train_logloss: 0.25562 | train_accuracy: 0.90189 | valid_logloss: 0.35044 | valid_accuracy: 0.87956 |  0:02:09s\n",
            "epoch 93 | loss: 0.25417 | train_logloss: 0.25314 | train_accuracy: 0.90322 | valid_logloss: 0.35673 | valid_accuracy: 0.88144 |  0:02:11s\n",
            "epoch 94 | loss: 0.25297 | train_logloss: 0.24862 | train_accuracy: 0.90178 | valid_logloss: 0.35984 | valid_accuracy: 0.87889 |  0:02:12s\n",
            "epoch 95 | loss: 0.25239 | train_logloss: 0.24725 | train_accuracy: 0.90378 | valid_logloss: 0.3595  | valid_accuracy: 0.87833 |  0:02:13s\n",
            "epoch 96 | loss: 0.25203 | train_logloss: 0.24198 | train_accuracy: 0.90844 | valid_logloss: 0.34056 | valid_accuracy: 0.88278 |  0:02:15s\n",
            "epoch 97 | loss: 0.24806 | train_logloss: 0.24634 | train_accuracy: 0.90578 | valid_logloss: 0.36766 | valid_accuracy: 0.877   |  0:02:16s\n",
            "epoch 98 | loss: 0.24605 | train_logloss: 0.24092 | train_accuracy: 0.90667 | valid_logloss: 0.35098 | valid_accuracy: 0.88122 |  0:02:18s\n",
            "epoch 99 | loss: 0.24632 | train_logloss: 0.23392 | train_accuracy: 0.90933 | valid_logloss: 0.35656 | valid_accuracy: 0.88211 |  0:02:19s\n",
            "epoch 100| loss: 0.24012 | train_logloss: 0.24067 | train_accuracy: 0.90644 | valid_logloss: 0.35481 | valid_accuracy: 0.881   |  0:02:20s\n",
            "epoch 101| loss: 0.2418  | train_logloss: 0.23449 | train_accuracy: 0.90856 | valid_logloss: 0.35847 | valid_accuracy: 0.88267 |  0:02:22s\n",
            "epoch 102| loss: 0.23554 | train_logloss: 0.22536 | train_accuracy: 0.91189 | valid_logloss: 0.3674  | valid_accuracy: 0.87989 |  0:02:23s\n",
            "epoch 103| loss: 0.23927 | train_logloss: 0.2326  | train_accuracy: 0.91056 | valid_logloss: 0.36797 | valid_accuracy: 0.88111 |  0:02:25s\n",
            "epoch 104| loss: 0.23851 | train_logloss: 0.2287  | train_accuracy: 0.91033 | valid_logloss: 0.36233 | valid_accuracy: 0.87878 |  0:02:26s\n",
            "epoch 105| loss: 0.23249 | train_logloss: 0.22562 | train_accuracy: 0.91044 | valid_logloss: 0.36536 | valid_accuracy: 0.88111 |  0:02:27s\n",
            "epoch 106| loss: 0.23565 | train_logloss: 0.23275 | train_accuracy: 0.91122 | valid_logloss: 0.364   | valid_accuracy: 0.87922 |  0:02:29s\n",
            "epoch 107| loss: 0.23235 | train_logloss: 0.2175  | train_accuracy: 0.917   | valid_logloss: 0.35986 | valid_accuracy: 0.88167 |  0:02:30s\n",
            "epoch 108| loss: 0.23174 | train_logloss: 0.22091 | train_accuracy: 0.91489 | valid_logloss: 0.36333 | valid_accuracy: 0.87967 |  0:02:32s\n",
            "epoch 109| loss: 0.22929 | train_logloss: 0.2217  | train_accuracy: 0.91467 | valid_logloss: 0.36652 | valid_accuracy: 0.88178 |  0:02:33s\n",
            "epoch 110| loss: 0.23206 | train_logloss: 0.22342 | train_accuracy: 0.91411 | valid_logloss: 0.37172 | valid_accuracy: 0.87933 |  0:02:35s\n",
            "epoch 111| loss: 0.2375  | train_logloss: 0.22771 | train_accuracy: 0.91133 | valid_logloss: 0.36936 | valid_accuracy: 0.877   |  0:02:36s\n",
            "epoch 112| loss: 0.23686 | train_logloss: 0.23071 | train_accuracy: 0.91189 | valid_logloss: 0.37282 | valid_accuracy: 0.88022 |  0:02:37s\n",
            "epoch 113| loss: 0.23886 | train_logloss: 0.2345  | train_accuracy: 0.91078 | valid_logloss: 0.37223 | valid_accuracy: 0.87944 |  0:02:39s\n",
            "epoch 114| loss: 0.23481 | train_logloss: 0.22031 | train_accuracy: 0.91389 | valid_logloss: 0.3664  | valid_accuracy: 0.87911 |  0:02:40s\n",
            "epoch 115| loss: 0.23322 | train_logloss: 0.22367 | train_accuracy: 0.91211 | valid_logloss: 0.37677 | valid_accuracy: 0.87944 |  0:02:42s\n",
            "epoch 116| loss: 0.23103 | train_logloss: 0.2164  | train_accuracy: 0.916   | valid_logloss: 0.37241 | valid_accuracy: 0.88078 |  0:02:43s\n",
            "epoch 117| loss: 0.22787 | train_logloss: 0.21544 | train_accuracy: 0.91711 | valid_logloss: 0.38076 | valid_accuracy: 0.87511 |  0:02:44s\n",
            "epoch 118| loss: 0.22366 | train_logloss: 0.21504 | train_accuracy: 0.91611 | valid_logloss: 0.38654 | valid_accuracy: 0.87722 |  0:02:46s\n",
            "epoch 119| loss: 0.22521 | train_logloss: 0.21403 | train_accuracy: 0.91844 | valid_logloss: 0.37947 | valid_accuracy: 0.87911 |  0:02:47s\n",
            "epoch 120| loss: 0.23016 | train_logloss: 0.21754 | train_accuracy: 0.91556 | valid_logloss: 0.37558 | valid_accuracy: 0.88122 |  0:02:49s\n",
            "epoch 121| loss: 0.23107 | train_logloss: 0.22697 | train_accuracy: 0.90989 | valid_logloss: 0.37994 | valid_accuracy: 0.87556 |  0:02:50s\n",
            "epoch 122| loss: 0.23085 | train_logloss: 0.22517 | train_accuracy: 0.90767 | valid_logloss: 0.38706 | valid_accuracy: 0.87233 |  0:02:52s\n",
            "epoch 123| loss: 0.22945 | train_logloss: 0.22731 | train_accuracy: 0.91    | valid_logloss: 0.38285 | valid_accuracy: 0.87356 |  0:02:53s\n",
            "epoch 124| loss: 0.23742 | train_logloss: 0.21746 | train_accuracy: 0.91467 | valid_logloss: 0.38447 | valid_accuracy: 0.87789 |  0:02:54s\n",
            "epoch 125| loss: 0.2345  | train_logloss: 0.22629 | train_accuracy: 0.91189 | valid_logloss: 0.37787 | valid_accuracy: 0.874   |  0:02:56s\n",
            "epoch 126| loss: 0.23408 | train_logloss: 0.22069 | train_accuracy: 0.91333 | valid_logloss: 0.37924 | valid_accuracy: 0.87822 |  0:02:57s\n",
            "\n",
            "Early stopping occurred at epoch 126 with best_epoch = 96 and best_valid_accuracy = 0.88278\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[409962  15645  54575]\n",
            " [ 18358 944231  37411]\n",
            " [ 35571  28996 368020]]\n",
            "Testing Score:  0.8827777777777778\n",
            "100%|██████████| 10/10 [00:52<00:00,  5.22s/it, best loss: -0.8846666666666667]\n",
            "({'colsample_bytree': 0.4866201567582985, 'min_child_samples': 0, 'min_child_weight': 0.4960240566827622, 'num_leaves': 18}, <hyperopt.base.Trials object at 0x7fcdd8255e90>)\n",
            "3.373206377029419\n",
            "Confusion Matrix: \n",
            " [[415972  13237  50973]\n",
            " [ 14368 953256  32376]\n",
            " [ 37461  28964 366162]]\n",
            "Testing Score:  0.8864444444444445\n",
            "{'Rows': 9000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 178.27749347686768, 'time_tn': 70.83791446685791, 'accuracy_tn': 0.8827777777777778, 'time_learn_gb': 1618973313.5955942, 'time_gb': 16.88725233078003, 'accuracy_gb': 0.8864444444444445}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRI66ThhH2iA",
        "outputId": "0d941098-5818-4b5a-ecd3-50baa75158a1"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000  8  ...  70.837914    0.886444  1.618973e+09  16.887252\n",
              "\n",
              "[1 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9QOZGb_nrT",
        "outputId": "0bc4aca8-ba10-4c3f-a131-5c8e925cad36"
      },
      "source": [
        "time_model(number_exp=2, \n",
        "     Rows=9000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.87557 | train_logloss: 15.07834| train_accuracy: 0.31756 | valid_logloss: 14.89747| valid_accuracy: 0.31889 |  0:00:02s\n",
            "epoch 1  | loss: 0.49194 | train_logloss: 2.60644 | train_accuracy: 0.43078 | valid_logloss: 2.61422 | valid_accuracy: 0.429   |  0:00:04s\n",
            "epoch 2  | loss: 0.42861 | train_logloss: 9.11321 | train_accuracy: 0.35956 | valid_logloss: 9.25903 | valid_accuracy: 0.35644 |  0:00:06s\n",
            "epoch 3  | loss: 0.40157 | train_logloss: 2.5082  | train_accuracy: 0.54678 | valid_logloss: 2.62221 | valid_accuracy: 0.54222 |  0:00:09s\n",
            "epoch 4  | loss: 0.39266 | train_logloss: 1.66468 | train_accuracy: 0.55467 | valid_logloss: 1.66118 | valid_accuracy: 0.55667 |  0:00:11s\n",
            "epoch 5  | loss: 0.3892  | train_logloss: 1.64852 | train_accuracy: 0.32356 | valid_logloss: 1.6426  | valid_accuracy: 0.32878 |  0:00:13s\n",
            "epoch 6  | loss: 0.38471 | train_logloss: 0.8525  | train_accuracy: 0.59689 | valid_logloss: 0.84201 | valid_accuracy: 0.60711 |  0:00:16s\n",
            "epoch 7  | loss: 0.37163 | train_logloss: 1.78941 | train_accuracy: 0.40356 | valid_logloss: 1.7856  | valid_accuracy: 0.40889 |  0:00:18s\n",
            "epoch 8  | loss: 0.36432 | train_logloss: 1.2134  | train_accuracy: 0.55222 | valid_logloss: 1.23071 | valid_accuracy: 0.54956 |  0:00:20s\n",
            "epoch 9  | loss: 0.35119 | train_logloss: 1.19796 | train_accuracy: 0.628   | valid_logloss: 1.18973 | valid_accuracy: 0.63244 |  0:00:23s\n",
            "epoch 10 | loss: 0.35227 | train_logloss: 1.38598 | train_accuracy: 0.562   | valid_logloss: 1.38444 | valid_accuracy: 0.55944 |  0:00:25s\n",
            "epoch 11 | loss: 0.34374 | train_logloss: 1.2764  | train_accuracy: 0.61989 | valid_logloss: 1.27607 | valid_accuracy: 0.62056 |  0:00:28s\n",
            "epoch 12 | loss: 0.35508 | train_logloss: 0.75825 | train_accuracy: 0.65711 | valid_logloss: 0.76373 | valid_accuracy: 0.65656 |  0:00:30s\n",
            "epoch 13 | loss: 0.33862 | train_logloss: 0.57215 | train_accuracy: 0.80033 | valid_logloss: 0.58419 | valid_accuracy: 0.79867 |  0:00:32s\n",
            "epoch 14 | loss: 0.33268 | train_logloss: 0.42276 | train_accuracy: 0.85322 | valid_logloss: 0.43673 | valid_accuracy: 0.84978 |  0:00:35s\n",
            "epoch 15 | loss: 0.32871 | train_logloss: 0.41172 | train_accuracy: 0.85233 | valid_logloss: 0.42828 | valid_accuracy: 0.85033 |  0:00:37s\n",
            "epoch 16 | loss: 0.33197 | train_logloss: 0.35061 | train_accuracy: 0.86656 | valid_logloss: 0.3743  | valid_accuracy: 0.86244 |  0:00:39s\n",
            "epoch 17 | loss: 0.32481 | train_logloss: 0.39051 | train_accuracy: 0.84889 | valid_logloss: 0.42252 | valid_accuracy: 0.84456 |  0:00:42s\n",
            "epoch 18 | loss: 0.33237 | train_logloss: 0.35136 | train_accuracy: 0.869   | valid_logloss: 0.38248 | valid_accuracy: 0.86144 |  0:00:44s\n",
            "epoch 19 | loss: 0.3244  | train_logloss: 0.34907 | train_accuracy: 0.86722 | valid_logloss: 0.37364 | valid_accuracy: 0.86711 |  0:00:46s\n",
            "epoch 20 | loss: 0.32447 | train_logloss: 0.3351  | train_accuracy: 0.87233 | valid_logloss: 0.36323 | valid_accuracy: 0.86833 |  0:00:49s\n",
            "epoch 21 | loss: 0.32189 | train_logloss: 0.32488 | train_accuracy: 0.874   | valid_logloss: 0.35799 | valid_accuracy: 0.87011 |  0:00:51s\n",
            "epoch 22 | loss: 0.32465 | train_logloss: 0.31498 | train_accuracy: 0.88033 | valid_logloss: 0.35294 | valid_accuracy: 0.87556 |  0:00:53s\n",
            "epoch 23 | loss: 0.32022 | train_logloss: 0.31594 | train_accuracy: 0.88111 | valid_logloss: 0.34903 | valid_accuracy: 0.87533 |  0:00:56s\n",
            "epoch 24 | loss: 0.31436 | train_logloss: 0.30899 | train_accuracy: 0.88278 | valid_logloss: 0.35029 | valid_accuracy: 0.877   |  0:00:58s\n",
            "epoch 25 | loss: 0.31583 | train_logloss: 0.30905 | train_accuracy: 0.88411 | valid_logloss: 0.34453 | valid_accuracy: 0.87633 |  0:01:00s\n",
            "epoch 26 | loss: 0.31222 | train_logloss: 0.3019  | train_accuracy: 0.88622 | valid_logloss: 0.33968 | valid_accuracy: 0.87822 |  0:01:03s\n",
            "epoch 27 | loss: 0.31405 | train_logloss: 0.29129 | train_accuracy: 0.88856 | valid_logloss: 0.33946 | valid_accuracy: 0.87978 |  0:01:05s\n",
            "epoch 28 | loss: 0.30624 | train_logloss: 0.29258 | train_accuracy: 0.88744 | valid_logloss: 0.34811 | valid_accuracy: 0.87789 |  0:01:07s\n",
            "epoch 29 | loss: 0.31011 | train_logloss: 0.29044 | train_accuracy: 0.88722 | valid_logloss: 0.34644 | valid_accuracy: 0.87489 |  0:01:10s\n",
            "epoch 30 | loss: 0.3055  | train_logloss: 0.2838  | train_accuracy: 0.89111 | valid_logloss: 0.33417 | valid_accuracy: 0.88156 |  0:01:12s\n",
            "epoch 31 | loss: 0.30628 | train_logloss: 0.29874 | train_accuracy: 0.88478 | valid_logloss: 0.35266 | valid_accuracy: 0.87678 |  0:01:14s\n",
            "epoch 32 | loss: 0.30891 | train_logloss: 0.28674 | train_accuracy: 0.89267 | valid_logloss: 0.3406  | valid_accuracy: 0.88078 |  0:01:17s\n",
            "epoch 33 | loss: 0.3137  | train_logloss: 0.2997  | train_accuracy: 0.88578 | valid_logloss: 0.35125 | valid_accuracy: 0.87522 |  0:01:19s\n",
            "epoch 34 | loss: 0.30966 | train_logloss: 0.28994 | train_accuracy: 0.89033 | valid_logloss: 0.34015 | valid_accuracy: 0.881   |  0:01:21s\n",
            "epoch 35 | loss: 0.3026  | train_logloss: 0.28099 | train_accuracy: 0.89233 | valid_logloss: 0.33462 | valid_accuracy: 0.88267 |  0:01:24s\n",
            "epoch 36 | loss: 0.3     | train_logloss: 0.28125 | train_accuracy: 0.89333 | valid_logloss: 0.34368 | valid_accuracy: 0.87711 |  0:01:26s\n",
            "epoch 37 | loss: 0.29843 | train_logloss: 0.27838 | train_accuracy: 0.89289 | valid_logloss: 0.34332 | valid_accuracy: 0.88144 |  0:01:28s\n",
            "epoch 38 | loss: 0.30204 | train_logloss: 0.29016 | train_accuracy: 0.89022 | valid_logloss: 0.35085 | valid_accuracy: 0.88056 |  0:01:31s\n",
            "epoch 39 | loss: 0.29736 | train_logloss: 0.29775 | train_accuracy: 0.88411 | valid_logloss: 0.35574 | valid_accuracy: 0.87389 |  0:01:33s\n",
            "epoch 40 | loss: 0.29412 | train_logloss: 0.27981 | train_accuracy: 0.89222 | valid_logloss: 0.35447 | valid_accuracy: 0.87956 |  0:01:35s\n",
            "epoch 41 | loss: 0.2964  | train_logloss: 0.28104 | train_accuracy: 0.89622 | valid_logloss: 0.34831 | valid_accuracy: 0.88256 |  0:01:38s\n",
            "epoch 42 | loss: 0.29794 | train_logloss: 0.28452 | train_accuracy: 0.89111 | valid_logloss: 0.35524 | valid_accuracy: 0.88044 |  0:01:40s\n",
            "epoch 43 | loss: 0.29389 | train_logloss: 0.28886 | train_accuracy: 0.89111 | valid_logloss: 0.34563 | valid_accuracy: 0.87956 |  0:01:42s\n",
            "epoch 44 | loss: 0.30405 | train_logloss: 0.28492 | train_accuracy: 0.88833 | valid_logloss: 0.33692 | valid_accuracy: 0.88022 |  0:01:45s\n",
            "epoch 45 | loss: 0.30327 | train_logloss: 0.28371 | train_accuracy: 0.88644 | valid_logloss: 0.34025 | valid_accuracy: 0.874   |  0:01:47s\n",
            "epoch 46 | loss: 0.2909  | train_logloss: 0.27213 | train_accuracy: 0.89722 | valid_logloss: 0.33679 | valid_accuracy: 0.88356 |  0:01:49s\n",
            "epoch 47 | loss: 0.29401 | train_logloss: 0.28933 | train_accuracy: 0.89133 | valid_logloss: 0.33747 | valid_accuracy: 0.88011 |  0:01:51s\n",
            "epoch 48 | loss: 0.29924 | train_logloss: 0.28212 | train_accuracy: 0.88978 | valid_logloss: 0.34294 | valid_accuracy: 0.88144 |  0:01:54s\n",
            "epoch 49 | loss: 0.29636 | train_logloss: 0.28254 | train_accuracy: 0.89144 | valid_logloss: 0.34242 | valid_accuracy: 0.87822 |  0:01:56s\n",
            "epoch 50 | loss: 0.29225 | train_logloss: 0.26659 | train_accuracy: 0.89467 | valid_logloss: 0.32999 | valid_accuracy: 0.88122 |  0:01:59s\n",
            "epoch 51 | loss: 0.29132 | train_logloss: 0.28049 | train_accuracy: 0.89011 | valid_logloss: 0.34988 | valid_accuracy: 0.87722 |  0:02:01s\n",
            "epoch 52 | loss: 0.29238 | train_logloss: 0.27814 | train_accuracy: 0.88711 | valid_logloss: 0.34464 | valid_accuracy: 0.875   |  0:02:03s\n",
            "epoch 53 | loss: 0.29156 | train_logloss: 0.26994 | train_accuracy: 0.89633 | valid_logloss: 0.3232  | valid_accuracy: 0.88311 |  0:02:06s\n",
            "epoch 54 | loss: 0.28745 | train_logloss: 0.27454 | train_accuracy: 0.89544 | valid_logloss: 0.33311 | valid_accuracy: 0.88378 |  0:02:08s\n",
            "epoch 55 | loss: 0.2856  | train_logloss: 0.26706 | train_accuracy: 0.89667 | valid_logloss: 0.3388  | valid_accuracy: 0.88156 |  0:02:10s\n",
            "epoch 56 | loss: 0.2815  | train_logloss: 0.26624 | train_accuracy: 0.89578 | valid_logloss: 0.33719 | valid_accuracy: 0.88256 |  0:02:13s\n",
            "epoch 57 | loss: 0.28443 | train_logloss: 0.26101 | train_accuracy: 0.89644 | valid_logloss: 0.33286 | valid_accuracy: 0.88389 |  0:02:15s\n",
            "epoch 58 | loss: 0.28166 | train_logloss: 0.26273 | train_accuracy: 0.89722 | valid_logloss: 0.33957 | valid_accuracy: 0.88478 |  0:02:17s\n",
            "epoch 59 | loss: 0.28311 | train_logloss: 0.26269 | train_accuracy: 0.89889 | valid_logloss: 0.3275  | valid_accuracy: 0.887   |  0:02:20s\n",
            "epoch 60 | loss: 0.28125 | train_logloss: 0.26391 | train_accuracy: 0.89767 | valid_logloss: 0.33316 | valid_accuracy: 0.88478 |  0:02:22s\n",
            "epoch 61 | loss: 0.28123 | train_logloss: 0.26732 | train_accuracy: 0.89567 | valid_logloss: 0.34557 | valid_accuracy: 0.87978 |  0:02:24s\n",
            "epoch 62 | loss: 0.2861  | train_logloss: 0.26108 | train_accuracy: 0.89767 | valid_logloss: 0.32649 | valid_accuracy: 0.88367 |  0:02:27s\n",
            "epoch 63 | loss: 0.28693 | train_logloss: 0.27199 | train_accuracy: 0.89589 | valid_logloss: 0.32856 | valid_accuracy: 0.88533 |  0:02:29s\n",
            "epoch 64 | loss: 0.27873 | train_logloss: 0.26233 | train_accuracy: 0.89956 | valid_logloss: 0.33096 | valid_accuracy: 0.88511 |  0:02:31s\n",
            "epoch 65 | loss: 0.28254 | train_logloss: 0.26021 | train_accuracy: 0.89722 | valid_logloss: 0.33083 | valid_accuracy: 0.88422 |  0:02:35s\n",
            "epoch 66 | loss: 0.27896 | train_logloss: 0.25986 | train_accuracy: 0.89589 | valid_logloss: 0.34193 | valid_accuracy: 0.88278 |  0:02:41s\n",
            "epoch 67 | loss: 0.28215 | train_logloss: 0.28595 | train_accuracy: 0.89433 | valid_logloss: 0.34652 | valid_accuracy: 0.88144 |  0:02:44s\n",
            "epoch 68 | loss: 0.28672 | train_logloss: 0.26594 | train_accuracy: 0.89767 | valid_logloss: 0.33318 | valid_accuracy: 0.88111 |  0:02:47s\n",
            "epoch 69 | loss: 0.28543 | train_logloss: 0.27111 | train_accuracy: 0.89622 | valid_logloss: 0.33642 | valid_accuracy: 0.88011 |  0:02:49s\n",
            "epoch 70 | loss: 0.28362 | train_logloss: 0.26187 | train_accuracy: 0.89822 | valid_logloss: 0.33543 | valid_accuracy: 0.88267 |  0:02:51s\n",
            "epoch 71 | loss: 0.27019 | train_logloss: 0.26257 | train_accuracy: 0.899   | valid_logloss: 0.34414 | valid_accuracy: 0.88656 |  0:02:54s\n",
            "epoch 72 | loss: 0.27354 | train_logloss: 0.26005 | train_accuracy: 0.89933 | valid_logloss: 0.33748 | valid_accuracy: 0.88333 |  0:02:56s\n",
            "epoch 73 | loss: 0.27228 | train_logloss: 0.26123 | train_accuracy: 0.89922 | valid_logloss: 0.34229 | valid_accuracy: 0.88356 |  0:02:58s\n",
            "epoch 74 | loss: 0.27474 | train_logloss: 0.27044 | train_accuracy: 0.896   | valid_logloss: 0.34604 | valid_accuracy: 0.88044 |  0:03:01s\n",
            "epoch 75 | loss: 0.27597 | train_logloss: 0.28815 | train_accuracy: 0.89044 | valid_logloss: 0.36675 | valid_accuracy: 0.87344 |  0:03:03s\n",
            "epoch 76 | loss: 0.29577 | train_logloss: 0.28084 | train_accuracy: 0.89189 | valid_logloss: 0.33766 | valid_accuracy: 0.88067 |  0:03:05s\n",
            "epoch 77 | loss: 0.29022 | train_logloss: 0.2657  | train_accuracy: 0.89544 | valid_logloss: 0.32775 | valid_accuracy: 0.88489 |  0:03:08s\n",
            "epoch 78 | loss: 0.28284 | train_logloss: 0.26184 | train_accuracy: 0.89944 | valid_logloss: 0.33479 | valid_accuracy: 0.88089 |  0:03:10s\n",
            "epoch 79 | loss: 0.27667 | train_logloss: 0.25519 | train_accuracy: 0.9     | valid_logloss: 0.33313 | valid_accuracy: 0.88733 |  0:03:12s\n",
            "epoch 80 | loss: 0.27549 | train_logloss: 0.25888 | train_accuracy: 0.89844 | valid_logloss: 0.34058 | valid_accuracy: 0.88111 |  0:03:15s\n",
            "epoch 81 | loss: 0.2685  | train_logloss: 0.26736 | train_accuracy: 0.89956 | valid_logloss: 0.33755 | valid_accuracy: 0.88156 |  0:03:17s\n",
            "epoch 82 | loss: 0.27282 | train_logloss: 0.26663 | train_accuracy: 0.89889 | valid_logloss: 0.34357 | valid_accuracy: 0.88233 |  0:03:20s\n",
            "epoch 83 | loss: 0.28006 | train_logloss: 0.26451 | train_accuracy: 0.89744 | valid_logloss: 0.33729 | valid_accuracy: 0.88422 |  0:03:22s\n",
            "epoch 84 | loss: 0.2727  | train_logloss: 0.25165 | train_accuracy: 0.90044 | valid_logloss: 0.33279 | valid_accuracy: 0.88156 |  0:03:24s\n",
            "epoch 85 | loss: 0.26551 | train_logloss: 0.246   | train_accuracy: 0.90444 | valid_logloss: 0.33437 | valid_accuracy: 0.88256 |  0:03:27s\n",
            "epoch 86 | loss: 0.27954 | train_logloss: 0.25087 | train_accuracy: 0.902   | valid_logloss: 0.32974 | valid_accuracy: 0.88411 |  0:03:29s\n",
            "epoch 87 | loss: 0.27054 | train_logloss: 0.25703 | train_accuracy: 0.904   | valid_logloss: 0.33794 | valid_accuracy: 0.88322 |  0:03:31s\n",
            "epoch 88 | loss: 0.27414 | train_logloss: 0.25774 | train_accuracy: 0.90322 | valid_logloss: 0.33436 | valid_accuracy: 0.88367 |  0:03:34s\n",
            "epoch 89 | loss: 0.28049 | train_logloss: 0.25911 | train_accuracy: 0.90011 | valid_logloss: 0.34048 | valid_accuracy: 0.885   |  0:03:36s\n",
            "epoch 90 | loss: 0.27385 | train_logloss: 0.24699 | train_accuracy: 0.90367 | valid_logloss: 0.33181 | valid_accuracy: 0.88344 |  0:03:38s\n",
            "epoch 91 | loss: 0.26798 | train_logloss: 0.25209 | train_accuracy: 0.90178 | valid_logloss: 0.33555 | valid_accuracy: 0.88044 |  0:03:41s\n",
            "epoch 92 | loss: 0.26092 | train_logloss: 0.24172 | train_accuracy: 0.90467 | valid_logloss: 0.32965 | valid_accuracy: 0.88578 |  0:03:43s\n",
            "epoch 93 | loss: 0.25546 | train_logloss: 0.25411 | train_accuracy: 0.89789 | valid_logloss: 0.34901 | valid_accuracy: 0.88033 |  0:03:45s\n",
            "epoch 94 | loss: 0.26408 | train_logloss: 0.24978 | train_accuracy: 0.90289 | valid_logloss: 0.33123 | valid_accuracy: 0.88667 |  0:03:48s\n",
            "epoch 95 | loss: 0.26532 | train_logloss: 0.25138 | train_accuracy: 0.90122 | valid_logloss: 0.3332  | valid_accuracy: 0.88578 |  0:03:50s\n",
            "epoch 96 | loss: 0.26352 | train_logloss: 0.24463 | train_accuracy: 0.90556 | valid_logloss: 0.33582 | valid_accuracy: 0.88467 |  0:03:53s\n",
            "epoch 97 | loss: 0.26097 | train_logloss: 0.25024 | train_accuracy: 0.90322 | valid_logloss: 0.33854 | valid_accuracy: 0.88389 |  0:03:55s\n",
            "epoch 98 | loss: 0.26416 | train_logloss: 0.24829 | train_accuracy: 0.90389 | valid_logloss: 0.33781 | valid_accuracy: 0.88544 |  0:03:57s\n",
            "epoch 99 | loss: 0.27331 | train_logloss: 0.25602 | train_accuracy: 0.89889 | valid_logloss: 0.34078 | valid_accuracy: 0.88322 |  0:04:00s\n",
            "epoch 100| loss: 0.26753 | train_logloss: 0.24529 | train_accuracy: 0.90433 | valid_logloss: 0.33645 | valid_accuracy: 0.88867 |  0:04:02s\n",
            "epoch 101| loss: 0.26226 | train_logloss: 0.25916 | train_accuracy: 0.89867 | valid_logloss: 0.35391 | valid_accuracy: 0.88011 |  0:04:04s\n",
            "epoch 102| loss: 0.2666  | train_logloss: 0.24541 | train_accuracy: 0.90244 | valid_logloss: 0.34048 | valid_accuracy: 0.88189 |  0:04:07s\n",
            "epoch 103| loss: 0.26318 | train_logloss: 0.24498 | train_accuracy: 0.90789 | valid_logloss: 0.35033 | valid_accuracy: 0.88367 |  0:04:09s\n",
            "epoch 104| loss: 0.27708 | train_logloss: 0.2422  | train_accuracy: 0.906   | valid_logloss: 0.3318  | valid_accuracy: 0.88367 |  0:04:12s\n",
            "epoch 105| loss: 0.26606 | train_logloss: 0.25392 | train_accuracy: 0.89989 | valid_logloss: 0.33995 | valid_accuracy: 0.88333 |  0:04:14s\n",
            "epoch 106| loss: 0.2737  | train_logloss: 0.26235 | train_accuracy: 0.89722 | valid_logloss: 0.33986 | valid_accuracy: 0.882   |  0:04:16s\n",
            "epoch 107| loss: 0.27841 | train_logloss: 0.26195 | train_accuracy: 0.89589 | valid_logloss: 0.34442 | valid_accuracy: 0.87633 |  0:04:19s\n",
            "epoch 108| loss: 0.27045 | train_logloss: 0.25307 | train_accuracy: 0.90244 | valid_logloss: 0.33555 | valid_accuracy: 0.884   |  0:04:21s\n",
            "epoch 109| loss: 0.26994 | train_logloss: 0.26844 | train_accuracy: 0.89967 | valid_logloss: 0.35795 | valid_accuracy: 0.87656 |  0:04:23s\n",
            "epoch 110| loss: 0.26089 | train_logloss: 0.23546 | train_accuracy: 0.90744 | valid_logloss: 0.33031 | valid_accuracy: 0.88567 |  0:04:26s\n",
            "epoch 111| loss: 0.25221 | train_logloss: 0.25135 | train_accuracy: 0.90433 | valid_logloss: 0.35883 | valid_accuracy: 0.87933 |  0:04:28s\n",
            "epoch 112| loss: 0.25453 | train_logloss: 0.23786 | train_accuracy: 0.90511 | valid_logloss: 0.33987 | valid_accuracy: 0.88133 |  0:04:31s\n",
            "epoch 113| loss: 0.25615 | train_logloss: 0.24304 | train_accuracy: 0.90467 | valid_logloss: 0.34012 | valid_accuracy: 0.88622 |  0:04:33s\n",
            "epoch 114| loss: 0.26171 | train_logloss: 0.2469  | train_accuracy: 0.903   | valid_logloss: 0.34842 | valid_accuracy: 0.88267 |  0:04:35s\n",
            "epoch 115| loss: 0.26111 | train_logloss: 0.23648 | train_accuracy: 0.909   | valid_logloss: 0.33373 | valid_accuracy: 0.88589 |  0:04:38s\n",
            "epoch 116| loss: 0.25437 | train_logloss: 0.25023 | train_accuracy: 0.90333 | valid_logloss: 0.35124 | valid_accuracy: 0.87967 |  0:04:40s\n",
            "epoch 117| loss: 0.25199 | train_logloss: 0.2381  | train_accuracy: 0.90711 | valid_logloss: 0.34542 | valid_accuracy: 0.887   |  0:04:42s\n",
            "epoch 118| loss: 0.25377 | train_logloss: 0.23595 | train_accuracy: 0.90722 | valid_logloss: 0.35495 | valid_accuracy: 0.88156 |  0:04:45s\n",
            "epoch 119| loss: 0.24571 | train_logloss: 0.2394  | train_accuracy: 0.90444 | valid_logloss: 0.3588  | valid_accuracy: 0.88033 |  0:04:47s\n",
            "epoch 120| loss: 0.25182 | train_logloss: 0.24004 | train_accuracy: 0.90733 | valid_logloss: 0.34353 | valid_accuracy: 0.883   |  0:04:49s\n",
            "epoch 121| loss: 0.25548 | train_logloss: 0.25081 | train_accuracy: 0.90256 | valid_logloss: 0.35096 | valid_accuracy: 0.88522 |  0:04:52s\n",
            "epoch 122| loss: 0.2507  | train_logloss: 0.23328 | train_accuracy: 0.90633 | valid_logloss: 0.33395 | valid_accuracy: 0.88578 |  0:04:54s\n",
            "epoch 123| loss: 0.25089 | train_logloss: 0.23849 | train_accuracy: 0.90411 | valid_logloss: 0.36135 | valid_accuracy: 0.87922 |  0:04:56s\n",
            "epoch 124| loss: 0.25827 | train_logloss: 0.2478  | train_accuracy: 0.90233 | valid_logloss: 0.34873 | valid_accuracy: 0.886   |  0:04:59s\n",
            "epoch 125| loss: 0.25934 | train_logloss: 0.23813 | train_accuracy: 0.90567 | valid_logloss: 0.35014 | valid_accuracy: 0.88222 |  0:05:01s\n",
            "epoch 126| loss: 0.24903 | train_logloss: 0.24202 | train_accuracy: 0.90544 | valid_logloss: 0.34589 | valid_accuracy: 0.88311 |  0:05:04s\n",
            "epoch 127| loss: 0.25252 | train_logloss: 0.22937 | train_accuracy: 0.90811 | valid_logloss: 0.35491 | valid_accuracy: 0.886   |  0:05:06s\n",
            "epoch 128| loss: 0.25221 | train_logloss: 0.23492 | train_accuracy: 0.90856 | valid_logloss: 0.34216 | valid_accuracy: 0.88256 |  0:05:08s\n",
            "epoch 129| loss: 0.25395 | train_logloss: 0.23    | train_accuracy: 0.909   | valid_logloss: 0.33547 | valid_accuracy: 0.88489 |  0:05:11s\n",
            "epoch 130| loss: 0.24362 | train_logloss: 0.22519 | train_accuracy: 0.912   | valid_logloss: 0.36653 | valid_accuracy: 0.88067 |  0:05:13s\n",
            "\n",
            "Early stopping occurred at epoch 130 with best_epoch = 100 and best_valid_accuracy = 0.88867\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[406911  15131  58140]\n",
            " [ 14470 948392  37138]\n",
            " [ 30755  27342 374490]]\n",
            "Testing Score:  0.8886666666666667\n",
            "100%|██████████| 10/10 [00:56<00:00,  5.64s/it, best loss: -0.887888888888889]\n",
            "({'colsample_bytree': 0.38320470308204635, 'min_child_samples': 22, 'min_child_weight': 0.703570258869821, 'num_leaves': 70}, <hyperopt.base.Trials object at 0x7fcdd2a993d0>)\n",
            "3.8639848232269287\n",
            "Confusion Matrix: \n",
            " [[415552  13684  50946]\n",
            " [ 14331 953817  31852]\n",
            " [ 37157  28825 366605]]\n",
            "Testing Score:  0.8873333333333333\n",
            "{'Rows': 9000, 'Nd': 16, 'Na': 16, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 313.88328194618225, 'time_tn': 92.41554379463196, 'accuracy_tn': 0.8886666666666667, 'time_learn_gb': 1618973809.9167619, 'time_gb': 13.39914083480835, 'accuracy_gb': 0.8873333333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uatx344UH3la",
        "outputId": "ac2276f1-5270-48a3-e419-74d9f20f78b4"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...    time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...  70.837914    0.886444  1.618973e+09  16.887252\n",
              "1  9000  9000  9000  16  ...  92.415544    0.887333  1.618974e+09  13.399141\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xJNKwSI-ldE",
        "outputId": "f6f16094-9e32-4d66-b2a2-2cc15a0e2ad4"
      },
      "source": [
        "time_model(number_exp=3, \n",
        "          Rows=9000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=512,\tBV=128,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 1.13429 | train_logloss: 20.65988| train_accuracy: 0.14711 | valid_logloss: 20.57821| valid_accuracy: 0.14922 |  0:00:06s\n",
            "epoch 1  | loss: 0.63151 | train_logloss: 14.86422| train_accuracy: 0.32056 | valid_logloss: 14.98605| valid_accuracy: 0.32089 |  0:00:14s\n",
            "epoch 2  | loss: 0.54626 | train_logloss: 7.24506 | train_accuracy: 0.36867 | valid_logloss: 7.27262 | valid_accuracy: 0.36989 |  0:00:21s\n",
            "epoch 3  | loss: 0.48646 | train_logloss: 8.98477 | train_accuracy: 0.26111 | valid_logloss: 8.9797  | valid_accuracy: 0.26633 |  0:00:28s\n",
            "epoch 4  | loss: 0.46108 | train_logloss: 5.03758 | train_accuracy: 0.55211 | valid_logloss: 5.11444 | valid_accuracy: 0.54878 |  0:00:35s\n",
            "epoch 5  | loss: 0.43743 | train_logloss: 3.09253 | train_accuracy: 0.601   | valid_logloss: 3.13618 | valid_accuracy: 0.60589 |  0:00:42s\n",
            "epoch 6  | loss: 0.41653 | train_logloss: 2.67541 | train_accuracy: 0.32233 | valid_logloss: 2.67537 | valid_accuracy: 0.32611 |  0:00:51s\n",
            "epoch 7  | loss: 0.39827 | train_logloss: 1.56524 | train_accuracy: 0.42367 | valid_logloss: 1.59581 | valid_accuracy: 0.42233 |  0:01:02s\n",
            "epoch 8  | loss: 0.39474 | train_logloss: 1.22983 | train_accuracy: 0.49411 | valid_logloss: 1.22679 | valid_accuracy: 0.49067 |  0:01:12s\n",
            "epoch 9  | loss: 0.37534 | train_logloss: 1.20098 | train_accuracy: 0.52011 | valid_logloss: 1.20194 | valid_accuracy: 0.52022 |  0:01:19s\n",
            "epoch 10 | loss: 0.36481 | train_logloss: 1.02398 | train_accuracy: 0.65022 | valid_logloss: 1.03321 | valid_accuracy: 0.64811 |  0:01:26s\n",
            "epoch 11 | loss: 0.36559 | train_logloss: 1.10683 | train_accuracy: 0.62578 | valid_logloss: 1.13029 | valid_accuracy: 0.61956 |  0:01:33s\n",
            "epoch 12 | loss: 0.37091 | train_logloss: 0.71038 | train_accuracy: 0.74433 | valid_logloss: 0.73137 | valid_accuracy: 0.73722 |  0:01:40s\n",
            "epoch 13 | loss: 0.3538  | train_logloss: 0.55516 | train_accuracy: 0.79567 | valid_logloss: 0.57798 | valid_accuracy: 0.78267 |  0:01:47s\n",
            "epoch 14 | loss: 0.35122 | train_logloss: 0.5327  | train_accuracy: 0.81044 | valid_logloss: 0.54671 | valid_accuracy: 0.80356 |  0:01:54s\n",
            "epoch 15 | loss: 0.34929 | train_logloss: 0.47305 | train_accuracy: 0.81789 | valid_logloss: 0.48593 | valid_accuracy: 0.81489 |  0:02:01s\n",
            "epoch 16 | loss: 0.3458  | train_logloss: 0.40155 | train_accuracy: 0.84956 | valid_logloss: 0.41528 | valid_accuracy: 0.84889 |  0:02:08s\n",
            "epoch 17 | loss: 0.34119 | train_logloss: 0.37378 | train_accuracy: 0.85756 | valid_logloss: 0.38928 | valid_accuracy: 0.85244 |  0:02:15s\n",
            "epoch 18 | loss: 0.3379  | train_logloss: 0.36679 | train_accuracy: 0.86033 | valid_logloss: 0.3923  | valid_accuracy: 0.85622 |  0:02:23s\n",
            "epoch 19 | loss: 0.33063 | train_logloss: 0.34879 | train_accuracy: 0.86722 | valid_logloss: 0.37636 | valid_accuracy: 0.86256 |  0:02:30s\n",
            "epoch 20 | loss: 0.33448 | train_logloss: 0.36312 | train_accuracy: 0.86378 | valid_logloss: 0.38944 | valid_accuracy: 0.85933 |  0:02:37s\n",
            "epoch 21 | loss: 0.33498 | train_logloss: 0.34282 | train_accuracy: 0.87011 | valid_logloss: 0.37497 | valid_accuracy: 0.86456 |  0:02:44s\n",
            "epoch 22 | loss: 0.32099 | train_logloss: 0.32234 | train_accuracy: 0.87633 | valid_logloss: 0.35947 | valid_accuracy: 0.87089 |  0:02:51s\n",
            "epoch 23 | loss: 0.32142 | train_logloss: 0.34796 | train_accuracy: 0.87111 | valid_logloss: 0.38572 | valid_accuracy: 0.866   |  0:02:58s\n",
            "epoch 24 | loss: 0.34592 | train_logloss: 0.32956 | train_accuracy: 0.87833 | valid_logloss: 0.35891 | valid_accuracy: 0.874   |  0:03:05s\n",
            "epoch 25 | loss: 0.33694 | train_logloss: 0.33611 | train_accuracy: 0.87133 | valid_logloss: 0.36627 | valid_accuracy: 0.86644 |  0:03:12s\n",
            "epoch 26 | loss: 0.34307 | train_logloss: 0.33342 | train_accuracy: 0.87222 | valid_logloss: 0.36077 | valid_accuracy: 0.86889 |  0:03:19s\n",
            "epoch 27 | loss: 0.34268 | train_logloss: 0.33593 | train_accuracy: 0.87189 | valid_logloss: 0.36431 | valid_accuracy: 0.86578 |  0:03:26s\n",
            "epoch 28 | loss: 0.34362 | train_logloss: 0.31693 | train_accuracy: 0.87833 | valid_logloss: 0.35117 | valid_accuracy: 0.87033 |  0:03:33s\n",
            "epoch 29 | loss: 0.33416 | train_logloss: 0.3155  | train_accuracy: 0.87844 | valid_logloss: 0.34899 | valid_accuracy: 0.87289 |  0:03:40s\n",
            "epoch 30 | loss: 0.3308  | train_logloss: 0.31813 | train_accuracy: 0.87444 | valid_logloss: 0.3567  | valid_accuracy: 0.87178 |  0:03:47s\n",
            "epoch 31 | loss: 0.3434  | train_logloss: 0.33574 | train_accuracy: 0.87178 | valid_logloss: 0.36065 | valid_accuracy: 0.86756 |  0:03:54s\n",
            "epoch 32 | loss: 0.33943 | train_logloss: 0.33973 | train_accuracy: 0.86911 | valid_logloss: 0.37504 | valid_accuracy: 0.865   |  0:04:02s\n",
            "epoch 33 | loss: 0.36082 | train_logloss: 0.34073 | train_accuracy: 0.87189 | valid_logloss: 0.36923 | valid_accuracy: 0.86767 |  0:04:09s\n",
            "epoch 34 | loss: 0.34389 | train_logloss: 0.32774 | train_accuracy: 0.87689 | valid_logloss: 0.35315 | valid_accuracy: 0.86989 |  0:04:16s\n",
            "epoch 35 | loss: 0.33239 | train_logloss: 0.32903 | train_accuracy: 0.87633 | valid_logloss: 0.3557  | valid_accuracy: 0.87244 |  0:04:23s\n",
            "epoch 36 | loss: 0.32574 | train_logloss: 0.31422 | train_accuracy: 0.88378 | valid_logloss: 0.35019 | valid_accuracy: 0.87378 |  0:04:30s\n",
            "epoch 37 | loss: 0.33602 | train_logloss: 0.32167 | train_accuracy: 0.88089 | valid_logloss: 0.35759 | valid_accuracy: 0.87378 |  0:04:38s\n",
            "epoch 38 | loss: 0.32942 | train_logloss: 0.31002 | train_accuracy: 0.88122 | valid_logloss: 0.34263 | valid_accuracy: 0.87767 |  0:04:47s\n",
            "epoch 39 | loss: 0.32551 | train_logloss: 0.31295 | train_accuracy: 0.87944 | valid_logloss: 0.348   | valid_accuracy: 0.87322 |  0:04:56s\n",
            "epoch 40 | loss: 0.32849 | train_logloss: 0.30927 | train_accuracy: 0.88056 | valid_logloss: 0.34875 | valid_accuracy: 0.87256 |  0:05:04s\n",
            "epoch 41 | loss: 0.32126 | train_logloss: 0.30781 | train_accuracy: 0.88233 | valid_logloss: 0.34798 | valid_accuracy: 0.87511 |  0:05:11s\n",
            "epoch 42 | loss: 0.32201 | train_logloss: 0.30823 | train_accuracy: 0.88189 | valid_logloss: 0.34399 | valid_accuracy: 0.87289 |  0:05:18s\n",
            "epoch 43 | loss: 0.32438 | train_logloss: 0.31216 | train_accuracy: 0.88078 | valid_logloss: 0.3448  | valid_accuracy: 0.87578 |  0:05:25s\n",
            "epoch 44 | loss: 0.32875 | train_logloss: 0.31456 | train_accuracy: 0.88222 | valid_logloss: 0.34622 | valid_accuracy: 0.87367 |  0:05:32s\n",
            "epoch 45 | loss: 0.33005 | train_logloss: 0.32785 | train_accuracy: 0.86933 | valid_logloss: 0.35748 | valid_accuracy: 0.86778 |  0:05:39s\n",
            "epoch 46 | loss: 0.32783 | train_logloss: 0.31085 | train_accuracy: 0.881   | valid_logloss: 0.34711 | valid_accuracy: 0.87789 |  0:05:46s\n",
            "epoch 47 | loss: 0.32918 | train_logloss: 0.32694 | train_accuracy: 0.87589 | valid_logloss: 0.35831 | valid_accuracy: 0.87289 |  0:05:53s\n",
            "epoch 48 | loss: 0.32919 | train_logloss: 0.30616 | train_accuracy: 0.88311 | valid_logloss: 0.33975 | valid_accuracy: 0.87433 |  0:06:00s\n",
            "epoch 49 | loss: 0.32006 | train_logloss: 0.30964 | train_accuracy: 0.88067 | valid_logloss: 0.34728 | valid_accuracy: 0.87244 |  0:06:07s\n",
            "epoch 50 | loss: 0.3139  | train_logloss: 0.30178 | train_accuracy: 0.88356 | valid_logloss: 0.33667 | valid_accuracy: 0.87711 |  0:06:14s\n",
            "epoch 51 | loss: 0.31507 | train_logloss: 0.29697 | train_accuracy: 0.88522 | valid_logloss: 0.33209 | valid_accuracy: 0.879   |  0:06:21s\n",
            "epoch 52 | loss: 0.3122  | train_logloss: 0.29381 | train_accuracy: 0.88722 | valid_logloss: 0.33201 | valid_accuracy: 0.87978 |  0:06:28s\n",
            "epoch 53 | loss: 0.30893 | train_logloss: 0.29136 | train_accuracy: 0.886   | valid_logloss: 0.33146 | valid_accuracy: 0.88144 |  0:06:35s\n",
            "epoch 54 | loss: 0.3074  | train_logloss: 0.2926  | train_accuracy: 0.88522 | valid_logloss: 0.33429 | valid_accuracy: 0.87889 |  0:06:42s\n",
            "epoch 55 | loss: 0.31578 | train_logloss: 0.31131 | train_accuracy: 0.88278 | valid_logloss: 0.35699 | valid_accuracy: 0.87556 |  0:06:49s\n",
            "epoch 56 | loss: 0.30934 | train_logloss: 0.28879 | train_accuracy: 0.88667 | valid_logloss: 0.3335  | valid_accuracy: 0.88278 |  0:06:56s\n",
            "epoch 57 | loss: 0.30335 | train_logloss: 0.30003 | train_accuracy: 0.885   | valid_logloss: 0.33862 | valid_accuracy: 0.87944 |  0:07:03s\n",
            "epoch 58 | loss: 0.30891 | train_logloss: 0.29745 | train_accuracy: 0.88633 | valid_logloss: 0.33493 | valid_accuracy: 0.88211 |  0:07:10s\n",
            "epoch 59 | loss: 0.30487 | train_logloss: 0.28203 | train_accuracy: 0.89056 | valid_logloss: 0.33525 | valid_accuracy: 0.88033 |  0:07:17s\n",
            "epoch 60 | loss: 0.29522 | train_logloss: 0.27829 | train_accuracy: 0.89056 | valid_logloss: 0.33669 | valid_accuracy: 0.88256 |  0:07:24s\n",
            "epoch 61 | loss: 0.30362 | train_logloss: 0.28905 | train_accuracy: 0.886   | valid_logloss: 0.3425  | valid_accuracy: 0.87567 |  0:07:31s\n",
            "epoch 62 | loss: 0.29973 | train_logloss: 0.28107 | train_accuracy: 0.89033 | valid_logloss: 0.33273 | valid_accuracy: 0.88122 |  0:07:38s\n",
            "epoch 63 | loss: 0.29284 | train_logloss: 0.27641 | train_accuracy: 0.89256 | valid_logloss: 0.3272  | valid_accuracy: 0.88122 |  0:07:45s\n",
            "epoch 64 | loss: 0.29333 | train_logloss: 0.27627 | train_accuracy: 0.89289 | valid_logloss: 0.34117 | valid_accuracy: 0.88211 |  0:07:52s\n",
            "epoch 65 | loss: 0.2906  | train_logloss: 0.275   | train_accuracy: 0.89267 | valid_logloss: 0.33241 | valid_accuracy: 0.88289 |  0:07:59s\n",
            "epoch 66 | loss: 0.28739 | train_logloss: 0.27908 | train_accuracy: 0.89189 | valid_logloss: 0.34076 | valid_accuracy: 0.88056 |  0:08:06s\n",
            "epoch 67 | loss: 0.28973 | train_logloss: 0.26695 | train_accuracy: 0.89644 | valid_logloss: 0.33656 | valid_accuracy: 0.88433 |  0:08:13s\n",
            "epoch 68 | loss: 0.28228 | train_logloss: 0.28112 | train_accuracy: 0.89278 | valid_logloss: 0.34998 | valid_accuracy: 0.88056 |  0:08:20s\n",
            "epoch 69 | loss: 0.28847 | train_logloss: 0.26858 | train_accuracy: 0.892   | valid_logloss: 0.33739 | valid_accuracy: 0.88089 |  0:08:27s\n",
            "epoch 70 | loss: 0.28876 | train_logloss: 0.2659  | train_accuracy: 0.89533 | valid_logloss: 0.32832 | valid_accuracy: 0.88267 |  0:08:35s\n",
            "epoch 71 | loss: 0.2801  | train_logloss: 0.2655  | train_accuracy: 0.89744 | valid_logloss: 0.34303 | valid_accuracy: 0.88033 |  0:08:42s\n",
            "epoch 72 | loss: 0.28431 | train_logloss: 0.26585 | train_accuracy: 0.89578 | valid_logloss: 0.33746 | valid_accuracy: 0.88189 |  0:08:49s\n",
            "epoch 73 | loss: 0.27847 | train_logloss: 0.26129 | train_accuracy: 0.89689 | valid_logloss: 0.3442  | valid_accuracy: 0.88522 |  0:08:56s\n",
            "epoch 74 | loss: 0.27835 | train_logloss: 0.26119 | train_accuracy: 0.89622 | valid_logloss: 0.3478  | valid_accuracy: 0.87889 |  0:09:03s\n",
            "epoch 75 | loss: 0.29683 | train_logloss: 0.29227 | train_accuracy: 0.88978 | valid_logloss: 0.34839 | valid_accuracy: 0.876   |  0:09:10s\n",
            "epoch 76 | loss: 0.30499 | train_logloss: 0.29741 | train_accuracy: 0.88556 | valid_logloss: 0.35534 | valid_accuracy: 0.87089 |  0:09:17s\n",
            "epoch 77 | loss: 0.30425 | train_logloss: 0.28947 | train_accuracy: 0.88956 | valid_logloss: 0.34767 | valid_accuracy: 0.87856 |  0:09:24s\n",
            "epoch 78 | loss: 0.30149 | train_logloss: 0.28775 | train_accuracy: 0.88789 | valid_logloss: 0.34509 | valid_accuracy: 0.87789 |  0:09:31s\n",
            "epoch 79 | loss: 0.28736 | train_logloss: 0.27746 | train_accuracy: 0.89356 | valid_logloss: 0.34128 | valid_accuracy: 0.88378 |  0:09:39s\n",
            "epoch 80 | loss: 0.28626 | train_logloss: 0.26585 | train_accuracy: 0.893   | valid_logloss: 0.32672 | valid_accuracy: 0.88011 |  0:09:46s\n",
            "epoch 81 | loss: 0.28098 | train_logloss: 0.26571 | train_accuracy: 0.89922 | valid_logloss: 0.33764 | valid_accuracy: 0.88189 |  0:09:53s\n",
            "epoch 82 | loss: 0.27809 | train_logloss: 0.25816 | train_accuracy: 0.902   | valid_logloss: 0.33074 | valid_accuracy: 0.88356 |  0:10:01s\n",
            "epoch 83 | loss: 0.27718 | train_logloss: 0.26092 | train_accuracy: 0.89922 | valid_logloss: 0.3295  | valid_accuracy: 0.881   |  0:10:08s\n",
            "epoch 84 | loss: 0.27905 | train_logloss: 0.26236 | train_accuracy: 0.89656 | valid_logloss: 0.33326 | valid_accuracy: 0.88322 |  0:10:15s\n",
            "epoch 85 | loss: 0.27986 | train_logloss: 0.27031 | train_accuracy: 0.896   | valid_logloss: 0.33234 | valid_accuracy: 0.88244 |  0:10:22s\n",
            "epoch 86 | loss: 0.29333 | train_logloss: 0.27812 | train_accuracy: 0.89222 | valid_logloss: 0.33451 | valid_accuracy: 0.87978 |  0:10:30s\n",
            "epoch 87 | loss: 0.2909  | train_logloss: 0.27013 | train_accuracy: 0.89589 | valid_logloss: 0.32968 | valid_accuracy: 0.88333 |  0:10:37s\n",
            "epoch 88 | loss: 0.28957 | train_logloss: 0.26817 | train_accuracy: 0.89567 | valid_logloss: 0.33032 | valid_accuracy: 0.88244 |  0:10:44s\n",
            "epoch 89 | loss: 0.2799  | train_logloss: 0.27415 | train_accuracy: 0.89256 | valid_logloss: 0.34013 | valid_accuracy: 0.886   |  0:10:52s\n",
            "epoch 90 | loss: 0.2803  | train_logloss: 0.28454 | train_accuracy: 0.88978 | valid_logloss: 0.35113 | valid_accuracy: 0.874   |  0:10:59s\n",
            "epoch 91 | loss: 0.28208 | train_logloss: 0.27983 | train_accuracy: 0.89256 | valid_logloss: 0.34768 | valid_accuracy: 0.879   |  0:11:06s\n",
            "epoch 92 | loss: 0.28081 | train_logloss: 0.26267 | train_accuracy: 0.89844 | valid_logloss: 0.33937 | valid_accuracy: 0.88278 |  0:11:14s\n",
            "epoch 93 | loss: 0.2848  | train_logloss: 0.27342 | train_accuracy: 0.89433 | valid_logloss: 0.3436  | valid_accuracy: 0.88111 |  0:11:21s\n",
            "epoch 94 | loss: 0.27614 | train_logloss: 0.26649 | train_accuracy: 0.89922 | valid_logloss: 0.34697 | valid_accuracy: 0.88267 |  0:11:28s\n",
            "epoch 95 | loss: 0.27462 | train_logloss: 0.2629  | train_accuracy: 0.89422 | valid_logloss: 0.33787 | valid_accuracy: 0.88256 |  0:11:35s\n",
            "epoch 96 | loss: 0.2704  | train_logloss: 0.25193 | train_accuracy: 0.90067 | valid_logloss: 0.33336 | valid_accuracy: 0.886   |  0:11:42s\n",
            "epoch 97 | loss: 0.27402 | train_logloss: 0.26437 | train_accuracy: 0.89411 | valid_logloss: 0.34159 | valid_accuracy: 0.88189 |  0:11:50s\n",
            "epoch 98 | loss: 0.2848  | train_logloss: 0.27399 | train_accuracy: 0.89311 | valid_logloss: 0.35884 | valid_accuracy: 0.88011 |  0:11:57s\n",
            "epoch 99 | loss: 0.28258 | train_logloss: 0.26075 | train_accuracy: 0.89733 | valid_logloss: 0.3427  | valid_accuracy: 0.88022 |  0:12:04s\n",
            "epoch 100| loss: 0.27798 | train_logloss: 0.26078 | train_accuracy: 0.89756 | valid_logloss: 0.3427  | valid_accuracy: 0.88322 |  0:12:11s\n",
            "epoch 101| loss: 0.27941 | train_logloss: 0.26063 | train_accuracy: 0.89678 | valid_logloss: 0.33851 | valid_accuracy: 0.87822 |  0:12:18s\n",
            "epoch 102| loss: 0.28254 | train_logloss: 0.25229 | train_accuracy: 0.89933 | valid_logloss: 0.32769 | valid_accuracy: 0.88356 |  0:12:25s\n",
            "epoch 103| loss: 0.27569 | train_logloss: 0.25242 | train_accuracy: 0.89911 | valid_logloss: 0.3283  | valid_accuracy: 0.88689 |  0:12:33s\n",
            "epoch 104| loss: 0.26992 | train_logloss: 0.2599  | train_accuracy: 0.89633 | valid_logloss: 0.34157 | valid_accuracy: 0.88089 |  0:12:40s\n",
            "epoch 105| loss: 0.27574 | train_logloss: 0.25987 | train_accuracy: 0.89644 | valid_logloss: 0.3391  | valid_accuracy: 0.88111 |  0:12:47s\n",
            "epoch 106| loss: 0.27749 | train_logloss: 0.28261 | train_accuracy: 0.89311 | valid_logloss: 0.35891 | valid_accuracy: 0.87689 |  0:12:55s\n",
            "epoch 107| loss: 0.27395 | train_logloss: 0.2546  | train_accuracy: 0.90056 | valid_logloss: 0.338   | valid_accuracy: 0.883   |  0:13:02s\n",
            "epoch 108| loss: 0.27218 | train_logloss: 0.25593 | train_accuracy: 0.90222 | valid_logloss: 0.34243 | valid_accuracy: 0.88278 |  0:13:09s\n",
            "epoch 109| loss: 0.26752 | train_logloss: 0.25277 | train_accuracy: 0.90167 | valid_logloss: 0.34377 | valid_accuracy: 0.88033 |  0:13:16s\n",
            "epoch 110| loss: 0.26729 | train_logloss: 0.24778 | train_accuracy: 0.904   | valid_logloss: 0.34859 | valid_accuracy: 0.88344 |  0:13:23s\n",
            "epoch 111| loss: 0.26508 | train_logloss: 0.25273 | train_accuracy: 0.90567 | valid_logloss: 0.33886 | valid_accuracy: 0.88356 |  0:13:30s\n",
            "epoch 112| loss: 0.26676 | train_logloss: 0.27084 | train_accuracy: 0.89711 | valid_logloss: 0.35231 | valid_accuracy: 0.87878 |  0:13:37s\n",
            "epoch 113| loss: 0.27759 | train_logloss: 0.26909 | train_accuracy: 0.89022 | valid_logloss: 0.35362 | valid_accuracy: 0.87522 |  0:13:45s\n",
            "epoch 114| loss: 0.28101 | train_logloss: 0.25911 | train_accuracy: 0.89856 | valid_logloss: 0.34423 | valid_accuracy: 0.88367 |  0:13:52s\n",
            "epoch 115| loss: 0.26996 | train_logloss: 0.24829 | train_accuracy: 0.90267 | valid_logloss: 0.33497 | valid_accuracy: 0.88389 |  0:13:59s\n",
            "epoch 116| loss: 0.26598 | train_logloss: 0.25071 | train_accuracy: 0.90256 | valid_logloss: 0.33698 | valid_accuracy: 0.88456 |  0:14:06s\n",
            "epoch 117| loss: 0.26553 | train_logloss: 0.25638 | train_accuracy: 0.90033 | valid_logloss: 0.35016 | valid_accuracy: 0.88    |  0:14:13s\n",
            "epoch 118| loss: 0.25594 | train_logloss: 0.23341 | train_accuracy: 0.90878 | valid_logloss: 0.33787 | valid_accuracy: 0.885   |  0:14:20s\n",
            "epoch 119| loss: 0.25894 | train_logloss: 0.23832 | train_accuracy: 0.90722 | valid_logloss: 0.358   | valid_accuracy: 0.87933 |  0:14:27s\n",
            "epoch 120| loss: 0.26244 | train_logloss: 0.23839 | train_accuracy: 0.905   | valid_logloss: 0.33809 | valid_accuracy: 0.88344 |  0:14:34s\n",
            "epoch 121| loss: 0.25531 | train_logloss: 0.24111 | train_accuracy: 0.90078 | valid_logloss: 0.35095 | valid_accuracy: 0.87811 |  0:14:42s\n",
            "epoch 122| loss: 0.25253 | train_logloss: 0.23428 | train_accuracy: 0.90578 | valid_logloss: 0.34879 | valid_accuracy: 0.87911 |  0:14:49s\n",
            "epoch 123| loss: 0.24711 | train_logloss: 0.23694 | train_accuracy: 0.903   | valid_logloss: 0.3564  | valid_accuracy: 0.87689 |  0:14:56s\n",
            "epoch 124| loss: 0.25885 | train_logloss: 0.2538  | train_accuracy: 0.89656 | valid_logloss: 0.37028 | valid_accuracy: 0.87144 |  0:15:03s\n",
            "epoch 125| loss: 0.2572  | train_logloss: 0.23512 | train_accuracy: 0.90511 | valid_logloss: 0.34577 | valid_accuracy: 0.88078 |  0:15:10s\n",
            "epoch 126| loss: 0.25665 | train_logloss: 0.23969 | train_accuracy: 0.90522 | valid_logloss: 0.35177 | valid_accuracy: 0.87633 |  0:15:17s\n",
            "epoch 127| loss: 0.25548 | train_logloss: 0.2349  | train_accuracy: 0.90522 | valid_logloss: 0.35392 | valid_accuracy: 0.88233 |  0:15:25s\n",
            "epoch 128| loss: 0.25776 | train_logloss: 0.24962 | train_accuracy: 0.9     | valid_logloss: 0.35756 | valid_accuracy: 0.88278 |  0:15:32s\n",
            "epoch 129| loss: 0.25436 | train_logloss: 0.23269 | train_accuracy: 0.90522 | valid_logloss: 0.33447 | valid_accuracy: 0.88233 |  0:15:39s\n",
            "epoch 130| loss: 0.24849 | train_logloss: 0.22824 | train_accuracy: 0.90633 | valid_logloss: 0.34985 | valid_accuracy: 0.88556 |  0:15:46s\n",
            "epoch 131| loss: 0.24298 | train_logloss: 0.23503 | train_accuracy: 0.90378 | valid_logloss: 0.35009 | valid_accuracy: 0.87978 |  0:15:54s\n",
            "epoch 132| loss: 0.24281 | train_logloss: 0.22512 | train_accuracy: 0.91144 | valid_logloss: 0.34225 | valid_accuracy: 0.88244 |  0:16:01s\n",
            "epoch 133| loss: 0.23961 | train_logloss: 0.22205 | train_accuracy: 0.91111 | valid_logloss: 0.35696 | valid_accuracy: 0.87956 |  0:16:08s\n",
            "\n",
            "Early stopping occurred at epoch 133 with best_epoch = 103 and best_valid_accuracy = 0.88689\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[421067  14291  44824]\n",
            " [ 19732 945635  34633]\n",
            " [ 43767  27776 361044]]\n",
            "Testing Score:  0.8868888888888888\n",
            "100%|██████████| 10/10 [00:58<00:00,  5.89s/it, best loss: -0.8856666666666666]\n",
            "({'colsample_bytree': 0.5060126076536782, 'min_child_samples': 27, 'min_child_weight': 0.8930263552041086, 'num_leaves': 94}, <hyperopt.base.Trials object at 0x7fcdd825a4d0>)\n",
            "5.6568992137908936\n",
            "Confusion Matrix: \n",
            " [[415644  13428  51110]\n",
            " [ 14678 953446  31876]\n",
            " [ 37517  28520 366550]]\n",
            "Testing Score:  0.8862222222222222\n",
            "{'Rows': 9000, 'Nd': 64, 'Na': 64, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 969.4727473258972, 'time_tn': 205.96458435058594, 'accuracy_tn': 0.8868888888888888, 'time_learn_gb': 1618975074.5923975, 'time_gb': 18.22988224029541, 'accuracy_gb': 0.8862222222222222}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1caD4daqH5qn",
        "outputId": "ab603b70-a40b-4935-ac3c-0d29705869ae"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1  9000  9000  9000  16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2  9000  9000  9000  64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "\n",
              "[3 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT6LTX2j_R1T",
        "outputId": "cff86d53-aefa-47d4-ea30-6cf491d5afc8"
      },
      "source": [
        "time_model(number_exp=4, \n",
        "     Rows=9000, \n",
        "     Nd=32,\tNa=32,\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=3, decision=3, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.83492 | train_logloss: 14.82981| train_accuracy: 0.41533 | valid_logloss: 14.74397| valid_accuracy: 0.41367 |  0:00:06s\n",
            "epoch 1  | loss: 0.53344 | train_logloss: 9.74513 | train_accuracy: 0.33378 | valid_logloss: 9.76129 | valid_accuracy: 0.33344 |  0:00:11s\n",
            "epoch 2  | loss: 0.43706 | train_logloss: 2.67965 | train_accuracy: 0.54944 | valid_logloss: 2.67902 | valid_accuracy: 0.55289 |  0:00:16s\n",
            "epoch 3  | loss: 0.41002 | train_logloss: 2.99112 | train_accuracy: 0.54978 | valid_logloss: 2.99254 | valid_accuracy: 0.54856 |  0:00:22s\n",
            "epoch 4  | loss: 0.39658 | train_logloss: 3.13579 | train_accuracy: 0.47222 | valid_logloss: 3.11745 | valid_accuracy: 0.466   |  0:00:27s\n",
            "epoch 5  | loss: 0.37451 | train_logloss: 3.17404 | train_accuracy: 0.38578 | valid_logloss: 3.16253 | valid_accuracy: 0.38478 |  0:00:31s\n",
            "epoch 6  | loss: 0.36359 | train_logloss: 2.62959 | train_accuracy: 0.37622 | valid_logloss: 2.61132 | valid_accuracy: 0.37944 |  0:00:35s\n",
            "epoch 7  | loss: 0.35351 | train_logloss: 1.58521 | train_accuracy: 0.38522 | valid_logloss: 1.59292 | valid_accuracy: 0.38722 |  0:00:39s\n",
            "epoch 8  | loss: 0.35993 | train_logloss: 1.48278 | train_accuracy: 0.64322 | valid_logloss: 1.46489 | valid_accuracy: 0.64811 |  0:00:43s\n",
            "epoch 9  | loss: 0.34754 | train_logloss: 0.8887  | train_accuracy: 0.70511 | valid_logloss: 0.87036 | valid_accuracy: 0.70778 |  0:00:47s\n",
            "epoch 10 | loss: 0.34815 | train_logloss: 0.91543 | train_accuracy: 0.698   | valid_logloss: 0.90201 | valid_accuracy: 0.70144 |  0:00:51s\n",
            "epoch 11 | loss: 0.34725 | train_logloss: 0.65703 | train_accuracy: 0.76689 | valid_logloss: 0.65471 | valid_accuracy: 0.771   |  0:00:55s\n",
            "epoch 12 | loss: 0.34812 | train_logloss: 0.59295 | train_accuracy: 0.79389 | valid_logloss: 0.59014 | valid_accuracy: 0.79333 |  0:00:59s\n",
            "epoch 13 | loss: 0.33908 | train_logloss: 0.55748 | train_accuracy: 0.80533 | valid_logloss: 0.55669 | valid_accuracy: 0.80778 |  0:01:03s\n",
            "epoch 14 | loss: 0.34328 | train_logloss: 0.49656 | train_accuracy: 0.81289 | valid_logloss: 0.49176 | valid_accuracy: 0.81633 |  0:01:07s\n",
            "epoch 15 | loss: 0.33378 | train_logloss: 0.42802 | train_accuracy: 0.858   | valid_logloss: 0.43517 | valid_accuracy: 0.85744 |  0:01:11s\n",
            "epoch 16 | loss: 0.33516 | train_logloss: 0.38723 | train_accuracy: 0.86089 | valid_logloss: 0.41108 | valid_accuracy: 0.85422 |  0:01:15s\n",
            "epoch 17 | loss: 0.3263  | train_logloss: 0.37268 | train_accuracy: 0.86622 | valid_logloss: 0.39393 | valid_accuracy: 0.86    |  0:01:19s\n",
            "epoch 18 | loss: 0.32486 | train_logloss: 0.35907 | train_accuracy: 0.86389 | valid_logloss: 0.38173 | valid_accuracy: 0.85889 |  0:01:23s\n",
            "epoch 19 | loss: 0.31941 | train_logloss: 0.3433  | train_accuracy: 0.86811 | valid_logloss: 0.37007 | valid_accuracy: 0.86156 |  0:01:27s\n",
            "epoch 20 | loss: 0.32633 | train_logloss: 0.32977 | train_accuracy: 0.87578 | valid_logloss: 0.36877 | valid_accuracy: 0.87189 |  0:01:31s\n",
            "epoch 21 | loss: 0.33573 | train_logloss: 0.33982 | train_accuracy: 0.877   | valid_logloss: 0.3651  | valid_accuracy: 0.86833 |  0:01:35s\n",
            "epoch 22 | loss: 0.34804 | train_logloss: 0.3342  | train_accuracy: 0.87778 | valid_logloss: 0.35675 | valid_accuracy: 0.86978 |  0:01:39s\n",
            "epoch 23 | loss: 0.34508 | train_logloss: 0.36633 | train_accuracy: 0.86111 | valid_logloss: 0.38651 | valid_accuracy: 0.85856 |  0:01:43s\n",
            "epoch 24 | loss: 0.35019 | train_logloss: 0.33742 | train_accuracy: 0.87556 | valid_logloss: 0.35365 | valid_accuracy: 0.86867 |  0:01:47s\n",
            "epoch 25 | loss: 0.33368 | train_logloss: 0.3288  | train_accuracy: 0.86956 | valid_logloss: 0.35989 | valid_accuracy: 0.862   |  0:01:51s\n",
            "epoch 26 | loss: 0.32296 | train_logloss: 0.31191 | train_accuracy: 0.88444 | valid_logloss: 0.34007 | valid_accuracy: 0.87456 |  0:01:55s\n",
            "epoch 27 | loss: 0.31874 | train_logloss: 0.30606 | train_accuracy: 0.88122 | valid_logloss: 0.34655 | valid_accuracy: 0.87211 |  0:01:59s\n",
            "epoch 28 | loss: 0.31183 | train_logloss: 0.30642 | train_accuracy: 0.88478 | valid_logloss: 0.33931 | valid_accuracy: 0.87567 |  0:02:03s\n",
            "epoch 29 | loss: 0.30783 | train_logloss: 0.29597 | train_accuracy: 0.88811 | valid_logloss: 0.33395 | valid_accuracy: 0.87989 |  0:02:07s\n",
            "epoch 30 | loss: 0.3122  | train_logloss: 0.29327 | train_accuracy: 0.88711 | valid_logloss: 0.33414 | valid_accuracy: 0.876   |  0:02:11s\n",
            "epoch 31 | loss: 0.30691 | train_logloss: 0.28775 | train_accuracy: 0.89067 | valid_logloss: 0.32838 | valid_accuracy: 0.87611 |  0:02:15s\n",
            "epoch 32 | loss: 0.30236 | train_logloss: 0.29208 | train_accuracy: 0.88811 | valid_logloss: 0.33231 | valid_accuracy: 0.87767 |  0:02:19s\n",
            "epoch 33 | loss: 0.30494 | train_logloss: 0.29836 | train_accuracy: 0.88367 | valid_logloss: 0.33952 | valid_accuracy: 0.87833 |  0:02:23s\n",
            "epoch 34 | loss: 0.29933 | train_logloss: 0.29656 | train_accuracy: 0.88844 | valid_logloss: 0.34721 | valid_accuracy: 0.87678 |  0:02:27s\n",
            "epoch 35 | loss: 0.31195 | train_logloss: 0.30115 | train_accuracy: 0.88856 | valid_logloss: 0.33188 | valid_accuracy: 0.88244 |  0:02:31s\n",
            "epoch 36 | loss: 0.31109 | train_logloss: 0.3003  | train_accuracy: 0.88478 | valid_logloss: 0.33523 | valid_accuracy: 0.87678 |  0:02:36s\n",
            "epoch 37 | loss: 0.30937 | train_logloss: 0.293   | train_accuracy: 0.88722 | valid_logloss: 0.33331 | valid_accuracy: 0.87744 |  0:02:40s\n",
            "epoch 38 | loss: 0.30499 | train_logloss: 0.28651 | train_accuracy: 0.89033 | valid_logloss: 0.33801 | valid_accuracy: 0.87689 |  0:02:44s\n",
            "epoch 39 | loss: 0.29724 | train_logloss: 0.28757 | train_accuracy: 0.88811 | valid_logloss: 0.33927 | valid_accuracy: 0.878   |  0:02:48s\n",
            "epoch 40 | loss: 0.29405 | train_logloss: 0.28382 | train_accuracy: 0.89078 | valid_logloss: 0.33101 | valid_accuracy: 0.87967 |  0:02:52s\n",
            "epoch 41 | loss: 0.29347 | train_logloss: 0.28147 | train_accuracy: 0.89278 | valid_logloss: 0.33439 | valid_accuracy: 0.87667 |  0:02:55s\n",
            "epoch 42 | loss: 0.29679 | train_logloss: 0.28363 | train_accuracy: 0.89056 | valid_logloss: 0.34011 | valid_accuracy: 0.87611 |  0:02:59s\n",
            "epoch 43 | loss: 0.29997 | train_logloss: 0.29216 | train_accuracy: 0.88744 | valid_logloss: 0.33955 | valid_accuracy: 0.877   |  0:03:04s\n",
            "epoch 44 | loss: 0.30426 | train_logloss: 0.28791 | train_accuracy: 0.88811 | valid_logloss: 0.34751 | valid_accuracy: 0.87856 |  0:03:08s\n",
            "epoch 45 | loss: 0.29611 | train_logloss: 0.29167 | train_accuracy: 0.88644 | valid_logloss: 0.34865 | valid_accuracy: 0.87256 |  0:03:12s\n",
            "epoch 46 | loss: 0.29766 | train_logloss: 0.28003 | train_accuracy: 0.89067 | valid_logloss: 0.33889 | valid_accuracy: 0.87856 |  0:03:16s\n",
            "epoch 47 | loss: 0.29702 | train_logloss: 0.28253 | train_accuracy: 0.88844 | valid_logloss: 0.33653 | valid_accuracy: 0.87789 |  0:03:20s\n",
            "epoch 48 | loss: 0.29322 | train_logloss: 0.28007 | train_accuracy: 0.89322 | valid_logloss: 0.34381 | valid_accuracy: 0.88067 |  0:03:24s\n",
            "epoch 49 | loss: 0.29557 | train_logloss: 0.27906 | train_accuracy: 0.89111 | valid_logloss: 0.33601 | valid_accuracy: 0.87989 |  0:03:28s\n",
            "epoch 50 | loss: 0.28802 | train_logloss: 0.27291 | train_accuracy: 0.894   | valid_logloss: 0.32569 | valid_accuracy: 0.88411 |  0:03:32s\n",
            "epoch 51 | loss: 0.29123 | train_logloss: 0.28339 | train_accuracy: 0.89156 | valid_logloss: 0.34683 | valid_accuracy: 0.87744 |  0:03:36s\n",
            "epoch 52 | loss: 0.28672 | train_logloss: 0.27646 | train_accuracy: 0.89356 | valid_logloss: 0.34076 | valid_accuracy: 0.87933 |  0:03:40s\n",
            "epoch 53 | loss: 0.28646 | train_logloss: 0.27563 | train_accuracy: 0.89356 | valid_logloss: 0.34024 | valid_accuracy: 0.87989 |  0:03:44s\n",
            "epoch 54 | loss: 0.28681 | train_logloss: 0.26406 | train_accuracy: 0.89889 | valid_logloss: 0.32915 | valid_accuracy: 0.882   |  0:03:48s\n",
            "epoch 55 | loss: 0.2852  | train_logloss: 0.26654 | train_accuracy: 0.89656 | valid_logloss: 0.32873 | valid_accuracy: 0.88367 |  0:03:52s\n",
            "epoch 56 | loss: 0.27895 | train_logloss: 0.2608  | train_accuracy: 0.897   | valid_logloss: 0.33879 | valid_accuracy: 0.87989 |  0:03:56s\n",
            "epoch 57 | loss: 0.27712 | train_logloss: 0.26842 | train_accuracy: 0.894   | valid_logloss: 0.34807 | valid_accuracy: 0.87967 |  0:04:00s\n",
            "epoch 58 | loss: 0.27923 | train_logloss: 0.27385 | train_accuracy: 0.89411 | valid_logloss: 0.33922 | valid_accuracy: 0.87978 |  0:04:04s\n",
            "epoch 59 | loss: 0.28544 | train_logloss: 0.26674 | train_accuracy: 0.89644 | valid_logloss: 0.33639 | valid_accuracy: 0.881   |  0:04:08s\n",
            "epoch 60 | loss: 0.28123 | train_logloss: 0.2641  | train_accuracy: 0.89711 | valid_logloss: 0.32833 | valid_accuracy: 0.88144 |  0:04:12s\n",
            "epoch 61 | loss: 0.27922 | train_logloss: 0.25918 | train_accuracy: 0.89956 | valid_logloss: 0.33038 | valid_accuracy: 0.88356 |  0:04:16s\n",
            "epoch 62 | loss: 0.28245 | train_logloss: 0.25986 | train_accuracy: 0.89922 | valid_logloss: 0.33598 | valid_accuracy: 0.88233 |  0:04:20s\n",
            "epoch 63 | loss: 0.27691 | train_logloss: 0.25375 | train_accuracy: 0.89989 | valid_logloss: 0.33734 | valid_accuracy: 0.88456 |  0:04:24s\n",
            "epoch 64 | loss: 0.27157 | train_logloss: 0.27097 | train_accuracy: 0.89878 | valid_logloss: 0.34601 | valid_accuracy: 0.882   |  0:04:28s\n",
            "epoch 65 | loss: 0.27524 | train_logloss: 0.25883 | train_accuracy: 0.89867 | valid_logloss: 0.32588 | valid_accuracy: 0.88567 |  0:04:32s\n",
            "epoch 66 | loss: 0.276   | train_logloss: 0.25203 | train_accuracy: 0.902   | valid_logloss: 0.32662 | valid_accuracy: 0.88711 |  0:04:36s\n",
            "epoch 67 | loss: 0.27081 | train_logloss: 0.25938 | train_accuracy: 0.89878 | valid_logloss: 0.33886 | valid_accuracy: 0.88267 |  0:04:40s\n",
            "epoch 68 | loss: 0.27526 | train_logloss: 0.25927 | train_accuracy: 0.89867 | valid_logloss: 0.34035 | valid_accuracy: 0.88333 |  0:04:44s\n",
            "epoch 69 | loss: 0.27453 | train_logloss: 0.26189 | train_accuracy: 0.901   | valid_logloss: 0.33424 | valid_accuracy: 0.88589 |  0:04:48s\n",
            "epoch 70 | loss: 0.27515 | train_logloss: 0.25151 | train_accuracy: 0.90022 | valid_logloss: 0.33143 | valid_accuracy: 0.88456 |  0:04:52s\n",
            "epoch 71 | loss: 0.2658  | train_logloss: 0.24882 | train_accuracy: 0.90033 | valid_logloss: 0.33249 | valid_accuracy: 0.88167 |  0:04:56s\n",
            "epoch 72 | loss: 0.26907 | train_logloss: 0.26382 | train_accuracy: 0.89611 | valid_logloss: 0.34382 | valid_accuracy: 0.88022 |  0:05:00s\n",
            "epoch 73 | loss: 0.28895 | train_logloss: 0.27018 | train_accuracy: 0.89567 | valid_logloss: 0.33863 | valid_accuracy: 0.88189 |  0:05:04s\n",
            "epoch 74 | loss: 0.27755 | train_logloss: 0.26662 | train_accuracy: 0.89822 | valid_logloss: 0.33917 | valid_accuracy: 0.88122 |  0:05:08s\n",
            "epoch 75 | loss: 0.27084 | train_logloss: 0.25505 | train_accuracy: 0.90033 | valid_logloss: 0.34203 | valid_accuracy: 0.87767 |  0:05:12s\n",
            "epoch 76 | loss: 0.27081 | train_logloss: 0.25411 | train_accuracy: 0.90033 | valid_logloss: 0.34428 | valid_accuracy: 0.87833 |  0:05:16s\n",
            "epoch 77 | loss: 0.2642  | train_logloss: 0.24828 | train_accuracy: 0.902   | valid_logloss: 0.32663 | valid_accuracy: 0.88289 |  0:05:20s\n",
            "epoch 78 | loss: 0.26455 | train_logloss: 0.24706 | train_accuracy: 0.903   | valid_logloss: 0.33475 | valid_accuracy: 0.88267 |  0:05:24s\n",
            "epoch 79 | loss: 0.26838 | train_logloss: 0.24844 | train_accuracy: 0.90278 | valid_logloss: 0.33624 | valid_accuracy: 0.88022 |  0:05:28s\n",
            "epoch 80 | loss: 0.26397 | train_logloss: 0.2443  | train_accuracy: 0.90344 | valid_logloss: 0.33699 | valid_accuracy: 0.88322 |  0:05:32s\n",
            "epoch 81 | loss: 0.26611 | train_logloss: 0.24445 | train_accuracy: 0.90689 | valid_logloss: 0.34674 | valid_accuracy: 0.88033 |  0:05:36s\n",
            "epoch 82 | loss: 0.26124 | train_logloss: 0.24628 | train_accuracy: 0.90411 | valid_logloss: 0.34592 | valid_accuracy: 0.88456 |  0:05:40s\n",
            "epoch 83 | loss: 0.26759 | train_logloss: 0.25131 | train_accuracy: 0.90367 | valid_logloss: 0.33708 | valid_accuracy: 0.882   |  0:05:44s\n",
            "epoch 84 | loss: 0.26357 | train_logloss: 0.24031 | train_accuracy: 0.90589 | valid_logloss: 0.3382  | valid_accuracy: 0.88244 |  0:05:48s\n",
            "epoch 85 | loss: 0.26559 | train_logloss: 0.24312 | train_accuracy: 0.90522 | valid_logloss: 0.33753 | valid_accuracy: 0.88389 |  0:05:52s\n",
            "epoch 86 | loss: 0.25895 | train_logloss: 0.23893 | train_accuracy: 0.90678 | valid_logloss: 0.33319 | valid_accuracy: 0.88378 |  0:05:56s\n",
            "epoch 87 | loss: 0.26031 | train_logloss: 0.24681 | train_accuracy: 0.90322 | valid_logloss: 0.33925 | valid_accuracy: 0.88211 |  0:06:00s\n",
            "epoch 88 | loss: 0.26119 | train_logloss: 0.2482  | train_accuracy: 0.90367 | valid_logloss: 0.34902 | valid_accuracy: 0.88289 |  0:06:04s\n",
            "epoch 89 | loss: 0.25559 | train_logloss: 0.23932 | train_accuracy: 0.90456 | valid_logloss: 0.34532 | valid_accuracy: 0.885   |  0:06:08s\n",
            "epoch 90 | loss: 0.25162 | train_logloss: 0.23993 | train_accuracy: 0.905   | valid_logloss: 0.34279 | valid_accuracy: 0.87911 |  0:06:12s\n",
            "epoch 91 | loss: 0.25547 | train_logloss: 0.22914 | train_accuracy: 0.912   | valid_logloss: 0.34412 | valid_accuracy: 0.88144 |  0:06:16s\n",
            "epoch 92 | loss: 0.25071 | train_logloss: 0.25318 | train_accuracy: 0.89956 | valid_logloss: 0.35475 | valid_accuracy: 0.877   |  0:06:20s\n",
            "epoch 93 | loss: 0.25819 | train_logloss: 0.22821 | train_accuracy: 0.90856 | valid_logloss: 0.35583 | valid_accuracy: 0.88256 |  0:06:24s\n",
            "epoch 94 | loss: 0.2509  | train_logloss: 0.23672 | train_accuracy: 0.90756 | valid_logloss: 0.34815 | valid_accuracy: 0.88156 |  0:06:28s\n",
            "epoch 95 | loss: 0.25377 | train_logloss: 0.23945 | train_accuracy: 0.90889 | valid_logloss: 0.3551  | valid_accuracy: 0.87933 |  0:06:32s\n",
            "epoch 96 | loss: 0.24744 | train_logloss: 0.23472 | train_accuracy: 0.90467 | valid_logloss: 0.35332 | valid_accuracy: 0.87989 |  0:06:36s\n",
            "\n",
            "Early stopping occurred at epoch 96 with best_epoch = 66 and best_valid_accuracy = 0.88711\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[410943  15211  54028]\n",
            " [ 15430 950820  33750]\n",
            " [ 35725  28813 368049]]\n",
            "Testing Score:  0.8871111111111111\n",
            "100%|██████████| 10/10 [01:03<00:00,  6.37s/it, best loss: -0.8834444444444445]\n",
            "({'colsample_bytree': 0.2745406394672486, 'min_child_samples': 46, 'min_child_weight': 0.35520305369169697, 'num_leaves': 75}, <hyperopt.base.Trials object at 0x7fce33767c90>)\n",
            "3.16560959815979\n",
            "Confusion Matrix: \n",
            " [[414554  13690  51938]\n",
            " [ 14203 953551  32246]\n",
            " [ 36307  28737 367543]]\n",
            "Testing Score:  0.8886666666666667\n",
            "{'Rows': 9000, 'Nd': 32, 'Na': 32, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 3, 'decision': 3, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 396.8239207267761, 'time_tn': 124.64023351669312, 'accuracy_tn': 0.8871111111111111, 'time_learn_gb': 1618975702.5644608, 'time_gb': 19.22441005706787, 'accuracy_gb': 0.8886666666666667}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeOxiMxyH6_A",
        "outputId": "342b3597-142a-4c1c-96da-770d372b13b2"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test  Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000   8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1  9000  9000  9000  16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2  9000  9000  9000  64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3  9000  9000  9000  32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "\n",
              "[4 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSD7AS2k_a7N",
        "outputId": "3dc3c443-3e7c-450c-a603-ea327df3d499"
      },
      "source": [
        "time_model(number_exp=5, \n",
        "     Rows=9000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 1.40662 | train_logloss: 12.09789| train_accuracy: 0.41956 | valid_logloss: 12.04145| valid_accuracy: 0.423   |  0:00:13s\n",
            "epoch 1  | loss: 0.8546  | train_logloss: 12.74374| train_accuracy: 0.45633 | valid_logloss: 12.83551| valid_accuracy: 0.45444 |  0:00:26s\n",
            "epoch 2  | loss: 0.56966 | train_logloss: 6.77982 | train_accuracy: 0.44544 | valid_logloss: 6.67853 | valid_accuracy: 0.44922 |  0:00:40s\n",
            "epoch 3  | loss: 0.46991 | train_logloss: 4.47451 | train_accuracy: 0.359   | valid_logloss: 4.38163 | valid_accuracy: 0.36278 |  0:00:54s\n",
            "epoch 4  | loss: 0.44643 | train_logloss: 2.29361 | train_accuracy: 0.30333 | valid_logloss: 2.32588 | valid_accuracy: 0.30789 |  0:01:07s\n",
            "epoch 5  | loss: 0.45202 | train_logloss: 4.65098 | train_accuracy: 0.34567 | valid_logloss: 4.69503 | valid_accuracy: 0.34478 |  0:01:21s\n",
            "epoch 6  | loss: 0.43189 | train_logloss: 1.19607 | train_accuracy: 0.665   | valid_logloss: 1.1846  | valid_accuracy: 0.66033 |  0:01:35s\n",
            "epoch 7  | loss: 0.43847 | train_logloss: 0.7559  | train_accuracy: 0.69367 | valid_logloss: 0.75316 | valid_accuracy: 0.69178 |  0:01:48s\n",
            "epoch 8  | loss: 0.41822 | train_logloss: 1.06667 | train_accuracy: 0.63033 | valid_logloss: 1.05855 | valid_accuracy: 0.63189 |  0:02:02s\n",
            "epoch 9  | loss: 0.40008 | train_logloss: 0.95608 | train_accuracy: 0.64711 | valid_logloss: 0.95616 | valid_accuracy: 0.64344 |  0:02:15s\n",
            "epoch 10 | loss: 0.38619 | train_logloss: 0.74861 | train_accuracy: 0.705   | valid_logloss: 0.74153 | valid_accuracy: 0.70333 |  0:02:28s\n",
            "epoch 11 | loss: 0.38511 | train_logloss: 0.71362 | train_accuracy: 0.74356 | valid_logloss: 0.69975 | valid_accuracy: 0.75067 |  0:02:41s\n",
            "epoch 12 | loss: 0.39146 | train_logloss: 0.52452 | train_accuracy: 0.82411 | valid_logloss: 0.52432 | valid_accuracy: 0.82422 |  0:02:54s\n",
            "epoch 13 | loss: 0.37135 | train_logloss: 0.52617 | train_accuracy: 0.81822 | valid_logloss: 0.53297 | valid_accuracy: 0.81422 |  0:03:08s\n",
            "epoch 14 | loss: 0.36564 | train_logloss: 0.51373 | train_accuracy: 0.81478 | valid_logloss: 0.52644 | valid_accuracy: 0.81656 |  0:03:23s\n",
            "epoch 15 | loss: 0.36432 | train_logloss: 0.43662 | train_accuracy: 0.83833 | valid_logloss: 0.44026 | valid_accuracy: 0.84178 |  0:03:39s\n",
            "epoch 16 | loss: 0.3603  | train_logloss: 0.42754 | train_accuracy: 0.83767 | valid_logloss: 0.43786 | valid_accuracy: 0.83911 |  0:03:56s\n",
            "epoch 17 | loss: 0.35242 | train_logloss: 0.43874 | train_accuracy: 0.83878 | valid_logloss: 0.44311 | valid_accuracy: 0.83678 |  0:04:11s\n",
            "epoch 18 | loss: 0.35507 | train_logloss: 0.41705 | train_accuracy: 0.84544 | valid_logloss: 0.42702 | valid_accuracy: 0.84422 |  0:04:28s\n",
            "epoch 19 | loss: 0.36342 | train_logloss: 0.38768 | train_accuracy: 0.86022 | valid_logloss: 0.39578 | valid_accuracy: 0.858   |  0:04:41s\n",
            "epoch 20 | loss: 0.36426 | train_logloss: 0.39871 | train_accuracy: 0.85211 | valid_logloss: 0.40715 | valid_accuracy: 0.85211 |  0:04:54s\n",
            "epoch 21 | loss: 0.36605 | train_logloss: 0.358   | train_accuracy: 0.86767 | valid_logloss: 0.37777 | valid_accuracy: 0.86222 |  0:05:07s\n",
            "epoch 22 | loss: 0.36524 | train_logloss: 0.37956 | train_accuracy: 0.85811 | valid_logloss: 0.3871  | valid_accuracy: 0.85656 |  0:05:20s\n",
            "epoch 23 | loss: 0.38456 | train_logloss: 0.37476 | train_accuracy: 0.86133 | valid_logloss: 0.38717 | valid_accuracy: 0.863   |  0:05:33s\n",
            "epoch 24 | loss: 0.37978 | train_logloss: 0.36394 | train_accuracy: 0.866   | valid_logloss: 0.3794  | valid_accuracy: 0.86644 |  0:05:46s\n",
            "epoch 25 | loss: 0.3772  | train_logloss: 0.36117 | train_accuracy: 0.86633 | valid_logloss: 0.37731 | valid_accuracy: 0.86578 |  0:05:59s\n",
            "epoch 26 | loss: 0.36449 | train_logloss: 0.35823 | train_accuracy: 0.86911 | valid_logloss: 0.38033 | valid_accuracy: 0.86422 |  0:06:12s\n",
            "epoch 27 | loss: 0.36306 | train_logloss: 0.35563 | train_accuracy: 0.86867 | valid_logloss: 0.37741 | valid_accuracy: 0.86411 |  0:06:25s\n",
            "epoch 28 | loss: 0.35858 | train_logloss: 0.34143 | train_accuracy: 0.86856 | valid_logloss: 0.36095 | valid_accuracy: 0.86989 |  0:06:38s\n",
            "epoch 29 | loss: 0.35167 | train_logloss: 0.33767 | train_accuracy: 0.87456 | valid_logloss: 0.35929 | valid_accuracy: 0.87056 |  0:06:51s\n",
            "epoch 30 | loss: 0.34842 | train_logloss: 0.33495 | train_accuracy: 0.87478 | valid_logloss: 0.35412 | valid_accuracy: 0.87267 |  0:07:04s\n",
            "epoch 31 | loss: 0.34615 | train_logloss: 0.33695 | train_accuracy: 0.87044 | valid_logloss: 0.36137 | valid_accuracy: 0.86811 |  0:07:17s\n",
            "epoch 32 | loss: 0.33712 | train_logloss: 0.34285 | train_accuracy: 0.87122 | valid_logloss: 0.36794 | valid_accuracy: 0.86878 |  0:07:29s\n",
            "epoch 33 | loss: 0.34198 | train_logloss: 0.32068 | train_accuracy: 0.87711 | valid_logloss: 0.35169 | valid_accuracy: 0.87278 |  0:07:42s\n",
            "epoch 34 | loss: 0.35168 | train_logloss: 0.32998 | train_accuracy: 0.87656 | valid_logloss: 0.35713 | valid_accuracy: 0.86833 |  0:07:55s\n",
            "epoch 35 | loss: 0.33964 | train_logloss: 0.32286 | train_accuracy: 0.87833 | valid_logloss: 0.34996 | valid_accuracy: 0.87133 |  0:08:08s\n",
            "epoch 36 | loss: 0.34247 | train_logloss: 0.32914 | train_accuracy: 0.87467 | valid_logloss: 0.36295 | valid_accuracy: 0.86956 |  0:08:21s\n",
            "epoch 37 | loss: 0.342   | train_logloss: 0.35729 | train_accuracy: 0.86589 | valid_logloss: 0.39534 | valid_accuracy: 0.85644 |  0:08:34s\n",
            "epoch 38 | loss: 0.36515 | train_logloss: 0.3346  | train_accuracy: 0.87    | valid_logloss: 0.36474 | valid_accuracy: 0.86633 |  0:08:46s\n",
            "epoch 39 | loss: 0.34848 | train_logloss: 0.32953 | train_accuracy: 0.87744 | valid_logloss: 0.35806 | valid_accuracy: 0.87222 |  0:08:59s\n",
            "epoch 40 | loss: 0.33707 | train_logloss: 0.31956 | train_accuracy: 0.881   | valid_logloss: 0.35706 | valid_accuracy: 0.87156 |  0:09:12s\n",
            "epoch 41 | loss: 0.33475 | train_logloss: 0.31173 | train_accuracy: 0.88144 | valid_logloss: 0.35687 | valid_accuracy: 0.87167 |  0:09:25s\n",
            "epoch 42 | loss: 0.32327 | train_logloss: 0.31002 | train_accuracy: 0.88    | valid_logloss: 0.3615  | valid_accuracy: 0.87289 |  0:09:38s\n",
            "epoch 43 | loss: 0.32039 | train_logloss: 0.31217 | train_accuracy: 0.876   | valid_logloss: 0.36276 | valid_accuracy: 0.86678 |  0:09:50s\n",
            "epoch 44 | loss: 0.32094 | train_logloss: 0.30434 | train_accuracy: 0.883   | valid_logloss: 0.34907 | valid_accuracy: 0.87644 |  0:10:03s\n",
            "epoch 45 | loss: 0.31784 | train_logloss: 0.3061  | train_accuracy: 0.882   | valid_logloss: 0.35858 | valid_accuracy: 0.873   |  0:10:16s\n",
            "epoch 46 | loss: 0.3251  | train_logloss: 0.31678 | train_accuracy: 0.87978 | valid_logloss: 0.36193 | valid_accuracy: 0.87156 |  0:10:29s\n",
            "epoch 47 | loss: 0.33624 | train_logloss: 0.31805 | train_accuracy: 0.87833 | valid_logloss: 0.35912 | valid_accuracy: 0.87611 |  0:10:42s\n",
            "epoch 48 | loss: 0.32553 | train_logloss: 0.3133  | train_accuracy: 0.88244 | valid_logloss: 0.35367 | valid_accuracy: 0.87456 |  0:10:54s\n",
            "epoch 49 | loss: 0.33267 | train_logloss: 0.33048 | train_accuracy: 0.87478 | valid_logloss: 0.36222 | valid_accuracy: 0.86844 |  0:11:07s\n",
            "epoch 50 | loss: 0.33994 | train_logloss: 0.32503 | train_accuracy: 0.87667 | valid_logloss: 0.36405 | valid_accuracy: 0.86722 |  0:11:20s\n",
            "epoch 51 | loss: 0.32443 | train_logloss: 0.32249 | train_accuracy: 0.875   | valid_logloss: 0.37424 | valid_accuracy: 0.86922 |  0:11:33s\n",
            "epoch 52 | loss: 0.32557 | train_logloss: 0.31601 | train_accuracy: 0.88256 | valid_logloss: 0.3654  | valid_accuracy: 0.87511 |  0:11:46s\n",
            "epoch 53 | loss: 0.325   | train_logloss: 0.30937 | train_accuracy: 0.88133 | valid_logloss: 0.35797 | valid_accuracy: 0.87656 |  0:11:59s\n",
            "epoch 54 | loss: 0.32011 | train_logloss: 0.312   | train_accuracy: 0.87822 | valid_logloss: 0.35927 | valid_accuracy: 0.87044 |  0:12:11s\n",
            "epoch 55 | loss: 0.32607 | train_logloss: 0.30611 | train_accuracy: 0.88378 | valid_logloss: 0.3593  | valid_accuracy: 0.875   |  0:12:24s\n",
            "epoch 56 | loss: 0.32194 | train_logloss: 0.30163 | train_accuracy: 0.88244 | valid_logloss: 0.35    | valid_accuracy: 0.87722 |  0:12:37s\n",
            "epoch 57 | loss: 0.3117  | train_logloss: 0.31041 | train_accuracy: 0.884   | valid_logloss: 0.36304 | valid_accuracy: 0.87622 |  0:12:50s\n",
            "epoch 58 | loss: 0.31228 | train_logloss: 0.30046 | train_accuracy: 0.88578 | valid_logloss: 0.34364 | valid_accuracy: 0.878   |  0:13:03s\n",
            "epoch 59 | loss: 0.3148  | train_logloss: 0.29744 | train_accuracy: 0.88778 | valid_logloss: 0.33756 | valid_accuracy: 0.88022 |  0:13:16s\n",
            "epoch 60 | loss: 0.30838 | train_logloss: 0.29721 | train_accuracy: 0.88822 | valid_logloss: 0.34177 | valid_accuracy: 0.87911 |  0:13:29s\n",
            "epoch 61 | loss: 0.30632 | train_logloss: 0.293   | train_accuracy: 0.88878 | valid_logloss: 0.34381 | valid_accuracy: 0.87811 |  0:13:42s\n",
            "epoch 62 | loss: 0.30021 | train_logloss: 0.29731 | train_accuracy: 0.88844 | valid_logloss: 0.35214 | valid_accuracy: 0.87567 |  0:13:55s\n",
            "epoch 63 | loss: 0.29917 | train_logloss: 0.29419 | train_accuracy: 0.88811 | valid_logloss: 0.34894 | valid_accuracy: 0.87856 |  0:14:08s\n",
            "epoch 64 | loss: 0.30055 | train_logloss: 0.28473 | train_accuracy: 0.89333 | valid_logloss: 0.34187 | valid_accuracy: 0.87978 |  0:14:21s\n",
            "epoch 65 | loss: 0.30542 | train_logloss: 0.29258 | train_accuracy: 0.88889 | valid_logloss: 0.33873 | valid_accuracy: 0.88022 |  0:14:34s\n",
            "epoch 66 | loss: 0.30235 | train_logloss: 0.30204 | train_accuracy: 0.88544 | valid_logloss: 0.35105 | valid_accuracy: 0.87456 |  0:14:46s\n",
            "epoch 67 | loss: 0.30234 | train_logloss: 0.2857  | train_accuracy: 0.88978 | valid_logloss: 0.34867 | valid_accuracy: 0.87933 |  0:14:59s\n",
            "epoch 68 | loss: 0.29509 | train_logloss: 0.28255 | train_accuracy: 0.89344 | valid_logloss: 0.34157 | valid_accuracy: 0.88111 |  0:15:12s\n",
            "epoch 69 | loss: 0.29843 | train_logloss: 0.28092 | train_accuracy: 0.89422 | valid_logloss: 0.33544 | valid_accuracy: 0.87867 |  0:15:25s\n",
            "epoch 70 | loss: 0.2931  | train_logloss: 0.28239 | train_accuracy: 0.89344 | valid_logloss: 0.34285 | valid_accuracy: 0.88    |  0:15:38s\n",
            "epoch 71 | loss: 0.29324 | train_logloss: 0.27939 | train_accuracy: 0.89289 | valid_logloss: 0.34182 | valid_accuracy: 0.87767 |  0:15:50s\n",
            "epoch 72 | loss: 0.29137 | train_logloss: 0.29757 | train_accuracy: 0.88678 | valid_logloss: 0.35713 | valid_accuracy: 0.87856 |  0:16:03s\n",
            "epoch 73 | loss: 0.29185 | train_logloss: 0.27837 | train_accuracy: 0.89489 | valid_logloss: 0.34719 | valid_accuracy: 0.88156 |  0:16:17s\n",
            "epoch 74 | loss: 0.28925 | train_logloss: 0.27775 | train_accuracy: 0.898   | valid_logloss: 0.35279 | valid_accuracy: 0.88167 |  0:16:30s\n",
            "epoch 75 | loss: 0.28877 | train_logloss: 0.26843 | train_accuracy: 0.89811 | valid_logloss: 0.34692 | valid_accuracy: 0.88167 |  0:16:43s\n",
            "epoch 76 | loss: 0.30263 | train_logloss: 0.30074 | train_accuracy: 0.88756 | valid_logloss: 0.36313 | valid_accuracy: 0.87467 |  0:16:56s\n",
            "epoch 77 | loss: 0.30632 | train_logloss: 0.29472 | train_accuracy: 0.88889 | valid_logloss: 0.35835 | valid_accuracy: 0.88022 |  0:17:09s\n",
            "epoch 78 | loss: 0.30231 | train_logloss: 0.28755 | train_accuracy: 0.89189 | valid_logloss: 0.34596 | valid_accuracy: 0.88411 |  0:17:22s\n",
            "epoch 79 | loss: 0.29706 | train_logloss: 0.28655 | train_accuracy: 0.89678 | valid_logloss: 0.35062 | valid_accuracy: 0.88089 |  0:17:35s\n",
            "epoch 80 | loss: 0.2948  | train_logloss: 0.28809 | train_accuracy: 0.89089 | valid_logloss: 0.35592 | valid_accuracy: 0.88011 |  0:17:48s\n",
            "epoch 81 | loss: 0.31166 | train_logloss: 0.30939 | train_accuracy: 0.88378 | valid_logloss: 0.35705 | valid_accuracy: 0.87856 |  0:18:01s\n",
            "epoch 82 | loss: 0.31566 | train_logloss: 0.30338 | train_accuracy: 0.887   | valid_logloss: 0.35889 | valid_accuracy: 0.87344 |  0:18:14s\n",
            "epoch 83 | loss: 0.30656 | train_logloss: 0.29347 | train_accuracy: 0.88978 | valid_logloss: 0.35738 | valid_accuracy: 0.87422 |  0:18:26s\n",
            "epoch 84 | loss: 0.29749 | train_logloss: 0.28935 | train_accuracy: 0.88778 | valid_logloss: 0.35083 | valid_accuracy: 0.87489 |  0:18:39s\n",
            "epoch 85 | loss: 0.29738 | train_logloss: 0.27664 | train_accuracy: 0.89467 | valid_logloss: 0.3418  | valid_accuracy: 0.87856 |  0:18:52s\n",
            "epoch 86 | loss: 0.29233 | train_logloss: 0.27755 | train_accuracy: 0.89456 | valid_logloss: 0.33847 | valid_accuracy: 0.87822 |  0:19:05s\n",
            "epoch 87 | loss: 0.29308 | train_logloss: 0.29369 | train_accuracy: 0.88911 | valid_logloss: 0.33983 | valid_accuracy: 0.87833 |  0:19:18s\n",
            "epoch 88 | loss: 0.30444 | train_logloss: 0.30585 | train_accuracy: 0.88489 | valid_logloss: 0.34833 | valid_accuracy: 0.88111 |  0:19:31s\n",
            "epoch 89 | loss: 0.31693 | train_logloss: 0.30214 | train_accuracy: 0.88667 | valid_logloss: 0.35023 | valid_accuracy: 0.879   |  0:19:44s\n",
            "epoch 90 | loss: 0.30505 | train_logloss: 0.30607 | train_accuracy: 0.88333 | valid_logloss: 0.35746 | valid_accuracy: 0.87067 |  0:19:56s\n",
            "epoch 91 | loss: 0.29425 | train_logloss: 0.292   | train_accuracy: 0.88922 | valid_logloss: 0.3427  | valid_accuracy: 0.87967 |  0:20:09s\n",
            "epoch 92 | loss: 0.30566 | train_logloss: 0.29243 | train_accuracy: 0.89044 | valid_logloss: 0.35595 | valid_accuracy: 0.87778 |  0:20:22s\n",
            "epoch 93 | loss: 0.29772 | train_logloss: 0.28939 | train_accuracy: 0.88733 | valid_logloss: 0.35445 | valid_accuracy: 0.87744 |  0:20:35s\n",
            "epoch 94 | loss: 0.29998 | train_logloss: 0.278   | train_accuracy: 0.89222 | valid_logloss: 0.34699 | valid_accuracy: 0.87989 |  0:20:47s\n",
            "epoch 95 | loss: 0.29415 | train_logloss: 0.29201 | train_accuracy: 0.89056 | valid_logloss: 0.35071 | valid_accuracy: 0.87967 |  0:21:00s\n",
            "epoch 96 | loss: 0.28693 | train_logloss: 0.26762 | train_accuracy: 0.89733 | valid_logloss: 0.34243 | valid_accuracy: 0.88256 |  0:21:13s\n",
            "epoch 97 | loss: 0.28306 | train_logloss: 0.27271 | train_accuracy: 0.89367 | valid_logloss: 0.34994 | valid_accuracy: 0.87789 |  0:21:26s\n",
            "epoch 98 | loss: 0.29388 | train_logloss: 0.2973  | train_accuracy: 0.88478 | valid_logloss: 0.35243 | valid_accuracy: 0.87478 |  0:21:39s\n",
            "epoch 99 | loss: 0.30636 | train_logloss: 0.28397 | train_accuracy: 0.89022 | valid_logloss: 0.34291 | valid_accuracy: 0.87622 |  0:21:51s\n",
            "epoch 100| loss: 0.29239 | train_logloss: 0.27619 | train_accuracy: 0.89111 | valid_logloss: 0.34729 | valid_accuracy: 0.87856 |  0:22:04s\n",
            "epoch 101| loss: 0.28677 | train_logloss: 0.29642 | train_accuracy: 0.88167 | valid_logloss: 0.37745 | valid_accuracy: 0.86889 |  0:22:17s\n",
            "epoch 102| loss: 0.2918  | train_logloss: 0.27311 | train_accuracy: 0.89189 | valid_logloss: 0.34674 | valid_accuracy: 0.88056 |  0:22:29s\n",
            "epoch 103| loss: 0.28428 | train_logloss: 0.27975 | train_accuracy: 0.89144 | valid_logloss: 0.33796 | valid_accuracy: 0.87611 |  0:22:42s\n",
            "epoch 104| loss: 0.28891 | train_logloss: 0.29809 | train_accuracy: 0.88333 | valid_logloss: 0.371   | valid_accuracy: 0.87056 |  0:22:55s\n",
            "epoch 105| loss: 0.2901  | train_logloss: 0.27869 | train_accuracy: 0.89433 | valid_logloss: 0.35312 | valid_accuracy: 0.87744 |  0:23:08s\n",
            "epoch 106| loss: 0.28751 | train_logloss: 0.27261 | train_accuracy: 0.89433 | valid_logloss: 0.33603 | valid_accuracy: 0.87956 |  0:23:20s\n",
            "epoch 107| loss: 0.28335 | train_logloss: 0.27208 | train_accuracy: 0.89344 | valid_logloss: 0.34022 | valid_accuracy: 0.88267 |  0:23:33s\n",
            "epoch 108| loss: 0.29069 | train_logloss: 0.26585 | train_accuracy: 0.89767 | valid_logloss: 0.33636 | valid_accuracy: 0.88456 |  0:23:46s\n",
            "epoch 109| loss: 0.28333 | train_logloss: 0.2728  | train_accuracy: 0.89533 | valid_logloss: 0.35705 | valid_accuracy: 0.87544 |  0:23:58s\n",
            "epoch 110| loss: 0.27587 | train_logloss: 0.26205 | train_accuracy: 0.897   | valid_logloss: 0.34447 | valid_accuracy: 0.88178 |  0:24:11s\n",
            "epoch 111| loss: 0.2725  | train_logloss: 0.2587  | train_accuracy: 0.89889 | valid_logloss: 0.33778 | valid_accuracy: 0.88411 |  0:24:24s\n",
            "epoch 112| loss: 0.27404 | train_logloss: 0.25661 | train_accuracy: 0.90122 | valid_logloss: 0.33391 | valid_accuracy: 0.882   |  0:24:37s\n",
            "epoch 113| loss: 0.2693  | train_logloss: 0.25811 | train_accuracy: 0.903   | valid_logloss: 0.34479 | valid_accuracy: 0.88167 |  0:24:50s\n",
            "epoch 114| loss: 0.27205 | train_logloss: 0.25458 | train_accuracy: 0.89889 | valid_logloss: 0.33784 | valid_accuracy: 0.88178 |  0:25:03s\n",
            "epoch 115| loss: 0.26903 | train_logloss: 0.257   | train_accuracy: 0.90322 | valid_logloss: 0.33981 | valid_accuracy: 0.888   |  0:25:16s\n",
            "epoch 116| loss: 0.29427 | train_logloss: 0.29479 | train_accuracy: 0.88611 | valid_logloss: 0.35819 | valid_accuracy: 0.875   |  0:25:29s\n",
            "epoch 117| loss: 0.28735 | train_logloss: 0.28153 | train_accuracy: 0.891   | valid_logloss: 0.34099 | valid_accuracy: 0.87711 |  0:25:41s\n",
            "epoch 118| loss: 0.27285 | train_logloss: 0.25665 | train_accuracy: 0.89989 | valid_logloss: 0.33846 | valid_accuracy: 0.88222 |  0:25:55s\n",
            "epoch 119| loss: 0.2742  | train_logloss: 0.25807 | train_accuracy: 0.90122 | valid_logloss: 0.34245 | valid_accuracy: 0.88378 |  0:26:08s\n",
            "epoch 120| loss: 0.26798 | train_logloss: 0.26043 | train_accuracy: 0.90144 | valid_logloss: 0.35013 | valid_accuracy: 0.881   |  0:26:20s\n",
            "epoch 121| loss: 0.27108 | train_logloss: 0.25976 | train_accuracy: 0.89933 | valid_logloss: 0.34278 | valid_accuracy: 0.88444 |  0:26:33s\n",
            "epoch 122| loss: 0.26891 | train_logloss: 0.24481 | train_accuracy: 0.90422 | valid_logloss: 0.33894 | valid_accuracy: 0.88344 |  0:26:46s\n",
            "epoch 123| loss: 0.26014 | train_logloss: 0.24996 | train_accuracy: 0.90489 | valid_logloss: 0.3386  | valid_accuracy: 0.88522 |  0:26:59s\n",
            "epoch 124| loss: 0.26756 | train_logloss: 0.25313 | train_accuracy: 0.90267 | valid_logloss: 0.34795 | valid_accuracy: 0.88322 |  0:27:12s\n",
            "epoch 125| loss: 0.25573 | train_logloss: 0.2429  | train_accuracy: 0.90322 | valid_logloss: 0.3537  | valid_accuracy: 0.88078 |  0:27:25s\n",
            "epoch 126| loss: 0.2717  | train_logloss: 0.31477 | train_accuracy: 0.87711 | valid_logloss: 0.37755 | valid_accuracy: 0.86789 |  0:27:38s\n",
            "epoch 127| loss: 0.30297 | train_logloss: 0.27912 | train_accuracy: 0.89489 | valid_logloss: 0.34308 | valid_accuracy: 0.88056 |  0:27:51s\n",
            "epoch 128| loss: 0.27854 | train_logloss: 0.26582 | train_accuracy: 0.89567 | valid_logloss: 0.3483  | valid_accuracy: 0.878   |  0:28:05s\n",
            "epoch 129| loss: 0.27165 | train_logloss: 0.25469 | train_accuracy: 0.89856 | valid_logloss: 0.34315 | valid_accuracy: 0.88167 |  0:28:18s\n",
            "epoch 130| loss: 0.26847 | train_logloss: 0.2503  | train_accuracy: 0.90133 | valid_logloss: 0.34083 | valid_accuracy: 0.88444 |  0:28:31s\n",
            "epoch 131| loss: 0.25961 | train_logloss: 0.24351 | train_accuracy: 0.90411 | valid_logloss: 0.33524 | valid_accuracy: 0.88144 |  0:28:44s\n",
            "epoch 132| loss: 0.25769 | train_logloss: 0.242   | train_accuracy: 0.90722 | valid_logloss: 0.34868 | valid_accuracy: 0.87878 |  0:28:57s\n",
            "epoch 133| loss: 0.25939 | train_logloss: 0.24203 | train_accuracy: 0.90533 | valid_logloss: 0.34481 | valid_accuracy: 0.88256 |  0:29:10s\n",
            "epoch 134| loss: 0.25991 | train_logloss: 0.23918 | train_accuracy: 0.90844 | valid_logloss: 0.36504 | valid_accuracy: 0.884   |  0:29:23s\n",
            "epoch 135| loss: 0.25334 | train_logloss: 0.24559 | train_accuracy: 0.90322 | valid_logloss: 0.36014 | valid_accuracy: 0.87956 |  0:29:36s\n",
            "epoch 136| loss: 0.25586 | train_logloss: 0.23347 | train_accuracy: 0.909   | valid_logloss: 0.36235 | valid_accuracy: 0.882   |  0:29:49s\n",
            "epoch 137| loss: 0.25761 | train_logloss: 0.25384 | train_accuracy: 0.89944 | valid_logloss: 0.36835 | valid_accuracy: 0.87367 |  0:30:01s\n",
            "epoch 138| loss: 0.25252 | train_logloss: 0.22768 | train_accuracy: 0.91044 | valid_logloss: 0.35773 | valid_accuracy: 0.88122 |  0:30:14s\n",
            "epoch 139| loss: 0.24298 | train_logloss: 0.25284 | train_accuracy: 0.90311 | valid_logloss: 0.37747 | valid_accuracy: 0.87422 |  0:30:27s\n",
            "epoch 140| loss: 0.25634 | train_logloss: 0.31638 | train_accuracy: 0.87911 | valid_logloss: 0.41749 | valid_accuracy: 0.85944 |  0:30:40s\n",
            "epoch 141| loss: 0.25113 | train_logloss: 0.24098 | train_accuracy: 0.90511 | valid_logloss: 0.35203 | valid_accuracy: 0.87978 |  0:30:53s\n",
            "epoch 142| loss: 0.25817 | train_logloss: 0.25866 | train_accuracy: 0.89878 | valid_logloss: 0.37092 | valid_accuracy: 0.873   |  0:31:06s\n",
            "epoch 143| loss: 0.27164 | train_logloss: 0.24973 | train_accuracy: 0.90389 | valid_logloss: 0.346   | valid_accuracy: 0.88644 |  0:31:19s\n",
            "epoch 144| loss: 0.26041 | train_logloss: 0.25307 | train_accuracy: 0.90167 | valid_logloss: 0.35095 | valid_accuracy: 0.87556 |  0:31:32s\n",
            "epoch 145| loss: 0.25213 | train_logloss: 0.25259 | train_accuracy: 0.90311 | valid_logloss: 0.36864 | valid_accuracy: 0.88033 |  0:31:45s\n",
            "\n",
            "Early stopping occurred at epoch 145 with best_epoch = 115 and best_valid_accuracy = 0.888\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[403556  17771  58855]\n",
            " [ 12475 957773  29752]\n",
            " [ 28303  31515 372769]]\n",
            "Testing Score:  0.888\n",
            "100%|██████████| 10/10 [01:10<00:00,  7.00s/it, best loss: -0.8836666666666666]\n",
            "({'colsample_bytree': 0.6612152711488598, 'min_child_samples': 40, 'min_child_weight': 0.6391987025092781, 'num_leaves': 54}, <hyperopt.base.Trials object at 0x7fcdd6cae590>)\n",
            "5.100636720657349\n",
            "Confusion Matrix: \n",
            " [[415617  13569  50996]\n",
            " [ 14569 953490  31941]\n",
            " [ 37794  28853 365940]]\n",
            "Testing Score:  0.886\n",
            "{'Rows': 9000, 'Nd': 128, 'Na': 128, 'B': 512, 'BV': 128, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 9000, 'test': 9000, 'time_learn_tn': 1907.2278344631195, 'time_tn': 324.13326597213745, 'accuracy_tn': 0.888, 'time_learn_gb': 1618978040.6986535, 'time_gb': 15.819995641708374, 'accuracy_gb': 0.886}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCsVElMBCf9_",
        "outputId": "e4c798ed-6a6e-435c-87f9-433ad4310a7c"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rows train  test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0  9000  9000  9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1  9000  9000  9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2  9000  9000  9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3  9000  9000  9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4  9000  9000  9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ql0Rid9-mW5"
      },
      "source": [
        "## 30000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JE4AgcBLAU",
        "outputId": "a7ce9257-5c44-4746-a355-4aba5e6f2139"
      },
      "source": [
        "time_model(number_exp=6, \n",
        "     Rows=30000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.76254 | train_logloss: 18.8653 | train_accuracy: 0.39357 | valid_logloss: 18.88225| valid_accuracy: 0.39423 |  0:00:04s\n",
            "epoch 1  | loss: 0.44937 | train_logloss: 21.20599| train_accuracy: 0.303   | valid_logloss: 21.04457| valid_accuracy: 0.30923 |  0:00:08s\n",
            "epoch 2  | loss: 0.39321 | train_logloss: 9.52474 | train_accuracy: 0.32583 | valid_logloss: 9.53722 | valid_accuracy: 0.3262  |  0:00:12s\n",
            "epoch 3  | loss: 0.36977 | train_logloss: 6.32504 | train_accuracy: 0.45577 | valid_logloss: 6.32006 | valid_accuracy: 0.45453 |  0:00:16s\n",
            "epoch 4  | loss: 0.35028 | train_logloss: 3.24219 | train_accuracy: 0.40293 | valid_logloss: 3.25428 | valid_accuracy: 0.40547 |  0:00:20s\n",
            "epoch 5  | loss: 0.34286 | train_logloss: 2.16249 | train_accuracy: 0.37507 | valid_logloss: 2.1924  | valid_accuracy: 0.37143 |  0:00:24s\n",
            "epoch 6  | loss: 0.33521 | train_logloss: 2.68844 | train_accuracy: 0.12427 | valid_logloss: 2.66716 | valid_accuracy: 0.12457 |  0:00:29s\n",
            "epoch 7  | loss: 0.33238 | train_logloss: 1.94602 | train_accuracy: 0.19203 | valid_logloss: 1.95077 | valid_accuracy: 0.19257 |  0:00:33s\n",
            "epoch 8  | loss: 0.33116 | train_logloss: 2.39949 | train_accuracy: 0.16467 | valid_logloss: 2.41922 | valid_accuracy: 0.16667 |  0:00:37s\n",
            "epoch 9  | loss: 0.32562 | train_logloss: 2.21126 | train_accuracy: 0.24603 | valid_logloss: 2.22983 | valid_accuracy: 0.2465  |  0:00:42s\n",
            "epoch 10 | loss: 0.32187 | train_logloss: 2.02781 | train_accuracy: 0.3256  | valid_logloss: 2.03751 | valid_accuracy: 0.32437 |  0:00:46s\n",
            "epoch 11 | loss: 0.32005 | train_logloss: 1.3392  | train_accuracy: 0.4699  | valid_logloss: 1.35104 | valid_accuracy: 0.46253 |  0:00:50s\n",
            "epoch 12 | loss: 0.31598 | train_logloss: 1.16197 | train_accuracy: 0.52637 | valid_logloss: 1.1811  | valid_accuracy: 0.5177  |  0:00:54s\n",
            "epoch 13 | loss: 0.31328 | train_logloss: 1.09557 | train_accuracy: 0.5712  | valid_logloss: 1.11306 | valid_accuracy: 0.55793 |  0:00:59s\n",
            "epoch 14 | loss: 0.31469 | train_logloss: 0.73815 | train_accuracy: 0.7048  | valid_logloss: 0.75184 | valid_accuracy: 0.69507 |  0:01:03s\n",
            "epoch 15 | loss: 0.31409 | train_logloss: 0.60562 | train_accuracy: 0.76263 | valid_logloss: 0.61796 | valid_accuracy: 0.7551  |  0:01:07s\n",
            "epoch 16 | loss: 0.31107 | train_logloss: 0.61137 | train_accuracy: 0.713   | valid_logloss: 0.62492 | valid_accuracy: 0.70417 |  0:01:12s\n",
            "epoch 17 | loss: 0.31318 | train_logloss: 0.49747 | train_accuracy: 0.81253 | valid_logloss: 0.5085  | valid_accuracy: 0.807   |  0:01:16s\n",
            "epoch 18 | loss: 0.31043 | train_logloss: 0.4965  | train_accuracy: 0.80863 | valid_logloss: 0.50694 | valid_accuracy: 0.803   |  0:01:20s\n",
            "epoch 19 | loss: 0.30602 | train_logloss: 0.43462 | train_accuracy: 0.84403 | valid_logloss: 0.44228 | valid_accuracy: 0.84077 |  0:01:25s\n",
            "epoch 20 | loss: 0.30337 | train_logloss: 0.40247 | train_accuracy: 0.85413 | valid_logloss: 0.41344 | valid_accuracy: 0.85    |  0:01:29s\n",
            "epoch 21 | loss: 0.30744 | train_logloss: 0.37312 | train_accuracy: 0.86257 | valid_logloss: 0.38555 | valid_accuracy: 0.85753 |  0:01:33s\n",
            "epoch 22 | loss: 0.30383 | train_logloss: 0.3419  | train_accuracy: 0.8711  | valid_logloss: 0.35231 | valid_accuracy: 0.8675  |  0:01:38s\n",
            "epoch 23 | loss: 0.30208 | train_logloss: 0.35506 | train_accuracy: 0.86693 | valid_logloss: 0.37021 | valid_accuracy: 0.86057 |  0:01:42s\n",
            "epoch 24 | loss: 0.30634 | train_logloss: 0.32835 | train_accuracy: 0.87407 | valid_logloss: 0.34192 | valid_accuracy: 0.8683  |  0:01:46s\n",
            "epoch 25 | loss: 0.30475 | train_logloss: 0.31713 | train_accuracy: 0.88497 | valid_logloss: 0.33144 | valid_accuracy: 0.87773 |  0:01:50s\n",
            "epoch 26 | loss: 0.30263 | train_logloss: 0.30605 | train_accuracy: 0.88387 | valid_logloss: 0.32049 | valid_accuracy: 0.8785  |  0:01:55s\n",
            "epoch 27 | loss: 0.30069 | train_logloss: 0.30736 | train_accuracy: 0.88657 | valid_logloss: 0.32384 | valid_accuracy: 0.87983 |  0:01:59s\n",
            "epoch 28 | loss: 0.29615 | train_logloss: 0.30296 | train_accuracy: 0.88667 | valid_logloss: 0.32041 | valid_accuracy: 0.88023 |  0:02:04s\n",
            "epoch 29 | loss: 0.29563 | train_logloss: 0.29911 | train_accuracy: 0.88683 | valid_logloss: 0.31807 | valid_accuracy: 0.882   |  0:02:08s\n",
            "epoch 30 | loss: 0.2949  | train_logloss: 0.29431 | train_accuracy: 0.886   | valid_logloss: 0.31278 | valid_accuracy: 0.88003 |  0:02:12s\n",
            "epoch 31 | loss: 0.29058 | train_logloss: 0.28712 | train_accuracy: 0.88993 | valid_logloss: 0.30744 | valid_accuracy: 0.88543 |  0:02:16s\n",
            "epoch 32 | loss: 0.2901  | train_logloss: 0.28557 | train_accuracy: 0.89083 | valid_logloss: 0.30576 | valid_accuracy: 0.8838  |  0:02:21s\n",
            "epoch 33 | loss: 0.28886 | train_logloss: 0.28624 | train_accuracy: 0.89147 | valid_logloss: 0.30837 | valid_accuracy: 0.8833  |  0:02:25s\n",
            "epoch 34 | loss: 0.28915 | train_logloss: 0.28167 | train_accuracy: 0.89267 | valid_logloss: 0.3019  | valid_accuracy: 0.88727 |  0:02:29s\n",
            "epoch 35 | loss: 0.288   | train_logloss: 0.28212 | train_accuracy: 0.8916  | valid_logloss: 0.30489 | valid_accuracy: 0.88297 |  0:02:34s\n",
            "epoch 36 | loss: 0.29101 | train_logloss: 0.29563 | train_accuracy: 0.891   | valid_logloss: 0.31171 | valid_accuracy: 0.885   |  0:02:38s\n",
            "epoch 37 | loss: 0.29769 | train_logloss: 0.28614 | train_accuracy: 0.89063 | valid_logloss: 0.30468 | valid_accuracy: 0.8837  |  0:02:42s\n",
            "epoch 38 | loss: 0.29197 | train_logloss: 0.28405 | train_accuracy: 0.89043 | valid_logloss: 0.30391 | valid_accuracy: 0.88507 |  0:02:47s\n",
            "epoch 39 | loss: 0.28773 | train_logloss: 0.27788 | train_accuracy: 0.89397 | valid_logloss: 0.29882 | valid_accuracy: 0.88683 |  0:02:51s\n",
            "epoch 40 | loss: 0.2852  | train_logloss: 0.28196 | train_accuracy: 0.8906  | valid_logloss: 0.30109 | valid_accuracy: 0.88353 |  0:02:56s\n",
            "epoch 41 | loss: 0.28437 | train_logloss: 0.27719 | train_accuracy: 0.894   | valid_logloss: 0.2985  | valid_accuracy: 0.88787 |  0:03:00s\n",
            "epoch 42 | loss: 0.28428 | train_logloss: 0.27392 | train_accuracy: 0.89673 | valid_logloss: 0.29599 | valid_accuracy: 0.8878  |  0:03:04s\n",
            "epoch 43 | loss: 0.28348 | train_logloss: 0.28255 | train_accuracy: 0.8907  | valid_logloss: 0.30452 | valid_accuracy: 0.88393 |  0:03:08s\n",
            "epoch 44 | loss: 0.28168 | train_logloss: 0.27722 | train_accuracy: 0.8931  | valid_logloss: 0.30033 | valid_accuracy: 0.8861  |  0:03:13s\n",
            "epoch 45 | loss: 0.2817  | train_logloss: 0.27384 | train_accuracy: 0.89437 | valid_logloss: 0.29768 | valid_accuracy: 0.888   |  0:03:17s\n",
            "epoch 46 | loss: 0.28381 | train_logloss: 0.27224 | train_accuracy: 0.8952  | valid_logloss: 0.29767 | valid_accuracy: 0.88887 |  0:03:22s\n",
            "epoch 47 | loss: 0.27718 | train_logloss: 0.27044 | train_accuracy: 0.89587 | valid_logloss: 0.2968  | valid_accuracy: 0.88797 |  0:03:26s\n",
            "epoch 48 | loss: 0.27914 | train_logloss: 0.27663 | train_accuracy: 0.8956  | valid_logloss: 0.30076 | valid_accuracy: 0.88787 |  0:03:30s\n",
            "epoch 49 | loss: 0.28428 | train_logloss: 0.2797  | train_accuracy: 0.89337 | valid_logloss: 0.30236 | valid_accuracy: 0.88473 |  0:03:34s\n",
            "epoch 50 | loss: 0.27663 | train_logloss: 0.2709  | train_accuracy: 0.8966  | valid_logloss: 0.29701 | valid_accuracy: 0.88927 |  0:03:39s\n",
            "epoch 51 | loss: 0.27795 | train_logloss: 0.26924 | train_accuracy: 0.89777 | valid_logloss: 0.29433 | valid_accuracy: 0.8885  |  0:03:43s\n",
            "epoch 52 | loss: 0.27485 | train_logloss: 0.26713 | train_accuracy: 0.89863 | valid_logloss: 0.29317 | valid_accuracy: 0.89097 |  0:03:47s\n",
            "epoch 53 | loss: 0.27815 | train_logloss: 0.26662 | train_accuracy: 0.89863 | valid_logloss: 0.29177 | valid_accuracy: 0.8909  |  0:03:52s\n",
            "epoch 54 | loss: 0.27413 | train_logloss: 0.26842 | train_accuracy: 0.89797 | valid_logloss: 0.29606 | valid_accuracy: 0.8892  |  0:03:56s\n",
            "epoch 55 | loss: 0.27215 | train_logloss: 0.26475 | train_accuracy: 0.8983  | valid_logloss: 0.2975  | valid_accuracy: 0.88957 |  0:04:01s\n",
            "epoch 56 | loss: 0.27439 | train_logloss: 0.26245 | train_accuracy: 0.8993  | valid_logloss: 0.29107 | valid_accuracy: 0.89027 |  0:04:05s\n",
            "epoch 57 | loss: 0.27192 | train_logloss: 0.26537 | train_accuracy: 0.89873 | valid_logloss: 0.29315 | valid_accuracy: 0.89033 |  0:04:09s\n",
            "epoch 58 | loss: 0.27285 | train_logloss: 0.26464 | train_accuracy: 0.8993  | valid_logloss: 0.29346 | valid_accuracy: 0.89063 |  0:04:14s\n",
            "epoch 59 | loss: 0.27041 | train_logloss: 0.26376 | train_accuracy: 0.8997  | valid_logloss: 0.29409 | valid_accuracy: 0.8901  |  0:04:18s\n",
            "epoch 60 | loss: 0.27088 | train_logloss: 0.26269 | train_accuracy: 0.8989  | valid_logloss: 0.2948  | valid_accuracy: 0.89    |  0:04:22s\n",
            "epoch 61 | loss: 0.26745 | train_logloss: 0.26105 | train_accuracy: 0.89973 | valid_logloss: 0.29291 | valid_accuracy: 0.8902  |  0:04:27s\n",
            "epoch 62 | loss: 0.2685  | train_logloss: 0.26203 | train_accuracy: 0.89953 | valid_logloss: 0.29572 | valid_accuracy: 0.88913 |  0:04:31s\n",
            "epoch 63 | loss: 0.2683  | train_logloss: 0.264   | train_accuracy: 0.89887 | valid_logloss: 0.2978  | valid_accuracy: 0.8895  |  0:04:35s\n",
            "epoch 64 | loss: 0.26782 | train_logloss: 0.26371 | train_accuracy: 0.89907 | valid_logloss: 0.29792 | valid_accuracy: 0.88733 |  0:04:40s\n",
            "epoch 65 | loss: 0.27409 | train_logloss: 0.26982 | train_accuracy: 0.8988  | valid_logloss: 0.29914 | valid_accuracy: 0.88877 |  0:04:44s\n",
            "epoch 66 | loss: 0.27    | train_logloss: 0.2575  | train_accuracy: 0.90147 | valid_logloss: 0.29089 | valid_accuracy: 0.89083 |  0:04:48s\n",
            "epoch 67 | loss: 0.26708 | train_logloss: 0.25935 | train_accuracy: 0.90123 | valid_logloss: 0.29377 | valid_accuracy: 0.8922  |  0:04:53s\n",
            "epoch 68 | loss: 0.26703 | train_logloss: 0.26056 | train_accuracy: 0.8985  | valid_logloss: 0.29336 | valid_accuracy: 0.8888  |  0:04:57s\n",
            "epoch 69 | loss: 0.26643 | train_logloss: 0.25619 | train_accuracy: 0.90163 | valid_logloss: 0.29178 | valid_accuracy: 0.89003 |  0:05:01s\n",
            "epoch 70 | loss: 0.2623  | train_logloss: 0.2545  | train_accuracy: 0.90227 | valid_logloss: 0.29083 | valid_accuracy: 0.89067 |  0:05:06s\n",
            "epoch 71 | loss: 0.26239 | train_logloss: 0.25858 | train_accuracy: 0.90013 | valid_logloss: 0.29222 | valid_accuracy: 0.89153 |  0:05:10s\n",
            "epoch 72 | loss: 0.26212 | train_logloss: 0.2554  | train_accuracy: 0.9013  | valid_logloss: 0.29251 | valid_accuracy: 0.8896  |  0:05:14s\n",
            "epoch 73 | loss: 0.26496 | train_logloss: 0.25834 | train_accuracy: 0.90093 | valid_logloss: 0.29544 | valid_accuracy: 0.8914  |  0:05:18s\n",
            "epoch 74 | loss: 0.26577 | train_logloss: 0.2545  | train_accuracy: 0.90197 | valid_logloss: 0.29299 | valid_accuracy: 0.8911  |  0:05:23s\n",
            "epoch 75 | loss: 0.26331 | train_logloss: 0.25849 | train_accuracy: 0.90123 | valid_logloss: 0.29397 | valid_accuracy: 0.8915  |  0:05:27s\n",
            "epoch 76 | loss: 0.26486 | train_logloss: 0.25349 | train_accuracy: 0.90063 | valid_logloss: 0.29448 | valid_accuracy: 0.89107 |  0:05:32s\n",
            "epoch 77 | loss: 0.26617 | train_logloss: 0.25751 | train_accuracy: 0.90147 | valid_logloss: 0.29639 | valid_accuracy: 0.89077 |  0:05:36s\n",
            "epoch 78 | loss: 0.26352 | train_logloss: 0.25585 | train_accuracy: 0.90253 | valid_logloss: 0.29382 | valid_accuracy: 0.8912  |  0:05:40s\n",
            "epoch 79 | loss: 0.26502 | train_logloss: 0.2651  | train_accuracy: 0.89993 | valid_logloss: 0.30065 | valid_accuracy: 0.88897 |  0:05:44s\n",
            "epoch 80 | loss: 0.2745  | train_logloss: 0.26971 | train_accuracy: 0.89693 | valid_logloss: 0.2958  | valid_accuracy: 0.88983 |  0:05:49s\n",
            "epoch 81 | loss: 0.2762  | train_logloss: 0.27056 | train_accuracy: 0.89677 | valid_logloss: 0.30076 | valid_accuracy: 0.88793 |  0:05:53s\n",
            "epoch 82 | loss: 0.27303 | train_logloss: 0.26169 | train_accuracy: 0.89957 | valid_logloss: 0.2914  | valid_accuracy: 0.88987 |  0:05:57s\n",
            "epoch 83 | loss: 0.26763 | train_logloss: 0.25726 | train_accuracy: 0.90157 | valid_logloss: 0.28977 | valid_accuracy: 0.8915  |  0:06:02s\n",
            "epoch 84 | loss: 0.26553 | train_logloss: 0.25376 | train_accuracy: 0.90347 | valid_logloss: 0.28836 | valid_accuracy: 0.893   |  0:06:06s\n",
            "epoch 85 | loss: 0.26342 | train_logloss: 0.25626 | train_accuracy: 0.90117 | valid_logloss: 0.29208 | valid_accuracy: 0.88893 |  0:06:10s\n",
            "epoch 86 | loss: 0.26363 | train_logloss: 0.25344 | train_accuracy: 0.90213 | valid_logloss: 0.29176 | valid_accuracy: 0.892   |  0:06:15s\n",
            "epoch 87 | loss: 0.26416 | train_logloss: 0.25682 | train_accuracy: 0.90173 | valid_logloss: 0.29138 | valid_accuracy: 0.89053 |  0:06:19s\n",
            "epoch 88 | loss: 0.26162 | train_logloss: 0.25158 | train_accuracy: 0.9027  | valid_logloss: 0.29213 | valid_accuracy: 0.88917 |  0:06:23s\n",
            "epoch 89 | loss: 0.2615  | train_logloss: 0.25554 | train_accuracy: 0.90253 | valid_logloss: 0.29722 | valid_accuracy: 0.89023 |  0:06:27s\n",
            "epoch 90 | loss: 0.25939 | train_logloss: 0.25215 | train_accuracy: 0.9026  | valid_logloss: 0.29194 | valid_accuracy: 0.8915  |  0:06:32s\n",
            "epoch 91 | loss: 0.2591  | train_logloss: 0.25491 | train_accuracy: 0.90303 | valid_logloss: 0.29687 | valid_accuracy: 0.8919  |  0:06:36s\n",
            "epoch 92 | loss: 0.26062 | train_logloss: 0.25508 | train_accuracy: 0.90207 | valid_logloss: 0.29164 | valid_accuracy: 0.89157 |  0:06:41s\n",
            "epoch 93 | loss: 0.26107 | train_logloss: 0.25225 | train_accuracy: 0.90233 | valid_logloss: 0.2922  | valid_accuracy: 0.89133 |  0:06:45s\n",
            "epoch 94 | loss: 0.26098 | train_logloss: 0.25206 | train_accuracy: 0.90473 | valid_logloss: 0.29494 | valid_accuracy: 0.89057 |  0:06:49s\n",
            "epoch 95 | loss: 0.25778 | train_logloss: 0.24925 | train_accuracy: 0.90343 | valid_logloss: 0.29378 | valid_accuracy: 0.89067 |  0:06:54s\n",
            "epoch 96 | loss: 0.2603  | train_logloss: 0.2497  | train_accuracy: 0.90317 | valid_logloss: 0.29378 | valid_accuracy: 0.8917  |  0:06:58s\n",
            "epoch 97 | loss: 0.25648 | train_logloss: 0.25136 | train_accuracy: 0.90223 | valid_logloss: 0.29147 | valid_accuracy: 0.89117 |  0:07:02s\n",
            "epoch 98 | loss: 0.2594  | train_logloss: 0.25293 | train_accuracy: 0.90273 | valid_logloss: 0.29613 | valid_accuracy: 0.89033 |  0:07:07s\n",
            "epoch 99 | loss: 0.26518 | train_logloss: 0.25687 | train_accuracy: 0.89963 | valid_logloss: 0.29419 | valid_accuracy: 0.89    |  0:07:11s\n",
            "epoch 100| loss: 0.26422 | train_logloss: 0.25638 | train_accuracy: 0.90243 | valid_logloss: 0.29335 | valid_accuracy: 0.89123 |  0:07:15s\n",
            "epoch 101| loss: 0.25995 | train_logloss: 0.25632 | train_accuracy: 0.90097 | valid_logloss: 0.29611 | valid_accuracy: 0.89083 |  0:07:20s\n",
            "epoch 102| loss: 0.26018 | train_logloss: 0.25313 | train_accuracy: 0.90247 | valid_logloss: 0.29005 | valid_accuracy: 0.89223 |  0:07:24s\n",
            "epoch 103| loss: 0.25993 | train_logloss: 0.25341 | train_accuracy: 0.90207 | valid_logloss: 0.29798 | valid_accuracy: 0.88943 |  0:07:29s\n",
            "epoch 104| loss: 0.25857 | train_logloss: 0.24944 | train_accuracy: 0.90407 | valid_logloss: 0.29102 | valid_accuracy: 0.8928  |  0:07:33s\n",
            "epoch 105| loss: 0.2592  | train_logloss: 0.25453 | train_accuracy: 0.90063 | valid_logloss: 0.29445 | valid_accuracy: 0.88973 |  0:07:37s\n",
            "epoch 106| loss: 0.25788 | train_logloss: 0.2505  | train_accuracy: 0.90337 | valid_logloss: 0.2937  | valid_accuracy: 0.8901  |  0:07:42s\n",
            "epoch 107| loss: 0.25706 | train_logloss: 0.25207 | train_accuracy: 0.9039  | valid_logloss: 0.29166 | valid_accuracy: 0.8915  |  0:07:46s\n",
            "epoch 108| loss: 0.25773 | train_logloss: 0.24925 | train_accuracy: 0.90547 | valid_logloss: 0.29591 | valid_accuracy: 0.89197 |  0:07:50s\n",
            "epoch 109| loss: 0.25693 | train_logloss: 0.25098 | train_accuracy: 0.90223 | valid_logloss: 0.29222 | valid_accuracy: 0.89117 |  0:07:54s\n",
            "epoch 110| loss: 0.25394 | train_logloss: 0.24902 | train_accuracy: 0.90443 | valid_logloss: 0.29474 | valid_accuracy: 0.8907  |  0:07:59s\n",
            "epoch 111| loss: 0.25755 | train_logloss: 0.2456  | train_accuracy: 0.9046  | valid_logloss: 0.29352 | valid_accuracy: 0.89147 |  0:08:03s\n",
            "epoch 112| loss: 0.25313 | train_logloss: 0.24771 | train_accuracy: 0.90447 | valid_logloss: 0.29432 | valid_accuracy: 0.89017 |  0:08:08s\n",
            "epoch 113| loss: 0.2562  | train_logloss: 0.2536  | train_accuracy: 0.90413 | valid_logloss: 0.29826 | valid_accuracy: 0.89007 |  0:08:12s\n",
            "epoch 114| loss: 0.26045 | train_logloss: 0.25122 | train_accuracy: 0.90453 | valid_logloss: 0.29127 | valid_accuracy: 0.8922  |  0:08:16s\n",
            "\n",
            "Early stopping occurred at epoch 114 with best_epoch = 84 and best_valid_accuracy = 0.893\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[414494  11674  54014]\n",
            " [ 17323 944540  38137]\n",
            " [ 29778  26561 376248]]\n",
            "Testing Score:  0.893\n",
            "100%|██████████| 10/10 [01:57<00:00, 11.73s/it, best loss: -0.8916999999999999]\n",
            "({'colsample_bytree': 0.24944688959507896, 'min_child_samples': 36, 'min_child_weight': 0.18514391376732692, 'num_leaves': 51}, <hyperopt.base.Trials object at 0x7fcddaa94910>)\n",
            "5.452325344085693\n",
            "Confusion Matrix: \n",
            " [[419992  12321  47869]\n",
            " [ 13882 954376  31742]\n",
            " [ 33996  27929 370662]]\n",
            "Testing Score:  0.8937333333333334\n",
            "{'Rows': 30000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 497.90298533439636, 'time_tn': 67.8069920539856, 'accuracy_tn': 0.893, 'time_learn_gb': 1618978760.137545, 'time_gb': 25.90647602081299, 'accuracy_gb': 0.8937333333333334}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "kLhfko9RWbI6",
        "outputId": "14b7d669-1a12-4408-80a7-a8edf9363aea"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1   9000   9000   9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2   9000   9000   9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3   9000   9000   9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4   9000   9000   9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "5  30000  30000  30000    8  ...   67.806992    0.893733  1.618979e+09  25.906476\n",
              "\n",
              "[6 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djIcrrHxBLAc",
        "outputId": "8e6635b3-16dd-4af0-8c55-daa4b71322a6"
      },
      "source": [
        "time_model(number_exp=7, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.7909  | train_logloss: 10.90168| train_accuracy: 0.42223 | valid_logloss: 10.94193| valid_accuracy: 0.42073 |  0:00:05s\n",
            "epoch 1  | loss: 0.44271 | train_logloss: 2.1243  | train_accuracy: 0.4728  | valid_logloss: 2.13487 | valid_accuracy: 0.46983 |  0:00:10s\n",
            "epoch 2  | loss: 0.37626 | train_logloss: 6.03606 | train_accuracy: 0.36993 | valid_logloss: 6.06867 | valid_accuracy: 0.37103 |  0:00:16s\n",
            "epoch 3  | loss: 0.36003 | train_logloss: 3.1375  | train_accuracy: 0.2847  | valid_logloss: 3.14876 | valid_accuracy: 0.28557 |  0:00:21s\n",
            "epoch 4  | loss: 0.34919 | train_logloss: 3.83346 | train_accuracy: 0.21207 | valid_logloss: 3.83416 | valid_accuracy: 0.20947 |  0:00:27s\n",
            "epoch 5  | loss: 0.3389  | train_logloss: 3.63812 | train_accuracy: 0.2327  | valid_logloss: 3.6591  | valid_accuracy: 0.2344  |  0:00:32s\n",
            "epoch 6  | loss: 0.33141 | train_logloss: 2.63396 | train_accuracy: 0.33393 | valid_logloss: 2.68704 | valid_accuracy: 0.32627 |  0:00:38s\n",
            "epoch 7  | loss: 0.3338  | train_logloss: 1.99517 | train_accuracy: 0.30027 | valid_logloss: 2.01521 | valid_accuracy: 0.30117 |  0:00:43s\n",
            "epoch 8  | loss: 0.33361 | train_logloss: 2.4814  | train_accuracy: 0.28427 | valid_logloss: 2.50751 | valid_accuracy: 0.2813  |  0:00:49s\n",
            "epoch 9  | loss: 0.33094 | train_logloss: 1.20007 | train_accuracy: 0.50613 | valid_logloss: 1.20965 | valid_accuracy: 0.50813 |  0:00:54s\n",
            "epoch 10 | loss: 0.32275 | train_logloss: 1.22768 | train_accuracy: 0.39813 | valid_logloss: 1.23796 | valid_accuracy: 0.39617 |  0:01:00s\n",
            "epoch 11 | loss: 0.31753 | train_logloss: 1.13775 | train_accuracy: 0.5316  | valid_logloss: 1.14885 | valid_accuracy: 0.5264  |  0:01:06s\n",
            "epoch 12 | loss: 0.31787 | train_logloss: 0.97136 | train_accuracy: 0.61867 | valid_logloss: 0.97873 | valid_accuracy: 0.6138  |  0:01:11s\n",
            "epoch 13 | loss: 0.31282 | train_logloss: 0.91798 | train_accuracy: 0.63493 | valid_logloss: 0.92338 | valid_accuracy: 0.6285  |  0:01:17s\n",
            "epoch 14 | loss: 0.31988 | train_logloss: 0.67145 | train_accuracy: 0.7306  | valid_logloss: 0.67979 | valid_accuracy: 0.7234  |  0:01:22s\n",
            "epoch 15 | loss: 0.32011 | train_logloss: 0.69785 | train_accuracy: 0.69593 | valid_logloss: 0.70631 | valid_accuracy: 0.6908  |  0:01:28s\n",
            "epoch 16 | loss: 0.31203 | train_logloss: 0.53215 | train_accuracy: 0.80577 | valid_logloss: 0.53804 | valid_accuracy: 0.79993 |  0:01:33s\n",
            "epoch 17 | loss: 0.31352 | train_logloss: 0.49352 | train_accuracy: 0.82863 | valid_logloss: 0.49875 | valid_accuracy: 0.8269  |  0:01:39s\n",
            "epoch 18 | loss: 0.30867 | train_logloss: 0.45733 | train_accuracy: 0.83173 | valid_logloss: 0.46553 | valid_accuracy: 0.82537 |  0:01:44s\n",
            "epoch 19 | loss: 0.30315 | train_logloss: 0.42121 | train_accuracy: 0.8341  | valid_logloss: 0.42844 | valid_accuracy: 0.83037 |  0:01:50s\n",
            "epoch 20 | loss: 0.30099 | train_logloss: 0.39578 | train_accuracy: 0.851   | valid_logloss: 0.40654 | valid_accuracy: 0.8465  |  0:01:55s\n",
            "epoch 21 | loss: 0.30351 | train_logloss: 0.3466  | train_accuracy: 0.868   | valid_logloss: 0.35679 | valid_accuracy: 0.8653  |  0:02:01s\n",
            "epoch 22 | loss: 0.29957 | train_logloss: 0.35874 | train_accuracy: 0.86427 | valid_logloss: 0.36984 | valid_accuracy: 0.85963 |  0:02:06s\n",
            "epoch 23 | loss: 0.29934 | train_logloss: 0.33551 | train_accuracy: 0.87097 | valid_logloss: 0.34848 | valid_accuracy: 0.86857 |  0:02:12s\n",
            "epoch 24 | loss: 0.29752 | train_logloss: 0.32148 | train_accuracy: 0.87953 | valid_logloss: 0.33633 | valid_accuracy: 0.8766  |  0:02:17s\n",
            "epoch 25 | loss: 0.29424 | train_logloss: 0.30171 | train_accuracy: 0.8868  | valid_logloss: 0.31692 | valid_accuracy: 0.8816  |  0:02:23s\n",
            "epoch 26 | loss: 0.29309 | train_logloss: 0.3021  | train_accuracy: 0.88417 | valid_logloss: 0.32012 | valid_accuracy: 0.88007 |  0:02:29s\n",
            "epoch 27 | loss: 0.29375 | train_logloss: 0.29465 | train_accuracy: 0.8882  | valid_logloss: 0.31419 | valid_accuracy: 0.8807  |  0:02:34s\n",
            "epoch 28 | loss: 0.28973 | train_logloss: 0.29568 | train_accuracy: 0.88653 | valid_logloss: 0.31465 | valid_accuracy: 0.88057 |  0:02:40s\n",
            "epoch 29 | loss: 0.28799 | train_logloss: 0.28758 | train_accuracy: 0.88873 | valid_logloss: 0.31108 | valid_accuracy: 0.88177 |  0:02:45s\n",
            "epoch 30 | loss: 0.28546 | train_logloss: 0.27998 | train_accuracy: 0.89147 | valid_logloss: 0.30479 | valid_accuracy: 0.88683 |  0:02:51s\n",
            "epoch 31 | loss: 0.28586 | train_logloss: 0.28764 | train_accuracy: 0.89007 | valid_logloss: 0.30956 | valid_accuracy: 0.8823  |  0:02:56s\n",
            "epoch 32 | loss: 0.28459 | train_logloss: 0.2801  | train_accuracy: 0.89163 | valid_logloss: 0.3036  | valid_accuracy: 0.88653 |  0:03:02s\n",
            "epoch 33 | loss: 0.28455 | train_logloss: 0.27655 | train_accuracy: 0.8942  | valid_logloss: 0.30062 | valid_accuracy: 0.8857  |  0:03:07s\n",
            "epoch 34 | loss: 0.28339 | train_logloss: 0.28391 | train_accuracy: 0.8907  | valid_logloss: 0.30819 | valid_accuracy: 0.88307 |  0:03:13s\n",
            "epoch 35 | loss: 0.28517 | train_logloss: 0.28021 | train_accuracy: 0.89377 | valid_logloss: 0.30657 | valid_accuracy: 0.886   |  0:03:18s\n",
            "epoch 36 | loss: 0.27817 | train_logloss: 0.27346 | train_accuracy: 0.89623 | valid_logloss: 0.30065 | valid_accuracy: 0.88793 |  0:03:24s\n",
            "epoch 37 | loss: 0.28046 | train_logloss: 0.27221 | train_accuracy: 0.89547 | valid_logloss: 0.30287 | valid_accuracy: 0.8868  |  0:03:29s\n",
            "epoch 38 | loss: 0.27992 | train_logloss: 0.27047 | train_accuracy: 0.89617 | valid_logloss: 0.29903 | valid_accuracy: 0.8868  |  0:03:35s\n",
            "epoch 39 | loss: 0.275   | train_logloss: 0.27081 | train_accuracy: 0.89593 | valid_logloss: 0.30019 | valid_accuracy: 0.88963 |  0:03:40s\n",
            "epoch 40 | loss: 0.2728  | train_logloss: 0.26964 | train_accuracy: 0.8962  | valid_logloss: 0.30347 | valid_accuracy: 0.8887  |  0:03:46s\n",
            "epoch 41 | loss: 0.27751 | train_logloss: 0.2692  | train_accuracy: 0.8989  | valid_logloss: 0.30222 | valid_accuracy: 0.88853 |  0:03:51s\n",
            "epoch 42 | loss: 0.27488 | train_logloss: 0.26453 | train_accuracy: 0.8989  | valid_logloss: 0.29651 | valid_accuracy: 0.88913 |  0:03:57s\n",
            "epoch 43 | loss: 0.27221 | train_logloss: 0.26435 | train_accuracy: 0.89843 | valid_logloss: 0.29728 | valid_accuracy: 0.8914  |  0:04:02s\n",
            "epoch 44 | loss: 0.2759  | train_logloss: 0.2637  | train_accuracy: 0.89807 | valid_logloss: 0.29979 | valid_accuracy: 0.88753 |  0:04:08s\n",
            "epoch 45 | loss: 0.27353 | train_logloss: 0.2643  | train_accuracy: 0.89837 | valid_logloss: 0.30002 | valid_accuracy: 0.88943 |  0:04:13s\n",
            "epoch 46 | loss: 0.27314 | train_logloss: 0.26357 | train_accuracy: 0.8989  | valid_logloss: 0.29721 | valid_accuracy: 0.88967 |  0:04:19s\n",
            "epoch 47 | loss: 0.27434 | train_logloss: 0.26992 | train_accuracy: 0.8984  | valid_logloss: 0.30242 | valid_accuracy: 0.88783 |  0:04:24s\n",
            "epoch 48 | loss: 0.28633 | train_logloss: 0.28753 | train_accuracy: 0.891   | valid_logloss: 0.3142  | valid_accuracy: 0.88363 |  0:04:30s\n",
            "epoch 49 | loss: 0.27801 | train_logloss: 0.26965 | train_accuracy: 0.8978  | valid_logloss: 0.29789 | valid_accuracy: 0.88883 |  0:04:35s\n",
            "epoch 50 | loss: 0.27335 | train_logloss: 0.27276 | train_accuracy: 0.8965  | valid_logloss: 0.30778 | valid_accuracy: 0.8857  |  0:04:41s\n",
            "epoch 51 | loss: 0.27588 | train_logloss: 0.26432 | train_accuracy: 0.89833 | valid_logloss: 0.29774 | valid_accuracy: 0.88797 |  0:04:46s\n",
            "epoch 52 | loss: 0.27039 | train_logloss: 0.26093 | train_accuracy: 0.90173 | valid_logloss: 0.29617 | valid_accuracy: 0.89067 |  0:04:52s\n",
            "epoch 53 | loss: 0.26875 | train_logloss: 0.26253 | train_accuracy: 0.90173 | valid_logloss: 0.29889 | valid_accuracy: 0.88893 |  0:04:57s\n",
            "epoch 54 | loss: 0.27129 | train_logloss: 0.26738 | train_accuracy: 0.89643 | valid_logloss: 0.30006 | valid_accuracy: 0.88713 |  0:05:03s\n",
            "epoch 55 | loss: 0.2729  | train_logloss: 0.26305 | train_accuracy: 0.89827 | valid_logloss: 0.29468 | valid_accuracy: 0.88893 |  0:05:08s\n",
            "epoch 56 | loss: 0.27048 | train_logloss: 0.26092 | train_accuracy: 0.9001  | valid_logloss: 0.29324 | valid_accuracy: 0.88983 |  0:05:13s\n",
            "epoch 57 | loss: 0.26666 | train_logloss: 0.26059 | train_accuracy: 0.9005  | valid_logloss: 0.29482 | valid_accuracy: 0.89173 |  0:05:19s\n",
            "epoch 58 | loss: 0.26706 | train_logloss: 0.25607 | train_accuracy: 0.9022  | valid_logloss: 0.29241 | valid_accuracy: 0.89307 |  0:05:24s\n",
            "epoch 59 | loss: 0.26658 | train_logloss: 0.2585  | train_accuracy: 0.90043 | valid_logloss: 0.29823 | valid_accuracy: 0.88907 |  0:05:35s\n",
            "epoch 60 | loss: 0.26638 | train_logloss: 0.25685 | train_accuracy: 0.90227 | valid_logloss: 0.29573 | valid_accuracy: 0.89123 |  0:05:44s\n",
            "epoch 61 | loss: 0.27341 | train_logloss: 0.2685  | train_accuracy: 0.89893 | valid_logloss: 0.29924 | valid_accuracy: 0.8888  |  0:05:49s\n",
            "epoch 62 | loss: 0.27424 | train_logloss: 0.26153 | train_accuracy: 0.9009  | valid_logloss: 0.29439 | valid_accuracy: 0.88977 |  0:05:55s\n",
            "epoch 63 | loss: 0.26578 | train_logloss: 0.25842 | train_accuracy: 0.90227 | valid_logloss: 0.29217 | valid_accuracy: 0.89177 |  0:06:01s\n",
            "epoch 64 | loss: 0.2642  | train_logloss: 0.25763 | train_accuracy: 0.9032  | valid_logloss: 0.29409 | valid_accuracy: 0.89107 |  0:06:06s\n",
            "epoch 65 | loss: 0.26426 | train_logloss: 0.25333 | train_accuracy: 0.90287 | valid_logloss: 0.29329 | valid_accuracy: 0.89247 |  0:06:12s\n",
            "epoch 66 | loss: 0.26079 | train_logloss: 0.24984 | train_accuracy: 0.9033  | valid_logloss: 0.29079 | valid_accuracy: 0.89157 |  0:06:17s\n",
            "epoch 67 | loss: 0.26182 | train_logloss: 0.2624  | train_accuracy: 0.89987 | valid_logloss: 0.30108 | valid_accuracy: 0.88807 |  0:06:23s\n",
            "epoch 68 | loss: 0.26044 | train_logloss: 0.24908 | train_accuracy: 0.90377 | valid_logloss: 0.29129 | valid_accuracy: 0.89247 |  0:06:28s\n",
            "epoch 69 | loss: 0.26387 | train_logloss: 0.25293 | train_accuracy: 0.90267 | valid_logloss: 0.29684 | valid_accuracy: 0.88903 |  0:06:34s\n",
            "epoch 70 | loss: 0.261   | train_logloss: 0.24999 | train_accuracy: 0.9031  | valid_logloss: 0.29353 | valid_accuracy: 0.89217 |  0:06:39s\n",
            "epoch 71 | loss: 0.25873 | train_logloss: 0.25382 | train_accuracy: 0.90493 | valid_logloss: 0.30011 | valid_accuracy: 0.89087 |  0:06:45s\n",
            "epoch 72 | loss: 0.26182 | train_logloss: 0.25828 | train_accuracy: 0.90027 | valid_logloss: 0.30543 | valid_accuracy: 0.88653 |  0:06:50s\n",
            "epoch 73 | loss: 0.26151 | train_logloss: 0.25127 | train_accuracy: 0.9034  | valid_logloss: 0.29477 | valid_accuracy: 0.8916  |  0:06:56s\n",
            "epoch 74 | loss: 0.26188 | train_logloss: 0.25413 | train_accuracy: 0.90133 | valid_logloss: 0.29318 | valid_accuracy: 0.89    |  0:07:01s\n",
            "epoch 75 | loss: 0.26114 | train_logloss: 0.25116 | train_accuracy: 0.90373 | valid_logloss: 0.29473 | valid_accuracy: 0.8902  |  0:07:07s\n",
            "epoch 76 | loss: 0.26005 | train_logloss: 0.24919 | train_accuracy: 0.90473 | valid_logloss: 0.29388 | valid_accuracy: 0.89197 |  0:07:12s\n",
            "epoch 77 | loss: 0.25925 | train_logloss: 0.25061 | train_accuracy: 0.9043  | valid_logloss: 0.29702 | valid_accuracy: 0.89083 |  0:07:18s\n",
            "epoch 78 | loss: 0.25542 | train_logloss: 0.24462 | train_accuracy: 0.9067  | valid_logloss: 0.29228 | valid_accuracy: 0.8926  |  0:07:23s\n",
            "epoch 79 | loss: 0.25644 | train_logloss: 0.24902 | train_accuracy: 0.90557 | valid_logloss: 0.29557 | valid_accuracy: 0.89273 |  0:07:29s\n",
            "epoch 80 | loss: 0.25772 | train_logloss: 0.25801 | train_accuracy: 0.89883 | valid_logloss: 0.30324 | valid_accuracy: 0.88803 |  0:07:35s\n",
            "epoch 81 | loss: 0.26122 | train_logloss: 0.24852 | train_accuracy: 0.90517 | valid_logloss: 0.29472 | valid_accuracy: 0.89033 |  0:07:40s\n",
            "epoch 82 | loss: 0.25543 | train_logloss: 0.24607 | train_accuracy: 0.90447 | valid_logloss: 0.29447 | valid_accuracy: 0.89217 |  0:07:46s\n",
            "epoch 83 | loss: 0.25581 | train_logloss: 0.24769 | train_accuracy: 0.90383 | valid_logloss: 0.29497 | valid_accuracy: 0.89147 |  0:07:51s\n",
            "epoch 84 | loss: 0.26371 | train_logloss: 0.26097 | train_accuracy: 0.9019  | valid_logloss: 0.30686 | valid_accuracy: 0.8895  |  0:07:57s\n",
            "epoch 85 | loss: 0.26356 | train_logloss: 0.25629 | train_accuracy: 0.90373 | valid_logloss: 0.29459 | valid_accuracy: 0.89153 |  0:08:02s\n",
            "epoch 86 | loss: 0.26252 | train_logloss: 0.25569 | train_accuracy: 0.9015  | valid_logloss: 0.29877 | valid_accuracy: 0.89077 |  0:08:08s\n",
            "epoch 87 | loss: 0.26715 | train_logloss: 0.25734 | train_accuracy: 0.90197 | valid_logloss: 0.30074 | valid_accuracy: 0.88947 |  0:08:13s\n",
            "epoch 88 | loss: 0.2654  | train_logloss: 0.26075 | train_accuracy: 0.9018  | valid_logloss: 0.29868 | valid_accuracy: 0.89177 |  0:08:19s\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 58 and best_valid_accuracy = 0.89307\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[417918  13974  48290]\n",
            " [ 15010 953437  31553]\n",
            " [ 33532  29235 369820]]\n",
            "Testing Score:  0.8930666666666667\n",
            "100%|██████████| 10/10 [02:09<00:00, 12.91s/it, best loss: -0.8919333333333334]\n",
            "({'colsample_bytree': 0.40989318456737933, 'min_child_samples': 27, 'min_child_weight': 0.5483062979181663, 'num_leaves': 99}, <hyperopt.base.Trials object at 0x7fcdd7fdead0>)\n",
            "8.562155723571777\n",
            "Confusion Matrix: \n",
            " [[420155  12494  47533]\n",
            " [ 13829 954356  31815]\n",
            " [ 34158  27816 370613]]\n",
            "Testing Score:  0.8936333333333333\n",
            "{'Rows': 30000, 'Nd': 16, 'Na': 16, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 500.8257670402527, 'time_tn': 78.5709490776062, 'accuracy_tn': 0.8930666666666667, 'time_learn_gb': 1618979513.2644746, 'time_gb': 21.026869297027588, 'accuracy_gb': 0.8936333333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "Rpysm9FvWbi7",
        "outputId": "602cd7e3-21a1-4fa0-abc0-19d616b910ec"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1   9000   9000   9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2   9000   9000   9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3   9000   9000   9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4   9000   9000   9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "5  30000  30000  30000    8  ...   67.806992    0.893733  1.618979e+09  25.906476\n",
              "6  30000  30000  30000   16  ...   78.570949    0.893633  1.618980e+09  21.026869\n",
              "\n",
              "[7 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuG8w6M3BLAd",
        "outputId": "688b35ec-f8d2-47b6-d8de-d3a4d2c4e1c6"
      },
      "source": [
        "time_model(number_exp=8, \n",
        "          Rows=30000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=2048,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.96105 | train_logloss: 19.67956| train_accuracy: 0.3174  | valid_logloss: 19.58548| valid_accuracy: 0.31993 |  0:00:17s\n",
            "epoch 1  | loss: 0.50731 | train_logloss: 19.11769| train_accuracy: 0.2322  | valid_logloss: 19.0889 | valid_accuracy: 0.23663 |  0:00:34s\n",
            "epoch 2  | loss: 0.42027 | train_logloss: 6.47861 | train_accuracy: 0.33577 | valid_logloss: 6.50551 | valid_accuracy: 0.335   |  0:00:51s\n",
            "epoch 3  | loss: 0.40221 | train_logloss: 4.47579 | train_accuracy: 0.3835  | valid_logloss: 4.50611 | valid_accuracy: 0.38297 |  0:01:08s\n",
            "epoch 4  | loss: 0.37895 | train_logloss: 2.39995 | train_accuracy: 0.57683 | valid_logloss: 2.41993 | valid_accuracy: 0.5737  |  0:01:26s\n",
            "epoch 5  | loss: 0.3701  | train_logloss: 2.6869  | train_accuracy: 0.45467 | valid_logloss: 2.71339 | valid_accuracy: 0.4521  |  0:01:43s\n",
            "epoch 6  | loss: 0.36842 | train_logloss: 1.70342 | train_accuracy: 0.56023 | valid_logloss: 1.69979 | valid_accuracy: 0.5587  |  0:02:00s\n",
            "epoch 7  | loss: 0.3622  | train_logloss: 0.96059 | train_accuracy: 0.5683  | valid_logloss: 0.96355 | valid_accuracy: 0.56737 |  0:02:17s\n",
            "epoch 8  | loss: 0.34898 | train_logloss: 1.20006 | train_accuracy: 0.56137 | valid_logloss: 1.20473 | valid_accuracy: 0.55997 |  0:02:35s\n",
            "epoch 9  | loss: 0.35652 | train_logloss: 1.01454 | train_accuracy: 0.58113 | valid_logloss: 1.01849 | valid_accuracy: 0.57937 |  0:02:52s\n",
            "epoch 10 | loss: 0.36326 | train_logloss: 0.96737 | train_accuracy: 0.69087 | valid_logloss: 0.98186 | valid_accuracy: 0.68963 |  0:03:09s\n",
            "epoch 11 | loss: 0.37849 | train_logloss: 1.57143 | train_accuracy: 0.46263 | valid_logloss: 1.58087 | valid_accuracy: 0.4591  |  0:03:29s\n",
            "epoch 12 | loss: 0.37237 | train_logloss: 0.86673 | train_accuracy: 0.65407 | valid_logloss: 0.87849 | valid_accuracy: 0.64887 |  0:03:50s\n",
            "epoch 13 | loss: 0.3619  | train_logloss: 0.8723  | train_accuracy: 0.63643 | valid_logloss: 0.8815  | valid_accuracy: 0.62677 |  0:04:07s\n",
            "epoch 14 | loss: 0.36527 | train_logloss: 0.76208 | train_accuracy: 0.6997  | valid_logloss: 0.77494 | valid_accuracy: 0.69623 |  0:04:23s\n",
            "epoch 15 | loss: 0.35384 | train_logloss: 0.64458 | train_accuracy: 0.73037 | valid_logloss: 0.65312 | valid_accuracy: 0.72693 |  0:04:40s\n",
            "epoch 16 | loss: 0.34089 | train_logloss: 0.62937 | train_accuracy: 0.76137 | valid_logloss: 0.63748 | valid_accuracy: 0.75417 |  0:04:56s\n",
            "epoch 17 | loss: 0.3444  | train_logloss: 0.60991 | train_accuracy: 0.78937 | valid_logloss: 0.61948 | valid_accuracy: 0.78657 |  0:05:13s\n",
            "epoch 18 | loss: 0.35332 | train_logloss: 0.50676 | train_accuracy: 0.82093 | valid_logloss: 0.51595 | valid_accuracy: 0.8176  |  0:05:29s\n",
            "epoch 19 | loss: 0.34395 | train_logloss: 0.44266 | train_accuracy: 0.83497 | valid_logloss: 0.44891 | valid_accuracy: 0.8306  |  0:05:46s\n",
            "epoch 20 | loss: 0.34871 | train_logloss: 0.42168 | train_accuracy: 0.8461  | valid_logloss: 0.42843 | valid_accuracy: 0.84443 |  0:06:02s\n",
            "epoch 21 | loss: 0.35609 | train_logloss: 0.38869 | train_accuracy: 0.8499  | valid_logloss: 0.39622 | valid_accuracy: 0.84677 |  0:06:19s\n",
            "epoch 22 | loss: 0.34609 | train_logloss: 0.38743 | train_accuracy: 0.854   | valid_logloss: 0.39284 | valid_accuracy: 0.85247 |  0:06:35s\n",
            "epoch 23 | loss: 0.34684 | train_logloss: 0.36408 | train_accuracy: 0.8646  | valid_logloss: 0.37253 | valid_accuracy: 0.86347 |  0:06:52s\n",
            "epoch 24 | loss: 0.33863 | train_logloss: 0.34283 | train_accuracy: 0.87187 | valid_logloss: 0.35437 | valid_accuracy: 0.86833 |  0:07:09s\n",
            "epoch 25 | loss: 0.33683 | train_logloss: 0.35437 | train_accuracy: 0.87    | valid_logloss: 0.36416 | valid_accuracy: 0.86543 |  0:07:26s\n",
            "epoch 26 | loss: 0.34385 | train_logloss: 0.34262 | train_accuracy: 0.8734  | valid_logloss: 0.3516  | valid_accuracy: 0.8691  |  0:07:43s\n",
            "epoch 27 | loss: 0.33237 | train_logloss: 0.34088 | train_accuracy: 0.8686  | valid_logloss: 0.35209 | valid_accuracy: 0.86693 |  0:07:59s\n",
            "epoch 28 | loss: 0.32772 | train_logloss: 0.33585 | train_accuracy: 0.87187 | valid_logloss: 0.34764 | valid_accuracy: 0.86937 |  0:08:16s\n",
            "epoch 29 | loss: 0.33146 | train_logloss: 0.33602 | train_accuracy: 0.8714  | valid_logloss: 0.34649 | valid_accuracy: 0.86897 |  0:08:32s\n",
            "epoch 30 | loss: 0.32792 | train_logloss: 0.33364 | train_accuracy: 0.87667 | valid_logloss: 0.34579 | valid_accuracy: 0.8735  |  0:08:49s\n",
            "epoch 31 | loss: 0.3258  | train_logloss: 0.31815 | train_accuracy: 0.87943 | valid_logloss: 0.33085 | valid_accuracy: 0.8755  |  0:09:06s\n",
            "epoch 32 | loss: 0.3245  | train_logloss: 0.32499 | train_accuracy: 0.87837 | valid_logloss: 0.33607 | valid_accuracy: 0.87287 |  0:09:23s\n",
            "epoch 33 | loss: 0.32062 | train_logloss: 0.31232 | train_accuracy: 0.88133 | valid_logloss: 0.32434 | valid_accuracy: 0.87733 |  0:09:40s\n",
            "epoch 34 | loss: 0.31652 | train_logloss: 0.31688 | train_accuracy: 0.88033 | valid_logloss: 0.33141 | valid_accuracy: 0.8766  |  0:09:56s\n",
            "epoch 35 | loss: 0.31367 | train_logloss: 0.30734 | train_accuracy: 0.88423 | valid_logloss: 0.32399 | valid_accuracy: 0.8804  |  0:10:13s\n",
            "epoch 36 | loss: 0.30587 | train_logloss: 0.30362 | train_accuracy: 0.8843  | valid_logloss: 0.32023 | valid_accuracy: 0.88047 |  0:10:29s\n",
            "epoch 37 | loss: 0.30346 | train_logloss: 0.30173 | train_accuracy: 0.88717 | valid_logloss: 0.31998 | valid_accuracy: 0.8809  |  0:10:46s\n",
            "epoch 38 | loss: 0.30369 | train_logloss: 0.29873 | train_accuracy: 0.8861  | valid_logloss: 0.31655 | valid_accuracy: 0.88293 |  0:11:03s\n",
            "epoch 39 | loss: 0.30373 | train_logloss: 0.29903 | train_accuracy: 0.88523 | valid_logloss: 0.31893 | valid_accuracy: 0.87923 |  0:11:19s\n",
            "epoch 40 | loss: 0.30253 | train_logloss: 0.2998  | train_accuracy: 0.8861  | valid_logloss: 0.3162  | valid_accuracy: 0.88113 |  0:11:36s\n",
            "epoch 41 | loss: 0.30112 | train_logloss: 0.30063 | train_accuracy: 0.88637 | valid_logloss: 0.32055 | valid_accuracy: 0.87947 |  0:11:53s\n",
            "epoch 42 | loss: 0.29868 | train_logloss: 0.29171 | train_accuracy: 0.88913 | valid_logloss: 0.31307 | valid_accuracy: 0.88287 |  0:12:10s\n",
            "epoch 43 | loss: 0.30958 | train_logloss: 0.30294 | train_accuracy: 0.88457 | valid_logloss: 0.32011 | valid_accuracy: 0.87967 |  0:12:27s\n",
            "epoch 44 | loss: 0.31165 | train_logloss: 0.32297 | train_accuracy: 0.8787  | valid_logloss: 0.33716 | valid_accuracy: 0.87373 |  0:12:44s\n",
            "epoch 45 | loss: 0.32323 | train_logloss: 0.31979 | train_accuracy: 0.87993 | valid_logloss: 0.33146 | valid_accuracy: 0.8774  |  0:13:01s\n",
            "epoch 46 | loss: 0.33553 | train_logloss: 0.35132 | train_accuracy: 0.86543 | valid_logloss: 0.36311 | valid_accuracy: 0.86353 |  0:13:18s\n",
            "epoch 47 | loss: 0.34426 | train_logloss: 0.32818 | train_accuracy: 0.87837 | valid_logloss: 0.3458  | valid_accuracy: 0.8745  |  0:13:34s\n",
            "epoch 48 | loss: 0.3284  | train_logloss: 0.32458 | train_accuracy: 0.87877 | valid_logloss: 0.33711 | valid_accuracy: 0.87383 |  0:13:51s\n",
            "epoch 49 | loss: 0.32263 | train_logloss: 0.31783 | train_accuracy: 0.8807  | valid_logloss: 0.32989 | valid_accuracy: 0.87763 |  0:14:07s\n",
            "epoch 50 | loss: 0.31521 | train_logloss: 0.312   | train_accuracy: 0.8811  | valid_logloss: 0.32526 | valid_accuracy: 0.87713 |  0:14:24s\n",
            "epoch 51 | loss: 0.30988 | train_logloss: 0.30392 | train_accuracy: 0.8834  | valid_logloss: 0.32016 | valid_accuracy: 0.87993 |  0:14:41s\n",
            "epoch 52 | loss: 0.31783 | train_logloss: 0.31398 | train_accuracy: 0.88093 | valid_logloss: 0.32746 | valid_accuracy: 0.87987 |  0:14:57s\n",
            "epoch 53 | loss: 0.31293 | train_logloss: 0.3036  | train_accuracy: 0.88347 | valid_logloss: 0.31821 | valid_accuracy: 0.88077 |  0:15:13s\n",
            "epoch 54 | loss: 0.31339 | train_logloss: 0.31185 | train_accuracy: 0.88347 | valid_logloss: 0.32424 | valid_accuracy: 0.87933 |  0:15:30s\n",
            "epoch 55 | loss: 0.31679 | train_logloss: 0.31155 | train_accuracy: 0.8814  | valid_logloss: 0.32446 | valid_accuracy: 0.87827 |  0:15:47s\n",
            "epoch 56 | loss: 0.32076 | train_logloss: 0.31394 | train_accuracy: 0.8782  | valid_logloss: 0.32585 | valid_accuracy: 0.87647 |  0:16:04s\n",
            "epoch 57 | loss: 0.31902 | train_logloss: 0.31112 | train_accuracy: 0.88063 | valid_logloss: 0.32589 | valid_accuracy: 0.87643 |  0:16:21s\n",
            "epoch 58 | loss: 0.3166  | train_logloss: 0.32023 | train_accuracy: 0.87773 | valid_logloss: 0.33087 | valid_accuracy: 0.8761  |  0:16:38s\n",
            "epoch 59 | loss: 0.31708 | train_logloss: 0.31488 | train_accuracy: 0.88167 | valid_logloss: 0.32651 | valid_accuracy: 0.87703 |  0:16:55s\n",
            "epoch 60 | loss: 0.31676 | train_logloss: 0.31247 | train_accuracy: 0.88063 | valid_logloss: 0.32388 | valid_accuracy: 0.87643 |  0:17:12s\n",
            "epoch 61 | loss: 0.31585 | train_logloss: 0.30699 | train_accuracy: 0.88417 | valid_logloss: 0.32206 | valid_accuracy: 0.8805  |  0:17:29s\n",
            "epoch 62 | loss: 0.30808 | train_logloss: 0.30962 | train_accuracy: 0.88343 | valid_logloss: 0.32593 | valid_accuracy: 0.88017 |  0:17:45s\n",
            "epoch 63 | loss: 0.31011 | train_logloss: 0.30087 | train_accuracy: 0.8844  | valid_logloss: 0.31388 | valid_accuracy: 0.88197 |  0:18:02s\n",
            "epoch 64 | loss: 0.3046  | train_logloss: 0.29346 | train_accuracy: 0.8881  | valid_logloss: 0.30949 | valid_accuracy: 0.8832  |  0:18:18s\n",
            "epoch 65 | loss: 0.30238 | train_logloss: 0.3095  | train_accuracy: 0.8838  | valid_logloss: 0.32526 | valid_accuracy: 0.87823 |  0:18:35s\n",
            "epoch 66 | loss: 0.31214 | train_logloss: 0.30606 | train_accuracy: 0.88367 | valid_logloss: 0.32208 | valid_accuracy: 0.87917 |  0:18:52s\n",
            "epoch 67 | loss: 0.32431 | train_logloss: 0.30901 | train_accuracy: 0.88377 | valid_logloss: 0.32367 | valid_accuracy: 0.87983 |  0:19:08s\n",
            "epoch 68 | loss: 0.31384 | train_logloss: 0.31358 | train_accuracy: 0.88003 | valid_logloss: 0.32679 | valid_accuracy: 0.87643 |  0:19:25s\n",
            "epoch 69 | loss: 0.31441 | train_logloss: 0.3046  | train_accuracy: 0.88277 | valid_logloss: 0.31834 | valid_accuracy: 0.88023 |  0:19:42s\n",
            "epoch 70 | loss: 0.30724 | train_logloss: 0.29932 | train_accuracy: 0.88493 | valid_logloss: 0.31324 | valid_accuracy: 0.88037 |  0:19:59s\n",
            "epoch 71 | loss: 0.30308 | train_logloss: 0.29493 | train_accuracy: 0.88617 | valid_logloss: 0.31178 | valid_accuracy: 0.8825  |  0:20:16s\n",
            "epoch 72 | loss: 0.30009 | train_logloss: 0.30094 | train_accuracy: 0.88517 | valid_logloss: 0.32168 | valid_accuracy: 0.88093 |  0:20:33s\n",
            "epoch 73 | loss: 0.30096 | train_logloss: 0.29928 | train_accuracy: 0.88663 | valid_logloss: 0.31387 | valid_accuracy: 0.88153 |  0:20:50s\n",
            "epoch 74 | loss: 0.29969 | train_logloss: 0.30157 | train_accuracy: 0.8862  | valid_logloss: 0.31759 | valid_accuracy: 0.88227 |  0:21:07s\n",
            "epoch 75 | loss: 0.30065 | train_logloss: 0.29785 | train_accuracy: 0.8842  | valid_logloss: 0.31121 | valid_accuracy: 0.8805  |  0:21:24s\n",
            "epoch 76 | loss: 0.31393 | train_logloss: 0.3167  | train_accuracy: 0.88113 | valid_logloss: 0.33023 | valid_accuracy: 0.87663 |  0:21:41s\n",
            "epoch 77 | loss: 0.31918 | train_logloss: 0.3229  | train_accuracy: 0.87903 | valid_logloss: 0.33445 | valid_accuracy: 0.8746  |  0:21:57s\n",
            "epoch 78 | loss: 0.32116 | train_logloss: 0.31411 | train_accuracy: 0.8832  | valid_logloss: 0.32806 | valid_accuracy: 0.87897 |  0:22:14s\n",
            "epoch 79 | loss: 0.32301 | train_logloss: 0.32783 | train_accuracy: 0.8796  | valid_logloss: 0.3406  | valid_accuracy: 0.87563 |  0:22:30s\n",
            "epoch 80 | loss: 0.33107 | train_logloss: 0.31552 | train_accuracy: 0.88057 | valid_logloss: 0.33122 | valid_accuracy: 0.8783  |  0:22:47s\n",
            "epoch 81 | loss: 0.3151  | train_logloss: 0.31013 | train_accuracy: 0.8838  | valid_logloss: 0.3251  | valid_accuracy: 0.88003 |  0:23:03s\n",
            "epoch 82 | loss: 0.31676 | train_logloss: 0.30951 | train_accuracy: 0.88383 | valid_logloss: 0.32471 | valid_accuracy: 0.8816  |  0:23:20s\n",
            "epoch 83 | loss: 0.31503 | train_logloss: 0.31078 | train_accuracy: 0.88497 | valid_logloss: 0.33061 | valid_accuracy: 0.88067 |  0:23:36s\n",
            "epoch 84 | loss: 0.31477 | train_logloss: 0.30959 | train_accuracy: 0.88557 | valid_logloss: 0.32318 | valid_accuracy: 0.88137 |  0:23:53s\n",
            "epoch 85 | loss: 0.309   | train_logloss: 0.30049 | train_accuracy: 0.8868  | valid_logloss: 0.3179  | valid_accuracy: 0.8827  |  0:24:09s\n",
            "epoch 86 | loss: 0.30397 | train_logloss: 0.29986 | train_accuracy: 0.88477 | valid_logloss: 0.31853 | valid_accuracy: 0.881   |  0:24:26s\n",
            "epoch 87 | loss: 0.30201 | train_logloss: 0.29888 | train_accuracy: 0.8875  | valid_logloss: 0.31816 | valid_accuracy: 0.88233 |  0:24:42s\n",
            "epoch 88 | loss: 0.30537 | train_logloss: 0.29622 | train_accuracy: 0.88727 | valid_logloss: 0.31449 | valid_accuracy: 0.88367 |  0:24:59s\n",
            "epoch 89 | loss: 0.30015 | train_logloss: 0.29225 | train_accuracy: 0.889   | valid_logloss: 0.30917 | valid_accuracy: 0.88453 |  0:25:23s\n",
            "epoch 90 | loss: 0.30145 | train_logloss: 0.30395 | train_accuracy: 0.88697 | valid_logloss: 0.31971 | valid_accuracy: 0.88403 |  0:25:41s\n",
            "epoch 91 | loss: 0.30151 | train_logloss: 0.29674 | train_accuracy: 0.88783 | valid_logloss: 0.3156  | valid_accuracy: 0.8832  |  0:25:57s\n",
            "epoch 92 | loss: 0.29546 | train_logloss: 0.29391 | train_accuracy: 0.8878  | valid_logloss: 0.31385 | valid_accuracy: 0.88373 |  0:26:14s\n",
            "epoch 93 | loss: 0.30173 | train_logloss: 0.29431 | train_accuracy: 0.8876  | valid_logloss: 0.31522 | valid_accuracy: 0.88423 |  0:26:31s\n",
            "epoch 94 | loss: 0.29657 | train_logloss: 0.29357 | train_accuracy: 0.88713 | valid_logloss: 0.31227 | valid_accuracy: 0.88427 |  0:26:48s\n",
            "epoch 95 | loss: 0.29338 | train_logloss: 0.28399 | train_accuracy: 0.8917  | valid_logloss: 0.30315 | valid_accuracy: 0.88723 |  0:27:04s\n",
            "epoch 96 | loss: 0.28855 | train_logloss: 0.28537 | train_accuracy: 0.8907  | valid_logloss: 0.30248 | valid_accuracy: 0.8883  |  0:27:21s\n",
            "epoch 97 | loss: 0.28426 | train_logloss: 0.27977 | train_accuracy: 0.89233 | valid_logloss: 0.29799 | valid_accuracy: 0.88733 |  0:27:37s\n",
            "epoch 98 | loss: 0.28322 | train_logloss: 0.27637 | train_accuracy: 0.89317 | valid_logloss: 0.29804 | valid_accuracy: 0.88843 |  0:27:54s\n",
            "epoch 99 | loss: 0.28323 | train_logloss: 0.27992 | train_accuracy: 0.89127 | valid_logloss: 0.30121 | valid_accuracy: 0.8865  |  0:28:10s\n",
            "epoch 100| loss: 0.28331 | train_logloss: 0.27832 | train_accuracy: 0.8932  | valid_logloss: 0.29882 | valid_accuracy: 0.8873  |  0:28:27s\n",
            "epoch 101| loss: 0.28239 | train_logloss: 0.27327 | train_accuracy: 0.8952  | valid_logloss: 0.29452 | valid_accuracy: 0.89003 |  0:28:43s\n",
            "epoch 102| loss: 0.27889 | train_logloss: 0.27464 | train_accuracy: 0.8947  | valid_logloss: 0.29619 | valid_accuracy: 0.88977 |  0:29:00s\n",
            "epoch 103| loss: 0.28175 | train_logloss: 0.27444 | train_accuracy: 0.8931  | valid_logloss: 0.29557 | valid_accuracy: 0.88753 |  0:29:16s\n",
            "epoch 104| loss: 0.28175 | train_logloss: 0.27719 | train_accuracy: 0.8925  | valid_logloss: 0.29746 | valid_accuracy: 0.88777 |  0:29:32s\n",
            "epoch 105| loss: 0.28003 | train_logloss: 0.27123 | train_accuracy: 0.89407 | valid_logloss: 0.29644 | valid_accuracy: 0.88907 |  0:29:49s\n",
            "epoch 106| loss: 0.27504 | train_logloss: 0.26966 | train_accuracy: 0.89563 | valid_logloss: 0.29238 | valid_accuracy: 0.8886  |  0:30:05s\n",
            "epoch 107| loss: 0.27362 | train_logloss: 0.27213 | train_accuracy: 0.8957  | valid_logloss: 0.29419 | valid_accuracy: 0.88987 |  0:30:22s\n",
            "epoch 108| loss: 0.27285 | train_logloss: 0.27627 | train_accuracy: 0.89383 | valid_logloss: 0.30047 | valid_accuracy: 0.8882  |  0:30:38s\n",
            "epoch 109| loss: 0.28644 | train_logloss: 0.2804  | train_accuracy: 0.8922  | valid_logloss: 0.30164 | valid_accuracy: 0.8858  |  0:30:54s\n",
            "epoch 110| loss: 0.27833 | train_logloss: 0.27279 | train_accuracy: 0.8948  | valid_logloss: 0.29745 | valid_accuracy: 0.88833 |  0:31:11s\n",
            "epoch 111| loss: 0.27294 | train_logloss: 0.26586 | train_accuracy: 0.89803 | valid_logloss: 0.29374 | valid_accuracy: 0.8908  |  0:31:27s\n",
            "epoch 112| loss: 0.27084 | train_logloss: 0.26686 | train_accuracy: 0.89763 | valid_logloss: 0.29778 | valid_accuracy: 0.88893 |  0:31:44s\n",
            "epoch 113| loss: 0.27193 | train_logloss: 0.26911 | train_accuracy: 0.8978  | valid_logloss: 0.29481 | valid_accuracy: 0.8899  |  0:32:00s\n",
            "epoch 114| loss: 0.27585 | train_logloss: 0.26687 | train_accuracy: 0.89753 | valid_logloss: 0.29281 | valid_accuracy: 0.89143 |  0:32:17s\n",
            "epoch 115| loss: 0.27564 | train_logloss: 0.27135 | train_accuracy: 0.89657 | valid_logloss: 0.29705 | valid_accuracy: 0.889   |  0:32:33s\n",
            "epoch 116| loss: 0.28196 | train_logloss: 0.28764 | train_accuracy: 0.8888  | valid_logloss: 0.30814 | valid_accuracy: 0.8839  |  0:32:50s\n",
            "epoch 117| loss: 0.28853 | train_logloss: 0.2817  | train_accuracy: 0.89307 | valid_logloss: 0.30346 | valid_accuracy: 0.8858  |  0:33:06s\n",
            "epoch 118| loss: 0.29684 | train_logloss: 0.29251 | train_accuracy: 0.89017 | valid_logloss: 0.31434 | valid_accuracy: 0.88417 |  0:33:23s\n",
            "epoch 119| loss: 0.29504 | train_logloss: 0.28304 | train_accuracy: 0.89233 | valid_logloss: 0.30354 | valid_accuracy: 0.88737 |  0:33:39s\n",
            "epoch 120| loss: 0.28601 | train_logloss: 0.27683 | train_accuracy: 0.8949  | valid_logloss: 0.29993 | valid_accuracy: 0.88917 |  0:33:56s\n",
            "epoch 121| loss: 0.28555 | train_logloss: 0.27824 | train_accuracy: 0.89307 | valid_logloss: 0.30203 | valid_accuracy: 0.88797 |  0:34:12s\n",
            "epoch 122| loss: 0.28211 | train_logloss: 0.29705 | train_accuracy: 0.8873  | valid_logloss: 0.31841 | valid_accuracy: 0.88157 |  0:34:28s\n",
            "epoch 123| loss: 0.29694 | train_logloss: 0.28522 | train_accuracy: 0.89227 | valid_logloss: 0.30751 | valid_accuracy: 0.88497 |  0:34:45s\n",
            "epoch 124| loss: 0.28718 | train_logloss: 0.28183 | train_accuracy: 0.89343 | valid_logloss: 0.30508 | valid_accuracy: 0.88423 |  0:35:01s\n",
            "epoch 125| loss: 0.28445 | train_logloss: 0.27693 | train_accuracy: 0.89493 | valid_logloss: 0.30101 | valid_accuracy: 0.8875  |  0:35:17s\n",
            "epoch 126| loss: 0.28169 | train_logloss: 0.27367 | train_accuracy: 0.89527 | valid_logloss: 0.298   | valid_accuracy: 0.88793 |  0:35:34s\n",
            "epoch 127| loss: 0.2786  | train_logloss: 0.27373 | train_accuracy: 0.8937  | valid_logloss: 0.29926 | valid_accuracy: 0.88707 |  0:35:50s\n",
            "epoch 128| loss: 0.27695 | train_logloss: 0.2699  | train_accuracy: 0.89773 | valid_logloss: 0.29604 | valid_accuracy: 0.88813 |  0:36:06s\n",
            "epoch 129| loss: 0.2752  | train_logloss: 0.26897 | train_accuracy: 0.89673 | valid_logloss: 0.29504 | valid_accuracy: 0.88847 |  0:36:23s\n",
            "epoch 130| loss: 0.27214 | train_logloss: 0.2661  | train_accuracy: 0.89763 | valid_logloss: 0.29494 | valid_accuracy: 0.88843 |  0:36:39s\n",
            "epoch 131| loss: 0.28085 | train_logloss: 0.28493 | train_accuracy: 0.89333 | valid_logloss: 0.3081  | valid_accuracy: 0.88707 |  0:36:56s\n",
            "epoch 132| loss: 0.28378 | train_logloss: 0.27542 | train_accuracy: 0.89607 | valid_logloss: 0.3009  | valid_accuracy: 0.88907 |  0:37:12s\n",
            "epoch 133| loss: 0.27761 | train_logloss: 0.27155 | train_accuracy: 0.89587 | valid_logloss: 0.29986 | valid_accuracy: 0.88713 |  0:37:29s\n",
            "epoch 134| loss: 0.27238 | train_logloss: 0.26913 | train_accuracy: 0.8951  | valid_logloss: 0.29667 | valid_accuracy: 0.886   |  0:37:45s\n",
            "epoch 135| loss: 0.27549 | train_logloss: 0.28965 | train_accuracy: 0.88803 | valid_logloss: 0.31253 | valid_accuracy: 0.88143 |  0:38:01s\n",
            "epoch 136| loss: 0.28442 | train_logloss: 0.27286 | train_accuracy: 0.89577 | valid_logloss: 0.29769 | valid_accuracy: 0.88943 |  0:38:18s\n",
            "epoch 137| loss: 0.27806 | train_logloss: 0.27038 | train_accuracy: 0.89813 | valid_logloss: 0.29526 | valid_accuracy: 0.89123 |  0:38:34s\n",
            "epoch 138| loss: 0.28563 | train_logloss: 0.27653 | train_accuracy: 0.89383 | valid_logloss: 0.29932 | valid_accuracy: 0.88767 |  0:38:50s\n",
            "epoch 139| loss: 0.28088 | train_logloss: 0.27529 | train_accuracy: 0.89607 | valid_logloss: 0.29816 | valid_accuracy: 0.88847 |  0:39:07s\n",
            "epoch 140| loss: 0.27739 | train_logloss: 0.26945 | train_accuracy: 0.8968  | valid_logloss: 0.29715 | valid_accuracy: 0.88997 |  0:39:23s\n",
            "epoch 141| loss: 0.27362 | train_logloss: 0.26849 | train_accuracy: 0.89727 | valid_logloss: 0.29636 | valid_accuracy: 0.88907 |  0:39:40s\n",
            "epoch 142| loss: 0.27336 | train_logloss: 0.26454 | train_accuracy: 0.89893 | valid_logloss: 0.29483 | valid_accuracy: 0.89013 |  0:39:56s\n",
            "epoch 143| loss: 0.27034 | train_logloss: 0.26824 | train_accuracy: 0.89763 | valid_logloss: 0.29937 | valid_accuracy: 0.88853 |  0:40:13s\n",
            "epoch 144| loss: 0.27257 | train_logloss: 0.27358 | train_accuracy: 0.8975  | valid_logloss: 0.30101 | valid_accuracy: 0.88913 |  0:40:30s\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 114 and best_valid_accuracy = 0.89143\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[413036  12948  54198]\n",
            " [ 14925 950652  34423]\n",
            " [ 31924  28030 372633]]\n",
            "Testing Score:  0.8914333333333333\n",
            "100%|██████████| 10/10 [02:01<00:00, 12.12s/it, best loss: -0.8922333333333334]\n",
            "({'colsample_bytree': 0.5489629528552913, 'min_child_samples': 38, 'min_child_weight': 0.280452581519467, 'num_leaves': 52}, <hyperopt.base.Trials object at 0x7fcddc084290>)\n",
            "7.940027713775635\n",
            "Confusion Matrix: \n",
            " [[420226  12417  47539]\n",
            " [ 13828 954946  31226]\n",
            " [ 34199  27955 370433]]\n",
            "Testing Score:  0.8942\n",
            "{'Rows': 30000, 'Nd': 64, 'Na': 64, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 2433.8832054138184, 'time_tn': 201.5446376800537, 'accuracy_tn': 0.8914333333333333, 'time_learn_gb': 1618982318.1305528, 'time_gb': 20.734107494354248, 'accuracy_gb': 0.8942}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9iTUqIHhWcYu",
        "outputId": "fe996736-2b93-4362-c9c3-f4288a30b162"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1   9000   9000   9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2   9000   9000   9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3   9000   9000   9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4   9000   9000   9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "5  30000  30000  30000    8  ...   67.806992    0.893733  1.618979e+09  25.906476\n",
              "6  30000  30000  30000   16  ...   78.570949    0.893633  1.618980e+09  21.026869\n",
              "7  30000  30000  30000   64  ...  201.544638    0.894200  1.618982e+09  20.734107\n",
              "\n",
              "[8 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ6c2Ct4BLAd",
        "outputId": "3ef707fc-a87c-4ffd-d715-54b89b81725a"
      },
      "source": [
        "time_model(number_exp=9, \n",
        "     Rows=30000, \n",
        "     Nd=32,\tNa=32,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=3, decision=3, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.80305 | train_logloss: 18.29121| train_accuracy: 0.27807 | valid_logloss: 18.18305| valid_accuracy: 0.28167 |  0:00:08s\n",
            "epoch 1  | loss: 0.44249 | train_logloss: 5.02792 | train_accuracy: 0.3149  | valid_logloss: 5.04669 | valid_accuracy: 0.31437 |  0:00:16s\n",
            "epoch 2  | loss: 0.37984 | train_logloss: 3.61867 | train_accuracy: 0.24013 | valid_logloss: 3.64938 | valid_accuracy: 0.2383  |  0:00:24s\n",
            "epoch 3  | loss: 0.35812 | train_logloss: 9.74934 | train_accuracy: 0.3334  | valid_logloss: 9.79333 | valid_accuracy: 0.33343 |  0:00:31s\n",
            "epoch 4  | loss: 0.33666 | train_logloss: 7.24188 | train_accuracy: 0.33623 | valid_logloss: 7.26347 | valid_accuracy: 0.33567 |  0:00:39s\n",
            "epoch 5  | loss: 0.32911 | train_logloss: 7.53498 | train_accuracy: 0.25127 | valid_logloss: 7.56136 | valid_accuracy: 0.2509  |  0:00:47s\n",
            "epoch 6  | loss: 0.32112 | train_logloss: 3.33266 | train_accuracy: 0.33557 | valid_logloss: 3.3425  | valid_accuracy: 0.3359  |  0:00:55s\n",
            "epoch 7  | loss: 0.31643 | train_logloss: 2.57589 | train_accuracy: 0.29067 | valid_logloss: 2.61125 | valid_accuracy: 0.2832  |  0:01:03s\n",
            "epoch 8  | loss: 0.31088 | train_logloss: 1.48563 | train_accuracy: 0.44363 | valid_logloss: 1.48681 | valid_accuracy: 0.4459  |  0:01:11s\n",
            "epoch 9  | loss: 0.3055  | train_logloss: 1.58188 | train_accuracy: 0.49303 | valid_logloss: 1.60336 | valid_accuracy: 0.4861  |  0:01:20s\n",
            "epoch 10 | loss: 0.3027  | train_logloss: 1.80277 | train_accuracy: 0.42243 | valid_logloss: 1.80944 | valid_accuracy: 0.42047 |  0:01:28s\n",
            "epoch 11 | loss: 0.29958 | train_logloss: 1.16711 | train_accuracy: 0.56767 | valid_logloss: 1.17003 | valid_accuracy: 0.56793 |  0:01:36s\n",
            "epoch 12 | loss: 0.29668 | train_logloss: 1.05808 | train_accuracy: 0.5996  | valid_logloss: 1.06135 | valid_accuracy: 0.59427 |  0:01:44s\n",
            "epoch 13 | loss: 0.29453 | train_logloss: 0.86409 | train_accuracy: 0.67347 | valid_logloss: 0.87033 | valid_accuracy: 0.6694  |  0:01:52s\n",
            "epoch 14 | loss: 0.29336 | train_logloss: 0.70287 | train_accuracy: 0.67303 | valid_logloss: 0.71407 | valid_accuracy: 0.66487 |  0:02:01s\n",
            "epoch 15 | loss: 0.2927  | train_logloss: 0.63557 | train_accuracy: 0.74067 | valid_logloss: 0.64923 | valid_accuracy: 0.7334  |  0:02:09s\n",
            "epoch 16 | loss: 0.29055 | train_logloss: 0.5238  | train_accuracy: 0.81393 | valid_logloss: 0.53483 | valid_accuracy: 0.81023 |  0:02:17s\n",
            "epoch 17 | loss: 0.29183 | train_logloss: 0.52901 | train_accuracy: 0.79177 | valid_logloss: 0.53988 | valid_accuracy: 0.78807 |  0:02:25s\n",
            "epoch 18 | loss: 0.2903  | train_logloss: 0.49229 | train_accuracy: 0.7908  | valid_logloss: 0.50415 | valid_accuracy: 0.7863  |  0:02:33s\n",
            "epoch 19 | loss: 0.28838 | train_logloss: 0.38997 | train_accuracy: 0.84827 | valid_logloss: 0.39931 | valid_accuracy: 0.84567 |  0:02:42s\n",
            "epoch 20 | loss: 0.28711 | train_logloss: 0.3875  | train_accuracy: 0.84543 | valid_logloss: 0.40018 | valid_accuracy: 0.8416  |  0:02:50s\n",
            "epoch 21 | loss: 0.28812 | train_logloss: 0.34249 | train_accuracy: 0.8694  | valid_logloss: 0.3554  | valid_accuracy: 0.86467 |  0:02:58s\n",
            "epoch 22 | loss: 0.28529 | train_logloss: 0.33541 | train_accuracy: 0.87333 | valid_logloss: 0.35312 | valid_accuracy: 0.86837 |  0:03:06s\n",
            "epoch 23 | loss: 0.2815  | train_logloss: 0.30605 | train_accuracy: 0.88043 | valid_logloss: 0.324   | valid_accuracy: 0.8761  |  0:03:14s\n",
            "epoch 24 | loss: 0.2799  | train_logloss: 0.29639 | train_accuracy: 0.8859  | valid_logloss: 0.31685 | valid_accuracy: 0.88017 |  0:03:23s\n",
            "epoch 25 | loss: 0.27542 | train_logloss: 0.29549 | train_accuracy: 0.88757 | valid_logloss: 0.31698 | valid_accuracy: 0.8811  |  0:03:31s\n",
            "epoch 26 | loss: 0.27438 | train_logloss: 0.28532 | train_accuracy: 0.88983 | valid_logloss: 0.30918 | valid_accuracy: 0.88487 |  0:03:39s\n",
            "epoch 27 | loss: 0.27445 | train_logloss: 0.28193 | train_accuracy: 0.8932  | valid_logloss: 0.3039  | valid_accuracy: 0.8865  |  0:03:47s\n",
            "epoch 28 | loss: 0.27356 | train_logloss: 0.27659 | train_accuracy: 0.89567 | valid_logloss: 0.3056  | valid_accuracy: 0.88883 |  0:03:55s\n",
            "epoch 29 | loss: 0.27638 | train_logloss: 0.27316 | train_accuracy: 0.8953  | valid_logloss: 0.30047 | valid_accuracy: 0.8879  |  0:04:03s\n",
            "epoch 30 | loss: 0.27549 | train_logloss: 0.28197 | train_accuracy: 0.89143 | valid_logloss: 0.31065 | valid_accuracy: 0.88383 |  0:04:12s\n",
            "epoch 31 | loss: 0.26892 | train_logloss: 0.27082 | train_accuracy: 0.89687 | valid_logloss: 0.29954 | valid_accuracy: 0.8893  |  0:04:20s\n",
            "epoch 32 | loss: 0.26827 | train_logloss: 0.26027 | train_accuracy: 0.90013 | valid_logloss: 0.29397 | valid_accuracy: 0.89237 |  0:04:28s\n",
            "epoch 33 | loss: 0.26697 | train_logloss: 0.25609 | train_accuracy: 0.90243 | valid_logloss: 0.28995 | valid_accuracy: 0.89327 |  0:04:36s\n",
            "epoch 34 | loss: 0.26562 | train_logloss: 0.26567 | train_accuracy: 0.89973 | valid_logloss: 0.30466 | valid_accuracy: 0.8879  |  0:04:44s\n",
            "epoch 35 | loss: 0.26673 | train_logloss: 0.26023 | train_accuracy: 0.89943 | valid_logloss: 0.29679 | valid_accuracy: 0.89023 |  0:04:52s\n",
            "epoch 36 | loss: 0.2647  | train_logloss: 0.25466 | train_accuracy: 0.90157 | valid_logloss: 0.29514 | valid_accuracy: 0.88973 |  0:05:01s\n",
            "epoch 37 | loss: 0.26644 | train_logloss: 0.25667 | train_accuracy: 0.9021  | valid_logloss: 0.29243 | valid_accuracy: 0.89263 |  0:05:09s\n",
            "epoch 38 | loss: 0.26361 | train_logloss: 0.26054 | train_accuracy: 0.89957 | valid_logloss: 0.3003  | valid_accuracy: 0.8892  |  0:05:17s\n",
            "epoch 39 | loss: 0.26729 | train_logloss: 0.2594  | train_accuracy: 0.90003 | valid_logloss: 0.29697 | valid_accuracy: 0.8893  |  0:05:25s\n",
            "epoch 40 | loss: 0.26731 | train_logloss: 0.26092 | train_accuracy: 0.9007  | valid_logloss: 0.29555 | valid_accuracy: 0.89093 |  0:05:34s\n",
            "epoch 41 | loss: 0.26815 | train_logloss: 0.26237 | train_accuracy: 0.89917 | valid_logloss: 0.29229 | valid_accuracy: 0.89213 |  0:05:42s\n",
            "epoch 42 | loss: 0.26823 | train_logloss: 0.25616 | train_accuracy: 0.9012  | valid_logloss: 0.29111 | valid_accuracy: 0.8914  |  0:05:50s\n",
            "epoch 43 | loss: 0.2699  | train_logloss: 0.26453 | train_accuracy: 0.89793 | valid_logloss: 0.29688 | valid_accuracy: 0.88883 |  0:05:58s\n",
            "epoch 44 | loss: 0.27135 | train_logloss: 0.26097 | train_accuracy: 0.89977 | valid_logloss: 0.29633 | valid_accuracy: 0.89007 |  0:06:07s\n",
            "epoch 45 | loss: 0.26818 | train_logloss: 0.2555  | train_accuracy: 0.90337 | valid_logloss: 0.29357 | valid_accuracy: 0.89177 |  0:06:15s\n",
            "epoch 46 | loss: 0.26602 | train_logloss: 0.31987 | train_accuracy: 0.87847 | valid_logloss: 0.35206 | valid_accuracy: 0.86877 |  0:06:23s\n",
            "epoch 47 | loss: 0.27557 | train_logloss: 0.25901 | train_accuracy: 0.90157 | valid_logloss: 0.29525 | valid_accuracy: 0.89073 |  0:06:31s\n",
            "epoch 48 | loss: 0.26671 | train_logloss: 0.25101 | train_accuracy: 0.9026  | valid_logloss: 0.29213 | valid_accuracy: 0.89217 |  0:06:39s\n",
            "epoch 49 | loss: 0.25958 | train_logloss: 0.25123 | train_accuracy: 0.90417 | valid_logloss: 0.29202 | valid_accuracy: 0.89147 |  0:06:48s\n",
            "epoch 50 | loss: 0.26017 | train_logloss: 0.2521  | train_accuracy: 0.90443 | valid_logloss: 0.29438 | valid_accuracy: 0.89217 |  0:06:56s\n",
            "epoch 51 | loss: 0.26822 | train_logloss: 0.26236 | train_accuracy: 0.90023 | valid_logloss: 0.29958 | valid_accuracy: 0.88877 |  0:07:04s\n",
            "epoch 52 | loss: 0.26963 | train_logloss: 0.26551 | train_accuracy: 0.89967 | valid_logloss: 0.30559 | valid_accuracy: 0.8902  |  0:07:12s\n",
            "epoch 53 | loss: 0.27406 | train_logloss: 0.26495 | train_accuracy: 0.89917 | valid_logloss: 0.29889 | valid_accuracy: 0.88917 |  0:07:21s\n",
            "epoch 54 | loss: 0.26766 | train_logloss: 0.25643 | train_accuracy: 0.90007 | valid_logloss: 0.29449 | valid_accuracy: 0.89167 |  0:07:29s\n",
            "epoch 55 | loss: 0.26275 | train_logloss: 0.25363 | train_accuracy: 0.90157 | valid_logloss: 0.2948  | valid_accuracy: 0.8939  |  0:07:37s\n",
            "epoch 56 | loss: 0.26302 | train_logloss: 0.26211 | train_accuracy: 0.89887 | valid_logloss: 0.29981 | valid_accuracy: 0.8906  |  0:07:45s\n",
            "epoch 57 | loss: 0.27493 | train_logloss: 0.26474 | train_accuracy: 0.89883 | valid_logloss: 0.29424 | valid_accuracy: 0.8898  |  0:07:53s\n",
            "epoch 58 | loss: 0.27159 | train_logloss: 0.26587 | train_accuracy: 0.90007 | valid_logloss: 0.30076 | valid_accuracy: 0.89207 |  0:08:02s\n",
            "epoch 59 | loss: 0.27399 | train_logloss: 0.26276 | train_accuracy: 0.9013  | valid_logloss: 0.29895 | valid_accuracy: 0.89123 |  0:08:10s\n",
            "epoch 60 | loss: 0.27152 | train_logloss: 0.26974 | train_accuracy: 0.89687 | valid_logloss: 0.29903 | valid_accuracy: 0.88713 |  0:08:18s\n",
            "epoch 61 | loss: 0.27543 | train_logloss: 0.26419 | train_accuracy: 0.8996  | valid_logloss: 0.29742 | valid_accuracy: 0.88923 |  0:08:26s\n",
            "epoch 62 | loss: 0.2698  | train_logloss: 0.26474 | train_accuracy: 0.89873 | valid_logloss: 0.29867 | valid_accuracy: 0.89057 |  0:08:34s\n",
            "epoch 63 | loss: 0.26535 | train_logloss: 0.25722 | train_accuracy: 0.90137 | valid_logloss: 0.29388 | valid_accuracy: 0.8902  |  0:08:42s\n",
            "epoch 64 | loss: 0.26493 | train_logloss: 0.26569 | train_accuracy: 0.8989  | valid_logloss: 0.29772 | valid_accuracy: 0.88903 |  0:08:51s\n",
            "epoch 65 | loss: 0.26092 | train_logloss: 0.25155 | train_accuracy: 0.90237 | valid_logloss: 0.29172 | valid_accuracy: 0.8931  |  0:08:59s\n",
            "epoch 66 | loss: 0.26012 | train_logloss: 0.24835 | train_accuracy: 0.90483 | valid_logloss: 0.29398 | valid_accuracy: 0.89223 |  0:09:07s\n",
            "epoch 67 | loss: 0.25745 | train_logloss: 0.24807 | train_accuracy: 0.9035  | valid_logloss: 0.29397 | valid_accuracy: 0.89227 |  0:09:15s\n",
            "epoch 68 | loss: 0.25395 | train_logloss: 0.24106 | train_accuracy: 0.9063  | valid_logloss: 0.28985 | valid_accuracy: 0.89483 |  0:09:23s\n",
            "epoch 69 | loss: 0.25249 | train_logloss: 0.24508 | train_accuracy: 0.9063  | valid_logloss: 0.29318 | valid_accuracy: 0.89453 |  0:09:32s\n",
            "epoch 70 | loss: 0.25247 | train_logloss: 0.23893 | train_accuracy: 0.90793 | valid_logloss: 0.29394 | valid_accuracy: 0.89463 |  0:09:40s\n",
            "epoch 71 | loss: 0.25133 | train_logloss: 0.23941 | train_accuracy: 0.90737 | valid_logloss: 0.29522 | valid_accuracy: 0.89437 |  0:09:48s\n",
            "epoch 72 | loss: 0.25153 | train_logloss: 0.25124 | train_accuracy: 0.90307 | valid_logloss: 0.29906 | valid_accuracy: 0.89103 |  0:09:56s\n",
            "epoch 73 | loss: 0.25326 | train_logloss: 0.24642 | train_accuracy: 0.90593 | valid_logloss: 0.29412 | valid_accuracy: 0.891   |  0:10:04s\n",
            "epoch 74 | loss: 0.25344 | train_logloss: 0.24129 | train_accuracy: 0.9061  | valid_logloss: 0.29731 | valid_accuracy: 0.89123 |  0:10:12s\n",
            "epoch 75 | loss: 0.25175 | train_logloss: 0.24318 | train_accuracy: 0.90767 | valid_logloss: 0.29293 | valid_accuracy: 0.89387 |  0:10:20s\n",
            "epoch 76 | loss: 0.25152 | train_logloss: 0.23541 | train_accuracy: 0.90787 | valid_logloss: 0.29298 | valid_accuracy: 0.89287 |  0:10:29s\n",
            "epoch 77 | loss: 0.24666 | train_logloss: 0.23729 | train_accuracy: 0.90673 | valid_logloss: 0.29874 | valid_accuracy: 0.893   |  0:10:37s\n",
            "epoch 78 | loss: 0.24841 | train_logloss: 0.24137 | train_accuracy: 0.9056  | valid_logloss: 0.30196 | valid_accuracy: 0.89083 |  0:10:48s\n",
            "epoch 79 | loss: 0.24977 | train_logloss: 0.23966 | train_accuracy: 0.90763 | valid_logloss: 0.29531 | valid_accuracy: 0.89247 |  0:10:56s\n",
            "epoch 80 | loss: 0.24293 | train_logloss: 0.23182 | train_accuracy: 0.90963 | valid_logloss: 0.30054 | valid_accuracy: 0.89157 |  0:11:04s\n",
            "epoch 81 | loss: 0.23976 | train_logloss: 0.22881 | train_accuracy: 0.91043 | valid_logloss: 0.29875 | valid_accuracy: 0.8926  |  0:11:13s\n",
            "epoch 82 | loss: 0.23968 | train_logloss: 0.23322 | train_accuracy: 0.90827 | valid_logloss: 0.30517 | valid_accuracy: 0.89077 |  0:11:21s\n",
            "epoch 83 | loss: 0.24102 | train_logloss: 0.22847 | train_accuracy: 0.9105  | valid_logloss: 0.30339 | valid_accuracy: 0.8927  |  0:11:29s\n",
            "epoch 84 | loss: 0.24168 | train_logloss: 0.23203 | train_accuracy: 0.90903 | valid_logloss: 0.30196 | valid_accuracy: 0.8909  |  0:11:37s\n",
            "epoch 85 | loss: 0.23927 | train_logloss: 0.2249  | train_accuracy: 0.91157 | valid_logloss: 0.29777 | valid_accuracy: 0.8945  |  0:11:46s\n",
            "epoch 86 | loss: 0.23781 | train_logloss: 0.22605 | train_accuracy: 0.91263 | valid_logloss: 0.30363 | valid_accuracy: 0.89443 |  0:11:54s\n",
            "epoch 87 | loss: 0.23838 | train_logloss: 0.22538 | train_accuracy: 0.91183 | valid_logloss: 0.30426 | valid_accuracy: 0.8929  |  0:12:02s\n",
            "epoch 88 | loss: 0.23589 | train_logloss: 0.22368 | train_accuracy: 0.9129  | valid_logloss: 0.30489 | valid_accuracy: 0.8927  |  0:12:10s\n",
            "epoch 89 | loss: 0.23217 | train_logloss: 0.21904 | train_accuracy: 0.91323 | valid_logloss: 0.30305 | valid_accuracy: 0.89227 |  0:12:19s\n",
            "epoch 90 | loss: 0.23388 | train_logloss: 0.22956 | train_accuracy: 0.91097 | valid_logloss: 0.31326 | valid_accuracy: 0.89073 |  0:12:27s\n",
            "epoch 91 | loss: 0.23354 | train_logloss: 0.22605 | train_accuracy: 0.91173 | valid_logloss: 0.31018 | valid_accuracy: 0.88987 |  0:12:35s\n",
            "epoch 92 | loss: 0.24002 | train_logloss: 0.22816 | train_accuracy: 0.9125  | valid_logloss: 0.30675 | valid_accuracy: 0.89273 |  0:12:43s\n",
            "epoch 93 | loss: 0.23608 | train_logloss: 0.22452 | train_accuracy: 0.91247 | valid_logloss: 0.3075  | valid_accuracy: 0.89153 |  0:12:52s\n",
            "epoch 94 | loss: 0.23503 | train_logloss: 0.22668 | train_accuracy: 0.91143 | valid_logloss: 0.30924 | valid_accuracy: 0.89147 |  0:13:00s\n",
            "epoch 95 | loss: 0.23481 | train_logloss: 0.2236  | train_accuracy: 0.91197 | valid_logloss: 0.30853 | valid_accuracy: 0.89123 |  0:13:08s\n",
            "epoch 96 | loss: 0.23457 | train_logloss: 0.22478 | train_accuracy: 0.91347 | valid_logloss: 0.29904 | valid_accuracy: 0.89177 |  0:13:16s\n",
            "epoch 97 | loss: 0.23595 | train_logloss: 0.2189  | train_accuracy: 0.9138  | valid_logloss: 0.30128 | valid_accuracy: 0.89473 |  0:13:24s\n",
            "epoch 98 | loss: 0.23257 | train_logloss: 0.21901 | train_accuracy: 0.9138  | valid_logloss: 0.3026  | valid_accuracy: 0.89387 |  0:13:33s\n",
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 68 and best_valid_accuracy = 0.89483\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[415983  12860  51339]\n",
            " [ 16195 955035  28770]\n",
            " [ 30457  29948 372182]]\n",
            "Testing Score:  0.8948333333333334\n",
            "100%|██████████| 10/10 [02:15<00:00, 13.59s/it, best loss: -0.8932333333333333]\n",
            "({'colsample_bytree': 0.8128624737720715, 'min_child_samples': 44, 'min_child_weight': 0.23003296786544222, 'num_leaves': 96}, <hyperopt.base.Trials object at 0x7fcdd30dd210>)\n",
            "13.619279146194458\n",
            "Confusion Matrix: \n",
            " [[420304  12509  47369]\n",
            " [ 13844 953927  32229]\n",
            " [ 34896  27793 369898]]\n",
            "Testing Score:  0.8935\n",
            "{'Rows': 30000, 'Nd': 32, 'Na': 32, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 3, 'decision': 3, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 814.8159272670746, 'time_tn': 103.36721324920654, 'accuracy_tn': 0.8948333333333334, 'time_learn_gb': 1618983419.809925, 'time_gb': 20.529149055480957, 'accuracy_gb': 0.8935}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "oqa-eziBWc2M",
        "outputId": "6b2023b4-e269-4998-a701-019ea5d3fb34"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894833</td>\n",
              "      <td>814.815927</td>\n",
              "      <td>103.367213</td>\n",
              "      <td>0.893500</td>\n",
              "      <td>1.618983e+09</td>\n",
              "      <td>20.529149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1   9000   9000   9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2   9000   9000   9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3   9000   9000   9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4   9000   9000   9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "5  30000  30000  30000    8  ...   67.806992    0.893733  1.618979e+09  25.906476\n",
              "6  30000  30000  30000   16  ...   78.570949    0.893633  1.618980e+09  21.026869\n",
              "7  30000  30000  30000   64  ...  201.544638    0.894200  1.618982e+09  20.734107\n",
              "8  30000  30000  30000   32  ...  103.367213    0.893500  1.618983e+09  20.529149\n",
              "\n",
              "[9 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DHTu9LrBLAd",
        "outputId": "a1a41259-c915-475f-aaab-ec3f8d069cba"
      },
      "source": [
        "time_model(number_exp=10, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 1.18848 | train_logloss: 14.69675| train_accuracy: 0.5055  | valid_logloss: 14.76413| valid_accuracy: 0.50353 |  0:00:39s\n",
            "epoch 1  | loss: 0.5972  | train_logloss: 13.91245| train_accuracy: 0.42417 | valid_logloss: 13.73418| valid_accuracy: 0.42817 |  0:01:17s\n",
            "epoch 2  | loss: 0.47271 | train_logloss: 10.07237| train_accuracy: 0.32757 | valid_logloss: 10.11241| valid_accuracy: 0.32747 |  0:01:55s\n",
            "epoch 3  | loss: 0.43034 | train_logloss: 3.41025 | train_accuracy: 0.44623 | valid_logloss: 3.40612 | valid_accuracy: 0.44213 |  0:02:34s\n",
            "epoch 4  | loss: 0.40223 | train_logloss: 5.54895 | train_accuracy: 0.34387 | valid_logloss: 5.55962 | valid_accuracy: 0.345   |  0:03:13s\n",
            "epoch 5  | loss: 0.39747 | train_logloss: 5.99466 | train_accuracy: 0.3342  | valid_logloss: 6.0253  | valid_accuracy: 0.33417 |  0:03:54s\n",
            "epoch 6  | loss: 0.38004 | train_logloss: 2.504   | train_accuracy: 0.33583 | valid_logloss: 2.50736 | valid_accuracy: 0.33653 |  0:04:35s\n",
            "epoch 7  | loss: 0.3841  | train_logloss: 2.19926 | train_accuracy: 0.47343 | valid_logloss: 2.21322 | valid_accuracy: 0.4734  |  0:05:17s\n",
            "epoch 8  | loss: 0.37357 | train_logloss: 1.61197 | train_accuracy: 0.4963  | valid_logloss: 1.62324 | valid_accuracy: 0.49573 |  0:05:58s\n",
            "epoch 9  | loss: 0.37054 | train_logloss: 1.12035 | train_accuracy: 0.52463 | valid_logloss: 1.1209  | valid_accuracy: 0.52047 |  0:06:40s\n",
            "epoch 10 | loss: 0.36028 | train_logloss: 1.16751 | train_accuracy: 0.55457 | valid_logloss: 1.17723 | valid_accuracy: 0.54947 |  0:07:21s\n",
            "epoch 11 | loss: 0.35392 | train_logloss: 1.00124 | train_accuracy: 0.64053 | valid_logloss: 1.00616 | valid_accuracy: 0.63393 |  0:08:04s\n",
            "epoch 12 | loss: 0.34906 | train_logloss: 0.69081 | train_accuracy: 0.7327  | valid_logloss: 0.69699 | valid_accuracy: 0.73157 |  0:08:45s\n",
            "epoch 13 | loss: 0.34624 | train_logloss: 0.65153 | train_accuracy: 0.7493  | valid_logloss: 0.65988 | valid_accuracy: 0.74433 |  0:09:26s\n",
            "epoch 14 | loss: 0.33916 | train_logloss: 0.57293 | train_accuracy: 0.7674  | valid_logloss: 0.58581 | valid_accuracy: 0.7595  |  0:10:07s\n",
            "epoch 15 | loss: 0.34054 | train_logloss: 0.58471 | train_accuracy: 0.76983 | valid_logloss: 0.59847 | valid_accuracy: 0.76343 |  0:10:48s\n",
            "epoch 16 | loss: 0.37847 | train_logloss: 0.50702 | train_accuracy: 0.81387 | valid_logloss: 0.5209  | valid_accuracy: 0.80793 |  0:11:29s\n",
            "epoch 17 | loss: 0.37322 | train_logloss: 0.4787  | train_accuracy: 0.81603 | valid_logloss: 0.49114 | valid_accuracy: 0.81137 |  0:12:15s\n",
            "epoch 18 | loss: 0.36506 | train_logloss: 0.44973 | train_accuracy: 0.83713 | valid_logloss: 0.45877 | valid_accuracy: 0.8367  |  0:12:54s\n",
            "epoch 19 | loss: 0.35493 | train_logloss: 0.46936 | train_accuracy: 0.8365  | valid_logloss: 0.48658 | valid_accuracy: 0.8318  |  0:13:32s\n",
            "epoch 20 | loss: 0.34828 | train_logloss: 0.38865 | train_accuracy: 0.85857 | valid_logloss: 0.40055 | valid_accuracy: 0.85427 |  0:14:11s\n",
            "epoch 21 | loss: 0.34629 | train_logloss: 0.36025 | train_accuracy: 0.8659  | valid_logloss: 0.37548 | valid_accuracy: 0.8593  |  0:14:50s\n",
            "epoch 22 | loss: 0.33281 | train_logloss: 0.33727 | train_accuracy: 0.8781  | valid_logloss: 0.34722 | valid_accuracy: 0.8721  |  0:15:28s\n",
            "epoch 23 | loss: 0.32647 | train_logloss: 0.33035 | train_accuracy: 0.87777 | valid_logloss: 0.3451  | valid_accuracy: 0.87363 |  0:16:08s\n",
            "epoch 24 | loss: 0.31756 | train_logloss: 0.32627 | train_accuracy: 0.87243 | valid_logloss: 0.34341 | valid_accuracy: 0.8678  |  0:16:48s\n",
            "epoch 25 | loss: 0.31869 | train_logloss: 0.3201  | train_accuracy: 0.88097 | valid_logloss: 0.33962 | valid_accuracy: 0.87277 |  0:17:26s\n",
            "epoch 26 | loss: 0.31739 | train_logloss: 0.32806 | train_accuracy: 0.8766  | valid_logloss: 0.34428 | valid_accuracy: 0.86973 |  0:18:05s\n",
            "epoch 27 | loss: 0.31495 | train_logloss: 0.32093 | train_accuracy: 0.87853 | valid_logloss: 0.33729 | valid_accuracy: 0.8726  |  0:18:43s\n",
            "epoch 28 | loss: 0.31777 | train_logloss: 0.32522 | train_accuracy: 0.8805  | valid_logloss: 0.33964 | valid_accuracy: 0.875   |  0:19:22s\n",
            "epoch 29 | loss: 0.31363 | train_logloss: 0.30979 | train_accuracy: 0.8834  | valid_logloss: 0.32819 | valid_accuracy: 0.87683 |  0:20:01s\n",
            "epoch 30 | loss: 0.30924 | train_logloss: 0.30468 | train_accuracy: 0.8877  | valid_logloss: 0.32058 | valid_accuracy: 0.88033 |  0:20:40s\n",
            "epoch 31 | loss: 0.31104 | train_logloss: 0.30896 | train_accuracy: 0.88597 | valid_logloss: 0.3271  | valid_accuracy: 0.88077 |  0:21:19s\n",
            "epoch 32 | loss: 0.31416 | train_logloss: 0.30449 | train_accuracy: 0.88753 | valid_logloss: 0.31973 | valid_accuracy: 0.88193 |  0:21:58s\n",
            "epoch 33 | loss: 0.31299 | train_logloss: 0.31252 | train_accuracy: 0.8818  | valid_logloss: 0.32812 | valid_accuracy: 0.87613 |  0:22:36s\n",
            "epoch 34 | loss: 0.30721 | train_logloss: 0.3006  | train_accuracy: 0.88753 | valid_logloss: 0.31739 | valid_accuracy: 0.88073 |  0:23:15s\n",
            "epoch 35 | loss: 0.30658 | train_logloss: 0.30001 | train_accuracy: 0.88603 | valid_logloss: 0.31743 | valid_accuracy: 0.8789  |  0:23:54s\n",
            "epoch 36 | loss: 0.30123 | train_logloss: 0.29917 | train_accuracy: 0.88797 | valid_logloss: 0.31682 | valid_accuracy: 0.8809  |  0:24:33s\n",
            "epoch 37 | loss: 0.30459 | train_logloss: 0.29591 | train_accuracy: 0.88893 | valid_logloss: 0.31224 | valid_accuracy: 0.88417 |  0:25:12s\n",
            "epoch 38 | loss: 0.30301 | train_logloss: 0.29604 | train_accuracy: 0.88743 | valid_logloss: 0.3139  | valid_accuracy: 0.8821  |  0:25:51s\n",
            "epoch 39 | loss: 0.2988  | train_logloss: 0.28966 | train_accuracy: 0.88993 | valid_logloss: 0.30855 | valid_accuracy: 0.8836  |  0:26:30s\n",
            "epoch 40 | loss: 0.29122 | train_logloss: 0.29235 | train_accuracy: 0.89103 | valid_logloss: 0.31277 | valid_accuracy: 0.88387 |  0:27:09s\n",
            "epoch 41 | loss: 0.29412 | train_logloss: 0.28553 | train_accuracy: 0.8922  | valid_logloss: 0.3066  | valid_accuracy: 0.88603 |  0:27:47s\n",
            "epoch 42 | loss: 0.29287 | train_logloss: 0.28815 | train_accuracy: 0.8916  | valid_logloss: 0.30918 | valid_accuracy: 0.8852  |  0:28:26s\n",
            "epoch 43 | loss: 0.2899  | train_logloss: 0.28179 | train_accuracy: 0.89267 | valid_logloss: 0.30646 | valid_accuracy: 0.88513 |  0:29:04s\n",
            "epoch 44 | loss: 0.28781 | train_logloss: 0.28305 | train_accuracy: 0.8922  | valid_logloss: 0.30715 | valid_accuracy: 0.88467 |  0:29:42s\n",
            "epoch 45 | loss: 0.28703 | train_logloss: 0.27827 | train_accuracy: 0.8936  | valid_logloss: 0.30305 | valid_accuracy: 0.8864  |  0:30:21s\n",
            "epoch 46 | loss: 0.28336 | train_logloss: 0.27417 | train_accuracy: 0.8966  | valid_logloss: 0.30032 | valid_accuracy: 0.88843 |  0:30:59s\n",
            "epoch 47 | loss: 0.28307 | train_logloss: 0.27867 | train_accuracy: 0.8941  | valid_logloss: 0.30688 | valid_accuracy: 0.88663 |  0:31:37s\n",
            "epoch 48 | loss: 0.28353 | train_logloss: 0.27481 | train_accuracy: 0.8957  | valid_logloss: 0.30074 | valid_accuracy: 0.88717 |  0:32:15s\n",
            "epoch 49 | loss: 0.28414 | train_logloss: 0.27195 | train_accuracy: 0.89793 | valid_logloss: 0.30006 | valid_accuracy: 0.88763 |  0:32:52s\n",
            "epoch 50 | loss: 0.28594 | train_logloss: 0.2848  | train_accuracy: 0.8925  | valid_logloss: 0.30948 | valid_accuracy: 0.8856  |  0:33:30s\n",
            "epoch 51 | loss: 0.2866  | train_logloss: 0.27975 | train_accuracy: 0.8941  | valid_logloss: 0.30908 | valid_accuracy: 0.88497 |  0:34:08s\n",
            "epoch 52 | loss: 0.28568 | train_logloss: 0.27502 | train_accuracy: 0.89647 | valid_logloss: 0.30231 | valid_accuracy: 0.88993 |  0:34:45s\n",
            "epoch 53 | loss: 0.28185 | train_logloss: 0.27173 | train_accuracy: 0.89573 | valid_logloss: 0.29824 | valid_accuracy: 0.88843 |  0:35:22s\n",
            "epoch 54 | loss: 0.27814 | train_logloss: 0.27511 | train_accuracy: 0.89637 | valid_logloss: 0.30578 | valid_accuracy: 0.887   |  0:36:00s\n",
            "epoch 55 | loss: 0.27645 | train_logloss: 0.2714  | train_accuracy: 0.89533 | valid_logloss: 0.3012  | valid_accuracy: 0.88593 |  0:36:45s\n",
            "epoch 56 | loss: 0.27654 | train_logloss: 0.26922 | train_accuracy: 0.8976  | valid_logloss: 0.29972 | valid_accuracy: 0.8881  |  0:37:23s\n",
            "epoch 57 | loss: 0.27513 | train_logloss: 0.27088 | train_accuracy: 0.89653 | valid_logloss: 0.30189 | valid_accuracy: 0.88777 |  0:38:00s\n",
            "epoch 58 | loss: 0.27474 | train_logloss: 0.26932 | train_accuracy: 0.89717 | valid_logloss: 0.3     | valid_accuracy: 0.8873  |  0:38:37s\n",
            "epoch 59 | loss: 0.27293 | train_logloss: 0.26801 | train_accuracy: 0.89737 | valid_logloss: 0.30071 | valid_accuracy: 0.8878  |  0:39:14s\n",
            "epoch 60 | loss: 0.28377 | train_logloss: 0.28921 | train_accuracy: 0.8918  | valid_logloss: 0.31371 | valid_accuracy: 0.88493 |  0:39:51s\n",
            "epoch 61 | loss: 0.29594 | train_logloss: 0.2923  | train_accuracy: 0.8929  | valid_logloss: 0.31467 | valid_accuracy: 0.8851  |  0:40:28s\n",
            "epoch 62 | loss: 0.29434 | train_logloss: 0.28797 | train_accuracy: 0.89083 | valid_logloss: 0.31271 | valid_accuracy: 0.88233 |  0:41:06s\n",
            "epoch 63 | loss: 0.28639 | train_logloss: 0.27875 | train_accuracy: 0.89597 | valid_logloss: 0.30137 | valid_accuracy: 0.88803 |  0:41:44s\n",
            "epoch 64 | loss: 0.28053 | train_logloss: 0.27286 | train_accuracy: 0.89667 | valid_logloss: 0.2994  | valid_accuracy: 0.887   |  0:42:22s\n",
            "epoch 65 | loss: 0.27813 | train_logloss: 0.28596 | train_accuracy: 0.88833 | valid_logloss: 0.31254 | valid_accuracy: 0.88097 |  0:43:00s\n",
            "epoch 66 | loss: 0.2772  | train_logloss: 0.26861 | train_accuracy: 0.89673 | valid_logloss: 0.29702 | valid_accuracy: 0.8882  |  0:43:38s\n",
            "epoch 67 | loss: 0.28002 | train_logloss: 0.33389 | train_accuracy: 0.87447 | valid_logloss: 0.35244 | valid_accuracy: 0.8683  |  0:44:15s\n",
            "epoch 68 | loss: 0.2981  | train_logloss: 0.28813 | train_accuracy: 0.8915  | valid_logloss: 0.31511 | valid_accuracy: 0.88223 |  0:44:53s\n",
            "epoch 69 | loss: 0.30038 | train_logloss: 0.28797 | train_accuracy: 0.89033 | valid_logloss: 0.30995 | valid_accuracy: 0.88417 |  0:45:30s\n",
            "epoch 70 | loss: 0.29173 | train_logloss: 0.28019 | train_accuracy: 0.89473 | valid_logloss: 0.30168 | valid_accuracy: 0.88757 |  0:46:07s\n",
            "epoch 71 | loss: 0.29145 | train_logloss: 0.28741 | train_accuracy: 0.8922  | valid_logloss: 0.30974 | valid_accuracy: 0.88403 |  0:46:44s\n",
            "epoch 72 | loss: 0.28458 | train_logloss: 0.27402 | train_accuracy: 0.8962  | valid_logloss: 0.29733 | valid_accuracy: 0.8877  |  0:47:21s\n",
            "epoch 73 | loss: 0.28106 | train_logloss: 0.27097 | train_accuracy: 0.8974  | valid_logloss: 0.29752 | valid_accuracy: 0.88933 |  0:47:58s\n",
            "epoch 74 | loss: 0.28131 | train_logloss: 0.28051 | train_accuracy: 0.89317 | valid_logloss: 0.30312 | valid_accuracy: 0.88603 |  0:48:36s\n",
            "epoch 75 | loss: 0.28575 | train_logloss: 0.27209 | train_accuracy: 0.8968  | valid_logloss: 0.29887 | valid_accuracy: 0.8887  |  0:49:14s\n",
            "epoch 76 | loss: 0.27884 | train_logloss: 0.27236 | train_accuracy: 0.89603 | valid_logloss: 0.30089 | valid_accuracy: 0.88827 |  0:49:51s\n",
            "epoch 77 | loss: 0.2755  | train_logloss: 0.26454 | train_accuracy: 0.89913 | valid_logloss: 0.29201 | valid_accuracy: 0.89063 |  0:50:29s\n",
            "epoch 78 | loss: 0.27242 | train_logloss: 0.26801 | train_accuracy: 0.89773 | valid_logloss: 0.29665 | valid_accuracy: 0.8891  |  0:51:06s\n",
            "epoch 79 | loss: 0.26982 | train_logloss: 0.26226 | train_accuracy: 0.89777 | valid_logloss: 0.29337 | valid_accuracy: 0.88933 |  0:51:43s\n",
            "epoch 80 | loss: 0.26838 | train_logloss: 0.25805 | train_accuracy: 0.89947 | valid_logloss: 0.29037 | valid_accuracy: 0.89117 |  0:52:21s\n",
            "epoch 81 | loss: 0.26494 | train_logloss: 0.25623 | train_accuracy: 0.89967 | valid_logloss: 0.29106 | valid_accuracy: 0.89007 |  0:52:59s\n",
            "epoch 82 | loss: 0.26461 | train_logloss: 0.25742 | train_accuracy: 0.9026  | valid_logloss: 0.29411 | valid_accuracy: 0.89    |  0:53:37s\n",
            "epoch 83 | loss: 0.2636  | train_logloss: 0.25652 | train_accuracy: 0.90143 | valid_logloss: 0.29376 | valid_accuracy: 0.88937 |  0:54:15s\n",
            "epoch 84 | loss: 0.26417 | train_logloss: 0.25652 | train_accuracy: 0.90063 | valid_logloss: 0.2922  | valid_accuracy: 0.89203 |  0:54:52s\n",
            "epoch 85 | loss: 0.265   | train_logloss: 0.25834 | train_accuracy: 0.90113 | valid_logloss: 0.29466 | valid_accuracy: 0.89047 |  0:55:29s\n",
            "epoch 86 | loss: 0.26447 | train_logloss: 0.26224 | train_accuracy: 0.89713 | valid_logloss: 0.29474 | valid_accuracy: 0.8888  |  0:56:07s\n",
            "epoch 87 | loss: 0.26341 | train_logloss: 0.25725 | train_accuracy: 0.90117 | valid_logloss: 0.29431 | valid_accuracy: 0.8908  |  0:56:46s\n",
            "epoch 88 | loss: 0.26391 | train_logloss: 0.25607 | train_accuracy: 0.90047 | valid_logloss: 0.29319 | valid_accuracy: 0.89    |  0:57:24s\n",
            "epoch 89 | loss: 0.26153 | train_logloss: 0.25089 | train_accuracy: 0.90263 | valid_logloss: 0.29371 | valid_accuracy: 0.89137 |  0:58:02s\n",
            "epoch 90 | loss: 0.26698 | train_logloss: 0.26116 | train_accuracy: 0.9009  | valid_logloss: 0.30055 | valid_accuracy: 0.8901  |  0:58:39s\n",
            "epoch 91 | loss: 0.26855 | train_logloss: 0.26212 | train_accuracy: 0.90077 | valid_logloss: 0.30149 | valid_accuracy: 0.8882  |  0:59:17s\n",
            "epoch 92 | loss: 0.2657  | train_logloss: 0.26095 | train_accuracy: 0.90133 | valid_logloss: 0.30248 | valid_accuracy: 0.8892  |  0:59:56s\n",
            "epoch 93 | loss: 0.26492 | train_logloss: 0.25709 | train_accuracy: 0.90247 | valid_logloss: 0.30087 | valid_accuracy: 0.89033 |  1:00:34s\n",
            "epoch 94 | loss: 0.26155 | train_logloss: 0.25203 | train_accuracy: 0.90233 | valid_logloss: 0.29494 | valid_accuracy: 0.89123 |  1:01:12s\n",
            "epoch 95 | loss: 0.26212 | train_logloss: 0.25386 | train_accuracy: 0.902   | valid_logloss: 0.2952  | valid_accuracy: 0.89233 |  1:01:50s\n",
            "epoch 96 | loss: 0.25868 | train_logloss: 0.24681 | train_accuracy: 0.90453 | valid_logloss: 0.29164 | valid_accuracy: 0.89153 |  1:02:28s\n",
            "epoch 97 | loss: 0.25448 | train_logloss: 0.24878 | train_accuracy: 0.90337 | valid_logloss: 0.29601 | valid_accuracy: 0.89107 |  1:03:06s\n",
            "epoch 98 | loss: 0.25248 | train_logloss: 0.24824 | train_accuracy: 0.90327 | valid_logloss: 0.29709 | valid_accuracy: 0.89103 |  1:03:43s\n",
            "epoch 99 | loss: 0.2541  | train_logloss: 0.24848 | train_accuracy: 0.9043  | valid_logloss: 0.2947  | valid_accuracy: 0.89163 |  1:04:22s\n",
            "epoch 100| loss: 0.25229 | train_logloss: 0.24699 | train_accuracy: 0.90413 | valid_logloss: 0.29626 | valid_accuracy: 0.89107 |  1:05:00s\n",
            "epoch 101| loss: 0.25475 | train_logloss: 0.24907 | train_accuracy: 0.9037  | valid_logloss: 0.29757 | valid_accuracy: 0.8894  |  1:05:38s\n",
            "epoch 102| loss: 0.25497 | train_logloss: 0.24446 | train_accuracy: 0.9055  | valid_logloss: 0.29749 | valid_accuracy: 0.89153 |  1:06:16s\n",
            "epoch 103| loss: 0.25169 | train_logloss: 0.24682 | train_accuracy: 0.90257 | valid_logloss: 0.30162 | valid_accuracy: 0.88907 |  1:06:53s\n",
            "epoch 104| loss: 0.25113 | train_logloss: 0.2403  | train_accuracy: 0.90647 | valid_logloss: 0.29683 | valid_accuracy: 0.8928  |  1:07:32s\n",
            "epoch 105| loss: 0.25362 | train_logloss: 0.24761 | train_accuracy: 0.90313 | valid_logloss: 0.3013  | valid_accuracy: 0.8924  |  1:08:10s\n",
            "epoch 106| loss: 0.28007 | train_logloss: 0.2885  | train_accuracy: 0.8919  | valid_logloss: 0.32193 | valid_accuracy: 0.8825  |  1:08:47s\n",
            "epoch 107| loss: 0.2791  | train_logloss: 0.26203 | train_accuracy: 0.89887 | valid_logloss: 0.30006 | valid_accuracy: 0.88877 |  1:09:24s\n",
            "epoch 108| loss: 0.26364 | train_logloss: 0.25812 | train_accuracy: 0.90047 | valid_logloss: 0.2981  | valid_accuracy: 0.88923 |  1:10:01s\n",
            "epoch 109| loss: 0.26164 | train_logloss: 0.24947 | train_accuracy: 0.90307 | valid_logloss: 0.29272 | valid_accuracy: 0.891   |  1:10:38s\n",
            "epoch 110| loss: 0.25782 | train_logloss: 0.24895 | train_accuracy: 0.90403 | valid_logloss: 0.29465 | valid_accuracy: 0.8916  |  1:11:15s\n",
            "epoch 111| loss: 0.25935 | train_logloss: 0.25247 | train_accuracy: 0.90003 | valid_logloss: 0.29598 | valid_accuracy: 0.88963 |  1:11:53s\n",
            "epoch 112| loss: 0.25761 | train_logloss: 0.24513 | train_accuracy: 0.90527 | valid_logloss: 0.29817 | valid_accuracy: 0.89163 |  1:12:30s\n",
            "epoch 113| loss: 0.25526 | train_logloss: 0.24326 | train_accuracy: 0.90593 | valid_logloss: 0.29319 | valid_accuracy: 0.89437 |  1:13:10s\n",
            "epoch 114| loss: 0.25062 | train_logloss: 0.24232 | train_accuracy: 0.9062  | valid_logloss: 0.29537 | valid_accuracy: 0.8918  |  1:13:47s\n",
            "epoch 115| loss: 0.24858 | train_logloss: 0.23959 | train_accuracy: 0.90667 | valid_logloss: 0.29458 | valid_accuracy: 0.8933  |  1:14:24s\n",
            "epoch 116| loss: 0.24973 | train_logloss: 0.24081 | train_accuracy: 0.90813 | valid_logloss: 0.29856 | valid_accuracy: 0.89137 |  1:15:01s\n",
            "epoch 117| loss: 0.24902 | train_logloss: 0.23849 | train_accuracy: 0.90757 | valid_logloss: 0.29832 | valid_accuracy: 0.89063 |  1:15:38s\n",
            "epoch 118| loss: 0.25025 | train_logloss: 0.23815 | train_accuracy: 0.90623 | valid_logloss: 0.2965  | valid_accuracy: 0.89157 |  1:16:15s\n",
            "epoch 119| loss: 0.24621 | train_logloss: 0.23966 | train_accuracy: 0.90633 | valid_logloss: 0.30118 | valid_accuracy: 0.8913  |  1:16:52s\n",
            "epoch 120| loss: 0.24633 | train_logloss: 0.23439 | train_accuracy: 0.90857 | valid_logloss: 0.29892 | valid_accuracy: 0.89197 |  1:17:29s\n",
            "epoch 121| loss: 0.24516 | train_logloss: 0.23211 | train_accuracy: 0.9101  | valid_logloss: 0.29394 | valid_accuracy: 0.89273 |  1:18:06s\n",
            "epoch 122| loss: 0.24088 | train_logloss: 0.2307  | train_accuracy: 0.90953 | valid_logloss: 0.30468 | valid_accuracy: 0.89187 |  1:18:42s\n",
            "epoch 123| loss: 0.24102 | train_logloss: 0.2335  | train_accuracy: 0.90963 | valid_logloss: 0.30649 | valid_accuracy: 0.88943 |  1:19:20s\n",
            "epoch 124| loss: 0.24001 | train_logloss: 0.22575 | train_accuracy: 0.91143 | valid_logloss: 0.29899 | valid_accuracy: 0.89313 |  1:19:57s\n",
            "epoch 125| loss: 0.24001 | train_logloss: 0.23721 | train_accuracy: 0.9065  | valid_logloss: 0.30546 | valid_accuracy: 0.89057 |  1:20:34s\n",
            "epoch 126| loss: 0.24232 | train_logloss: 0.231   | train_accuracy: 0.90933 | valid_logloss: 0.30102 | valid_accuracy: 0.89153 |  1:21:11s\n",
            "epoch 127| loss: 0.23586 | train_logloss: 0.23132 | train_accuracy: 0.90917 | valid_logloss: 0.30798 | valid_accuracy: 0.8897  |  1:21:49s\n",
            "epoch 128| loss: 0.23881 | train_logloss: 0.22796 | train_accuracy: 0.9104  | valid_logloss: 0.30271 | valid_accuracy: 0.89177 |  1:22:26s\n",
            "epoch 129| loss: 0.23923 | train_logloss: 0.23207 | train_accuracy: 0.90963 | valid_logloss: 0.30598 | valid_accuracy: 0.88977 |  1:23:04s\n",
            "epoch 130| loss: 0.23807 | train_logloss: 0.22593 | train_accuracy: 0.91153 | valid_logloss: 0.30392 | valid_accuracy: 0.88897 |  1:23:42s\n",
            "epoch 131| loss: 0.2411  | train_logloss: 0.24434 | train_accuracy: 0.9064  | valid_logloss: 0.3174  | valid_accuracy: 0.88943 |  1:24:20s\n",
            "epoch 132| loss: 0.24874 | train_logloss: 0.2357  | train_accuracy: 0.90787 | valid_logloss: 0.30974 | valid_accuracy: 0.88893 |  1:24:58s\n",
            "epoch 133| loss: 0.23932 | train_logloss: 0.22918 | train_accuracy: 0.91093 | valid_logloss: 0.30731 | valid_accuracy: 0.89203 |  1:25:37s\n",
            "epoch 134| loss: 0.23664 | train_logloss: 0.22861 | train_accuracy: 0.90973 | valid_logloss: 0.30727 | valid_accuracy: 0.89137 |  1:26:14s\n",
            "epoch 135| loss: 0.23455 | train_logloss: 0.22941 | train_accuracy: 0.911   | valid_logloss: 0.31182 | valid_accuracy: 0.8889  |  1:26:52s\n",
            "epoch 136| loss: 0.23379 | train_logloss: 0.21855 | train_accuracy: 0.914   | valid_logloss: 0.30969 | valid_accuracy: 0.89033 |  1:27:30s\n",
            "epoch 137| loss: 0.23034 | train_logloss: 0.22188 | train_accuracy: 0.91207 | valid_logloss: 0.3089  | valid_accuracy: 0.89253 |  1:28:08s\n",
            "epoch 138| loss: 0.23209 | train_logloss: 0.21731 | train_accuracy: 0.91483 | valid_logloss: 0.30977 | valid_accuracy: 0.89117 |  1:28:46s\n",
            "epoch 139| loss: 0.2307  | train_logloss: 0.21855 | train_accuracy: 0.91313 | valid_logloss: 0.31771 | valid_accuracy: 0.8876  |  1:29:24s\n",
            "epoch 140| loss: 0.24132 | train_logloss: 0.23634 | train_accuracy: 0.909   | valid_logloss: 0.31175 | valid_accuracy: 0.88927 |  1:30:02s\n",
            "epoch 141| loss: 0.23904 | train_logloss: 0.22914 | train_accuracy: 0.90897 | valid_logloss: 0.31292 | valid_accuracy: 0.89047 |  1:30:40s\n",
            "epoch 142| loss: 0.23442 | train_logloss: 0.2273  | train_accuracy: 0.91057 | valid_logloss: 0.31581 | valid_accuracy: 0.88927 |  1:31:18s\n",
            "epoch 143| loss: 0.23639 | train_logloss: 0.22186 | train_accuracy: 0.91333 | valid_logloss: 0.3056  | valid_accuracy: 0.89327 |  1:31:57s\n",
            "\n",
            "Early stopping occurred at epoch 143 with best_epoch = 113 and best_valid_accuracy = 0.89437\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[419031  12610  48541]\n",
            " [ 16678 950108  33214]\n",
            " [ 33456  28331 370800]]\n",
            "Testing Score:  0.8943666666666666\n",
            "100%|██████████| 10/10 [01:49<00:00, 10.93s/it, best loss: -0.8921333333333333]\n",
            "({'colsample_bytree': 0.3146423528745642, 'min_child_samples': 20, 'min_child_weight': 0.8145549011405879, 'num_leaves': 55}, <hyperopt.base.Trials object at 0x7fcddc031d10>)\n",
            "6.097266674041748\n",
            "Confusion Matrix: \n",
            " [[419779  12380  48023]\n",
            " [ 13805 954348  31847]\n",
            " [ 33646  27890 371051]]\n",
            "Testing Score:  0.8949333333333334\n",
            "{'Rows': 30000, 'Nd': 128, 'Na': 128, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 5, 'γ': 1.7, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 30000, 'test': 30000, 'time_learn_tn': 5522.418471336365, 'time_tn': 288.12083196640015, 'accuracy_tn': 0.8943666666666666, 'time_learn_gb': 1618989390.4620116, 'time_gb': 22.54999566078186, 'accuracy_gb': 0.8949333333333334}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "-wP-3LQlWeXC",
        "outputId": "aef588c2-201c-46fc-94f9-9a3fa06bea1d"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894833</td>\n",
              "      <td>814.815927</td>\n",
              "      <td>103.367213</td>\n",
              "      <td>0.893500</td>\n",
              "      <td>1.618983e+09</td>\n",
              "      <td>20.529149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894367</td>\n",
              "      <td>5522.418471</td>\n",
              "      <td>288.120832</td>\n",
              "      <td>0.894933</td>\n",
              "      <td>1.618989e+09</td>\n",
              "      <td>22.549996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Rows  train   test   Nd  ...     time_tn accuracy_gb time_learn_gb    time_gb\n",
              "0   9000   9000   9000    8  ...   70.837914    0.886444  1.618973e+09  16.887252\n",
              "1   9000   9000   9000   16  ...   92.415544    0.887333  1.618974e+09  13.399141\n",
              "2   9000   9000   9000   64  ...  205.964584    0.886222  1.618975e+09  18.229882\n",
              "3   9000   9000   9000   32  ...  124.640234    0.888667  1.618976e+09  19.224410\n",
              "4   9000   9000   9000  128  ...  324.133266    0.886000  1.618978e+09  15.819996\n",
              "5  30000  30000  30000    8  ...   67.806992    0.893733  1.618979e+09  25.906476\n",
              "6  30000  30000  30000   16  ...   78.570949    0.893633  1.618980e+09  21.026869\n",
              "7  30000  30000  30000   64  ...  201.544638    0.894200  1.618982e+09  20.734107\n",
              "8  30000  30000  30000   32  ...  103.367213    0.893500  1.618983e+09  20.529149\n",
              "9  30000  30000  30000  128  ...  288.120832    0.894933  1.618989e+09  22.549996\n",
              "\n",
              "[10 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QGMDAZ-rEW"
      },
      "source": [
        "##300000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4rLckKpRWYR",
        "outputId": "90d4dae8-f8a0-4620-fa25-5b747e3808f3"
      },
      "source": [
        "time_model(number_exp=11, \n",
        "     Rows=300000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.38118 | train_logloss: 1.77836 | train_accuracy: 0.27422 | valid_logloss: 1.77439 | valid_accuracy: 0.27495 |  0:00:41s\n",
            "epoch 1  | loss: 0.3087  | train_logloss: 0.42924 | train_accuracy: 0.84535 | valid_logloss: 0.42863 | valid_accuracy: 0.84601 |  0:01:24s\n",
            "epoch 2  | loss: 0.29966 | train_logloss: 0.29444 | train_accuracy: 0.88895 | valid_logloss: 0.29541 | valid_accuracy: 0.88872 |  0:02:07s\n",
            "epoch 3  | loss: 0.2945  | train_logloss: 0.28734 | train_accuracy: 0.8914  | valid_logloss: 0.28981 | valid_accuracy: 0.89154 |  0:02:50s\n",
            "epoch 4  | loss: 0.28859 | train_logloss: 0.28017 | train_accuracy: 0.89389 | valid_logloss: 0.28258 | valid_accuracy: 0.89364 |  0:03:32s\n",
            "epoch 5  | loss: 0.28419 | train_logloss: 0.27859 | train_accuracy: 0.89364 | valid_logloss: 0.2808  | valid_accuracy: 0.89308 |  0:04:14s\n",
            "epoch 6  | loss: 0.28185 | train_logloss: 0.2734  | train_accuracy: 0.89609 | valid_logloss: 0.27636 | valid_accuracy: 0.89558 |  0:04:57s\n",
            "epoch 7  | loss: 0.27935 | train_logloss: 0.27889 | train_accuracy: 0.89338 | valid_logloss: 0.28156 | valid_accuracy: 0.89354 |  0:05:48s\n",
            "epoch 8  | loss: 0.2776  | train_logloss: 0.27301 | train_accuracy: 0.89577 | valid_logloss: 0.27604 | valid_accuracy: 0.89515 |  0:06:32s\n",
            "epoch 9  | loss: 0.27627 | train_logloss: 0.27091 | train_accuracy: 0.89686 | valid_logloss: 0.2748  | valid_accuracy: 0.89595 |  0:07:14s\n",
            "epoch 10 | loss: 0.27522 | train_logloss: 0.26999 | train_accuracy: 0.89764 | valid_logloss: 0.27358 | valid_accuracy: 0.89686 |  0:07:57s\n",
            "epoch 11 | loss: 0.27349 | train_logloss: 0.26889 | train_accuracy: 0.89809 | valid_logloss: 0.27263 | valid_accuracy: 0.89742 |  0:08:40s\n",
            "epoch 12 | loss: 0.2735  | train_logloss: 0.2663  | train_accuracy: 0.89826 | valid_logloss: 0.27    | valid_accuracy: 0.89753 |  0:09:23s\n",
            "epoch 13 | loss: 0.27207 | train_logloss: 0.26804 | train_accuracy: 0.89822 | valid_logloss: 0.27248 | valid_accuracy: 0.89724 |  0:10:06s\n",
            "epoch 14 | loss: 0.27053 | train_logloss: 0.26914 | train_accuracy: 0.89759 | valid_logloss: 0.27418 | valid_accuracy: 0.8961  |  0:10:49s\n",
            "epoch 15 | loss: 0.27069 | train_logloss: 0.26696 | train_accuracy: 0.8984  | valid_logloss: 0.27216 | valid_accuracy: 0.89698 |  0:11:32s\n",
            "epoch 16 | loss: 0.26993 | train_logloss: 0.26783 | train_accuracy: 0.89776 | valid_logloss: 0.27201 | valid_accuracy: 0.89684 |  0:12:14s\n",
            "epoch 17 | loss: 0.26962 | train_logloss: 0.26554 | train_accuracy: 0.89863 | valid_logloss: 0.26919 | valid_accuracy: 0.89786 |  0:12:57s\n",
            "epoch 18 | loss: 0.26919 | train_logloss: 0.26658 | train_accuracy: 0.89834 | valid_logloss: 0.27127 | valid_accuracy: 0.89708 |  0:13:39s\n",
            "epoch 19 | loss: 0.26888 | train_logloss: 0.26626 | train_accuracy: 0.8987  | valid_logloss: 0.27102 | valid_accuracy: 0.89642 |  0:14:22s\n",
            "epoch 20 | loss: 0.26766 | train_logloss: 0.26484 | train_accuracy: 0.89977 | valid_logloss: 0.26931 | valid_accuracy: 0.89837 |  0:15:05s\n",
            "epoch 21 | loss: 0.26884 | train_logloss: 0.26616 | train_accuracy: 0.89876 | valid_logloss: 0.27106 | valid_accuracy: 0.8973  |  0:15:47s\n",
            "epoch 22 | loss: 0.26714 | train_logloss: 0.26187 | train_accuracy: 0.90033 | valid_logloss: 0.26719 | valid_accuracy: 0.89841 |  0:16:29s\n",
            "epoch 23 | loss: 0.26715 | train_logloss: 0.26657 | train_accuracy: 0.8998  | valid_logloss: 0.27126 | valid_accuracy: 0.89814 |  0:17:12s\n",
            "epoch 24 | loss: 0.26708 | train_logloss: 0.26364 | train_accuracy: 0.89974 | valid_logloss: 0.26859 | valid_accuracy: 0.89827 |  0:17:54s\n",
            "epoch 25 | loss: 0.26768 | train_logloss: 0.26326 | train_accuracy: 0.90002 | valid_logloss: 0.26801 | valid_accuracy: 0.89869 |  0:18:37s\n",
            "epoch 26 | loss: 0.26678 | train_logloss: 0.26351 | train_accuracy: 0.89852 | valid_logloss: 0.26835 | valid_accuracy: 0.89682 |  0:19:19s\n",
            "epoch 27 | loss: 0.26569 | train_logloss: 0.26105 | train_accuracy: 0.90114 | valid_logloss: 0.26597 | valid_accuracy: 0.89928 |  0:20:01s\n",
            "epoch 28 | loss: 0.26546 | train_logloss: 0.26409 | train_accuracy: 0.90027 | valid_logloss: 0.26914 | valid_accuracy: 0.899   |  0:20:44s\n",
            "epoch 29 | loss: 0.26528 | train_logloss: 0.2593  | train_accuracy: 0.90085 | valid_logloss: 0.26485 | valid_accuracy: 0.89952 |  0:21:26s\n",
            "epoch 30 | loss: 0.26441 | train_logloss: 0.2626  | train_accuracy: 0.90045 | valid_logloss: 0.26855 | valid_accuracy: 0.89874 |  0:22:08s\n",
            "epoch 31 | loss: 0.26494 | train_logloss: 0.25832 | train_accuracy: 0.90179 | valid_logloss: 0.26485 | valid_accuracy: 0.89987 |  0:22:51s\n",
            "epoch 32 | loss: 0.26389 | train_logloss: 0.26738 | train_accuracy: 0.89849 | valid_logloss: 0.27453 | valid_accuracy: 0.89653 |  0:23:33s\n",
            "epoch 33 | loss: 0.26476 | train_logloss: 0.26067 | train_accuracy: 0.90094 | valid_logloss: 0.26618 | valid_accuracy: 0.89927 |  0:24:16s\n",
            "epoch 34 | loss: 0.26404 | train_logloss: 0.26382 | train_accuracy: 0.89971 | valid_logloss: 0.26938 | valid_accuracy: 0.89815 |  0:24:58s\n",
            "epoch 35 | loss: 0.26524 | train_logloss: 0.2609  | train_accuracy: 0.90085 | valid_logloss: 0.26602 | valid_accuracy: 0.89875 |  0:25:41s\n",
            "epoch 36 | loss: 0.26507 | train_logloss: 0.25892 | train_accuracy: 0.90139 | valid_logloss: 0.26423 | valid_accuracy: 0.89937 |  0:26:23s\n",
            "epoch 37 | loss: 0.26483 | train_logloss: 0.25817 | train_accuracy: 0.90151 | valid_logloss: 0.26525 | valid_accuracy: 0.89944 |  0:27:05s\n",
            "epoch 38 | loss: 0.26357 | train_logloss: 0.2589  | train_accuracy: 0.90137 | valid_logloss: 0.26515 | valid_accuracy: 0.8991  |  0:27:48s\n",
            "epoch 39 | loss: 0.26328 | train_logloss: 0.25878 | train_accuracy: 0.90119 | valid_logloss: 0.26429 | valid_accuracy: 0.89981 |  0:28:30s\n",
            "epoch 40 | loss: 0.26276 | train_logloss: 0.25825 | train_accuracy: 0.90156 | valid_logloss: 0.26391 | valid_accuracy: 0.90006 |  0:29:12s\n",
            "epoch 41 | loss: 0.26304 | train_logloss: 0.25929 | train_accuracy: 0.90053 | valid_logloss: 0.26465 | valid_accuracy: 0.89897 |  0:29:54s\n",
            "epoch 42 | loss: 0.26252 | train_logloss: 0.25654 | train_accuracy: 0.90229 | valid_logloss: 0.2632  | valid_accuracy: 0.90017 |  0:30:37s\n",
            "epoch 43 | loss: 0.262   | train_logloss: 0.25844 | train_accuracy: 0.90154 | valid_logloss: 0.26522 | valid_accuracy: 0.89925 |  0:31:19s\n",
            "epoch 44 | loss: 0.26287 | train_logloss: 0.25957 | train_accuracy: 0.90129 | valid_logloss: 0.26706 | valid_accuracy: 0.8987  |  0:32:02s\n",
            "epoch 45 | loss: 0.26155 | train_logloss: 0.26001 | train_accuracy: 0.90053 | valid_logloss: 0.26587 | valid_accuracy: 0.89867 |  0:32:47s\n",
            "epoch 46 | loss: 0.2623  | train_logloss: 0.25603 | train_accuracy: 0.90263 | valid_logloss: 0.2628  | valid_accuracy: 0.90031 |  0:33:29s\n",
            "epoch 47 | loss: 0.26177 | train_logloss: 0.25651 | train_accuracy: 0.90228 | valid_logloss: 0.26333 | valid_accuracy: 0.89997 |  0:34:11s\n",
            "epoch 48 | loss: 0.2608  | train_logloss: 0.25599 | train_accuracy: 0.90223 | valid_logloss: 0.26334 | valid_accuracy: 0.90034 |  0:34:53s\n",
            "epoch 49 | loss: 0.26118 | train_logloss: 0.25815 | train_accuracy: 0.90191 | valid_logloss: 0.26559 | valid_accuracy: 0.89933 |  0:35:35s\n",
            "epoch 50 | loss: 0.2641  | train_logloss: 0.26143 | train_accuracy: 0.90022 | valid_logloss: 0.26719 | valid_accuracy: 0.89867 |  0:36:17s\n",
            "epoch 51 | loss: 0.26186 | train_logloss: 0.25688 | train_accuracy: 0.90261 | valid_logloss: 0.2642  | valid_accuracy: 0.90014 |  0:37:00s\n",
            "epoch 52 | loss: 0.26146 | train_logloss: 0.25731 | train_accuracy: 0.90195 | valid_logloss: 0.2644  | valid_accuracy: 0.89948 |  0:37:47s\n",
            "epoch 53 | loss: 0.26098 | train_logloss: 0.25789 | train_accuracy: 0.90119 | valid_logloss: 0.26521 | valid_accuracy: 0.89911 |  0:38:29s\n",
            "epoch 54 | loss: 0.26048 | train_logloss: 0.26292 | train_accuracy: 0.89978 | valid_logloss: 0.27121 | valid_accuracy: 0.8968  |  0:39:12s\n",
            "epoch 55 | loss: 0.26051 | train_logloss: 0.25878 | train_accuracy: 0.90133 | valid_logloss: 0.26661 | valid_accuracy: 0.89889 |  0:39:54s\n",
            "epoch 56 | loss: 0.25956 | train_logloss: 0.25778 | train_accuracy: 0.902   | valid_logloss: 0.2652  | valid_accuracy: 0.89954 |  0:40:36s\n",
            "epoch 57 | loss: 0.26051 | train_logloss: 0.2559  | train_accuracy: 0.90265 | valid_logloss: 0.26348 | valid_accuracy: 0.89972 |  0:41:18s\n",
            "epoch 58 | loss: 0.26029 | train_logloss: 0.257   | train_accuracy: 0.90217 | valid_logloss: 0.26401 | valid_accuracy: 0.90048 |  0:42:00s\n",
            "epoch 59 | loss: 0.25976 | train_logloss: 0.2547  | train_accuracy: 0.90285 | valid_logloss: 0.2622  | valid_accuracy: 0.90045 |  0:42:43s\n",
            "epoch 60 | loss: 0.2605  | train_logloss: 0.25601 | train_accuracy: 0.90223 | valid_logloss: 0.26329 | valid_accuracy: 0.89995 |  0:43:25s\n",
            "epoch 61 | loss: 0.25933 | train_logloss: 0.25609 | train_accuracy: 0.90227 | valid_logloss: 0.2654  | valid_accuracy: 0.89942 |  0:44:07s\n",
            "epoch 62 | loss: 0.25912 | train_logloss: 0.25509 | train_accuracy: 0.90274 | valid_logloss: 0.2642  | valid_accuracy: 0.90016 |  0:44:49s\n",
            "epoch 63 | loss: 0.26193 | train_logloss: 0.25571 | train_accuracy: 0.90253 | valid_logloss: 0.26415 | valid_accuracy: 0.9001  |  0:45:31s\n",
            "epoch 64 | loss: 0.25985 | train_logloss: 0.2578  | train_accuracy: 0.90116 | valid_logloss: 0.266   | valid_accuracy: 0.89879 |  0:46:13s\n",
            "epoch 65 | loss: 0.25922 | train_logloss: 0.25649 | train_accuracy: 0.90174 | valid_logloss: 0.26347 | valid_accuracy: 0.90002 |  0:46:56s\n",
            "epoch 66 | loss: 0.25971 | train_logloss: 0.25574 | train_accuracy: 0.90201 | valid_logloss: 0.26295 | valid_accuracy: 0.89979 |  0:47:38s\n",
            "epoch 67 | loss: 0.26004 | train_logloss: 0.25779 | train_accuracy: 0.9018  | valid_logloss: 0.26573 | valid_accuracy: 0.89909 |  0:48:20s\n",
            "epoch 68 | loss: 0.25907 | train_logloss: 0.25605 | train_accuracy: 0.90229 | valid_logloss: 0.26471 | valid_accuracy: 0.89951 |  0:49:02s\n",
            "epoch 69 | loss: 0.25948 | train_logloss: 0.26084 | train_accuracy: 0.90078 | valid_logloss: 0.2681  | valid_accuracy: 0.89852 |  0:49:44s\n",
            "epoch 70 | loss: 0.25893 | train_logloss: 0.25577 | train_accuracy: 0.90257 | valid_logloss: 0.26468 | valid_accuracy: 0.90045 |  0:50:27s\n",
            "epoch 71 | loss: 0.25842 | train_logloss: 0.25522 | train_accuracy: 0.903   | valid_logloss: 0.26359 | valid_accuracy: 0.90046 |  0:51:10s\n",
            "epoch 72 | loss: 0.25861 | train_logloss: 0.254   | train_accuracy: 0.90286 | valid_logloss: 0.26275 | valid_accuracy: 0.90043 |  0:51:53s\n",
            "epoch 73 | loss: 0.25891 | train_logloss: 0.25251 | train_accuracy: 0.90318 | valid_logloss: 0.26125 | valid_accuracy: 0.90074 |  0:52:37s\n",
            "epoch 74 | loss: 0.25777 | train_logloss: 0.25376 | train_accuracy: 0.9032  | valid_logloss: 0.26184 | valid_accuracy: 0.90104 |  0:53:21s\n",
            "epoch 75 | loss: 0.25824 | train_logloss: 0.25664 | train_accuracy: 0.90276 | valid_logloss: 0.26564 | valid_accuracy: 0.90003 |  0:54:04s\n",
            "epoch 76 | loss: 0.25797 | train_logloss: 0.25381 | train_accuracy: 0.90262 | valid_logloss: 0.26223 | valid_accuracy: 0.90056 |  0:54:47s\n",
            "epoch 77 | loss: 0.25792 | train_logloss: 0.25313 | train_accuracy: 0.90348 | valid_logloss: 0.2628  | valid_accuracy: 0.90101 |  0:55:30s\n",
            "epoch 78 | loss: 0.25817 | train_logloss: 0.25535 | train_accuracy: 0.90314 | valid_logloss: 0.26544 | valid_accuracy: 0.8998  |  0:56:12s\n",
            "epoch 79 | loss: 0.25792 | train_logloss: 0.25306 | train_accuracy: 0.90346 | valid_logloss: 0.26177 | valid_accuracy: 0.90103 |  0:56:54s\n",
            "epoch 80 | loss: 0.25782 | train_logloss: 0.25195 | train_accuracy: 0.90387 | valid_logloss: 0.26181 | valid_accuracy: 0.90076 |  0:57:36s\n",
            "epoch 81 | loss: 0.25767 | train_logloss: 0.2538  | train_accuracy: 0.90297 | valid_logloss: 0.26319 | valid_accuracy: 0.89982 |  0:58:19s\n",
            "epoch 82 | loss: 0.25806 | train_logloss: 0.25695 | train_accuracy: 0.90201 | valid_logloss: 0.26474 | valid_accuracy: 0.89985 |  0:59:02s\n",
            "epoch 83 | loss: 0.25754 | train_logloss: 0.25304 | train_accuracy: 0.90338 | valid_logloss: 0.26256 | valid_accuracy: 0.90071 |  0:59:44s\n",
            "epoch 84 | loss: 0.2574  | train_logloss: 0.25249 | train_accuracy: 0.90407 | valid_logloss: 0.26155 | valid_accuracy: 0.90152 |  1:00:26s\n",
            "epoch 85 | loss: 0.25739 | train_logloss: 0.25352 | train_accuracy: 0.90353 | valid_logloss: 0.26206 | valid_accuracy: 0.90101 |  1:01:09s\n",
            "epoch 86 | loss: 0.25691 | train_logloss: 0.25403 | train_accuracy: 0.90334 | valid_logloss: 0.26365 | valid_accuracy: 0.90033 |  1:01:51s\n",
            "epoch 87 | loss: 0.25704 | train_logloss: 0.25157 | train_accuracy: 0.90401 | valid_logloss: 0.26126 | valid_accuracy: 0.9012  |  1:02:34s\n",
            "epoch 88 | loss: 0.25656 | train_logloss: 0.25405 | train_accuracy: 0.90326 | valid_logloss: 0.26293 | valid_accuracy: 0.90042 |  1:03:17s\n",
            "epoch 89 | loss: 0.25644 | train_logloss: 0.25345 | train_accuracy: 0.90307 | valid_logloss: 0.26331 | valid_accuracy: 0.90056 |  1:04:01s\n",
            "epoch 90 | loss: 0.2568  | train_logloss: 0.25447 | train_accuracy: 0.90262 | valid_logloss: 0.26469 | valid_accuracy: 0.89944 |  1:04:44s\n",
            "epoch 91 | loss: 0.25691 | train_logloss: 0.25098 | train_accuracy: 0.904   | valid_logloss: 0.26081 | valid_accuracy: 0.90097 |  1:05:26s\n",
            "epoch 92 | loss: 0.25677 | train_logloss: 0.25143 | train_accuracy: 0.90401 | valid_logloss: 0.2614  | valid_accuracy: 0.90117 |  1:06:19s\n",
            "epoch 93 | loss: 0.25595 | train_logloss: 0.25129 | train_accuracy: 0.90391 | valid_logloss: 0.26166 | valid_accuracy: 0.90068 |  1:07:02s\n",
            "epoch 94 | loss: 0.2563  | train_logloss: 0.25306 | train_accuracy: 0.90319 | valid_logloss: 0.26312 | valid_accuracy: 0.90013 |  1:07:45s\n",
            "epoch 95 | loss: 0.25626 | train_logloss: 0.25186 | train_accuracy: 0.90412 | valid_logloss: 0.26218 | valid_accuracy: 0.90114 |  1:08:28s\n",
            "epoch 96 | loss: 0.25644 | train_logloss: 0.25613 | train_accuracy: 0.90208 | valid_logloss: 0.26587 | valid_accuracy: 0.89912 |  1:09:11s\n",
            "epoch 97 | loss: 0.25643 | train_logloss: 0.25166 | train_accuracy: 0.90395 | valid_logloss: 0.26195 | valid_accuracy: 0.90115 |  1:09:54s\n",
            "epoch 98 | loss: 0.25683 | train_logloss: 0.25281 | train_accuracy: 0.9035  | valid_logloss: 0.263   | valid_accuracy: 0.90059 |  1:10:37s\n",
            "epoch 99 | loss: 0.25653 | train_logloss: 0.25119 | train_accuracy: 0.90474 | valid_logloss: 0.26212 | valid_accuracy: 0.90147 |  1:11:19s\n",
            "epoch 100| loss: 0.25626 | train_logloss: 0.25255 | train_accuracy: 0.90318 | valid_logloss: 0.26267 | valid_accuracy: 0.90047 |  1:12:02s\n",
            "epoch 101| loss: 0.25578 | train_logloss: 0.25157 | train_accuracy: 0.90387 | valid_logloss: 0.26188 | valid_accuracy: 0.90065 |  1:12:45s\n",
            "epoch 102| loss: 0.25545 | train_logloss: 0.25003 | train_accuracy: 0.90455 | valid_logloss: 0.26015 | valid_accuracy: 0.90169 |  1:13:27s\n",
            "epoch 103| loss: 0.25862 | train_logloss: 0.25297 | train_accuracy: 0.9039  | valid_logloss: 0.26249 | valid_accuracy: 0.90098 |  1:14:10s\n",
            "epoch 104| loss: 0.25666 | train_logloss: 0.25311 | train_accuracy: 0.90303 | valid_logloss: 0.2635  | valid_accuracy: 0.89982 |  1:14:53s\n",
            "epoch 105| loss: 0.2558  | train_logloss: 0.2528  | train_accuracy: 0.90355 | valid_logloss: 0.26359 | valid_accuracy: 0.90013 |  1:15:35s\n",
            "epoch 106| loss: 0.25593 | train_logloss: 0.25294 | train_accuracy: 0.9034  | valid_logloss: 0.26373 | valid_accuracy: 0.90015 |  1:16:18s\n",
            "epoch 107| loss: 0.25573 | train_logloss: 0.25179 | train_accuracy: 0.90365 | valid_logloss: 0.26141 | valid_accuracy: 0.90097 |  1:17:01s\n",
            "epoch 108| loss: 0.25578 | train_logloss: 0.25035 | train_accuracy: 0.90472 | valid_logloss: 0.26082 | valid_accuracy: 0.90185 |  1:17:44s\n",
            "epoch 109| loss: 0.25535 | train_logloss: 0.25357 | train_accuracy: 0.90317 | valid_logloss: 0.2645  | valid_accuracy: 0.90032 |  1:18:26s\n",
            "epoch 110| loss: 0.25515 | train_logloss: 0.24917 | train_accuracy: 0.90525 | valid_logloss: 0.26047 | valid_accuracy: 0.90156 |  1:19:09s\n",
            "epoch 111| loss: 0.25644 | train_logloss: 0.2533  | train_accuracy: 0.90337 | valid_logloss: 0.26337 | valid_accuracy: 0.9008  |  1:19:52s\n",
            "epoch 112| loss: 0.25665 | train_logloss: 0.25268 | train_accuracy: 0.90357 | valid_logloss: 0.26257 | valid_accuracy: 0.90076 |  1:20:34s\n",
            "epoch 113| loss: 0.25732 | train_logloss: 0.25779 | train_accuracy: 0.90224 | valid_logloss: 0.26614 | valid_accuracy: 0.89942 |  1:21:17s\n",
            "epoch 114| loss: 0.25672 | train_logloss: 0.25187 | train_accuracy: 0.90368 | valid_logloss: 0.26202 | valid_accuracy: 0.90046 |  1:21:59s\n",
            "epoch 115| loss: 0.25566 | train_logloss: 0.2538  | train_accuracy: 0.90319 | valid_logloss: 0.2649  | valid_accuracy: 0.90024 |  1:22:42s\n",
            "epoch 116| loss: 0.2564  | train_logloss: 0.26537 | train_accuracy: 0.89999 | valid_logloss: 0.27239 | valid_accuracy: 0.89812 |  1:23:25s\n",
            "epoch 117| loss: 0.26634 | train_logloss: 0.25857 | train_accuracy: 0.90125 | valid_logloss: 0.26557 | valid_accuracy: 0.89951 |  1:24:07s\n",
            "epoch 118| loss: 0.25883 | train_logloss: 0.25337 | train_accuracy: 0.9039  | valid_logloss: 0.26133 | valid_accuracy: 0.90112 |  1:24:50s\n",
            "epoch 119| loss: 0.25751 | train_logloss: 0.25217 | train_accuracy: 0.90395 | valid_logloss: 0.26149 | valid_accuracy: 0.90103 |  1:25:33s\n",
            "epoch 120| loss: 0.25691 | train_logloss: 0.25269 | train_accuracy: 0.90374 | valid_logloss: 0.2629  | valid_accuracy: 0.90066 |  1:26:16s\n",
            "epoch 121| loss: 0.25587 | train_logloss: 0.25335 | train_accuracy: 0.90315 | valid_logloss: 0.26405 | valid_accuracy: 0.8995  |  1:26:59s\n",
            "epoch 122| loss: 0.25591 | train_logloss: 0.25375 | train_accuracy: 0.90339 | valid_logloss: 0.26295 | valid_accuracy: 0.90057 |  1:27:41s\n",
            "epoch 123| loss: 0.25513 | train_logloss: 0.25198 | train_accuracy: 0.90341 | valid_logloss: 0.26242 | valid_accuracy: 0.90016 |  1:28:24s\n",
            "epoch 124| loss: 0.25541 | train_logloss: 0.25117 | train_accuracy: 0.90442 | valid_logloss: 0.26186 | valid_accuracy: 0.90096 |  1:29:06s\n",
            "epoch 125| loss: 0.25518 | train_logloss: 0.2509  | train_accuracy: 0.90414 | valid_logloss: 0.26159 | valid_accuracy: 0.90089 |  1:29:49s\n",
            "epoch 126| loss: 0.25542 | train_logloss: 0.25255 | train_accuracy: 0.90334 | valid_logloss: 0.26337 | valid_accuracy: 0.90015 |  1:30:32s\n",
            "epoch 127| loss: 0.25582 | train_logloss: 0.25199 | train_accuracy: 0.90382 | valid_logloss: 0.26245 | valid_accuracy: 0.9004  |  1:31:14s\n",
            "epoch 128| loss: 0.25423 | train_logloss: 0.25274 | train_accuracy: 0.9038  | valid_logloss: 0.26293 | valid_accuracy: 0.9006  |  1:31:57s\n",
            "epoch 129| loss: 0.25546 | train_logloss: 0.24991 | train_accuracy: 0.90406 | valid_logloss: 0.26131 | valid_accuracy: 0.90092 |  1:32:41s\n",
            "epoch 130| loss: 0.25438 | train_logloss: 0.25054 | train_accuracy: 0.90409 | valid_logloss: 0.26123 | valid_accuracy: 0.90089 |  1:33:23s\n",
            "epoch 131| loss: 0.25583 | train_logloss: 0.24941 | train_accuracy: 0.90429 | valid_logloss: 0.26064 | valid_accuracy: 0.90121 |  1:34:07s\n",
            "epoch 132| loss: 0.25494 | train_logloss: 0.24907 | train_accuracy: 0.90505 | valid_logloss: 0.26033 | valid_accuracy: 0.90177 |  1:34:49s\n",
            "epoch 133| loss: 0.2548  | train_logloss: 0.25009 | train_accuracy: 0.90444 | valid_logloss: 0.26071 | valid_accuracy: 0.90116 |  1:35:32s\n",
            "epoch 134| loss: 0.25424 | train_logloss: 0.25383 | train_accuracy: 0.90366 | valid_logloss: 0.26391 | valid_accuracy: 0.90071 |  1:36:15s\n",
            "epoch 135| loss: 0.25658 | train_logloss: 0.2557  | train_accuracy: 0.90282 | valid_logloss: 0.2651  | valid_accuracy: 0.90011 |  1:36:58s\n",
            "epoch 136| loss: 0.25617 | train_logloss: 0.24989 | train_accuracy: 0.90481 | valid_logloss: 0.26136 | valid_accuracy: 0.90166 |  1:37:40s\n",
            "epoch 137| loss: 0.25505 | train_logloss: 0.25151 | train_accuracy: 0.90377 | valid_logloss: 0.26275 | valid_accuracy: 0.90064 |  1:38:27s\n",
            "epoch 138| loss: 0.25502 | train_logloss: 0.25079 | train_accuracy: 0.904   | valid_logloss: 0.2612  | valid_accuracy: 0.9012  |  1:39:10s\n",
            "\n",
            "Early stopping occurred at epoch 138 with best_epoch = 108 and best_valid_accuracy = 0.90185\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[424521  10820  44841]\n",
            " [ 13868 957856  28276]\n",
            " [ 31214  27904 373469]]\n",
            "Testing Score:  0.9018466666666667\n",
            "100%|██████████| 10/10 [17:37<00:00, 105.71s/it, best loss: -0.90083]\n",
            "({'colsample_bytree': 0.48891622968167503, 'min_child_samples': 32, 'min_child_weight': 0.39936759039610426, 'num_leaves': 97}, <hyperopt.base.Trials object at 0x7fcddabca490>)\n",
            "75.13816380500793\n",
            "Confusion Matrix: \n",
            " [[426229  10132  43821]\n",
            " [ 13858 956462  29680]\n",
            " [ 31961  26918 373708]]\n",
            "Testing Score:  0.9025233333333333\n",
            "{'Rows': 300000, 'Nd': 8, 'Na': 8, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 1, 'decision': 2, 'mask_type': 'entmax', 'train': 300000, 'test': 300000, 'time_learn_tn': 5962.013143062592, 'time_tn': 69.41409206390381, 'accuracy_tn': 0.9018466666666667, 'time_learn_gb': 1618996546.2971547, 'time_gb': 62.969158411026, 'accuracy_gb': 0.9025233333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "vyw5vT76WfDW",
        "outputId": "d36a3054-97e8-4f84-a4ba-d88453c563d5"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894833</td>\n",
              "      <td>814.815927</td>\n",
              "      <td>103.367213</td>\n",
              "      <td>0.893500</td>\n",
              "      <td>1.618983e+09</td>\n",
              "      <td>20.529149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894367</td>\n",
              "      <td>5522.418471</td>\n",
              "      <td>288.120832</td>\n",
              "      <td>0.894933</td>\n",
              "      <td>1.618989e+09</td>\n",
              "      <td>22.549996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901847</td>\n",
              "      <td>5962.013143</td>\n",
              "      <td>69.414092</td>\n",
              "      <td>0.902523</td>\n",
              "      <td>1.618997e+09</td>\n",
              "      <td>62.969158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ... accuracy_gb time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...    0.886444  1.618973e+09  16.887252\n",
              "1     9000    9000    9000  ...    0.887333  1.618974e+09  13.399141\n",
              "2     9000    9000    9000  ...    0.886222  1.618975e+09  18.229882\n",
              "3     9000    9000    9000  ...    0.888667  1.618976e+09  19.224410\n",
              "4     9000    9000    9000  ...    0.886000  1.618978e+09  15.819996\n",
              "5    30000   30000   30000  ...    0.893733  1.618979e+09  25.906476\n",
              "6    30000   30000   30000  ...    0.893633  1.618980e+09  21.026869\n",
              "7    30000   30000   30000  ...    0.894200  1.618982e+09  20.734107\n",
              "8    30000   30000   30000  ...    0.893500  1.618983e+09  20.529149\n",
              "9    30000   30000   30000  ...    0.894933  1.618989e+09  22.549996\n",
              "10  300000  300000  300000  ...    0.902523  1.618997e+09  62.969158\n",
              "\n",
              "[11 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CtomkZNzRWYZ",
        "outputId": "6d24070d-5bfd-489f-9589-92b23d6f33f3"
      },
      "source": [
        "time_model(number_exp=12, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.3988  | train_logloss: 1.00297 | train_accuracy: 0.6149  | valid_logloss: 1.00383 | valid_accuracy: 0.61598 |  0:00:52s\n",
            "epoch 1  | loss: 0.31908 | train_logloss: 0.47446 | train_accuracy: 0.78552 | valid_logloss: 0.47138 | valid_accuracy: 0.78743 |  0:01:44s\n",
            "epoch 2  | loss: 0.30617 | train_logloss: 0.30688 | train_accuracy: 0.88368 | valid_logloss: 0.30874 | valid_accuracy: 0.88335 |  0:02:38s\n",
            "epoch 3  | loss: 0.29601 | train_logloss: 0.29094 | train_accuracy: 0.89035 | valid_logloss: 0.29435 | valid_accuracy: 0.88975 |  0:03:31s\n",
            "epoch 4  | loss: 0.28984 | train_logloss: 0.28477 | train_accuracy: 0.88964 | valid_logloss: 0.28783 | valid_accuracy: 0.88917 |  0:04:23s\n",
            "epoch 5  | loss: 0.28528 | train_logloss: 0.28025 | train_accuracy: 0.89343 | valid_logloss: 0.284   | valid_accuracy: 0.8928  |  0:05:23s\n",
            "epoch 6  | loss: 0.28554 | train_logloss: 0.2934  | train_accuracy: 0.88938 | valid_logloss: 0.29604 | valid_accuracy: 0.88936 |  0:06:16s\n",
            "epoch 7  | loss: 0.28693 | train_logloss: 0.2762  | train_accuracy: 0.89573 | valid_logloss: 0.27987 | valid_accuracy: 0.89492 |  0:07:09s\n",
            "epoch 8  | loss: 0.28076 | train_logloss: 0.27545 | train_accuracy: 0.89535 | valid_logloss: 0.28013 | valid_accuracy: 0.89389 |  0:08:02s\n",
            "epoch 9  | loss: 0.27986 | train_logloss: 0.27557 | train_accuracy: 0.89515 | valid_logloss: 0.27958 | valid_accuracy: 0.89431 |  0:08:56s\n",
            "epoch 10 | loss: 0.27709 | train_logloss: 0.27198 | train_accuracy: 0.89631 | valid_logloss: 0.27611 | valid_accuracy: 0.8956  |  0:09:50s\n",
            "epoch 11 | loss: 0.27641 | train_logloss: 0.27746 | train_accuracy: 0.89395 | valid_logloss: 0.2819  | valid_accuracy: 0.89371 |  0:10:44s\n",
            "epoch 12 | loss: 0.27683 | train_logloss: 0.27205 | train_accuracy: 0.89652 | valid_logloss: 0.27567 | valid_accuracy: 0.89565 |  0:11:38s\n",
            "epoch 13 | loss: 0.27493 | train_logloss: 0.2678  | train_accuracy: 0.89908 | valid_logloss: 0.27251 | valid_accuracy: 0.89776 |  0:12:32s\n",
            "epoch 14 | loss: 0.27598 | train_logloss: 0.26844 | train_accuracy: 0.89875 | valid_logloss: 0.27328 | valid_accuracy: 0.89735 |  0:13:26s\n",
            "epoch 15 | loss: 0.27263 | train_logloss: 0.27015 | train_accuracy: 0.89621 | valid_logloss: 0.27458 | valid_accuracy: 0.89565 |  0:14:21s\n",
            "epoch 16 | loss: 0.2718  | train_logloss: 0.26617 | train_accuracy: 0.89804 | valid_logloss: 0.27223 | valid_accuracy: 0.89663 |  0:15:15s\n",
            "epoch 17 | loss: 0.2732  | train_logloss: 0.27382 | train_accuracy: 0.89701 | valid_logloss: 0.27828 | valid_accuracy: 0.89617 |  0:16:15s\n",
            "epoch 18 | loss: 0.27814 | train_logloss: 0.27349 | train_accuracy: 0.89553 | valid_logloss: 0.27685 | valid_accuracy: 0.89493 |  0:17:10s\n",
            "epoch 19 | loss: 0.27336 | train_logloss: 0.26896 | train_accuracy: 0.89759 | valid_logloss: 0.27299 | valid_accuracy: 0.89681 |  0:18:04s\n",
            "epoch 20 | loss: 0.27215 | train_logloss: 0.26916 | train_accuracy: 0.8973  | valid_logloss: 0.27264 | valid_accuracy: 0.8972  |  0:18:57s\n",
            "epoch 21 | loss: 0.27264 | train_logloss: 0.268   | train_accuracy: 0.89759 | valid_logloss: 0.27234 | valid_accuracy: 0.89614 |  0:19:51s\n",
            "epoch 22 | loss: 0.27224 | train_logloss: 0.26748 | train_accuracy: 0.89841 | valid_logloss: 0.27121 | valid_accuracy: 0.89776 |  0:20:45s\n",
            "epoch 23 | loss: 0.26998 | train_logloss: 0.26287 | train_accuracy: 0.89991 | valid_logloss: 0.2675  | valid_accuracy: 0.89877 |  0:21:40s\n",
            "epoch 24 | loss: 0.26855 | train_logloss: 0.2638  | train_accuracy: 0.89939 | valid_logloss: 0.26929 | valid_accuracy: 0.89748 |  0:22:36s\n",
            "epoch 25 | loss: 0.26798 | train_logloss: 0.26875 | train_accuracy: 0.89761 | valid_logloss: 0.27379 | valid_accuracy: 0.89591 |  0:23:30s\n",
            "epoch 26 | loss: 0.26945 | train_logloss: 0.26213 | train_accuracy: 0.90039 | valid_logloss: 0.26751 | valid_accuracy: 0.89894 |  0:24:25s\n",
            "epoch 27 | loss: 0.26637 | train_logloss: 0.25977 | train_accuracy: 0.90111 | valid_logloss: 0.26586 | valid_accuracy: 0.89949 |  0:25:19s\n",
            "epoch 28 | loss: 0.26524 | train_logloss: 0.26177 | train_accuracy: 0.90035 | valid_logloss: 0.26695 | valid_accuracy: 0.89914 |  0:26:14s\n",
            "epoch 29 | loss: 0.26481 | train_logloss: 0.26215 | train_accuracy: 0.89997 | valid_logloss: 0.26881 | valid_accuracy: 0.8985  |  0:27:08s\n",
            "epoch 30 | loss: 0.26883 | train_logloss: 0.27305 | train_accuracy: 0.89706 | valid_logloss: 0.27795 | valid_accuracy: 0.89672 |  0:28:03s\n",
            "epoch 31 | loss: 0.27649 | train_logloss: 0.2757  | train_accuracy: 0.89515 | valid_logloss: 0.27927 | valid_accuracy: 0.89447 |  0:28:57s\n",
            "epoch 32 | loss: 0.2721  | train_logloss: 0.26795 | train_accuracy: 0.89986 | valid_logloss: 0.27279 | valid_accuracy: 0.89854 |  0:29:52s\n",
            "epoch 33 | loss: 0.26798 | train_logloss: 0.2635  | train_accuracy: 0.89923 | valid_logloss: 0.26872 | valid_accuracy: 0.89803 |  0:30:46s\n",
            "epoch 34 | loss: 0.26606 | train_logloss: 0.26037 | train_accuracy: 0.90108 | valid_logloss: 0.2669  | valid_accuracy: 0.89945 |  0:31:41s\n",
            "epoch 35 | loss: 0.26477 | train_logloss: 0.25957 | train_accuracy: 0.90121 | valid_logloss: 0.26542 | valid_accuracy: 0.89976 |  0:32:35s\n",
            "epoch 36 | loss: 0.26429 | train_logloss: 0.26148 | train_accuracy: 0.89995 | valid_logloss: 0.26742 | valid_accuracy: 0.89834 |  0:33:30s\n",
            "epoch 37 | loss: 0.26366 | train_logloss: 0.25765 | train_accuracy: 0.90191 | valid_logloss: 0.26452 | valid_accuracy: 0.90044 |  0:34:24s\n",
            "epoch 38 | loss: 0.26267 | train_logloss: 0.2615  | train_accuracy: 0.90073 | valid_logloss: 0.26918 | valid_accuracy: 0.89866 |  0:35:19s\n",
            "epoch 39 | loss: 0.26307 | train_logloss: 0.25696 | train_accuracy: 0.90184 | valid_logloss: 0.26466 | valid_accuracy: 0.89979 |  0:36:14s\n",
            "epoch 40 | loss: 0.26195 | train_logloss: 0.25671 | train_accuracy: 0.9019  | valid_logloss: 0.26492 | valid_accuracy: 0.90035 |  0:37:08s\n",
            "epoch 41 | loss: 0.26335 | train_logloss: 0.25842 | train_accuracy: 0.90167 | valid_logloss: 0.26499 | valid_accuracy: 0.90002 |  0:38:02s\n",
            "epoch 42 | loss: 0.26269 | train_logloss: 0.26047 | train_accuracy: 0.90039 | valid_logloss: 0.26743 | valid_accuracy: 0.89909 |  0:38:57s\n",
            "epoch 43 | loss: 0.26548 | train_logloss: 0.26086 | train_accuracy: 0.90071 | valid_logloss: 0.2679  | valid_accuracy: 0.89911 |  0:39:51s\n",
            "epoch 44 | loss: 0.26398 | train_logloss: 0.25738 | train_accuracy: 0.90239 | valid_logloss: 0.26445 | valid_accuracy: 0.90082 |  0:40:46s\n",
            "epoch 45 | loss: 0.26323 | train_logloss: 0.26239 | train_accuracy: 0.90036 | valid_logloss: 0.269   | valid_accuracy: 0.89888 |  0:41:40s\n",
            "epoch 46 | loss: 0.26261 | train_logloss: 0.25986 | train_accuracy: 0.90013 | valid_logloss: 0.26815 | valid_accuracy: 0.89833 |  0:42:37s\n",
            "epoch 47 | loss: 0.26149 | train_logloss: 0.25691 | train_accuracy: 0.90157 | valid_logloss: 0.26535 | valid_accuracy: 0.89968 |  0:43:33s\n",
            "epoch 48 | loss: 0.26465 | train_logloss: 0.25768 | train_accuracy: 0.90206 | valid_logloss: 0.26531 | valid_accuracy: 0.90014 |  0:44:27s\n",
            "epoch 49 | loss: 0.26197 | train_logloss: 0.25738 | train_accuracy: 0.90163 | valid_logloss: 0.26545 | valid_accuracy: 0.89932 |  0:45:22s\n",
            "epoch 50 | loss: 0.26121 | train_logloss: 0.25555 | train_accuracy: 0.90355 | valid_logloss: 0.26444 | valid_accuracy: 0.90094 |  0:46:17s\n",
            "epoch 51 | loss: 0.26002 | train_logloss: 0.25839 | train_accuracy: 0.90125 | valid_logloss: 0.26628 | valid_accuracy: 0.89938 |  0:47:12s\n",
            "epoch 52 | loss: 0.26121 | train_logloss: 0.25498 | train_accuracy: 0.90279 | valid_logloss: 0.26321 | valid_accuracy: 0.90063 |  0:48:06s\n",
            "epoch 53 | loss: 0.25949 | train_logloss: 0.25605 | train_accuracy: 0.90231 | valid_logloss: 0.26515 | valid_accuracy: 0.89975 |  0:49:01s\n",
            "epoch 54 | loss: 0.26044 | train_logloss: 0.25599 | train_accuracy: 0.90207 | valid_logloss: 0.26458 | valid_accuracy: 0.89975 |  0:49:55s\n",
            "epoch 55 | loss: 0.26005 | train_logloss: 0.25414 | train_accuracy: 0.90304 | valid_logloss: 0.26381 | valid_accuracy: 0.90023 |  0:50:50s\n",
            "epoch 56 | loss: 0.25849 | train_logloss: 0.25377 | train_accuracy: 0.90258 | valid_logloss: 0.26287 | valid_accuracy: 0.90028 |  0:51:44s\n",
            "epoch 57 | loss: 0.25783 | train_logloss: 0.25184 | train_accuracy: 0.90343 | valid_logloss: 0.261   | valid_accuracy: 0.9007  |  0:52:39s\n",
            "epoch 58 | loss: 0.25747 | train_logloss: 0.25274 | train_accuracy: 0.90331 | valid_logloss: 0.26242 | valid_accuracy: 0.90097 |  0:53:33s\n",
            "epoch 59 | loss: 0.25675 | train_logloss: 0.25283 | train_accuracy: 0.90374 | valid_logloss: 0.26396 | valid_accuracy: 0.90072 |  0:54:27s\n",
            "epoch 60 | loss: 0.25703 | train_logloss: 0.25224 | train_accuracy: 0.90377 | valid_logloss: 0.26163 | valid_accuracy: 0.90156 |  0:55:21s\n",
            "epoch 61 | loss: 0.25714 | train_logloss: 0.25285 | train_accuracy: 0.90322 | valid_logloss: 0.26198 | valid_accuracy: 0.90033 |  0:56:16s\n",
            "epoch 62 | loss: 0.25832 | train_logloss: 0.25681 | train_accuracy: 0.90285 | valid_logloss: 0.26595 | valid_accuracy: 0.90009 |  0:57:11s\n",
            "epoch 63 | loss: 0.25756 | train_logloss: 0.25384 | train_accuracy: 0.90288 | valid_logloss: 0.26444 | valid_accuracy: 0.90039 |  0:58:06s\n",
            "epoch 64 | loss: 0.25671 | train_logloss: 0.25381 | train_accuracy: 0.90321 | valid_logloss: 0.26391 | valid_accuracy: 0.90021 |  0:59:01s\n",
            "epoch 65 | loss: 0.25675 | train_logloss: 0.25146 | train_accuracy: 0.90406 | valid_logloss: 0.26142 | valid_accuracy: 0.90054 |  0:59:55s\n",
            "epoch 66 | loss: 0.25621 | train_logloss: 0.25228 | train_accuracy: 0.90389 | valid_logloss: 0.26331 | valid_accuracy: 0.90106 |  1:00:51s\n",
            "epoch 67 | loss: 0.25615 | train_logloss: 0.25035 | train_accuracy: 0.90408 | valid_logloss: 0.26098 | valid_accuracy: 0.90112 |  1:01:46s\n",
            "epoch 68 | loss: 0.25905 | train_logloss: 0.25454 | train_accuracy: 0.9029  | valid_logloss: 0.2635  | valid_accuracy: 0.90072 |  1:02:42s\n",
            "epoch 69 | loss: 0.2573  | train_logloss: 0.25262 | train_accuracy: 0.90347 | valid_logloss: 0.26418 | valid_accuracy: 0.90042 |  1:03:38s\n",
            "epoch 70 | loss: 0.25698 | train_logloss: 0.25727 | train_accuracy: 0.90238 | valid_logloss: 0.26761 | valid_accuracy: 0.8995  |  1:04:34s\n",
            "epoch 71 | loss: 0.25693 | train_logloss: 0.24991 | train_accuracy: 0.90458 | valid_logloss: 0.26127 | valid_accuracy: 0.90139 |  1:05:29s\n",
            "epoch 72 | loss: 0.25505 | train_logloss: 0.25327 | train_accuracy: 0.90337 | valid_logloss: 0.26421 | valid_accuracy: 0.90051 |  1:06:26s\n",
            "epoch 73 | loss: 0.25561 | train_logloss: 0.24821 | train_accuracy: 0.90488 | valid_logloss: 0.25995 | valid_accuracy: 0.90182 |  1:07:22s\n",
            "epoch 74 | loss: 0.25522 | train_logloss: 0.25307 | train_accuracy: 0.90334 | valid_logloss: 0.2656  | valid_accuracy: 0.89999 |  1:08:17s\n",
            "epoch 75 | loss: 0.25508 | train_logloss: 0.24861 | train_accuracy: 0.90511 | valid_logloss: 0.26032 | valid_accuracy: 0.90169 |  1:09:13s\n",
            "epoch 76 | loss: 0.25505 | train_logloss: 0.2522  | train_accuracy: 0.90351 | valid_logloss: 0.26594 | valid_accuracy: 0.90016 |  1:10:09s\n",
            "epoch 77 | loss: 0.25462 | train_logloss: 0.24946 | train_accuracy: 0.90428 | valid_logloss: 0.26197 | valid_accuracy: 0.90096 |  1:11:06s\n",
            "epoch 78 | loss: 0.25433 | train_logloss: 0.25262 | train_accuracy: 0.90394 | valid_logloss: 0.26411 | valid_accuracy: 0.90045 |  1:12:03s\n",
            "epoch 79 | loss: 0.25931 | train_logloss: 0.25485 | train_accuracy: 0.90233 | valid_logloss: 0.26454 | valid_accuracy: 0.89941 |  1:12:59s\n",
            "epoch 80 | loss: 0.25503 | train_logloss: 0.25048 | train_accuracy: 0.90443 | valid_logloss: 0.26229 | valid_accuracy: 0.90099 |  1:13:56s\n",
            "epoch 81 | loss: 0.25443 | train_logloss: 0.2544  | train_accuracy: 0.90273 | valid_logloss: 0.26479 | valid_accuracy: 0.89999 |  1:14:53s\n",
            "epoch 82 | loss: 0.25479 | train_logloss: 0.25023 | train_accuracy: 0.90422 | valid_logloss: 0.26286 | valid_accuracy: 0.90053 |  1:15:50s\n",
            "epoch 83 | loss: 0.25329 | train_logloss: 0.25446 | train_accuracy: 0.90169 | valid_logloss: 0.26781 | valid_accuracy: 0.89778 |  1:16:46s\n",
            "epoch 84 | loss: 0.25514 | train_logloss: 0.25156 | train_accuracy: 0.90437 | valid_logloss: 0.26398 | valid_accuracy: 0.90082 |  1:17:44s\n",
            "epoch 85 | loss: 0.25383 | train_logloss: 0.24936 | train_accuracy: 0.90487 | valid_logloss: 0.2616  | valid_accuracy: 0.90149 |  1:18:40s\n",
            "epoch 86 | loss: 0.25606 | train_logloss: 0.25308 | train_accuracy: 0.90353 | valid_logloss: 0.26468 | valid_accuracy: 0.9005  |  1:19:37s\n",
            "epoch 87 | loss: 0.25465 | train_logloss: 0.25005 | train_accuracy: 0.90446 | valid_logloss: 0.26229 | valid_accuracy: 0.90086 |  1:20:33s\n",
            "epoch 88 | loss: 0.25642 | train_logloss: 0.25355 | train_accuracy: 0.90336 | valid_logloss: 0.26499 | valid_accuracy: 0.90021 |  1:21:30s\n",
            "epoch 89 | loss: 0.25604 | train_logloss: 0.25737 | train_accuracy: 0.90278 | valid_logloss: 0.26817 | valid_accuracy: 0.89937 |  1:22:25s\n",
            "epoch 90 | loss: 0.2574  | train_logloss: 0.25472 | train_accuracy: 0.90291 | valid_logloss: 0.26505 | valid_accuracy: 0.89963 |  1:23:36s\n",
            "epoch 91 | loss: 0.25928 | train_logloss: 0.25249 | train_accuracy: 0.90418 | valid_logloss: 0.26389 | valid_accuracy: 0.90104 |  1:24:32s\n",
            "epoch 92 | loss: 0.25621 | train_logloss: 0.25122 | train_accuracy: 0.90474 | valid_logloss: 0.26415 | valid_accuracy: 0.90071 |  1:25:28s\n",
            "epoch 93 | loss: 0.2565  | train_logloss: 0.25016 | train_accuracy: 0.90467 | valid_logloss: 0.26321 | valid_accuracy: 0.90121 |  1:26:25s\n",
            "epoch 94 | loss: 0.25808 | train_logloss: 0.26717 | train_accuracy: 0.89944 | valid_logloss: 0.27597 | valid_accuracy: 0.89693 |  1:27:21s\n",
            "epoch 95 | loss: 0.26463 | train_logloss: 0.25579 | train_accuracy: 0.90214 | valid_logloss: 0.26533 | valid_accuracy: 0.90004 |  1:28:18s\n",
            "epoch 96 | loss: 0.25949 | train_logloss: 0.25136 | train_accuracy: 0.90471 | valid_logloss: 0.26224 | valid_accuracy: 0.90119 |  1:29:15s\n",
            "epoch 97 | loss: 0.25593 | train_logloss: 0.25295 | train_accuracy: 0.90342 | valid_logloss: 0.26493 | valid_accuracy: 0.90017 |  1:30:11s\n",
            "epoch 98 | loss: 0.25585 | train_logloss: 0.25713 | train_accuracy: 0.90311 | valid_logloss: 0.26791 | valid_accuracy: 0.89968 |  1:31:06s\n",
            "epoch 99 | loss: 0.25664 | train_logloss: 0.25201 | train_accuracy: 0.90414 | valid_logloss: 0.26414 | valid_accuracy: 0.90028 |  1:32:01s\n",
            "epoch 100| loss: 0.25517 | train_logloss: 0.25086 | train_accuracy: 0.90459 | valid_logloss: 0.26192 | valid_accuracy: 0.9016  |  1:32:56s\n",
            "epoch 101| loss: 0.2542  | train_logloss: 0.24894 | train_accuracy: 0.90458 | valid_logloss: 0.26208 | valid_accuracy: 0.90151 |  1:33:51s\n",
            "epoch 102| loss: 0.25346 | train_logloss: 0.24872 | train_accuracy: 0.90509 | valid_logloss: 0.2622  | valid_accuracy: 0.90137 |  1:34:47s\n",
            "epoch 103| loss: 0.25257 | train_logloss: 0.247   | train_accuracy: 0.90607 | valid_logloss: 0.26147 | valid_accuracy: 0.90221 |  1:35:43s\n",
            "epoch 104| loss: 0.25184 | train_logloss: 0.24646 | train_accuracy: 0.90589 | valid_logloss: 0.26038 | valid_accuracy: 0.9018  |  1:36:40s\n",
            "epoch 105| loss: 0.2518  | train_logloss: 0.24799 | train_accuracy: 0.90514 | valid_logloss: 0.26279 | valid_accuracy: 0.90095 |  1:37:36s\n",
            "epoch 106| loss: 0.25258 | train_logloss: 0.24877 | train_accuracy: 0.90504 | valid_logloss: 0.26253 | valid_accuracy: 0.901   |  1:38:31s\n",
            "epoch 107| loss: 0.25231 | train_logloss: 0.25236 | train_accuracy: 0.90207 | valid_logloss: 0.26708 | valid_accuracy: 0.89759 |  1:39:27s\n",
            "epoch 108| loss: 0.25215 | train_logloss: 0.2468  | train_accuracy: 0.90555 | valid_logloss: 0.26198 | valid_accuracy: 0.90159 |  1:40:22s\n",
            "epoch 109| loss: 0.25892 | train_logloss: 0.26369 | train_accuracy: 0.90066 | valid_logloss: 0.27393 | valid_accuracy: 0.89808 |  1:41:19s\n",
            "epoch 110| loss: 0.27352 | train_logloss: 0.26529 | train_accuracy: 0.89978 | valid_logloss: 0.2729  | valid_accuracy: 0.89785 |  1:42:16s\n",
            "epoch 111| loss: 0.27728 | train_logloss: 0.26604 | train_accuracy: 0.89939 | valid_logloss: 0.27469 | valid_accuracy: 0.89712 |  1:43:12s\n",
            "epoch 112| loss: 0.26302 | train_logloss: 0.25795 | train_accuracy: 0.90269 | valid_logloss: 0.26735 | valid_accuracy: 0.90021 |  1:44:09s\n",
            "epoch 113| loss: 0.2639  | train_logloss: 0.25896 | train_accuracy: 0.90194 | valid_logloss: 0.2693  | valid_accuracy: 0.89926 |  1:45:06s\n",
            "epoch 114| loss: 0.26211 | train_logloss: 0.25885 | train_accuracy: 0.90222 | valid_logloss: 0.26867 | valid_accuracy: 0.8999  |  1:46:03s\n",
            "epoch 115| loss: 0.26461 | train_logloss: 0.25947 | train_accuracy: 0.9012  | valid_logloss: 0.26841 | valid_accuracy: 0.89936 |  1:47:00s\n",
            "epoch 116| loss: 0.26187 | train_logloss: 0.2548  | train_accuracy: 0.90253 | valid_logloss: 0.26553 | valid_accuracy: 0.90004 |  1:47:57s\n",
            "epoch 117| loss: 0.25751 | train_logloss: 0.25193 | train_accuracy: 0.90409 | valid_logloss: 0.26295 | valid_accuracy: 0.90073 |  1:48:54s\n",
            "epoch 118| loss: 0.25783 | train_logloss: 0.25161 | train_accuracy: 0.90431 | valid_logloss: 0.26315 | valid_accuracy: 0.90129 |  1:49:50s\n",
            "epoch 119| loss: 0.26282 | train_logloss: 0.25617 | train_accuracy: 0.90253 | valid_logloss: 0.26642 | valid_accuracy: 0.8995  |  1:50:47s\n",
            "epoch 120| loss: 0.26019 | train_logloss: 0.26525 | train_accuracy: 0.90013 | valid_logloss: 0.27486 | valid_accuracy: 0.89801 |  1:51:43s\n",
            "epoch 121| loss: 0.25966 | train_logloss: 0.25451 | train_accuracy: 0.90291 | valid_logloss: 0.26495 | valid_accuracy: 0.89983 |  1:52:40s\n",
            "epoch 122| loss: 0.27459 | train_logloss: 0.26706 | train_accuracy: 0.89825 | valid_logloss: 0.27404 | valid_accuracy: 0.89659 |  1:53:36s\n",
            "epoch 123| loss: 0.26576 | train_logloss: 0.26088 | train_accuracy: 0.9006  | valid_logloss: 0.26943 | valid_accuracy: 0.89836 |  1:54:32s\n",
            "epoch 124| loss: 0.26421 | train_logloss: 0.26043 | train_accuracy: 0.90106 | valid_logloss: 0.26923 | valid_accuracy: 0.89884 |  1:55:32s\n",
            "epoch 125| loss: 0.26333 | train_logloss: 0.258   | train_accuracy: 0.90217 | valid_logloss: 0.2667  | valid_accuracy: 0.8999  |  1:56:29s\n",
            "epoch 126| loss: 0.26097 | train_logloss: 0.25946 | train_accuracy: 0.90085 | valid_logloss: 0.26808 | valid_accuracy: 0.89882 |  1:57:26s\n",
            "epoch 127| loss: 0.25976 | train_logloss: 0.25439 | train_accuracy: 0.90391 | valid_logloss: 0.26418 | valid_accuracy: 0.90111 |  1:58:23s\n",
            "epoch 128| loss: 0.25956 | train_logloss: 0.25929 | train_accuracy: 0.90153 | valid_logloss: 0.26894 | valid_accuracy: 0.89896 |  1:59:20s\n",
            "epoch 129| loss: 0.25744 | train_logloss: 0.2574  | train_accuracy: 0.90049 | valid_logloss: 0.2681  | valid_accuracy: 0.89828 |  2:00:17s\n",
            "epoch 130| loss: 0.25669 | train_logloss: 0.25311 | train_accuracy: 0.90366 | valid_logloss: 0.26381 | valid_accuracy: 0.90045 |  2:01:17s\n",
            "epoch 131| loss: 0.25627 | train_logloss: 0.24984 | train_accuracy: 0.90458 | valid_logloss: 0.26174 | valid_accuracy: 0.90161 |  2:02:13s\n",
            "epoch 132| loss: 0.25529 | train_logloss: 0.25646 | train_accuracy: 0.90185 | valid_logloss: 0.2684  | valid_accuracy: 0.89843 |  2:03:10s\n",
            "epoch 133| loss: 0.2556  | train_logloss: 0.25449 | train_accuracy: 0.9034  | valid_logloss: 0.26612 | valid_accuracy: 0.89965 |  2:04:07s\n",
            "\n",
            "Early stopping occurred at epoch 133 with best_epoch = 103 and best_valid_accuracy = 0.90221\n",
            "Best weights from best epoch are automatically used!\n",
            "Confusion Matrix: \n",
            " [[426277  10087  43818]\n",
            " [ 16137 958024  25839]\n",
            " [ 31483  28878 372226]]\n",
            "Testing Score:  0.9022066666666667\n",
            "{'Rows': 300000, 'Nd': 16, 'Na': 16, 'B': 2048, 'BV': 512, 'mB': 0.7, 'λsparse': 0.001, 'Nsteps': 3, 'γ': 1.5, 'learning rate': 0.02, 'decay rate': 0.95, 'decay iterations': 200, 'shared': 2, 'decision': 2, 'mask_type': 'entmax', 'train': 300000, 'test': 300000, 'time_learn_tn': 7460.482656002045, 'time_tn': 76.50759196281433, 'accuracy_tn': 0.9022066666666667, 'time_learn_gb': 0, 'time_gb': 0, 'accuracy_gb': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-18eec0ac8420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0mdecay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0mdecay_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m      \u001b[0mshared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m      mask_type='entmax')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-f6da9e7c6683>\u001b[0m in \u001b[0;36mtime_model\u001b[0;34m(number_exp, Rows, Nd, Na, B, BV, mB, λsparse, Nsteps, γ, learning_rate, decay_rate, decay_iterations, shared, decision, mask_type, gb_do)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0mdftn_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Научная работа/Data/hyper/cpu_time.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Научная работа/Data/hyper/53/cpu_gb'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m   \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Научная работа/Data/hyper/53/cpu_gb'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'gb' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "tWdzAY2JxMIk",
        "outputId": "a8224bc7-36a7-41d4-9923-33f2d701a4e3"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rows</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>Nd</th>\n",
              "      <th>Na</th>\n",
              "      <th>B</th>\n",
              "      <th>BV</th>\n",
              "      <th>mB</th>\n",
              "      <th>λsparse</th>\n",
              "      <th>Nsteps</th>\n",
              "      <th>γ</th>\n",
              "      <th>learning rate</th>\n",
              "      <th>decay rate</th>\n",
              "      <th>decay iterations</th>\n",
              "      <th>shared</th>\n",
              "      <th>decision</th>\n",
              "      <th>mask_type</th>\n",
              "      <th>accuracy_tn</th>\n",
              "      <th>time_learn_tn</th>\n",
              "      <th>time_tn</th>\n",
              "      <th>accuracy_gb</th>\n",
              "      <th>time_learn_gb</th>\n",
              "      <th>time_gb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.882778</td>\n",
              "      <td>178.277493</td>\n",
              "      <td>70.837914</td>\n",
              "      <td>0.886444</td>\n",
              "      <td>1.618973e+09</td>\n",
              "      <td>16.887252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>313.883282</td>\n",
              "      <td>92.415544</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>1.618974e+09</td>\n",
              "      <td>13.399141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.886889</td>\n",
              "      <td>969.472747</td>\n",
              "      <td>205.964584</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>1.618975e+09</td>\n",
              "      <td>18.229882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.887111</td>\n",
              "      <td>396.823921</td>\n",
              "      <td>124.640234</td>\n",
              "      <td>0.888667</td>\n",
              "      <td>1.618976e+09</td>\n",
              "      <td>19.224410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>1907.227834</td>\n",
              "      <td>324.133266</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1.618978e+09</td>\n",
              "      <td>15.819996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>497.902985</td>\n",
              "      <td>67.806992</td>\n",
              "      <td>0.893733</td>\n",
              "      <td>1.618979e+09</td>\n",
              "      <td>25.906476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.893067</td>\n",
              "      <td>500.825767</td>\n",
              "      <td>78.570949</td>\n",
              "      <td>0.893633</td>\n",
              "      <td>1.618980e+09</td>\n",
              "      <td>21.026869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.891433</td>\n",
              "      <td>2433.883205</td>\n",
              "      <td>201.544638</td>\n",
              "      <td>0.894200</td>\n",
              "      <td>1.618982e+09</td>\n",
              "      <td>20.734107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894833</td>\n",
              "      <td>814.815927</td>\n",
              "      <td>103.367213</td>\n",
              "      <td>0.893500</td>\n",
              "      <td>1.618983e+09</td>\n",
              "      <td>20.529149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.894367</td>\n",
              "      <td>5522.418471</td>\n",
              "      <td>288.120832</td>\n",
              "      <td>0.894933</td>\n",
              "      <td>1.618989e+09</td>\n",
              "      <td>22.549996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.901847</td>\n",
              "      <td>5962.013143</td>\n",
              "      <td>69.414092</td>\n",
              "      <td>0.902523</td>\n",
              "      <td>1.618997e+09</td>\n",
              "      <td>62.969158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>300000</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2048</td>\n",
              "      <td>512</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>entmax</td>\n",
              "      <td>0.902207</td>\n",
              "      <td>7460.482656</td>\n",
              "      <td>76.507592</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rows   train    test  ...  accuracy_gb  time_learn_gb    time_gb\n",
              "0     9000    9000    9000  ...     0.886444   1.618973e+09  16.887252\n",
              "1     9000    9000    9000  ...     0.887333   1.618974e+09  13.399141\n",
              "2     9000    9000    9000  ...     0.886222   1.618975e+09  18.229882\n",
              "3     9000    9000    9000  ...     0.888667   1.618976e+09  19.224410\n",
              "4     9000    9000    9000  ...     0.886000   1.618978e+09  15.819996\n",
              "5    30000   30000   30000  ...     0.893733   1.618979e+09  25.906476\n",
              "6    30000   30000   30000  ...     0.893633   1.618980e+09  21.026869\n",
              "7    30000   30000   30000  ...     0.894200   1.618982e+09  20.734107\n",
              "8    30000   30000   30000  ...     0.893500   1.618983e+09  20.529149\n",
              "9    30000   30000   30000  ...     0.894933   1.618989e+09  22.549996\n",
              "10  300000  300000  300000  ...     0.902523   1.618997e+09  62.969158\n",
              "11  300000  300000  300000  ...     0.000000   0.000000e+00   0.000000\n",
              "\n",
              "[12 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7X5T7MCw9F3",
        "outputId": "832475d6-a7e1-4892-d974-f7633000d697"
      },
      "source": [
        "time_model(number_exp=13, \n",
        "          Rows=300000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=2048,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.46931 | train_logloss: 1.17069 | train_accuracy: 0.57971 | valid_logloss: 1.17644 | valid_accuracy: 0.57988 |  0:02:47s\n",
            "epoch 1  | loss: 0.34779 | train_logloss: 0.37316 | train_accuracy: 0.86468 | valid_logloss: 0.37202 | valid_accuracy: 0.86536 |  0:05:33s\n",
            "epoch 2  | loss: 0.3495  | train_logloss: 0.33396 | train_accuracy: 0.87419 | valid_logloss: 0.33572 | valid_accuracy: 0.8736  |  0:08:20s\n",
            "epoch 3  | loss: 0.32007 | train_logloss: 0.31716 | train_accuracy: 0.88242 | valid_logloss: 0.31847 | valid_accuracy: 0.88208 |  0:11:06s\n",
            "epoch 4  | loss: 0.33049 | train_logloss: 0.32295 | train_accuracy: 0.88005 | valid_logloss: 0.32446 | valid_accuracy: 0.88046 |  0:13:52s\n",
            "epoch 5  | loss: 0.31203 | train_logloss: 0.30438 | train_accuracy: 0.88645 | valid_logloss: 0.30602 | valid_accuracy: 0.88679 |  0:16:38s\n",
            "epoch 6  | loss: 0.30509 | train_logloss: 0.29164 | train_accuracy: 0.88911 | valid_logloss: 0.29357 | valid_accuracy: 0.88919 |  0:19:25s\n",
            "epoch 7  | loss: 0.29541 | train_logloss: 0.29279 | train_accuracy: 0.88765 | valid_logloss: 0.29419 | valid_accuracy: 0.88817 |  0:22:10s\n",
            "epoch 8  | loss: 0.29652 | train_logloss: 0.28944 | train_accuracy: 0.89013 | valid_logloss: 0.29293 | valid_accuracy: 0.88981 |  0:25:07s\n",
            "epoch 9  | loss: 0.2956  | train_logloss: 0.28526 | train_accuracy: 0.89324 | valid_logloss: 0.28709 | valid_accuracy: 0.89304 |  0:27:55s\n",
            "epoch 10 | loss: 0.28407 | train_logloss: 0.2801  | train_accuracy: 0.89486 | valid_logloss: 0.28171 | valid_accuracy: 0.89422 |  0:30:41s\n",
            "epoch 11 | loss: 0.28456 | train_logloss: 0.27899 | train_accuracy: 0.8947  | valid_logloss: 0.28149 | valid_accuracy: 0.89427 |  0:33:27s\n",
            "epoch 12 | loss: 0.28138 | train_logloss: 0.2785  | train_accuracy: 0.89459 | valid_logloss: 0.28069 | valid_accuracy: 0.8938  |  0:36:14s\n",
            "epoch 13 | loss: 0.28031 | train_logloss: 0.27781 | train_accuracy: 0.89502 | valid_logloss: 0.27971 | valid_accuracy: 0.89522 |  0:39:01s\n",
            "epoch 14 | loss: 0.27909 | train_logloss: 0.27894 | train_accuracy: 0.89542 | valid_logloss: 0.28126 | valid_accuracy: 0.895   |  0:41:49s\n",
            "epoch 15 | loss: 0.27884 | train_logloss: 0.27413 | train_accuracy: 0.89649 | valid_logloss: 0.27761 | valid_accuracy: 0.89564 |  0:44:39s\n",
            "epoch 16 | loss: 0.2773  | train_logloss: 0.28287 | train_accuracy: 0.89097 | valid_logloss: 0.28521 | valid_accuracy: 0.89103 |  0:47:24s\n",
            "epoch 17 | loss: 0.2782  | train_logloss: 0.27238 | train_accuracy: 0.89645 | valid_logloss: 0.27591 | valid_accuracy: 0.89568 |  0:50:09s\n",
            "epoch 18 | loss: 0.27402 | train_logloss: 0.26932 | train_accuracy: 0.89839 | valid_logloss: 0.27276 | valid_accuracy: 0.89735 |  0:52:56s\n",
            "epoch 19 | loss: 0.27549 | train_logloss: 0.26915 | train_accuracy: 0.89798 | valid_logloss: 0.27283 | valid_accuracy: 0.89687 |  0:55:42s\n",
            "epoch 20 | loss: 0.27213 | train_logloss: 0.26681 | train_accuracy: 0.89847 | valid_logloss: 0.271   | valid_accuracy: 0.89765 |  0:58:38s\n",
            "epoch 21 | loss: 0.27122 | train_logloss: 0.26494 | train_accuracy: 0.89867 | valid_logloss: 0.26939 | valid_accuracy: 0.89786 |  1:01:24s\n",
            "epoch 22 | loss: 0.26875 | train_logloss: 0.26575 | train_accuracy: 0.89912 | valid_logloss: 0.26979 | valid_accuracy: 0.89862 |  1:04:11s\n",
            "epoch 23 | loss: 0.26941 | train_logloss: 0.26409 | train_accuracy: 0.90003 | valid_logloss: 0.2688  | valid_accuracy: 0.89858 |  1:06:58s\n",
            "epoch 24 | loss: 0.26983 | train_logloss: 0.2627  | train_accuracy: 0.90044 | valid_logloss: 0.26784 | valid_accuracy: 0.89929 |  1:09:45s\n",
            "epoch 25 | loss: 0.26808 | train_logloss: 0.26675 | train_accuracy: 0.8986  | valid_logloss: 0.27129 | valid_accuracy: 0.89758 |  1:12:31s\n",
            "epoch 26 | loss: 0.26708 | train_logloss: 0.27507 | train_accuracy: 0.89652 | valid_logloss: 0.27912 | valid_accuracy: 0.89583 |  1:15:17s\n",
            "epoch 27 | loss: 0.27673 | train_logloss: 0.26921 | train_accuracy: 0.89812 | valid_logloss: 0.27348 | valid_accuracy: 0.89712 |  1:18:03s\n",
            "epoch 28 | loss: 0.26923 | train_logloss: 0.26911 | train_accuracy: 0.89667 | valid_logloss: 0.27483 | valid_accuracy: 0.89504 |  1:20:50s\n",
            "epoch 29 | loss: 0.26643 | train_logloss: 0.26416 | train_accuracy: 0.9003  | valid_logloss: 0.26934 | valid_accuracy: 0.89867 |  1:23:36s\n",
            "epoch 30 | loss: 0.27244 | train_logloss: 0.26851 | train_accuracy: 0.89808 | valid_logloss: 0.2726  | valid_accuracy: 0.89744 |  1:26:21s\n",
            "epoch 31 | loss: 0.27312 | train_logloss: 0.26353 | train_accuracy: 0.90053 | valid_logloss: 0.26792 | valid_accuracy: 0.89912 |  1:29:10s\n",
            "epoch 32 | loss: 0.26773 | train_logloss: 0.26427 | train_accuracy: 0.90028 | valid_logloss: 0.26889 | valid_accuracy: 0.89938 |  1:31:57s\n",
            "epoch 33 | loss: 0.26614 | train_logloss: 0.26176 | train_accuracy: 0.89991 | valid_logloss: 0.26714 | valid_accuracy: 0.89855 |  1:34:43s\n",
            "epoch 34 | loss: 0.26593 | train_logloss: 0.25909 | train_accuracy: 0.90159 | valid_logloss: 0.26438 | valid_accuracy: 0.89947 |  1:37:29s\n",
            "epoch 35 | loss: 0.26509 | train_logloss: 0.26226 | train_accuracy: 0.9002  | valid_logloss: 0.26794 | valid_accuracy: 0.89844 |  1:40:16s\n",
            "epoch 36 | loss: 0.26327 | train_logloss: 0.25996 | train_accuracy: 0.90074 | valid_logloss: 0.26639 | valid_accuracy: 0.89903 |  1:43:02s\n",
            "epoch 37 | loss: 0.2633  | train_logloss: 0.25863 | train_accuracy: 0.90092 | valid_logloss: 0.26477 | valid_accuracy: 0.89936 |  1:45:46s\n",
            "epoch 38 | loss: 0.26154 | train_logloss: 0.25866 | train_accuracy: 0.9015  | valid_logloss: 0.26528 | valid_accuracy: 0.89998 |  1:48:33s\n",
            "epoch 39 | loss: 0.26277 | train_logloss: 0.25833 | train_accuracy: 0.90208 | valid_logloss: 0.26413 | valid_accuracy: 0.90014 |  1:51:17s\n",
            "epoch 40 | loss: 0.26174 | train_logloss: 0.25804 | train_accuracy: 0.90122 | valid_logloss: 0.2649  | valid_accuracy: 0.89955 |  1:54:01s\n",
            "epoch 41 | loss: 0.26036 | train_logloss: 0.25647 | train_accuracy: 0.90233 | valid_logloss: 0.26404 | valid_accuracy: 0.89995 |  1:56:45s\n",
            "epoch 42 | loss: 0.26028 | train_logloss: 0.25667 | train_accuracy: 0.9023  | valid_logloss: 0.2646  | valid_accuracy: 0.89999 |  1:59:28s\n",
            "epoch 43 | loss: 0.25936 | train_logloss: 0.25673 | train_accuracy: 0.90214 | valid_logloss: 0.26435 | valid_accuracy: 0.89994 |  2:02:13s\n",
            "epoch 44 | loss: 0.25893 | train_logloss: 0.26187 | train_accuracy: 0.90032 | valid_logloss: 0.27021 | valid_accuracy: 0.89781 |  2:04:59s\n",
            "epoch 45 | loss: 0.2582  | train_logloss: 0.25452 | train_accuracy: 0.90313 | valid_logloss: 0.26246 | valid_accuracy: 0.9005  |  2:07:44s\n",
            "epoch 46 | loss: 0.25914 | train_logloss: 0.25966 | train_accuracy: 0.90173 | valid_logloss: 0.2678  | valid_accuracy: 0.8991  |  2:10:28s\n",
            "epoch 47 | loss: 0.26142 | train_logloss: 0.26468 | train_accuracy: 0.90033 | valid_logloss: 0.27117 | valid_accuracy: 0.89877 |  2:13:19s\n",
            "epoch 48 | loss: 0.26224 | train_logloss: 0.2565  | train_accuracy: 0.90269 | valid_logloss: 0.26389 | valid_accuracy: 0.89986 |  2:16:05s\n",
            "epoch 49 | loss: 0.25823 | train_logloss: 0.25272 | train_accuracy: 0.90381 | valid_logloss: 0.26164 | valid_accuracy: 0.90076 |  2:18:52s\n",
            "epoch 50 | loss: 0.25756 | train_logloss: 0.25262 | train_accuracy: 0.90366 | valid_logloss: 0.26189 | valid_accuracy: 0.90096 |  2:21:37s\n",
            "epoch 51 | loss: 0.25653 | train_logloss: 0.25682 | train_accuracy: 0.9006  | valid_logloss: 0.26551 | valid_accuracy: 0.89751 |  2:24:23s\n",
            "epoch 52 | loss: 0.25605 | train_logloss: 0.25163 | train_accuracy: 0.90347 | valid_logloss: 0.26155 | valid_accuracy: 0.90087 |  2:27:10s\n",
            "epoch 53 | loss: 0.25626 | train_logloss: 0.25289 | train_accuracy: 0.90346 | valid_logloss: 0.26291 | valid_accuracy: 0.90026 |  2:29:56s\n",
            "epoch 54 | loss: 0.2562  | train_logloss: 0.25247 | train_accuracy: 0.90376 | valid_logloss: 0.26252 | valid_accuracy: 0.90072 |  2:32:43s\n",
            "epoch 55 | loss: 0.25506 | train_logloss: 0.25171 | train_accuracy: 0.90377 | valid_logloss: 0.26233 | valid_accuracy: 0.90076 |  2:35:28s\n",
            "epoch 56 | loss: 0.25448 | train_logloss: 0.24969 | train_accuracy: 0.90456 | valid_logloss: 0.26249 | valid_accuracy: 0.90028 |  2:38:13s\n",
            "epoch 57 | loss: 0.25438 | train_logloss: 0.25043 | train_accuracy: 0.9047  | valid_logloss: 0.26158 | valid_accuracy: 0.90086 |  2:40:59s\n",
            "epoch 58 | loss: 0.25397 | train_logloss: 0.2503  | train_accuracy: 0.90447 | valid_logloss: 0.26086 | valid_accuracy: 0.90079 |  2:43:44s\n",
            "epoch 59 | loss: 0.25314 | train_logloss: 0.24968 | train_accuracy: 0.90443 | valid_logloss: 0.26216 | valid_accuracy: 0.9011  |  2:46:29s\n",
            "epoch 60 | loss: 0.25326 | train_logloss: 0.24892 | train_accuracy: 0.9054  | valid_logloss: 0.26138 | valid_accuracy: 0.90133 |  2:49:12s\n",
            "epoch 61 | loss: 0.25261 | train_logloss: 0.24795 | train_accuracy: 0.90585 | valid_logloss: 0.26154 | valid_accuracy: 0.90165 |  2:51:55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrNzBAh2xLsK"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHOJBMPCw9F_"
      },
      "source": [
        "time_model(number_exp=14, \n",
        "     Rows=300000, \n",
        "     Nd=32,\tNa=32,\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=3, decision=3, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yI1_hAVWghd"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Se6YseCw9F_"
      },
      "source": [
        "time_model(number_exp=15, \n",
        "     Rows=300000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEGbhxjyWfq5"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbiQR28y-7ye"
      },
      "source": [
        "##3000000 rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu55irMfRcgQ"
      },
      "source": [
        "time_model(number_exp=16, \n",
        "     Rows=3000000, \n",
        "     Nd=8,\tNa=8,\t\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=1, decision=2, \n",
        "     mask_type='entmax', gb_do=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pywqp7G0WhEl"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TXq3EEIRcgT"
      },
      "source": [
        "time_model(number_exp=17, \n",
        "     Rows=3000000, \n",
        "     Nd=16,\tNa=16,\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_vNM8tWh9D"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctuss_gtRcgU"
      },
      "source": [
        "time_model(number_exp=18, \n",
        "          Rows=3000000, \n",
        "          Nd=64,\tNa=64,\t\n",
        "          B=16384,\tBV=512,\tmB=0.7,\t\n",
        "          λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "          learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "          shared=2, decision=2, \n",
        "          mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAzIb5zsWieR"
      },
      "source": [
        "dftn_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVyOw_8lRfHj"
      },
      "source": [
        "##Другие"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnB1167-DZ0"
      },
      "source": [
        "ones(number_exp=0, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WQUysI2_Qfu"
      },
      "source": [
        "ones(number_exp=1, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_4XW1cErKy"
      },
      "source": [
        "ones(number_exp=2, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7G1oUnsFPdU"
      },
      "source": [
        "ones(number_exp=3, \n",
        "     Rows=30000, \n",
        "     Nd=32,\tNa=32,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_UFQFX_FZMH"
      },
      "source": [
        "ones(number_exp=4, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGShfVf7z0Dd",
        "outputId": "24e826cf-d3f5-440b-82c5-a3fad26df3e5"
      },
      "source": [
        "!pip install memory_profiler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/fd/d92b3295657f8837e0177e7b48b32d6651436f0293af42b76d134c3bb489/memory_profiler-0.58.0.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-cp37-none-any.whl size=30180 sha256=5dff132182fdc6bba109ce582d8d376653122098650b1ef6a0ce91aeb3c522fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/e4/0b/aaab481fc5dd2a4ea59e78bc7231bb6aae7635ca7ee79f8ae5\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.58.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9pxDTlfHi-Q",
        "outputId": "5e97186c-b6f4-4bc4-9889-3e2bcf881b23"
      },
      "source": [
        "memory(number_exp=5, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.97905 | train_logloss: 12.4491 | train_accuracy: 0.37373 | valid_logloss: 12.61953| valid_accuracy: 0.37013 |  0:00:02s\n",
            "epoch 1  | loss: 0.56978 | train_logloss: 14.36881| train_accuracy: 0.31703 | valid_logloss: 14.37557| valid_accuracy: 0.3213  |  0:00:04s\n",
            "epoch 2  | loss: 0.46949 | train_logloss: 4.74105 | train_accuracy: 0.52543 | valid_logloss: 4.76561 | valid_accuracy: 0.52323 |  0:00:06s\n",
            "epoch 3  | loss: 0.40363 | train_logloss: 5.61051 | train_accuracy: 0.30353 | valid_logloss: 5.61289 | valid_accuracy: 0.30657 |  0:00:08s\n",
            "epoch 4  | loss: 0.3827  | train_logloss: 2.38747 | train_accuracy: 0.30453 | valid_logloss: 2.3809  | valid_accuracy: 0.30113 |  0:00:11s\n",
            "epoch 5  | loss: 0.36719 | train_logloss: 1.9456  | train_accuracy: 0.32873 | valid_logloss: 1.95286 | valid_accuracy: 0.32877 |  0:00:13s\n",
            "epoch 6  | loss: 0.36546 | train_logloss: 1.8662  | train_accuracy: 0.36957 | valid_logloss: 1.87932 | valid_accuracy: 0.3725  |  0:00:15s\n",
            "epoch 7  | loss: 0.35385 | train_logloss: 1.85355 | train_accuracy: 0.43037 | valid_logloss: 1.86257 | valid_accuracy: 0.4297  |  0:00:17s\n",
            "epoch 8  | loss: 0.34529 | train_logloss: 1.92583 | train_accuracy: 0.26563 | valid_logloss: 1.93435 | valid_accuracy: 0.2617  |  0:00:19s\n",
            "epoch 9  | loss: 0.3531  | train_logloss: 1.54352 | train_accuracy: 0.6106  | valid_logloss: 1.54575 | valid_accuracy: 0.6044  |  0:00:21s\n",
            "epoch 10 | loss: 0.36885 | train_logloss: 1.36769 | train_accuracy: 0.47797 | valid_logloss: 1.37466 | valid_accuracy: 0.47293 |  0:00:24s\n",
            "epoch 11 | loss: 0.36291 | train_logloss: 0.96016 | train_accuracy: 0.63273 | valid_logloss: 0.98093 | valid_accuracy: 0.62423 |  0:00:26s\n",
            "epoch 12 | loss: 0.34936 | train_logloss: 0.73899 | train_accuracy: 0.7094  | valid_logloss: 0.75295 | valid_accuracy: 0.70293 |  0:00:28s\n",
            "epoch 13 | loss: 0.34543 | train_logloss: 0.60338 | train_accuracy: 0.80957 | valid_logloss: 0.61627 | valid_accuracy: 0.8034  |  0:00:30s\n",
            "epoch 14 | loss: 0.34373 | train_logloss: 0.6553  | train_accuracy: 0.77407 | valid_logloss: 0.66719 | valid_accuracy: 0.7696  |  0:00:32s\n",
            "epoch 15 | loss: 0.33511 | train_logloss: 0.55993 | train_accuracy: 0.79187 | valid_logloss: 0.57096 | valid_accuracy: 0.78863 |  0:00:35s\n",
            "epoch 16 | loss: 0.32642 | train_logloss: 0.53451 | train_accuracy: 0.80093 | valid_logloss: 0.54382 | valid_accuracy: 0.7994  |  0:00:37s\n",
            "epoch 17 | loss: 0.32882 | train_logloss: 0.46076 | train_accuracy: 0.82603 | valid_logloss: 0.47291 | valid_accuracy: 0.8215  |  0:00:39s\n",
            "epoch 18 | loss: 0.332   | train_logloss: 0.41772 | train_accuracy: 0.84733 | valid_logloss: 0.42707 | valid_accuracy: 0.84647 |  0:00:41s\n",
            "epoch 19 | loss: 0.33086 | train_logloss: 0.3881  | train_accuracy: 0.8589  | valid_logloss: 0.39446 | valid_accuracy: 0.8548  |  0:00:43s\n",
            "epoch 20 | loss: 0.32429 | train_logloss: 0.38051 | train_accuracy: 0.85453 | valid_logloss: 0.38792 | valid_accuracy: 0.8536  |  0:00:46s\n",
            "epoch 21 | loss: 0.32387 | train_logloss: 0.35743 | train_accuracy: 0.86827 | valid_logloss: 0.36519 | valid_accuracy: 0.8666  |  0:00:48s\n",
            "epoch 22 | loss: 0.3277  | train_logloss: 0.34388 | train_accuracy: 0.8761  | valid_logloss: 0.35529 | valid_accuracy: 0.87177 |  0:00:50s\n",
            "epoch 23 | loss: 0.32828 | train_logloss: 0.37716 | train_accuracy: 0.8667  | valid_logloss: 0.38501 | valid_accuracy: 0.86367 |  0:00:52s\n",
            "epoch 24 | loss: 0.33056 | train_logloss: 0.33555 | train_accuracy: 0.87477 | valid_logloss: 0.34323 | valid_accuracy: 0.87187 |  0:00:54s\n",
            "epoch 25 | loss: 0.3272  | train_logloss: 0.33469 | train_accuracy: 0.87443 | valid_logloss: 0.34333 | valid_accuracy: 0.8724  |  0:00:56s\n",
            "epoch 26 | loss: 0.32224 | train_logloss: 0.32578 | train_accuracy: 0.8778  | valid_logloss: 0.33451 | valid_accuracy: 0.8764  |  0:00:59s\n",
            "epoch 27 | loss: 0.32637 | train_logloss: 0.32538 | train_accuracy: 0.8784  | valid_logloss: 0.3362  | valid_accuracy: 0.87373 |  0:01:01s\n",
            "epoch 28 | loss: 0.32075 | train_logloss: 0.31854 | train_accuracy: 0.8792  | valid_logloss: 0.32856 | valid_accuracy: 0.87683 |  0:01:03s\n",
            "epoch 29 | loss: 0.31423 | train_logloss: 0.30562 | train_accuracy: 0.88577 | valid_logloss: 0.31782 | valid_accuracy: 0.8823  |  0:01:05s\n",
            "epoch 30 | loss: 0.32787 | train_logloss: 0.35006 | train_accuracy: 0.85887 | valid_logloss: 0.35993 | valid_accuracy: 0.8561  |  0:01:07s\n",
            "epoch 31 | loss: 0.32527 | train_logloss: 0.31964 | train_accuracy: 0.87607 | valid_logloss: 0.33061 | valid_accuracy: 0.8733  |  0:01:10s\n",
            "epoch 32 | loss: 0.31287 | train_logloss: 0.31665 | train_accuracy: 0.8785  | valid_logloss: 0.32985 | valid_accuracy: 0.8762  |  0:01:12s\n",
            "epoch 33 | loss: 0.3147  | train_logloss: 0.31728 | train_accuracy: 0.87887 | valid_logloss: 0.32836 | valid_accuracy: 0.87367 |  0:01:14s\n",
            "epoch 34 | loss: 0.30669 | train_logloss: 0.29686 | train_accuracy: 0.886   | valid_logloss: 0.31386 | valid_accuracy: 0.88143 |  0:01:16s\n",
            "epoch 35 | loss: 0.31008 | train_logloss: 0.30319 | train_accuracy: 0.88533 | valid_logloss: 0.31887 | valid_accuracy: 0.88037 |  0:01:18s\n",
            "epoch 36 | loss: 0.30761 | train_logloss: 0.29662 | train_accuracy: 0.88757 | valid_logloss: 0.31305 | valid_accuracy: 0.88097 |  0:01:21s\n",
            "epoch 37 | loss: 0.30404 | train_logloss: 0.30209 | train_accuracy: 0.88423 | valid_logloss: 0.31594 | valid_accuracy: 0.88173 |  0:01:23s\n",
            "epoch 38 | loss: 0.30785 | train_logloss: 0.3112  | train_accuracy: 0.88517 | valid_logloss: 0.32971 | valid_accuracy: 0.88077 |  0:01:25s\n",
            "epoch 39 | loss: 0.31085 | train_logloss: 0.30203 | train_accuracy: 0.88487 | valid_logloss: 0.3161  | valid_accuracy: 0.88007 |  0:01:27s\n",
            "epoch 40 | loss: 0.30089 | train_logloss: 0.29308 | train_accuracy: 0.8889  | valid_logloss: 0.30866 | valid_accuracy: 0.88373 |  0:01:29s\n",
            "epoch 41 | loss: 0.29935 | train_logloss: 0.29849 | train_accuracy: 0.88727 | valid_logloss: 0.31943 | valid_accuracy: 0.881   |  0:01:32s\n",
            "epoch 42 | loss: 0.29715 | train_logloss: 0.2925  | train_accuracy: 0.88943 | valid_logloss: 0.31371 | valid_accuracy: 0.88503 |  0:01:34s\n",
            "epoch 43 | loss: 0.29392 | train_logloss: 0.28365 | train_accuracy: 0.89127 | valid_logloss: 0.30535 | valid_accuracy: 0.8858  |  0:01:36s\n",
            "epoch 44 | loss: 0.28967 | train_logloss: 0.28508 | train_accuracy: 0.8905  | valid_logloss: 0.30791 | valid_accuracy: 0.88367 |  0:01:38s\n",
            "epoch 45 | loss: 0.29236 | train_logloss: 0.29584 | train_accuracy: 0.8876  | valid_logloss: 0.31726 | valid_accuracy: 0.88227 |  0:01:40s\n",
            "epoch 46 | loss: 0.29755 | train_logloss: 0.29315 | train_accuracy: 0.88827 | valid_logloss: 0.31389 | valid_accuracy: 0.88243 |  0:01:43s\n",
            "epoch 47 | loss: 0.29275 | train_logloss: 0.2859  | train_accuracy: 0.8922  | valid_logloss: 0.30784 | valid_accuracy: 0.88593 |  0:01:45s\n",
            "epoch 48 | loss: 0.28951 | train_logloss: 0.28184 | train_accuracy: 0.88977 | valid_logloss: 0.30245 | valid_accuracy: 0.8845  |  0:01:47s\n",
            "epoch 49 | loss: 0.28977 | train_logloss: 0.29449 | train_accuracy: 0.88533 | valid_logloss: 0.31502 | valid_accuracy: 0.881   |  0:01:49s\n",
            "epoch 50 | loss: 0.30073 | train_logloss: 0.31554 | train_accuracy: 0.8807  | valid_logloss: 0.33294 | valid_accuracy: 0.87637 |  0:01:51s\n",
            "epoch 51 | loss: 0.3108  | train_logloss: 0.30296 | train_accuracy: 0.88533 | valid_logloss: 0.32153 | valid_accuracy: 0.87937 |  0:01:54s\n",
            "epoch 52 | loss: 0.30582 | train_logloss: 0.30823 | train_accuracy: 0.8844  | valid_logloss: 0.32328 | valid_accuracy: 0.8809  |  0:01:56s\n",
            "epoch 53 | loss: 0.30711 | train_logloss: 0.29592 | train_accuracy: 0.88783 | valid_logloss: 0.31358 | valid_accuracy: 0.8834  |  0:01:58s\n",
            "epoch 54 | loss: 0.29994 | train_logloss: 0.29117 | train_accuracy: 0.8897  | valid_logloss: 0.30707 | valid_accuracy: 0.8864  |  0:02:00s\n",
            "epoch 55 | loss: 0.30592 | train_logloss: 0.30926 | train_accuracy: 0.8801  | valid_logloss: 0.32352 | valid_accuracy: 0.87327 |  0:02:03s\n",
            "epoch 56 | loss: 0.3032  | train_logloss: 0.29621 | train_accuracy: 0.88383 | valid_logloss: 0.31104 | valid_accuracy: 0.8807  |  0:02:05s\n",
            "epoch 57 | loss: 0.29733 | train_logloss: 0.28695 | train_accuracy: 0.88947 | valid_logloss: 0.30401 | valid_accuracy: 0.8836  |  0:02:07s\n",
            "epoch 58 | loss: 0.29487 | train_logloss: 0.28673 | train_accuracy: 0.88983 | valid_logloss: 0.30697 | valid_accuracy: 0.884   |  0:02:09s\n",
            "epoch 59 | loss: 0.28747 | train_logloss: 0.28264 | train_accuracy: 0.8901  | valid_logloss: 0.3038  | valid_accuracy: 0.8853  |  0:02:12s\n",
            "epoch 60 | loss: 0.2843  | train_logloss: 0.2783  | train_accuracy: 0.89277 | valid_logloss: 0.3015  | valid_accuracy: 0.887   |  0:02:14s\n",
            "epoch 61 | loss: 0.2869  | train_logloss: 0.28686 | train_accuracy: 0.8908  | valid_logloss: 0.30658 | valid_accuracy: 0.88613 |  0:02:16s\n",
            "epoch 62 | loss: 0.2877  | train_logloss: 0.27989 | train_accuracy: 0.8923  | valid_logloss: 0.30364 | valid_accuracy: 0.88587 |  0:02:18s\n",
            "epoch 63 | loss: 0.28477 | train_logloss: 0.27708 | train_accuracy: 0.89307 | valid_logloss: 0.30085 | valid_accuracy: 0.8863  |  0:02:21s\n",
            "epoch 64 | loss: 0.28547 | train_logloss: 0.28623 | train_accuracy: 0.89197 | valid_logloss: 0.30923 | valid_accuracy: 0.8857  |  0:02:23s\n",
            "epoch 65 | loss: 0.29204 | train_logloss: 0.28278 | train_accuracy: 0.89043 | valid_logloss: 0.30389 | valid_accuracy: 0.88547 |  0:02:25s\n",
            "epoch 66 | loss: 0.28597 | train_logloss: 0.27628 | train_accuracy: 0.89423 | valid_logloss: 0.29863 | valid_accuracy: 0.8876  |  0:02:27s\n",
            "epoch 67 | loss: 0.28349 | train_logloss: 0.27436 | train_accuracy: 0.89487 | valid_logloss: 0.29829 | valid_accuracy: 0.88917 |  0:02:30s\n",
            "epoch 68 | loss: 0.2798  | train_logloss: 0.27127 | train_accuracy: 0.89473 | valid_logloss: 0.29757 | valid_accuracy: 0.88847 |  0:02:32s\n",
            "epoch 69 | loss: 0.2833  | train_logloss: 0.27914 | train_accuracy: 0.89367 | valid_logloss: 0.30303 | valid_accuracy: 0.88787 |  0:02:34s\n",
            "epoch 70 | loss: 0.27968 | train_logloss: 0.27337 | train_accuracy: 0.896   | valid_logloss: 0.30016 | valid_accuracy: 0.88757 |  0:02:37s\n",
            "epoch 71 | loss: 0.27936 | train_logloss: 0.26939 | train_accuracy: 0.89603 | valid_logloss: 0.29306 | valid_accuracy: 0.89043 |  0:02:39s\n",
            "epoch 72 | loss: 0.27518 | train_logloss: 0.27008 | train_accuracy: 0.8957  | valid_logloss: 0.2966  | valid_accuracy: 0.88947 |  0:02:41s\n",
            "epoch 73 | loss: 0.28314 | train_logloss: 0.28498 | train_accuracy: 0.8893  | valid_logloss: 0.30627 | valid_accuracy: 0.88347 |  0:02:43s\n",
            "epoch 74 | loss: 0.28713 | train_logloss: 0.27665 | train_accuracy: 0.89463 | valid_logloss: 0.29996 | valid_accuracy: 0.8866  |  0:02:45s\n",
            "epoch 75 | loss: 0.27902 | train_logloss: 0.27327 | train_accuracy: 0.89563 | valid_logloss: 0.29478 | valid_accuracy: 0.88883 |  0:02:48s\n",
            "epoch 76 | loss: 0.27902 | train_logloss: 0.27501 | train_accuracy: 0.89663 | valid_logloss: 0.29729 | valid_accuracy: 0.88897 |  0:02:50s\n",
            "epoch 77 | loss: 0.27954 | train_logloss: 0.27598 | train_accuracy: 0.8948  | valid_logloss: 0.30121 | valid_accuracy: 0.88787 |  0:02:52s\n",
            "epoch 78 | loss: 0.27622 | train_logloss: 0.27014 | train_accuracy: 0.89677 | valid_logloss: 0.29308 | valid_accuracy: 0.88967 |  0:02:55s\n",
            "epoch 79 | loss: 0.27246 | train_logloss: 0.26798 | train_accuracy: 0.89783 | valid_logloss: 0.29556 | valid_accuracy: 0.8893  |  0:02:57s\n",
            "epoch 80 | loss: 0.27123 | train_logloss: 0.26514 | train_accuracy: 0.8988  | valid_logloss: 0.29273 | valid_accuracy: 0.89077 |  0:02:59s\n",
            "epoch 81 | loss: 0.27056 | train_logloss: 0.26272 | train_accuracy: 0.8967  | valid_logloss: 0.29323 | valid_accuracy: 0.8893  |  0:03:01s\n",
            "epoch 82 | loss: 0.27259 | train_logloss: 0.27664 | train_accuracy: 0.89473 | valid_logloss: 0.30252 | valid_accuracy: 0.8884  |  0:03:03s\n",
            "epoch 83 | loss: 0.27518 | train_logloss: 0.2741  | train_accuracy: 0.89667 | valid_logloss: 0.30608 | valid_accuracy: 0.8873  |  0:03:05s\n",
            "epoch 84 | loss: 0.27399 | train_logloss: 0.26742 | train_accuracy: 0.8969  | valid_logloss: 0.29482 | valid_accuracy: 0.8886  |  0:03:07s\n",
            "epoch 85 | loss: 0.27176 | train_logloss: 0.26468 | train_accuracy: 0.89863 | valid_logloss: 0.29395 | valid_accuracy: 0.8891  |  0:03:10s\n",
            "epoch 86 | loss: 0.27599 | train_logloss: 0.26946 | train_accuracy: 0.8966  | valid_logloss: 0.29539 | valid_accuracy: 0.88947 |  0:03:12s\n",
            "epoch 87 | loss: 0.27548 | train_logloss: 0.27249 | train_accuracy: 0.89597 | valid_logloss: 0.30289 | valid_accuracy: 0.88577 |  0:03:14s\n",
            "epoch 88 | loss: 0.27441 | train_logloss: 0.26797 | train_accuracy: 0.89893 | valid_logloss: 0.29994 | valid_accuracy: 0.88873 |  0:03:16s\n",
            "epoch 89 | loss: 0.27278 | train_logloss: 0.26455 | train_accuracy: 0.8981  | valid_logloss: 0.29379 | valid_accuracy: 0.89083 |  0:03:18s\n",
            "epoch 90 | loss: 0.28483 | train_logloss: 0.27475 | train_accuracy: 0.89593 | valid_logloss: 0.29878 | valid_accuracy: 0.8888  |  0:03:20s\n",
            "epoch 91 | loss: 0.2772  | train_logloss: 0.26589 | train_accuracy: 0.89763 | valid_logloss: 0.29391 | valid_accuracy: 0.89013 |  0:03:22s\n",
            "epoch 92 | loss: 0.27268 | train_logloss: 0.26492 | train_accuracy: 0.89967 | valid_logloss: 0.29426 | valid_accuracy: 0.89043 |  0:03:24s\n",
            "epoch 93 | loss: 0.27101 | train_logloss: 0.26461 | train_accuracy: 0.89853 | valid_logloss: 0.29421 | valid_accuracy: 0.8897  |  0:03:27s\n",
            "epoch 94 | loss: 0.26751 | train_logloss: 0.26042 | train_accuracy: 0.90063 | valid_logloss: 0.29237 | valid_accuracy: 0.89203 |  0:03:29s\n",
            "epoch 95 | loss: 0.268   | train_logloss: 0.26317 | train_accuracy: 0.89977 | valid_logloss: 0.2959  | valid_accuracy: 0.88863 |  0:03:31s\n",
            "epoch 96 | loss: 0.26938 | train_logloss: 0.25944 | train_accuracy: 0.90047 | valid_logloss: 0.29403 | valid_accuracy: 0.889   |  0:03:33s\n",
            "epoch 97 | loss: 0.2641  | train_logloss: 0.25672 | train_accuracy: 0.90157 | valid_logloss: 0.29248 | valid_accuracy: 0.89027 |  0:03:35s\n",
            "epoch 98 | loss: 0.26533 | train_logloss: 0.25793 | train_accuracy: 0.9005  | valid_logloss: 0.2938  | valid_accuracy: 0.89023 |  0:03:37s\n",
            "epoch 99 | loss: 0.26343 | train_logloss: 0.26057 | train_accuracy: 0.89953 | valid_logloss: 0.29541 | valid_accuracy: 0.8892  |  0:03:39s\n",
            "epoch 100| loss: 0.26208 | train_logloss: 0.25628 | train_accuracy: 0.90053 | valid_logloss: 0.29423 | valid_accuracy: 0.88953 |  0:03:42s\n",
            "epoch 101| loss: 0.26255 | train_logloss: 0.25914 | train_accuracy: 0.8999  | valid_logloss: 0.29751 | valid_accuracy: 0.88817 |  0:03:44s\n",
            "epoch 102| loss: 0.26052 | train_logloss: 0.25212 | train_accuracy: 0.90163 | valid_logloss: 0.29016 | valid_accuracy: 0.89113 |  0:03:46s\n",
            "epoch 103| loss: 0.25586 | train_logloss: 0.2526  | train_accuracy: 0.9027  | valid_logloss: 0.29393 | valid_accuracy: 0.89293 |  0:03:48s\n",
            "epoch 104| loss: 0.25851 | train_logloss: 0.25077 | train_accuracy: 0.90363 | valid_logloss: 0.29353 | valid_accuracy: 0.8915  |  0:03:50s\n",
            "epoch 105| loss: 0.25626 | train_logloss: 0.25198 | train_accuracy: 0.9026  | valid_logloss: 0.29673 | valid_accuracy: 0.89227 |  0:03:52s\n",
            "epoch 106| loss: 0.25923 | train_logloss: 0.25354 | train_accuracy: 0.90267 | valid_logloss: 0.29509 | valid_accuracy: 0.89123 |  0:03:54s\n",
            "epoch 107| loss: 0.25781 | train_logloss: 0.25163 | train_accuracy: 0.90283 | valid_logloss: 0.29753 | valid_accuracy: 0.89103 |  0:03:57s\n",
            "epoch 108| loss: 0.25661 | train_logloss: 0.2486  | train_accuracy: 0.90357 | valid_logloss: 0.29471 | valid_accuracy: 0.89163 |  0:03:59s\n",
            "epoch 109| loss: 0.25564 | train_logloss: 0.24838 | train_accuracy: 0.90517 | valid_logloss: 0.29885 | valid_accuracy: 0.88993 |  0:04:01s\n",
            "epoch 110| loss: 0.25444 | train_logloss: 0.24677 | train_accuracy: 0.9051  | valid_logloss: 0.29219 | valid_accuracy: 0.8928  |  0:04:03s\n",
            "epoch 111| loss: 0.25371 | train_logloss: 0.24358 | train_accuracy: 0.9057  | valid_logloss: 0.29406 | valid_accuracy: 0.8907  |  0:04:05s\n",
            "epoch 112| loss: 0.25121 | train_logloss: 0.24256 | train_accuracy: 0.90653 | valid_logloss: 0.29717 | valid_accuracy: 0.89203 |  0:04:07s\n",
            "epoch 113| loss: 0.25357 | train_logloss: 0.2452  | train_accuracy: 0.90473 | valid_logloss: 0.29589 | valid_accuracy: 0.89033 |  0:04:09s\n",
            "epoch 114| loss: 0.25284 | train_logloss: 0.24793 | train_accuracy: 0.9048  | valid_logloss: 0.30038 | valid_accuracy: 0.88933 |  0:04:12s\n",
            "epoch 115| loss: 0.25313 | train_logloss: 0.24265 | train_accuracy: 0.90623 | valid_logloss: 0.29613 | valid_accuracy: 0.89257 |  0:04:14s\n",
            "epoch 116| loss: 0.2508  | train_logloss: 0.24423 | train_accuracy: 0.90527 | valid_logloss: 0.29566 | valid_accuracy: 0.8905  |  0:04:16s\n",
            "epoch 117| loss: 0.2502  | train_logloss: 0.24441 | train_accuracy: 0.9058  | valid_logloss: 0.29898 | valid_accuracy: 0.89113 |  0:04:18s\n",
            "epoch 118| loss: 0.24916 | train_logloss: 0.24197 | train_accuracy: 0.90643 | valid_logloss: 0.29711 | valid_accuracy: 0.892   |  0:04:20s\n",
            "epoch 119| loss: 0.25319 | train_logloss: 0.25539 | train_accuracy: 0.90257 | valid_logloss: 0.30549 | valid_accuracy: 0.88883 |  0:04:22s\n",
            "epoch 120| loss: 0.26652 | train_logloss: 0.26247 | train_accuracy: 0.90117 | valid_logloss: 0.30016 | valid_accuracy: 0.89017 |  0:04:24s\n",
            "epoch 121| loss: 0.27249 | train_logloss: 0.26449 | train_accuracy: 0.89963 | valid_logloss: 0.29901 | valid_accuracy: 0.8886  |  0:04:27s\n",
            "epoch 122| loss: 0.27135 | train_logloss: 0.26047 | train_accuracy: 0.9007  | valid_logloss: 0.29678 | valid_accuracy: 0.88973 |  0:04:29s\n",
            "epoch 123| loss: 0.26678 | train_logloss: 0.25613 | train_accuracy: 0.90277 | valid_logloss: 0.29185 | valid_accuracy: 0.89137 |  0:04:31s\n",
            "epoch 124| loss: 0.26281 | train_logloss: 0.26444 | train_accuracy: 0.89693 | valid_logloss: 0.31038 | valid_accuracy: 0.88587 |  0:04:33s\n",
            "epoch 125| loss: 0.26687 | train_logloss: 0.26211 | train_accuracy: 0.8978  | valid_logloss: 0.2983  | valid_accuracy: 0.8894  |  0:04:35s\n",
            "epoch 126| loss: 0.26587 | train_logloss: 0.25389 | train_accuracy: 0.90173 | valid_logloss: 0.29404 | valid_accuracy: 0.89133 |  0:04:37s\n",
            "epoch 127| loss: 0.25967 | train_logloss: 0.25046 | train_accuracy: 0.90453 | valid_logloss: 0.29045 | valid_accuracy: 0.891   |  0:04:39s\n",
            "epoch 128| loss: 0.2578  | train_logloss: 0.24846 | train_accuracy: 0.9058  | valid_logloss: 0.29163 | valid_accuracy: 0.89243 |  0:04:42s\n",
            "epoch 129| loss: 0.25504 | train_logloss: 0.24565 | train_accuracy: 0.90423 | valid_logloss: 0.29582 | valid_accuracy: 0.8908  |  0:04:44s\n",
            "epoch 130| loss: 0.25352 | train_logloss: 0.24705 | train_accuracy: 0.9052  | valid_logloss: 0.29403 | valid_accuracy: 0.8902  |  0:04:46s\n",
            "epoch 131| loss: 0.25277 | train_logloss: 0.24267 | train_accuracy: 0.90613 | valid_logloss: 0.29331 | valid_accuracy: 0.89173 |  0:04:48s\n",
            "epoch 132| loss: 0.25263 | train_logloss: 0.24292 | train_accuracy: 0.90653 | valid_logloss: 0.29322 | valid_accuracy: 0.89063 |  0:04:50s\n",
            "epoch 133| loss: 0.24835 | train_logloss: 0.23953 | train_accuracy: 0.90613 | valid_logloss: 0.29604 | valid_accuracy: 0.8927  |  0:04:52s\n",
            "epoch 134| loss: 0.24808 | train_logloss: 0.24064 | train_accuracy: 0.90753 | valid_logloss: 0.29866 | valid_accuracy: 0.89197 |  0:04:54s\n",
            "epoch 135| loss: 0.24685 | train_logloss: 0.23822 | train_accuracy: 0.90803 | valid_logloss: 0.29688 | valid_accuracy: 0.89023 |  0:04:57s\n",
            "epoch 136| loss: 0.24936 | train_logloss: 0.23871 | train_accuracy: 0.9066  | valid_logloss: 0.29274 | valid_accuracy: 0.89133 |  0:04:59s\n",
            "epoch 137| loss: 0.25008 | train_logloss: 0.24246 | train_accuracy: 0.90587 | valid_logloss: 0.29693 | valid_accuracy: 0.89053 |  0:05:01s\n",
            "epoch 138| loss: 0.25028 | train_logloss: 0.24952 | train_accuracy: 0.90453 | valid_logloss: 0.30634 | valid_accuracy: 0.88817 |  0:05:03s\n",
            "epoch 139| loss: 0.25828 | train_logloss: 0.24596 | train_accuracy: 0.90467 | valid_logloss: 0.29939 | valid_accuracy: 0.8911  |  0:05:05s\n",
            "epoch 140| loss: 0.25418 | train_logloss: 0.24582 | train_accuracy: 0.90423 | valid_logloss: 0.2969  | valid_accuracy: 0.88917 |  0:05:07s\n",
            "epoch 141| loss: 0.25046 | train_logloss: 0.2442  | train_accuracy: 0.90697 | valid_logloss: 0.3023  | valid_accuracy: 0.88923 |  0:05:09s\n",
            "epoch 142| loss: 0.24713 | train_logloss: 0.23878 | train_accuracy: 0.9079  | valid_logloss: 0.29572 | valid_accuracy: 0.89083 |  0:05:12s\n",
            "epoch 143| loss: 0.24307 | train_logloss: 0.2354  | train_accuracy: 0.90817 | valid_logloss: 0.29799 | valid_accuracy: 0.89173 |  0:05:14s\n",
            "epoch 144| loss: 0.24463 | train_logloss: 0.23662 | train_accuracy: 0.9096  | valid_logloss: 0.29712 | valid_accuracy: 0.89083 |  0:05:16s\n",
            "epoch 145| loss: 0.24261 | train_logloss: 0.23206 | train_accuracy: 0.90947 | valid_logloss: 0.29989 | valid_accuracy: 0.89083 |  0:05:18s\n",
            "epoch 146| loss: 0.24088 | train_logloss: 0.2321  | train_accuracy: 0.91113 | valid_logloss: 0.29754 | valid_accuracy: 0.88967 |  0:05:20s\n",
            "epoch 147| loss: 0.24337 | train_logloss: 0.23131 | train_accuracy: 0.9105  | valid_logloss: 0.30043 | valid_accuracy: 0.8912  |  0:05:22s\n",
            "epoch 148| loss: 0.24063 | train_logloss: 0.23517 | train_accuracy: 0.90953 | valid_logloss: 0.30138 | valid_accuracy: 0.8907  |  0:05:24s\n",
            "epoch 149| loss: 0.24304 | train_logloss: 0.2372  | train_accuracy: 0.9094  | valid_logloss: 0.30225 | valid_accuracy: 0.88903 |  0:05:27s\n",
            "epoch 150| loss: 0.24496 | train_logloss: 0.2379  | train_accuracy: 0.9088  | valid_logloss: 0.30638 | valid_accuracy: 0.88997 |  0:05:29s\n",
            "epoch 151| loss: 0.24599 | train_logloss: 0.23585 | train_accuracy: 0.90893 | valid_logloss: 0.30486 | valid_accuracy: 0.88977 |  0:05:31s\n",
            "epoch 152| loss: 0.24439 | train_logloss: 0.23905 | train_accuracy: 0.90793 | valid_logloss: 0.30745 | valid_accuracy: 0.88843 |  0:05:33s\n",
            "epoch 153| loss: 0.24117 | train_logloss: 0.23141 | train_accuracy: 0.91043 | valid_logloss: 0.3028  | valid_accuracy: 0.88967 |  0:05:35s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 103 and best_valid_accuracy = 0.89293\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixw-bGGCzvyw",
        "outputId": "a271091e-e68a-47e0-dfbb-4f93da7c2b3a"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb5.pkl' 'tn5.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    253.5 MiB    253.5 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.1 MiB      5.6 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2319.0 MiB   2059.9 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkLeovCoHp2P",
        "outputId": "055a52a6-99db-40da-87c5-e0244603c51a"
      },
      "source": [
        "memory(number_exp=6, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.69956 | train_logloss: 19.1368 | train_accuracy: 0.30157 | valid_logloss: 18.98184| valid_accuracy: 0.3066  |  0:00:01s\n",
            "epoch 1  | loss: 0.39224 | train_logloss: 13.7197 | train_accuracy: 0.3147  | valid_logloss: 13.76676| valid_accuracy: 0.31487 |  0:00:03s\n",
            "epoch 2  | loss: 0.35065 | train_logloss: 17.28233| train_accuracy: 0.31073 | valid_logloss: 17.28015| valid_accuracy: 0.31097 |  0:00:04s\n",
            "epoch 3  | loss: 0.33239 | train_logloss: 9.25345 | train_accuracy: 0.26283 | valid_logloss: 9.27368 | valid_accuracy: 0.26153 |  0:00:06s\n",
            "epoch 4  | loss: 0.32662 | train_logloss: 5.23646 | train_accuracy: 0.19393 | valid_logloss: 5.25429 | valid_accuracy: 0.19507 |  0:00:07s\n",
            "epoch 5  | loss: 0.31889 | train_logloss: 5.17342 | train_accuracy: 0.1914  | valid_logloss: 5.18116 | valid_accuracy: 0.19033 |  0:00:09s\n",
            "epoch 6  | loss: 0.31295 | train_logloss: 4.63485 | train_accuracy: 0.28783 | valid_logloss: 4.65883 | valid_accuracy: 0.2864  |  0:00:10s\n",
            "epoch 7  | loss: 0.30388 | train_logloss: 3.62504 | train_accuracy: 0.3366  | valid_logloss: 3.65505 | valid_accuracy: 0.33487 |  0:00:12s\n",
            "epoch 8  | loss: 0.30345 | train_logloss: 2.99254 | train_accuracy: 0.3589  | valid_logloss: 3.01201 | valid_accuracy: 0.3564  |  0:00:14s\n",
            "epoch 9  | loss: 0.30469 | train_logloss: 3.13761 | train_accuracy: 0.32433 | valid_logloss: 3.16707 | valid_accuracy: 0.31933 |  0:00:15s\n",
            "epoch 10 | loss: 0.30184 | train_logloss: 1.48885 | train_accuracy: 0.4621  | valid_logloss: 1.50825 | valid_accuracy: 0.45667 |  0:00:17s\n",
            "epoch 11 | loss: 0.29804 | train_logloss: 1.54527 | train_accuracy: 0.38867 | valid_logloss: 1.55547 | valid_accuracy: 0.38277 |  0:00:18s\n",
            "epoch 12 | loss: 0.29027 | train_logloss: 1.28611 | train_accuracy: 0.52847 | valid_logloss: 1.29732 | valid_accuracy: 0.52183 |  0:00:20s\n",
            "epoch 13 | loss: 0.28815 | train_logloss: 0.9834  | train_accuracy: 0.62217 | valid_logloss: 0.99384 | valid_accuracy: 0.61327 |  0:00:22s\n",
            "epoch 14 | loss: 0.29073 | train_logloss: 0.91848 | train_accuracy: 0.6366  | valid_logloss: 0.92275 | valid_accuracy: 0.63147 |  0:00:23s\n",
            "epoch 15 | loss: 0.28638 | train_logloss: 0.94037 | train_accuracy: 0.69493 | valid_logloss: 0.94953 | valid_accuracy: 0.69363 |  0:00:25s\n",
            "epoch 16 | loss: 0.2875  | train_logloss: 0.60466 | train_accuracy: 0.79803 | valid_logloss: 0.61054 | valid_accuracy: 0.79257 |  0:00:26s\n",
            "epoch 17 | loss: 0.28078 | train_logloss: 0.57784 | train_accuracy: 0.78457 | valid_logloss: 0.58723 | valid_accuracy: 0.7824  |  0:00:28s\n",
            "epoch 18 | loss: 0.2783  | train_logloss: 0.45591 | train_accuracy: 0.83937 | valid_logloss: 0.46561 | valid_accuracy: 0.83653 |  0:00:30s\n",
            "epoch 19 | loss: 0.28025 | train_logloss: 0.44233 | train_accuracy: 0.8412  | valid_logloss: 0.45462 | valid_accuracy: 0.83833 |  0:00:31s\n",
            "epoch 20 | loss: 0.2725  | train_logloss: 0.37264 | train_accuracy: 0.86767 | valid_logloss: 0.38941 | valid_accuracy: 0.8633  |  0:00:33s\n",
            "epoch 21 | loss: 0.26917 | train_logloss: 0.333   | train_accuracy: 0.87883 | valid_logloss: 0.35173 | valid_accuracy: 0.87663 |  0:00:34s\n",
            "epoch 22 | loss: 0.26927 | train_logloss: 0.32288 | train_accuracy: 0.88023 | valid_logloss: 0.34692 | valid_accuracy: 0.87437 |  0:00:36s\n",
            "epoch 23 | loss: 0.27185 | train_logloss: 0.31845 | train_accuracy: 0.8815  | valid_logloss: 0.34465 | valid_accuracy: 0.87733 |  0:00:38s\n",
            "epoch 24 | loss: 0.27367 | train_logloss: 0.31359 | train_accuracy: 0.88363 | valid_logloss: 0.34104 | valid_accuracy: 0.8765  |  0:00:39s\n",
            "epoch 25 | loss: 0.27909 | train_logloss: 0.30434 | train_accuracy: 0.8861  | valid_logloss: 0.32642 | valid_accuracy: 0.8821  |  0:00:41s\n",
            "epoch 26 | loss: 0.28364 | train_logloss: 0.29199 | train_accuracy: 0.88867 | valid_logloss: 0.31711 | valid_accuracy: 0.88207 |  0:00:42s\n",
            "epoch 27 | loss: 0.28466 | train_logloss: 0.28037 | train_accuracy: 0.89047 | valid_logloss: 0.30541 | valid_accuracy: 0.88623 |  0:00:44s\n",
            "epoch 28 | loss: 0.27769 | train_logloss: 0.26998 | train_accuracy: 0.8972  | valid_logloss: 0.29998 | valid_accuracy: 0.88757 |  0:00:46s\n",
            "epoch 29 | loss: 0.27103 | train_logloss: 0.26981 | train_accuracy: 0.89503 | valid_logloss: 0.30161 | valid_accuracy: 0.88623 |  0:00:47s\n",
            "epoch 30 | loss: 0.26906 | train_logloss: 0.27144 | train_accuracy: 0.8954  | valid_logloss: 0.3076  | valid_accuracy: 0.88533 |  0:00:49s\n",
            "epoch 31 | loss: 0.26877 | train_logloss: 0.27427 | train_accuracy: 0.896   | valid_logloss: 0.30713 | valid_accuracy: 0.88647 |  0:00:50s\n",
            "epoch 32 | loss: 0.27251 | train_logloss: 0.25733 | train_accuracy: 0.9011  | valid_logloss: 0.29773 | valid_accuracy: 0.89037 |  0:00:52s\n",
            "epoch 33 | loss: 0.26587 | train_logloss: 0.2604  | train_accuracy: 0.89937 | valid_logloss: 0.30247 | valid_accuracy: 0.88803 |  0:00:54s\n",
            "epoch 34 | loss: 0.2613  | train_logloss: 0.25367 | train_accuracy: 0.90137 | valid_logloss: 0.30591 | valid_accuracy: 0.88687 |  0:00:55s\n",
            "epoch 35 | loss: 0.26402 | train_logloss: 0.25528 | train_accuracy: 0.90063 | valid_logloss: 0.30259 | valid_accuracy: 0.88727 |  0:00:57s\n",
            "epoch 36 | loss: 0.26577 | train_logloss: 0.26107 | train_accuracy: 0.89813 | valid_logloss: 0.301   | valid_accuracy: 0.88733 |  0:00:58s\n",
            "epoch 37 | loss: 0.26369 | train_logloss: 0.26373 | train_accuracy: 0.89727 | valid_logloss: 0.31182 | valid_accuracy: 0.88463 |  0:01:00s\n",
            "epoch 38 | loss: 0.26477 | train_logloss: 0.2507  | train_accuracy: 0.90283 | valid_logloss: 0.29684 | valid_accuracy: 0.88893 |  0:01:02s\n",
            "epoch 39 | loss: 0.26347 | train_logloss: 0.25581 | train_accuracy: 0.90033 | valid_logloss: 0.30639 | valid_accuracy: 0.88617 |  0:01:03s\n",
            "epoch 40 | loss: 0.26119 | train_logloss: 0.24904 | train_accuracy: 0.904   | valid_logloss: 0.29753 | valid_accuracy: 0.88897 |  0:01:05s\n",
            "epoch 41 | loss: 0.25957 | train_logloss: 0.24666 | train_accuracy: 0.90407 | valid_logloss: 0.29711 | valid_accuracy: 0.88943 |  0:01:06s\n",
            "epoch 42 | loss: 0.25297 | train_logloss: 0.24978 | train_accuracy: 0.90277 | valid_logloss: 0.30109 | valid_accuracy: 0.88813 |  0:01:08s\n",
            "epoch 43 | loss: 0.25328 | train_logloss: 0.24026 | train_accuracy: 0.9057  | valid_logloss: 0.29791 | valid_accuracy: 0.88957 |  0:01:10s\n",
            "epoch 44 | loss: 0.25606 | train_logloss: 0.25127 | train_accuracy: 0.90307 | valid_logloss: 0.3076  | valid_accuracy: 0.88537 |  0:01:11s\n",
            "epoch 45 | loss: 0.25609 | train_logloss: 0.24682 | train_accuracy: 0.90383 | valid_logloss: 0.30369 | valid_accuracy: 0.8887  |  0:01:13s\n",
            "epoch 46 | loss: 0.25034 | train_logloss: 0.23985 | train_accuracy: 0.9069  | valid_logloss: 0.30083 | valid_accuracy: 0.89083 |  0:01:14s\n",
            "epoch 47 | loss: 0.24797 | train_logloss: 0.24048 | train_accuracy: 0.9074  | valid_logloss: 0.3006  | valid_accuracy: 0.88963 |  0:01:16s\n",
            "epoch 48 | loss: 0.25109 | train_logloss: 0.24268 | train_accuracy: 0.90657 | valid_logloss: 0.30486 | valid_accuracy: 0.88903 |  0:01:18s\n",
            "epoch 49 | loss: 0.25102 | train_logloss: 0.24309 | train_accuracy: 0.9064  | valid_logloss: 0.30837 | valid_accuracy: 0.88833 |  0:01:19s\n",
            "epoch 50 | loss: 0.25224 | train_logloss: 0.2485  | train_accuracy: 0.90407 | valid_logloss: 0.31019 | valid_accuracy: 0.88427 |  0:01:21s\n",
            "epoch 51 | loss: 0.25019 | train_logloss: 0.24065 | train_accuracy: 0.90533 | valid_logloss: 0.30463 | valid_accuracy: 0.8896  |  0:01:22s\n",
            "epoch 52 | loss: 0.24656 | train_logloss: 0.2307  | train_accuracy: 0.91097 | valid_logloss: 0.30465 | valid_accuracy: 0.8892  |  0:01:24s\n",
            "epoch 53 | loss: 0.24424 | train_logloss: 0.23967 | train_accuracy: 0.90627 | valid_logloss: 0.30471 | valid_accuracy: 0.89063 |  0:01:26s\n",
            "epoch 54 | loss: 0.2461  | train_logloss: 0.24324 | train_accuracy: 0.90643 | valid_logloss: 0.31383 | valid_accuracy: 0.88823 |  0:01:27s\n",
            "epoch 55 | loss: 0.24916 | train_logloss: 0.23904 | train_accuracy: 0.9081  | valid_logloss: 0.30828 | valid_accuracy: 0.88883 |  0:01:29s\n",
            "epoch 56 | loss: 0.24739 | train_logloss: 0.23448 | train_accuracy: 0.90887 | valid_logloss: 0.31015 | valid_accuracy: 0.88807 |  0:01:31s\n",
            "epoch 57 | loss: 0.24598 | train_logloss: 0.23369 | train_accuracy: 0.91047 | valid_logloss: 0.30944 | valid_accuracy: 0.88847 |  0:01:32s\n",
            "epoch 58 | loss: 0.24714 | train_logloss: 0.23352 | train_accuracy: 0.9085  | valid_logloss: 0.30644 | valid_accuracy: 0.8894  |  0:01:34s\n",
            "epoch 59 | loss: 0.24613 | train_logloss: 0.2373  | train_accuracy: 0.9079  | valid_logloss: 0.30617 | valid_accuracy: 0.88863 |  0:01:35s\n",
            "epoch 60 | loss: 0.24218 | train_logloss: 0.22919 | train_accuracy: 0.91163 | valid_logloss: 0.30369 | valid_accuracy: 0.89073 |  0:01:37s\n",
            "epoch 61 | loss: 0.24196 | train_logloss: 0.22888 | train_accuracy: 0.91317 | valid_logloss: 0.30795 | valid_accuracy: 0.88707 |  0:01:38s\n",
            "epoch 62 | loss: 0.24117 | train_logloss: 0.23111 | train_accuracy: 0.90977 | valid_logloss: 0.30653 | valid_accuracy: 0.88883 |  0:01:40s\n",
            "epoch 63 | loss: 0.24151 | train_logloss: 0.22783 | train_accuracy: 0.91127 | valid_logloss: 0.30817 | valid_accuracy: 0.88817 |  0:01:42s\n",
            "epoch 64 | loss: 0.24689 | train_logloss: 0.25441 | train_accuracy: 0.9024  | valid_logloss: 0.32106 | valid_accuracy: 0.88523 |  0:01:43s\n",
            "epoch 65 | loss: 0.26073 | train_logloss: 0.24474 | train_accuracy: 0.9034  | valid_logloss: 0.30644 | valid_accuracy: 0.89037 |  0:01:45s\n",
            "epoch 66 | loss: 0.25074 | train_logloss: 0.23878 | train_accuracy: 0.90703 | valid_logloss: 0.30699 | valid_accuracy: 0.8892  |  0:01:46s\n",
            "epoch 67 | loss: 0.24481 | train_logloss: 0.23022 | train_accuracy: 0.90943 | valid_logloss: 0.30348 | valid_accuracy: 0.88967 |  0:01:48s\n",
            "epoch 68 | loss: 0.24071 | train_logloss: 0.23372 | train_accuracy: 0.90873 | valid_logloss: 0.30591 | valid_accuracy: 0.88743 |  0:01:50s\n",
            "epoch 69 | loss: 0.24946 | train_logloss: 0.2421  | train_accuracy: 0.90687 | valid_logloss: 0.3072  | valid_accuracy: 0.88793 |  0:01:51s\n",
            "epoch 70 | loss: 0.24434 | train_logloss: 0.23373 | train_accuracy: 0.90917 | valid_logloss: 0.30339 | valid_accuracy: 0.88993 |  0:01:53s\n",
            "epoch 71 | loss: 0.24311 | train_logloss: 0.2337  | train_accuracy: 0.90937 | valid_logloss: 0.30298 | valid_accuracy: 0.89047 |  0:01:54s\n",
            "epoch 72 | loss: 0.25989 | train_logloss: 0.24889 | train_accuracy: 0.9047  | valid_logloss: 0.30539 | valid_accuracy: 0.88787 |  0:01:56s\n",
            "epoch 73 | loss: 0.25501 | train_logloss: 0.24494 | train_accuracy: 0.90443 | valid_logloss: 0.30845 | valid_accuracy: 0.88653 |  0:01:58s\n",
            "epoch 74 | loss: 0.24917 | train_logloss: 0.23304 | train_accuracy: 0.90927 | valid_logloss: 0.29757 | valid_accuracy: 0.89063 |  0:01:59s\n",
            "epoch 75 | loss: 0.24532 | train_logloss: 0.22756 | train_accuracy: 0.91087 | valid_logloss: 0.30487 | valid_accuracy: 0.88717 |  0:02:01s\n",
            "epoch 76 | loss: 0.23789 | train_logloss: 0.22774 | train_accuracy: 0.91123 | valid_logloss: 0.30376 | valid_accuracy: 0.8906  |  0:02:02s\n",
            "epoch 77 | loss: 0.23767 | train_logloss: 0.22236 | train_accuracy: 0.9126  | valid_logloss: 0.30741 | valid_accuracy: 0.89003 |  0:02:04s\n",
            "epoch 78 | loss: 0.2378  | train_logloss: 0.22221 | train_accuracy: 0.91203 | valid_logloss: 0.30335 | valid_accuracy: 0.89047 |  0:02:06s\n",
            "epoch 79 | loss: 0.23598 | train_logloss: 0.22302 | train_accuracy: 0.9126  | valid_logloss: 0.31282 | valid_accuracy: 0.8861  |  0:02:07s\n",
            "epoch 80 | loss: 0.2354  | train_logloss: 0.23323 | train_accuracy: 0.90743 | valid_logloss: 0.31787 | valid_accuracy: 0.88623 |  0:02:09s\n",
            "epoch 81 | loss: 0.23833 | train_logloss: 0.22512 | train_accuracy: 0.91183 | valid_logloss: 0.31225 | valid_accuracy: 0.8879  |  0:02:10s\n",
            "epoch 82 | loss: 0.23431 | train_logloss: 0.22375 | train_accuracy: 0.91313 | valid_logloss: 0.31459 | valid_accuracy: 0.88593 |  0:02:12s\n",
            "epoch 83 | loss: 0.23525 | train_logloss: 0.22461 | train_accuracy: 0.91177 | valid_logloss: 0.31115 | valid_accuracy: 0.88827 |  0:02:14s\n",
            "epoch 84 | loss: 0.23213 | train_logloss: 0.22219 | train_accuracy: 0.9139  | valid_logloss: 0.31074 | valid_accuracy: 0.887   |  0:02:15s\n",
            "epoch 85 | loss: 0.22845 | train_logloss: 0.21538 | train_accuracy: 0.91457 | valid_logloss: 0.30896 | valid_accuracy: 0.8894  |  0:02:17s\n",
            "epoch 86 | loss: 0.22596 | train_logloss: 0.20994 | train_accuracy: 0.91637 | valid_logloss: 0.31454 | valid_accuracy: 0.88933 |  0:02:19s\n",
            "epoch 87 | loss: 0.2222  | train_logloss: 0.20753 | train_accuracy: 0.9198  | valid_logloss: 0.31779 | valid_accuracy: 0.88723 |  0:02:20s\n",
            "epoch 88 | loss: 0.22213 | train_logloss: 0.21591 | train_accuracy: 0.91367 | valid_logloss: 0.319   | valid_accuracy: 0.88697 |  0:02:22s\n",
            "epoch 89 | loss: 0.2248  | train_logloss: 0.22132 | train_accuracy: 0.91223 | valid_logloss: 0.32317 | valid_accuracy: 0.88733 |  0:02:24s\n",
            "epoch 90 | loss: 0.23105 | train_logloss: 0.21612 | train_accuracy: 0.9153  | valid_logloss: 0.31653 | valid_accuracy: 0.88683 |  0:02:25s\n",
            "epoch 91 | loss: 0.22671 | train_logloss: 0.21115 | train_accuracy: 0.9172  | valid_logloss: 0.31153 | valid_accuracy: 0.88743 |  0:02:27s\n",
            "epoch 92 | loss: 0.22346 | train_logloss: 0.20663 | train_accuracy: 0.9188  | valid_logloss: 0.32332 | valid_accuracy: 0.88657 |  0:02:28s\n",
            "epoch 93 | loss: 0.21903 | train_logloss: 0.206   | train_accuracy: 0.9173  | valid_logloss: 0.32889 | valid_accuracy: 0.88733 |  0:02:30s\n",
            "epoch 94 | loss: 0.21815 | train_logloss: 0.20233 | train_accuracy: 0.9198  | valid_logloss: 0.32306 | valid_accuracy: 0.88823 |  0:02:32s\n",
            "epoch 95 | loss: 0.21441 | train_logloss: 0.20266 | train_accuracy: 0.9196  | valid_logloss: 0.32242 | valid_accuracy: 0.88853 |  0:02:33s\n",
            "epoch 96 | loss: 0.216   | train_logloss: 0.20427 | train_accuracy: 0.9182  | valid_logloss: 0.32594 | valid_accuracy: 0.88753 |  0:02:35s\n",
            "\n",
            "Early stopping occurred at epoch 96 with best_epoch = 46 and best_valid_accuracy = 0.89083\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtGVjktM0GXU",
        "outputId": "e6a8d8ab-592b-44db-b22e-311f8b2393c2"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb6.pkl' 'tn6.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    250.9 MiB    250.9 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    256.5 MiB      5.7 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2320.1 MiB   2063.6 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9w7lgB0IYng",
        "outputId": "45c5117c-182c-4573-d443-c8813e333853"
      },
      "source": [
        "memory(number_exp=7, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.79359 | train_logloss: 15.44778| train_accuracy: 0.33477 | valid_logloss: 15.47836| valid_accuracy: 0.33477 |  0:00:01s\n",
            "epoch 1  | loss: 0.42554 | train_logloss: 22.44025| train_accuracy: 0.33337 | valid_logloss: 22.46656| valid_accuracy: 0.33317 |  0:00:03s\n",
            "epoch 2  | loss: 0.35965 | train_logloss: 12.26977| train_accuracy: 0.3281  | valid_logloss: 12.2782 | valid_accuracy: 0.3278  |  0:00:05s\n",
            "epoch 3  | loss: 0.34007 | train_logloss: 5.79972 | train_accuracy: 0.3622  | valid_logloss: 5.79605 | valid_accuracy: 0.36187 |  0:00:06s\n",
            "epoch 4  | loss: 0.32537 | train_logloss: 3.26044 | train_accuracy: 0.46783 | valid_logloss: 3.29715 | valid_accuracy: 0.4634  |  0:00:08s\n",
            "epoch 5  | loss: 0.31751 | train_logloss: 5.39595 | train_accuracy: 0.31773 | valid_logloss: 5.41002 | valid_accuracy: 0.31917 |  0:00:09s\n",
            "epoch 6  | loss: 0.31533 | train_logloss: 4.50698 | train_accuracy: 0.31837 | valid_logloss: 4.50554 | valid_accuracy: 0.3204  |  0:00:11s\n",
            "epoch 7  | loss: 0.30677 | train_logloss: 3.2863  | train_accuracy: 0.33573 | valid_logloss: 3.3033  | valid_accuracy: 0.3354  |  0:00:13s\n",
            "epoch 8  | loss: 0.30873 | train_logloss: 2.68492 | train_accuracy: 0.35813 | valid_logloss: 2.70148 | valid_accuracy: 0.35693 |  0:00:14s\n",
            "epoch 9  | loss: 0.30147 | train_logloss: 2.91223 | train_accuracy: 0.40167 | valid_logloss: 2.93773 | valid_accuracy: 0.39707 |  0:00:16s\n",
            "epoch 10 | loss: 0.29434 | train_logloss: 1.61735 | train_accuracy: 0.52577 | valid_logloss: 1.62845 | valid_accuracy: 0.52507 |  0:00:18s\n",
            "epoch 11 | loss: 0.29324 | train_logloss: 1.3859  | train_accuracy: 0.4636  | valid_logloss: 1.40105 | valid_accuracy: 0.4581  |  0:00:19s\n",
            "epoch 12 | loss: 0.29139 | train_logloss: 1.02658 | train_accuracy: 0.5322  | valid_logloss: 1.03835 | valid_accuracy: 0.5285  |  0:00:21s\n",
            "epoch 13 | loss: 0.29132 | train_logloss: 1.44733 | train_accuracy: 0.62137 | valid_logloss: 1.46033 | valid_accuracy: 0.61743 |  0:00:23s\n",
            "epoch 14 | loss: 0.28904 | train_logloss: 0.8384  | train_accuracy: 0.7256  | valid_logloss: 0.84476 | valid_accuracy: 0.72307 |  0:00:24s\n",
            "epoch 15 | loss: 0.28373 | train_logloss: 0.85121 | train_accuracy: 0.66403 | valid_logloss: 0.85675 | valid_accuracy: 0.65807 |  0:00:26s\n",
            "epoch 16 | loss: 0.2789  | train_logloss: 0.6688  | train_accuracy: 0.7656  | valid_logloss: 0.67791 | valid_accuracy: 0.75947 |  0:00:28s\n",
            "epoch 17 | loss: 0.28185 | train_logloss: 0.47702 | train_accuracy: 0.8336  | valid_logloss: 0.49217 | valid_accuracy: 0.82593 |  0:00:29s\n",
            "epoch 18 | loss: 0.27548 | train_logloss: 0.48007 | train_accuracy: 0.8245  | valid_logloss: 0.49615 | valid_accuracy: 0.81793 |  0:00:31s\n",
            "epoch 19 | loss: 0.27358 | train_logloss: 0.42041 | train_accuracy: 0.8531  | valid_logloss: 0.43445 | valid_accuracy: 0.84933 |  0:00:33s\n",
            "epoch 20 | loss: 0.26928 | train_logloss: 0.40095 | train_accuracy: 0.844   | valid_logloss: 0.4172  | valid_accuracy: 0.83973 |  0:00:34s\n",
            "epoch 21 | loss: 0.27007 | train_logloss: 0.36678 | train_accuracy: 0.86093 | valid_logloss: 0.3876  | valid_accuracy: 0.85657 |  0:00:36s\n",
            "epoch 22 | loss: 0.26547 | train_logloss: 0.32718 | train_accuracy: 0.8788  | valid_logloss: 0.35201 | valid_accuracy: 0.876   |  0:00:38s\n",
            "epoch 23 | loss: 0.26777 | train_logloss: 0.30843 | train_accuracy: 0.88503 | valid_logloss: 0.33385 | valid_accuracy: 0.87827 |  0:00:40s\n",
            "epoch 24 | loss: 0.26605 | train_logloss: 0.2887  | train_accuracy: 0.88963 | valid_logloss: 0.31845 | valid_accuracy: 0.88373 |  0:00:41s\n",
            "epoch 25 | loss: 0.26306 | train_logloss: 0.27829 | train_accuracy: 0.89303 | valid_logloss: 0.31631 | valid_accuracy: 0.88473 |  0:00:43s\n",
            "epoch 26 | loss: 0.26043 | train_logloss: 0.2755  | train_accuracy: 0.89407 | valid_logloss: 0.31676 | valid_accuracy: 0.8852  |  0:00:45s\n",
            "epoch 27 | loss: 0.26189 | train_logloss: 0.2585  | train_accuracy: 0.89967 | valid_logloss: 0.30405 | valid_accuracy: 0.8873  |  0:00:46s\n",
            "epoch 28 | loss: 0.26169 | train_logloss: 0.26381 | train_accuracy: 0.8972  | valid_logloss: 0.30764 | valid_accuracy: 0.88713 |  0:00:48s\n",
            "epoch 29 | loss: 0.26255 | train_logloss: 0.26428 | train_accuracy: 0.8991  | valid_logloss: 0.30946 | valid_accuracy: 0.88917 |  0:00:50s\n",
            "epoch 30 | loss: 0.26245 | train_logloss: 0.25599 | train_accuracy: 0.90183 | valid_logloss: 0.30148 | valid_accuracy: 0.89003 |  0:00:52s\n",
            "epoch 31 | loss: 0.25836 | train_logloss: 0.25031 | train_accuracy: 0.90307 | valid_logloss: 0.30817 | valid_accuracy: 0.8878  |  0:00:53s\n",
            "epoch 32 | loss: 0.25958 | train_logloss: 0.25021 | train_accuracy: 0.90253 | valid_logloss: 0.30012 | valid_accuracy: 0.88913 |  0:00:55s\n",
            "epoch 33 | loss: 0.25676 | train_logloss: 0.25046 | train_accuracy: 0.9012  | valid_logloss: 0.30374 | valid_accuracy: 0.8884  |  0:00:57s\n",
            "epoch 34 | loss: 0.25389 | train_logloss: 0.24458 | train_accuracy: 0.90557 | valid_logloss: 0.3027  | valid_accuracy: 0.88917 |  0:00:58s\n",
            "epoch 35 | loss: 0.25389 | train_logloss: 0.24661 | train_accuracy: 0.90397 | valid_logloss: 0.30201 | valid_accuracy: 0.88977 |  0:01:00s\n",
            "epoch 36 | loss: 0.25546 | train_logloss: 0.25023 | train_accuracy: 0.90377 | valid_logloss: 0.30482 | valid_accuracy: 0.88763 |  0:01:02s\n",
            "epoch 37 | loss: 0.26177 | train_logloss: 0.2526  | train_accuracy: 0.89953 | valid_logloss: 0.30358 | valid_accuracy: 0.88703 |  0:01:03s\n",
            "epoch 38 | loss: 0.25881 | train_logloss: 0.24518 | train_accuracy: 0.9052  | valid_logloss: 0.29718 | valid_accuracy: 0.89083 |  0:01:05s\n",
            "epoch 39 | loss: 0.25767 | train_logloss: 0.24799 | train_accuracy: 0.90333 | valid_logloss: 0.30333 | valid_accuracy: 0.8905  |  0:01:07s\n",
            "epoch 40 | loss: 0.25548 | train_logloss: 0.2453  | train_accuracy: 0.9029  | valid_logloss: 0.29998 | valid_accuracy: 0.8896  |  0:01:09s\n",
            "epoch 41 | loss: 0.25174 | train_logloss: 0.2406  | train_accuracy: 0.90517 | valid_logloss: 0.30059 | valid_accuracy: 0.88997 |  0:01:10s\n",
            "epoch 42 | loss: 0.25108 | train_logloss: 0.23984 | train_accuracy: 0.9066  | valid_logloss: 0.30205 | valid_accuracy: 0.89153 |  0:01:12s\n",
            "epoch 43 | loss: 0.24897 | train_logloss: 0.2385  | train_accuracy: 0.90817 | valid_logloss: 0.30428 | valid_accuracy: 0.8916  |  0:01:14s\n",
            "epoch 44 | loss: 0.24498 | train_logloss: 0.2337  | train_accuracy: 0.90923 | valid_logloss: 0.29929 | valid_accuracy: 0.8917  |  0:01:15s\n",
            "epoch 45 | loss: 0.24234 | train_logloss: 0.23411 | train_accuracy: 0.9097  | valid_logloss: 0.30068 | valid_accuracy: 0.8906  |  0:01:17s\n",
            "epoch 46 | loss: 0.24475 | train_logloss: 0.23368 | train_accuracy: 0.90753 | valid_logloss: 0.30605 | valid_accuracy: 0.8894  |  0:01:19s\n",
            "epoch 47 | loss: 0.24618 | train_logloss: 0.23277 | train_accuracy: 0.90883 | valid_logloss: 0.29665 | valid_accuracy: 0.89093 |  0:01:20s\n",
            "epoch 48 | loss: 0.24497 | train_logloss: 0.2379  | train_accuracy: 0.9079  | valid_logloss: 0.30145 | valid_accuracy: 0.89063 |  0:01:22s\n",
            "epoch 49 | loss: 0.24839 | train_logloss: 0.23948 | train_accuracy: 0.90613 | valid_logloss: 0.30505 | valid_accuracy: 0.88773 |  0:01:23s\n",
            "epoch 50 | loss: 0.2463  | train_logloss: 0.23881 | train_accuracy: 0.9067  | valid_logloss: 0.30594 | valid_accuracy: 0.89043 |  0:01:25s\n",
            "epoch 51 | loss: 0.24446 | train_logloss: 0.23717 | train_accuracy: 0.90827 | valid_logloss: 0.30703 | valid_accuracy: 0.88883 |  0:01:27s\n",
            "epoch 52 | loss: 0.24869 | train_logloss: 0.24203 | train_accuracy: 0.90557 | valid_logloss: 0.30739 | valid_accuracy: 0.89197 |  0:01:28s\n",
            "epoch 53 | loss: 0.25123 | train_logloss: 0.23404 | train_accuracy: 0.90973 | valid_logloss: 0.30154 | valid_accuracy: 0.892   |  0:01:30s\n",
            "epoch 54 | loss: 0.24388 | train_logloss: 0.23062 | train_accuracy: 0.91053 | valid_logloss: 0.30173 | valid_accuracy: 0.8923  |  0:01:31s\n",
            "epoch 55 | loss: 0.24478 | train_logloss: 0.23268 | train_accuracy: 0.9084  | valid_logloss: 0.30649 | valid_accuracy: 0.8893  |  0:01:33s\n",
            "epoch 56 | loss: 0.24706 | train_logloss: 0.24197 | train_accuracy: 0.9055  | valid_logloss: 0.31403 | valid_accuracy: 0.88713 |  0:01:34s\n",
            "epoch 57 | loss: 0.25383 | train_logloss: 0.24185 | train_accuracy: 0.90573 | valid_logloss: 0.30404 | valid_accuracy: 0.89    |  0:01:36s\n",
            "epoch 58 | loss: 0.24967 | train_logloss: 0.23638 | train_accuracy: 0.9076  | valid_logloss: 0.30362 | valid_accuracy: 0.89167 |  0:01:37s\n",
            "epoch 59 | loss: 0.24497 | train_logloss: 0.23835 | train_accuracy: 0.9068  | valid_logloss: 0.31302 | valid_accuracy: 0.88753 |  0:01:39s\n",
            "epoch 60 | loss: 0.24018 | train_logloss: 0.22958 | train_accuracy: 0.90843 | valid_logloss: 0.30496 | valid_accuracy: 0.8913  |  0:01:40s\n",
            "epoch 61 | loss: 0.23455 | train_logloss: 0.22123 | train_accuracy: 0.91313 | valid_logloss: 0.30849 | valid_accuracy: 0.8905  |  0:01:42s\n",
            "epoch 62 | loss: 0.2332  | train_logloss: 0.22367 | train_accuracy: 0.9133  | valid_logloss: 0.31857 | valid_accuracy: 0.89063 |  0:01:43s\n",
            "epoch 63 | loss: 0.23411 | train_logloss: 0.22364 | train_accuracy: 0.91067 | valid_logloss: 0.30946 | valid_accuracy: 0.89127 |  0:01:45s\n",
            "epoch 64 | loss: 0.238   | train_logloss: 0.22348 | train_accuracy: 0.91117 | valid_logloss: 0.31683 | valid_accuracy: 0.88853 |  0:01:46s\n",
            "epoch 65 | loss: 0.23534 | train_logloss: 0.228   | train_accuracy: 0.9105  | valid_logloss: 0.31744 | valid_accuracy: 0.8887  |  0:01:48s\n",
            "epoch 66 | loss: 0.23711 | train_logloss: 0.22069 | train_accuracy: 0.91253 | valid_logloss: 0.30627 | valid_accuracy: 0.89187 |  0:01:49s\n",
            "epoch 67 | loss: 0.23319 | train_logloss: 0.22268 | train_accuracy: 0.91117 | valid_logloss: 0.31411 | valid_accuracy: 0.8905  |  0:01:51s\n",
            "epoch 68 | loss: 0.23191 | train_logloss: 0.21853 | train_accuracy: 0.9141  | valid_logloss: 0.31164 | valid_accuracy: 0.8893  |  0:01:52s\n",
            "epoch 69 | loss: 0.23042 | train_logloss: 0.21363 | train_accuracy: 0.91447 | valid_logloss: 0.31137 | valid_accuracy: 0.891   |  0:01:54s\n",
            "epoch 70 | loss: 0.22882 | train_logloss: 0.21618 | train_accuracy: 0.91367 | valid_logloss: 0.31632 | valid_accuracy: 0.88883 |  0:01:55s\n",
            "epoch 71 | loss: 0.2233  | train_logloss: 0.21142 | train_accuracy: 0.91587 | valid_logloss: 0.32107 | valid_accuracy: 0.8904  |  0:01:57s\n",
            "epoch 72 | loss: 0.22758 | train_logloss: 0.2192  | train_accuracy: 0.91417 | valid_logloss: 0.31569 | valid_accuracy: 0.88727 |  0:01:58s\n",
            "epoch 73 | loss: 0.23087 | train_logloss: 0.21684 | train_accuracy: 0.91237 | valid_logloss: 0.31926 | valid_accuracy: 0.8883  |  0:02:00s\n",
            "epoch 74 | loss: 0.22484 | train_logloss: 0.2055  | train_accuracy: 0.91817 | valid_logloss: 0.31241 | valid_accuracy: 0.8923  |  0:02:01s\n",
            "epoch 75 | loss: 0.22223 | train_logloss: 0.21643 | train_accuracy: 0.91393 | valid_logloss: 0.31864 | valid_accuracy: 0.89073 |  0:02:02s\n",
            "epoch 76 | loss: 0.21979 | train_logloss: 0.2079  | train_accuracy: 0.91747 | valid_logloss: 0.32905 | valid_accuracy: 0.8891  |  0:02:04s\n",
            "epoch 77 | loss: 0.22384 | train_logloss: 0.2123  | train_accuracy: 0.9143  | valid_logloss: 0.33187 | valid_accuracy: 0.88557 |  0:02:05s\n",
            "epoch 78 | loss: 0.22349 | train_logloss: 0.21522 | train_accuracy: 0.91423 | valid_logloss: 0.32448 | valid_accuracy: 0.88837 |  0:02:07s\n",
            "epoch 79 | loss: 0.22886 | train_logloss: 0.21819 | train_accuracy: 0.91437 | valid_logloss: 0.31961 | valid_accuracy: 0.88913 |  0:02:08s\n",
            "epoch 80 | loss: 0.22519 | train_logloss: 0.20985 | train_accuracy: 0.91563 | valid_logloss: 0.3245  | valid_accuracy: 0.8905  |  0:02:10s\n",
            "epoch 81 | loss: 0.2175  | train_logloss: 0.20254 | train_accuracy: 0.9202  | valid_logloss: 0.32864 | valid_accuracy: 0.89027 |  0:02:11s\n",
            "epoch 82 | loss: 0.21537 | train_logloss: 0.20684 | train_accuracy: 0.9179  | valid_logloss: 0.33416 | valid_accuracy: 0.88527 |  0:02:13s\n",
            "epoch 83 | loss: 0.21131 | train_logloss: 0.1983  | train_accuracy: 0.92037 | valid_logloss: 0.33141 | valid_accuracy: 0.88913 |  0:02:14s\n",
            "epoch 84 | loss: 0.21523 | train_logloss: 0.20074 | train_accuracy: 0.91953 | valid_logloss: 0.32987 | valid_accuracy: 0.89027 |  0:02:16s\n",
            "epoch 85 | loss: 0.21012 | train_logloss: 0.19345 | train_accuracy: 0.92397 | valid_logloss: 0.33412 | valid_accuracy: 0.88713 |  0:02:17s\n",
            "epoch 86 | loss: 0.21073 | train_logloss: 0.20084 | train_accuracy: 0.92003 | valid_logloss: 0.3421  | valid_accuracy: 0.88423 |  0:02:19s\n",
            "epoch 87 | loss: 0.2128  | train_logloss: 0.19826 | train_accuracy: 0.92093 | valid_logloss: 0.33535 | valid_accuracy: 0.888   |  0:02:20s\n",
            "epoch 88 | loss: 0.212   | train_logloss: 0.21402 | train_accuracy: 0.91387 | valid_logloss: 0.3464  | valid_accuracy: 0.88597 |  0:02:22s\n",
            "epoch 89 | loss: 0.21115 | train_logloss: 0.20069 | train_accuracy: 0.91997 | valid_logloss: 0.33834 | valid_accuracy: 0.88657 |  0:02:23s\n",
            "epoch 90 | loss: 0.20698 | train_logloss: 0.19017 | train_accuracy: 0.92457 | valid_logloss: 0.34375 | valid_accuracy: 0.8875  |  0:02:25s\n",
            "epoch 91 | loss: 0.20605 | train_logloss: 0.19094 | train_accuracy: 0.9234  | valid_logloss: 0.34153 | valid_accuracy: 0.88613 |  0:02:26s\n",
            "epoch 92 | loss: 0.20664 | train_logloss: 0.19037 | train_accuracy: 0.92283 | valid_logloss: 0.336   | valid_accuracy: 0.88907 |  0:02:28s\n",
            "epoch 93 | loss: 0.20459 | train_logloss: 0.19482 | train_accuracy: 0.9224  | valid_logloss: 0.3491  | valid_accuracy: 0.8855  |  0:02:29s\n",
            "epoch 94 | loss: 0.20525 | train_logloss: 0.19032 | train_accuracy: 0.9235  | valid_logloss: 0.35517 | valid_accuracy: 0.8838  |  0:02:31s\n",
            "epoch 95 | loss: 0.20186 | train_logloss: 0.19056 | train_accuracy: 0.92453 | valid_logloss: 0.34896 | valid_accuracy: 0.8863  |  0:02:32s\n",
            "epoch 96 | loss: 0.20005 | train_logloss: 0.19149 | train_accuracy: 0.92213 | valid_logloss: 0.35078 | valid_accuracy: 0.8843  |  0:02:34s\n",
            "epoch 97 | loss: 0.20041 | train_logloss: 0.19384 | train_accuracy: 0.92383 | valid_logloss: 0.35476 | valid_accuracy: 0.88387 |  0:02:35s\n",
            "epoch 98 | loss: 0.21803 | train_logloss: 0.20454 | train_accuracy: 0.9192  | valid_logloss: 0.3369  | valid_accuracy: 0.88573 |  0:02:37s\n",
            "epoch 99 | loss: 0.21213 | train_logloss: 0.19357 | train_accuracy: 0.92327 | valid_logloss: 0.33846 | valid_accuracy: 0.88827 |  0:02:38s\n",
            "epoch 100| loss: 0.20318 | train_logloss: 0.18808 | train_accuracy: 0.9255  | valid_logloss: 0.3457  | valid_accuracy: 0.88627 |  0:02:40s\n",
            "epoch 101| loss: 0.21092 | train_logloss: 0.19344 | train_accuracy: 0.9223  | valid_logloss: 0.33589 | valid_accuracy: 0.8869  |  0:02:41s\n",
            "epoch 102| loss: 0.20221 | train_logloss: 0.1844  | train_accuracy: 0.9271  | valid_logloss: 0.34939 | valid_accuracy: 0.8845  |  0:02:43s\n",
            "epoch 103| loss: 0.19594 | train_logloss: 0.17713 | train_accuracy: 0.92877 | valid_logloss: 0.35972 | valid_accuracy: 0.8858  |  0:02:44s\n",
            "epoch 104| loss: 0.19272 | train_logloss: 0.17574 | train_accuracy: 0.92913 | valid_logloss: 0.3654  | valid_accuracy: 0.8827  |  0:02:46s\n",
            "\n",
            "Early stopping occurred at epoch 104 with best_epoch = 54 and best_valid_accuracy = 0.8923\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mQDonDa0Ksa",
        "outputId": "a472ea6d-9aba-4a35-a27f-f421bc3385c3"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb7.pkl' 'tn7.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    254.0 MiB    254.0 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.6 MiB      5.7 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2332.1 MiB   2072.5 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFy9iLyKIlVO"
      },
      "source": [
        "ones(number_exp=8, \n",
        "     Rows=30000, \n",
        "     Nd=128,\tNa=128,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YzTgGm7JB0s"
      },
      "source": [
        "ones(number_exp=9, \n",
        "     Rows=30000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=7,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=20,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weawcLGoJaYl",
        "outputId": "483cd3ed-f9cd-41ce-8cbe-1de7c5e91050"
      },
      "source": [
        "memory(number_exp=10, \n",
        "     Rows=9000, \n",
        "     Nd=64,\tNa=64,\t\n",
        "     B=512,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.04583 | train_logloss: 11.84129| train_accuracy: 0.354   | valid_logloss: 11.79447| valid_accuracy: 0.35944 |  0:00:01s\n",
            "epoch 1  | loss: 0.61206 | train_logloss: 8.09429 | train_accuracy: 0.61311 | valid_logloss: 7.91257 | valid_accuracy: 0.62033 |  0:00:03s\n",
            "epoch 2  | loss: 0.52071 | train_logloss: 20.06188| train_accuracy: 0.35133 | valid_logloss: 20.21294| valid_accuracy: 0.35156 |  0:00:05s\n",
            "epoch 3  | loss: 0.46929 | train_logloss: 8.4903  | train_accuracy: 0.22744 | valid_logloss: 8.40832 | valid_accuracy: 0.22922 |  0:00:07s\n",
            "epoch 4  | loss: 0.41851 | train_logloss: 1.92663 | train_accuracy: 0.55256 | valid_logloss: 1.9921  | valid_accuracy: 0.54578 |  0:00:09s\n",
            "epoch 5  | loss: 0.41071 | train_logloss: 1.43395 | train_accuracy: 0.52244 | valid_logloss: 1.43481 | valid_accuracy: 0.53011 |  0:00:11s\n",
            "epoch 6  | loss: 0.40321 | train_logloss: 1.31418 | train_accuracy: 0.51211 | valid_logloss: 1.28267 | valid_accuracy: 0.52656 |  0:00:13s\n",
            "epoch 7  | loss: 0.39996 | train_logloss: 1.47851 | train_accuracy: 0.57622 | valid_logloss: 1.48036 | valid_accuracy: 0.57933 |  0:00:15s\n",
            "epoch 8  | loss: 0.40898 | train_logloss: 0.98344 | train_accuracy: 0.64356 | valid_logloss: 0.97977 | valid_accuracy: 0.63956 |  0:00:17s\n",
            "epoch 9  | loss: 0.3975  | train_logloss: 0.98865 | train_accuracy: 0.64433 | valid_logloss: 0.97478 | valid_accuracy: 0.64611 |  0:00:19s\n",
            "epoch 10 | loss: 0.39431 | train_logloss: 0.63677 | train_accuracy: 0.74133 | valid_logloss: 0.6478  | valid_accuracy: 0.73833 |  0:00:21s\n",
            "epoch 11 | loss: 0.38798 | train_logloss: 0.91954 | train_accuracy: 0.63056 | valid_logloss: 0.92723 | valid_accuracy: 0.62789 |  0:00:22s\n",
            "epoch 12 | loss: 0.372   | train_logloss: 0.62025 | train_accuracy: 0.73389 | valid_logloss: 0.64695 | valid_accuracy: 0.72878 |  0:00:24s\n",
            "epoch 13 | loss: 0.36427 | train_logloss: 0.50053 | train_accuracy: 0.79667 | valid_logloss: 0.52873 | valid_accuracy: 0.78767 |  0:00:26s\n",
            "epoch 14 | loss: 0.36237 | train_logloss: 0.56726 | train_accuracy: 0.78744 | valid_logloss: 0.59053 | valid_accuracy: 0.78078 |  0:00:28s\n",
            "epoch 15 | loss: 0.36229 | train_logloss: 0.44811 | train_accuracy: 0.83144 | valid_logloss: 0.47408 | valid_accuracy: 0.82278 |  0:00:30s\n",
            "epoch 16 | loss: 0.36508 | train_logloss: 0.39371 | train_accuracy: 0.853   | valid_logloss: 0.42392 | valid_accuracy: 0.83978 |  0:00:32s\n",
            "epoch 17 | loss: 0.3555  | train_logloss: 0.38993 | train_accuracy: 0.851   | valid_logloss: 0.41895 | valid_accuracy: 0.843   |  0:00:34s\n",
            "epoch 18 | loss: 0.35264 | train_logloss: 0.36284 | train_accuracy: 0.86311 | valid_logloss: 0.3895  | valid_accuracy: 0.85233 |  0:00:37s\n",
            "epoch 19 | loss: 0.36107 | train_logloss: 0.34254 | train_accuracy: 0.86933 | valid_logloss: 0.37496 | valid_accuracy: 0.857   |  0:00:39s\n",
            "epoch 20 | loss: 0.36331 | train_logloss: 0.36227 | train_accuracy: 0.85978 | valid_logloss: 0.39719 | valid_accuracy: 0.85111 |  0:00:41s\n",
            "epoch 21 | loss: 0.36393 | train_logloss: 0.39241 | train_accuracy: 0.84911 | valid_logloss: 0.429   | valid_accuracy: 0.84389 |  0:00:43s\n",
            "epoch 22 | loss: 0.35307 | train_logloss: 0.36009 | train_accuracy: 0.86044 | valid_logloss: 0.39132 | valid_accuracy: 0.85378 |  0:00:45s\n",
            "epoch 23 | loss: 0.34196 | train_logloss: 0.33231 | train_accuracy: 0.87467 | valid_logloss: 0.36496 | valid_accuracy: 0.86733 |  0:00:47s\n",
            "epoch 24 | loss: 0.33722 | train_logloss: 0.3236  | train_accuracy: 0.87678 | valid_logloss: 0.36074 | valid_accuracy: 0.86989 |  0:00:49s\n",
            "epoch 25 | loss: 0.32656 | train_logloss: 0.31357 | train_accuracy: 0.877   | valid_logloss: 0.35068 | valid_accuracy: 0.87056 |  0:00:51s\n",
            "epoch 26 | loss: 0.32546 | train_logloss: 0.32161 | train_accuracy: 0.87689 | valid_logloss: 0.36071 | valid_accuracy: 0.86967 |  0:00:53s\n",
            "epoch 27 | loss: 0.32891 | train_logloss: 0.3132  | train_accuracy: 0.87833 | valid_logloss: 0.35176 | valid_accuracy: 0.86822 |  0:00:55s\n",
            "epoch 28 | loss: 0.32553 | train_logloss: 0.31257 | train_accuracy: 0.87844 | valid_logloss: 0.3569  | valid_accuracy: 0.86733 |  0:00:57s\n",
            "epoch 29 | loss: 0.32456 | train_logloss: 0.31595 | train_accuracy: 0.87911 | valid_logloss: 0.35905 | valid_accuracy: 0.86733 |  0:00:59s\n",
            "epoch 30 | loss: 0.32618 | train_logloss: 0.30924 | train_accuracy: 0.87922 | valid_logloss: 0.35473 | valid_accuracy: 0.86778 |  0:01:01s\n",
            "epoch 31 | loss: 0.31934 | train_logloss: 0.31365 | train_accuracy: 0.87844 | valid_logloss: 0.35525 | valid_accuracy: 0.86967 |  0:01:03s\n",
            "epoch 32 | loss: 0.32174 | train_logloss: 0.31301 | train_accuracy: 0.88133 | valid_logloss: 0.35463 | valid_accuracy: 0.86789 |  0:01:05s\n",
            "epoch 33 | loss: 0.3151  | train_logloss: 0.31321 | train_accuracy: 0.87922 | valid_logloss: 0.36038 | valid_accuracy: 0.86933 |  0:01:07s\n",
            "epoch 34 | loss: 0.31774 | train_logloss: 0.30344 | train_accuracy: 0.88256 | valid_logloss: 0.34917 | valid_accuracy: 0.87178 |  0:01:09s\n",
            "epoch 35 | loss: 0.31183 | train_logloss: 0.29325 | train_accuracy: 0.88533 | valid_logloss: 0.34891 | valid_accuracy: 0.87056 |  0:01:11s\n",
            "epoch 36 | loss: 0.31406 | train_logloss: 0.3     | train_accuracy: 0.88033 | valid_logloss: 0.34982 | valid_accuracy: 0.87111 |  0:01:13s\n",
            "epoch 37 | loss: 0.31161 | train_logloss: 0.30615 | train_accuracy: 0.87811 | valid_logloss: 0.3573  | valid_accuracy: 0.86544 |  0:01:15s\n",
            "epoch 38 | loss: 0.30173 | train_logloss: 0.29461 | train_accuracy: 0.88022 | valid_logloss: 0.35389 | valid_accuracy: 0.86822 |  0:01:17s\n",
            "epoch 39 | loss: 0.30822 | train_logloss: 0.29926 | train_accuracy: 0.886   | valid_logloss: 0.35045 | valid_accuracy: 0.87422 |  0:01:19s\n",
            "epoch 40 | loss: 0.30894 | train_logloss: 0.29511 | train_accuracy: 0.88411 | valid_logloss: 0.3453  | valid_accuracy: 0.87222 |  0:01:21s\n",
            "epoch 41 | loss: 0.3166  | train_logloss: 0.2906  | train_accuracy: 0.88689 | valid_logloss: 0.33706 | valid_accuracy: 0.87578 |  0:01:23s\n",
            "epoch 42 | loss: 0.31188 | train_logloss: 0.28577 | train_accuracy: 0.88556 | valid_logloss: 0.3363  | valid_accuracy: 0.87589 |  0:01:25s\n",
            "epoch 43 | loss: 0.30563 | train_logloss: 0.29005 | train_accuracy: 0.88722 | valid_logloss: 0.3456  | valid_accuracy: 0.87422 |  0:01:27s\n",
            "epoch 44 | loss: 0.305   | train_logloss: 0.29736 | train_accuracy: 0.88522 | valid_logloss: 0.35129 | valid_accuracy: 0.87444 |  0:01:29s\n",
            "epoch 45 | loss: 0.30928 | train_logloss: 0.29258 | train_accuracy: 0.88789 | valid_logloss: 0.34715 | valid_accuracy: 0.87422 |  0:01:31s\n",
            "epoch 46 | loss: 0.3009  | train_logloss: 0.28914 | train_accuracy: 0.88756 | valid_logloss: 0.34172 | valid_accuracy: 0.87444 |  0:01:33s\n",
            "epoch 47 | loss: 0.31248 | train_logloss: 0.3048  | train_accuracy: 0.87978 | valid_logloss: 0.34667 | valid_accuracy: 0.87267 |  0:01:35s\n",
            "epoch 48 | loss: 0.31687 | train_logloss: 0.29751 | train_accuracy: 0.88522 | valid_logloss: 0.34183 | valid_accuracy: 0.87667 |  0:01:37s\n",
            "epoch 49 | loss: 0.31895 | train_logloss: 0.30424 | train_accuracy: 0.88567 | valid_logloss: 0.35005 | valid_accuracy: 0.87644 |  0:01:39s\n",
            "epoch 50 | loss: 0.31699 | train_logloss: 0.30241 | train_accuracy: 0.88367 | valid_logloss: 0.34736 | valid_accuracy: 0.875   |  0:01:41s\n",
            "epoch 51 | loss: 0.30978 | train_logloss: 0.29737 | train_accuracy: 0.887   | valid_logloss: 0.34197 | valid_accuracy: 0.88111 |  0:01:43s\n",
            "epoch 52 | loss: 0.29914 | train_logloss: 0.28576 | train_accuracy: 0.887   | valid_logloss: 0.33582 | valid_accuracy: 0.87956 |  0:01:45s\n",
            "epoch 53 | loss: 0.30161 | train_logloss: 0.28188 | train_accuracy: 0.88956 | valid_logloss: 0.33304 | valid_accuracy: 0.87744 |  0:01:47s\n",
            "epoch 54 | loss: 0.29153 | train_logloss: 0.28149 | train_accuracy: 0.888   | valid_logloss: 0.34981 | valid_accuracy: 0.87622 |  0:01:49s\n",
            "epoch 55 | loss: 0.28735 | train_logloss: 0.28768 | train_accuracy: 0.88822 | valid_logloss: 0.33971 | valid_accuracy: 0.87822 |  0:01:51s\n",
            "epoch 56 | loss: 0.30547 | train_logloss: 0.31377 | train_accuracy: 0.87733 | valid_logloss: 0.36815 | valid_accuracy: 0.867   |  0:01:53s\n",
            "epoch 57 | loss: 0.32209 | train_logloss: 0.29942 | train_accuracy: 0.88689 | valid_logloss: 0.34779 | valid_accuracy: 0.87622 |  0:01:55s\n",
            "epoch 58 | loss: 0.31295 | train_logloss: 0.29109 | train_accuracy: 0.887   | valid_logloss: 0.34116 | valid_accuracy: 0.878   |  0:01:57s\n",
            "epoch 59 | loss: 0.3123  | train_logloss: 0.31084 | train_accuracy: 0.88    | valid_logloss: 0.35884 | valid_accuracy: 0.87    |  0:01:59s\n",
            "epoch 60 | loss: 0.30589 | train_logloss: 0.2953  | train_accuracy: 0.88933 | valid_logloss: 0.3526  | valid_accuracy: 0.87489 |  0:02:01s\n",
            "epoch 61 | loss: 0.30539 | train_logloss: 0.29593 | train_accuracy: 0.88667 | valid_logloss: 0.34411 | valid_accuracy: 0.87911 |  0:02:03s\n",
            "epoch 62 | loss: 0.30813 | train_logloss: 0.29116 | train_accuracy: 0.88689 | valid_logloss: 0.34127 | valid_accuracy: 0.87344 |  0:02:04s\n",
            "epoch 63 | loss: 0.30151 | train_logloss: 0.28024 | train_accuracy: 0.894   | valid_logloss: 0.34282 | valid_accuracy: 0.87256 |  0:02:06s\n",
            "epoch 64 | loss: 0.29391 | train_logloss: 0.28435 | train_accuracy: 0.89344 | valid_logloss: 0.34465 | valid_accuracy: 0.87867 |  0:02:08s\n",
            "epoch 65 | loss: 0.29133 | train_logloss: 0.27433 | train_accuracy: 0.89133 | valid_logloss: 0.34109 | valid_accuracy: 0.87744 |  0:02:10s\n",
            "epoch 66 | loss: 0.29248 | train_logloss: 0.28419 | train_accuracy: 0.89044 | valid_logloss: 0.34152 | valid_accuracy: 0.87644 |  0:02:12s\n",
            "epoch 67 | loss: 0.30378 | train_logloss: 0.27152 | train_accuracy: 0.89744 | valid_logloss: 0.33502 | valid_accuracy: 0.87744 |  0:02:14s\n",
            "epoch 68 | loss: 0.29001 | train_logloss: 0.28401 | train_accuracy: 0.88878 | valid_logloss: 0.33691 | valid_accuracy: 0.87667 |  0:02:16s\n",
            "epoch 69 | loss: 0.30214 | train_logloss: 0.28237 | train_accuracy: 0.893   | valid_logloss: 0.33563 | valid_accuracy: 0.88067 |  0:02:18s\n",
            "epoch 70 | loss: 0.29559 | train_logloss: 0.28766 | train_accuracy: 0.88978 | valid_logloss: 0.33814 | valid_accuracy: 0.87844 |  0:02:19s\n",
            "epoch 71 | loss: 0.29486 | train_logloss: 0.27996 | train_accuracy: 0.89267 | valid_logloss: 0.33691 | valid_accuracy: 0.88022 |  0:02:21s\n",
            "epoch 72 | loss: 0.29686 | train_logloss: 0.28444 | train_accuracy: 0.89144 | valid_logloss: 0.3439  | valid_accuracy: 0.88111 |  0:02:23s\n",
            "epoch 73 | loss: 0.29667 | train_logloss: 0.28432 | train_accuracy: 0.89078 | valid_logloss: 0.33367 | valid_accuracy: 0.88033 |  0:02:25s\n",
            "epoch 74 | loss: 0.29816 | train_logloss: 0.28671 | train_accuracy: 0.89344 | valid_logloss: 0.33212 | valid_accuracy: 0.88011 |  0:02:27s\n",
            "epoch 75 | loss: 0.28517 | train_logloss: 0.28279 | train_accuracy: 0.89211 | valid_logloss: 0.34299 | valid_accuracy: 0.87689 |  0:02:29s\n",
            "epoch 76 | loss: 0.29627 | train_logloss: 0.30279 | train_accuracy: 0.88678 | valid_logloss: 0.35479 | valid_accuracy: 0.87078 |  0:02:31s\n",
            "epoch 77 | loss: 0.30051 | train_logloss: 0.27891 | train_accuracy: 0.89367 | valid_logloss: 0.3298  | valid_accuracy: 0.88089 |  0:02:33s\n",
            "epoch 78 | loss: 0.29409 | train_logloss: 0.28235 | train_accuracy: 0.89111 | valid_logloss: 0.34331 | valid_accuracy: 0.87878 |  0:02:34s\n",
            "epoch 79 | loss: 0.29114 | train_logloss: 0.27611 | train_accuracy: 0.89367 | valid_logloss: 0.33935 | valid_accuracy: 0.87778 |  0:02:36s\n",
            "epoch 80 | loss: 0.28235 | train_logloss: 0.27857 | train_accuracy: 0.89444 | valid_logloss: 0.33218 | valid_accuracy: 0.88189 |  0:02:38s\n",
            "epoch 81 | loss: 0.29212 | train_logloss: 0.27559 | train_accuracy: 0.89567 | valid_logloss: 0.32543 | valid_accuracy: 0.88122 |  0:02:40s\n",
            "epoch 82 | loss: 0.2869  | train_logloss: 0.27514 | train_accuracy: 0.89289 | valid_logloss: 0.33018 | valid_accuracy: 0.88256 |  0:02:42s\n",
            "epoch 83 | loss: 0.30314 | train_logloss: 0.30337 | train_accuracy: 0.88378 | valid_logloss: 0.3519  | valid_accuracy: 0.87256 |  0:02:44s\n",
            "epoch 84 | loss: 0.31862 | train_logloss: 0.31835 | train_accuracy: 0.88244 | valid_logloss: 0.37024 | valid_accuracy: 0.86878 |  0:02:46s\n",
            "epoch 85 | loss: 0.32648 | train_logloss: 0.33073 | train_accuracy: 0.87811 | valid_logloss: 0.36936 | valid_accuracy: 0.86389 |  0:02:48s\n",
            "epoch 86 | loss: 0.30808 | train_logloss: 0.28556 | train_accuracy: 0.88911 | valid_logloss: 0.33835 | valid_accuracy: 0.87844 |  0:02:50s\n",
            "epoch 87 | loss: 0.3006  | train_logloss: 0.28685 | train_accuracy: 0.89289 | valid_logloss: 0.33998 | valid_accuracy: 0.87733 |  0:02:51s\n",
            "epoch 88 | loss: 0.29333 | train_logloss: 0.28205 | train_accuracy: 0.89367 | valid_logloss: 0.33313 | valid_accuracy: 0.883   |  0:02:53s\n",
            "epoch 89 | loss: 0.29516 | train_logloss: 0.28179 | train_accuracy: 0.891   | valid_logloss: 0.33127 | valid_accuracy: 0.87822 |  0:02:55s\n",
            "epoch 90 | loss: 0.29318 | train_logloss: 0.28324 | train_accuracy: 0.894   | valid_logloss: 0.34019 | valid_accuracy: 0.88256 |  0:02:57s\n",
            "epoch 91 | loss: 0.28459 | train_logloss: 0.27575 | train_accuracy: 0.89422 | valid_logloss: 0.3335  | valid_accuracy: 0.88044 |  0:02:59s\n",
            "epoch 92 | loss: 0.28386 | train_logloss: 0.26321 | train_accuracy: 0.898   | valid_logloss: 0.33195 | valid_accuracy: 0.88133 |  0:03:01s\n",
            "epoch 93 | loss: 0.29131 | train_logloss: 0.2772  | train_accuracy: 0.89389 | valid_logloss: 0.32916 | valid_accuracy: 0.88033 |  0:03:03s\n",
            "epoch 94 | loss: 0.28853 | train_logloss: 0.27393 | train_accuracy: 0.89822 | valid_logloss: 0.33653 | valid_accuracy: 0.87844 |  0:03:04s\n",
            "epoch 95 | loss: 0.27955 | train_logloss: 0.27655 | train_accuracy: 0.89467 | valid_logloss: 0.34995 | valid_accuracy: 0.87744 |  0:03:06s\n",
            "epoch 96 | loss: 0.28521 | train_logloss: 0.27151 | train_accuracy: 0.89467 | valid_logloss: 0.34204 | valid_accuracy: 0.88322 |  0:03:08s\n",
            "epoch 97 | loss: 0.284   | train_logloss: 0.26164 | train_accuracy: 0.89878 | valid_logloss: 0.33414 | valid_accuracy: 0.882   |  0:03:10s\n",
            "epoch 98 | loss: 0.28391 | train_logloss: 0.27207 | train_accuracy: 0.89644 | valid_logloss: 0.34125 | valid_accuracy: 0.882   |  0:03:12s\n",
            "epoch 99 | loss: 0.27974 | train_logloss: 0.28001 | train_accuracy: 0.89444 | valid_logloss: 0.3419  | valid_accuracy: 0.87978 |  0:03:14s\n",
            "epoch 100| loss: 0.27955 | train_logloss: 0.25986 | train_accuracy: 0.89967 | valid_logloss: 0.33039 | valid_accuracy: 0.88222 |  0:03:16s\n",
            "epoch 101| loss: 0.28688 | train_logloss: 0.28066 | train_accuracy: 0.89289 | valid_logloss: 0.34935 | valid_accuracy: 0.87556 |  0:03:18s\n",
            "epoch 102| loss: 0.29271 | train_logloss: 0.28329 | train_accuracy: 0.88867 | valid_logloss: 0.33729 | valid_accuracy: 0.87644 |  0:03:20s\n",
            "epoch 103| loss: 0.28457 | train_logloss: 0.26907 | train_accuracy: 0.89622 | valid_logloss: 0.32813 | valid_accuracy: 0.88644 |  0:03:22s\n",
            "epoch 104| loss: 0.28505 | train_logloss: 0.26933 | train_accuracy: 0.89444 | valid_logloss: 0.33263 | valid_accuracy: 0.88367 |  0:03:23s\n",
            "epoch 105| loss: 0.28174 | train_logloss: 0.26248 | train_accuracy: 0.897   | valid_logloss: 0.34026 | valid_accuracy: 0.87978 |  0:03:25s\n",
            "epoch 106| loss: 0.27335 | train_logloss: 0.26777 | train_accuracy: 0.89422 | valid_logloss: 0.34763 | valid_accuracy: 0.87911 |  0:03:27s\n",
            "epoch 107| loss: 0.27255 | train_logloss: 0.26114 | train_accuracy: 0.89944 | valid_logloss: 0.33569 | valid_accuracy: 0.88267 |  0:03:29s\n",
            "epoch 108| loss: 0.2705  | train_logloss: 0.26947 | train_accuracy: 0.90022 | valid_logloss: 0.35037 | valid_accuracy: 0.88144 |  0:03:31s\n",
            "epoch 109| loss: 0.26956 | train_logloss: 0.25215 | train_accuracy: 0.90289 | valid_logloss: 0.33183 | valid_accuracy: 0.886   |  0:03:33s\n",
            "epoch 110| loss: 0.26734 | train_logloss: 0.25267 | train_accuracy: 0.90322 | valid_logloss: 0.32737 | valid_accuracy: 0.88511 |  0:03:35s\n",
            "epoch 111| loss: 0.26771 | train_logloss: 0.24955 | train_accuracy: 0.90356 | valid_logloss: 0.33858 | valid_accuracy: 0.88289 |  0:03:37s\n",
            "epoch 112| loss: 0.2688  | train_logloss: 0.25822 | train_accuracy: 0.90089 | valid_logloss: 0.34265 | valid_accuracy: 0.88044 |  0:03:38s\n",
            "epoch 113| loss: 0.27534 | train_logloss: 0.26633 | train_accuracy: 0.89767 | valid_logloss: 0.34334 | valid_accuracy: 0.883   |  0:03:40s\n",
            "epoch 114| loss: 0.27351 | train_logloss: 0.2644  | train_accuracy: 0.89933 | valid_logloss: 0.33465 | valid_accuracy: 0.88144 |  0:03:42s\n",
            "epoch 115| loss: 0.26735 | train_logloss: 0.24537 | train_accuracy: 0.905   | valid_logloss: 0.32395 | valid_accuracy: 0.88611 |  0:03:44s\n",
            "epoch 116| loss: 0.26285 | train_logloss: 0.25443 | train_accuracy: 0.89978 | valid_logloss: 0.33432 | valid_accuracy: 0.884   |  0:03:46s\n",
            "epoch 117| loss: 0.26907 | train_logloss: 0.25134 | train_accuracy: 0.90444 | valid_logloss: 0.33676 | valid_accuracy: 0.88267 |  0:03:48s\n",
            "epoch 118| loss: 0.26441 | train_logloss: 0.2428  | train_accuracy: 0.908   | valid_logloss: 0.33447 | valid_accuracy: 0.88467 |  0:03:50s\n",
            "epoch 119| loss: 0.25948 | train_logloss: 0.24019 | train_accuracy: 0.90756 | valid_logloss: 0.32966 | valid_accuracy: 0.88256 |  0:03:52s\n",
            "epoch 120| loss: 0.25961 | train_logloss: 0.23962 | train_accuracy: 0.90578 | valid_logloss: 0.3311  | valid_accuracy: 0.88489 |  0:03:53s\n",
            "epoch 121| loss: 0.26105 | train_logloss: 0.25395 | train_accuracy: 0.90411 | valid_logloss: 0.34879 | valid_accuracy: 0.88078 |  0:03:55s\n",
            "epoch 122| loss: 0.27014 | train_logloss: 0.25432 | train_accuracy: 0.90322 | valid_logloss: 0.34525 | valid_accuracy: 0.88256 |  0:03:57s\n",
            "epoch 123| loss: 0.26347 | train_logloss: 0.24508 | train_accuracy: 0.90689 | valid_logloss: 0.33624 | valid_accuracy: 0.88222 |  0:03:59s\n",
            "epoch 124| loss: 0.26257 | train_logloss: 0.24241 | train_accuracy: 0.90533 | valid_logloss: 0.33986 | valid_accuracy: 0.883   |  0:04:01s\n",
            "epoch 125| loss: 0.25643 | train_logloss: 0.23501 | train_accuracy: 0.90744 | valid_logloss: 0.33605 | valid_accuracy: 0.88111 |  0:04:03s\n",
            "epoch 126| loss: 0.25646 | train_logloss: 0.24082 | train_accuracy: 0.90689 | valid_logloss: 0.33861 | valid_accuracy: 0.88356 |  0:04:05s\n",
            "epoch 127| loss: 0.25602 | train_logloss: 0.24345 | train_accuracy: 0.90644 | valid_logloss: 0.34043 | valid_accuracy: 0.88122 |  0:04:07s\n",
            "epoch 128| loss: 0.25964 | train_logloss: 0.25109 | train_accuracy: 0.90622 | valid_logloss: 0.35544 | valid_accuracy: 0.881   |  0:04:09s\n",
            "epoch 129| loss: 0.25402 | train_logloss: 0.2426  | train_accuracy: 0.90433 | valid_logloss: 0.35536 | valid_accuracy: 0.87856 |  0:04:11s\n",
            "epoch 130| loss: 0.25075 | train_logloss: 0.24075 | train_accuracy: 0.90767 | valid_logloss: 0.3424  | valid_accuracy: 0.88056 |  0:04:13s\n",
            "epoch 131| loss: 0.2495  | train_logloss: 0.23724 | train_accuracy: 0.90689 | valid_logloss: 0.34599 | valid_accuracy: 0.885   |  0:04:15s\n",
            "epoch 132| loss: 0.25845 | train_logloss: 0.23544 | train_accuracy: 0.90867 | valid_logloss: 0.34175 | valid_accuracy: 0.87978 |  0:04:17s\n",
            "epoch 133| loss: 0.25481 | train_logloss: 0.24521 | train_accuracy: 0.90311 | valid_logloss: 0.35676 | valid_accuracy: 0.88167 |  0:04:19s\n",
            "epoch 134| loss: 0.25068 | train_logloss: 0.24251 | train_accuracy: 0.90678 | valid_logloss: 0.35733 | valid_accuracy: 0.88011 |  0:04:21s\n",
            "epoch 135| loss: 0.26222 | train_logloss: 0.2419  | train_accuracy: 0.90689 | valid_logloss: 0.35108 | valid_accuracy: 0.87733 |  0:04:23s\n",
            "epoch 136| loss: 0.25261 | train_logloss: 0.2274  | train_accuracy: 0.91278 | valid_logloss: 0.34505 | valid_accuracy: 0.88489 |  0:04:25s\n",
            "epoch 137| loss: 0.24834 | train_logloss: 0.24453 | train_accuracy: 0.90511 | valid_logloss: 0.35112 | valid_accuracy: 0.88133 |  0:04:27s\n",
            "epoch 138| loss: 0.24984 | train_logloss: 0.25387 | train_accuracy: 0.89789 | valid_logloss: 0.35961 | valid_accuracy: 0.87656 |  0:04:29s\n",
            "epoch 139| loss: 0.2591  | train_logloss: 0.23466 | train_accuracy: 0.91    | valid_logloss: 0.34212 | valid_accuracy: 0.88522 |  0:04:31s\n",
            "epoch 140| loss: 0.25594 | train_logloss: 0.23933 | train_accuracy: 0.90756 | valid_logloss: 0.34009 | valid_accuracy: 0.88178 |  0:04:33s\n",
            "epoch 141| loss: 0.25499 | train_logloss: 0.2342  | train_accuracy: 0.90922 | valid_logloss: 0.35274 | valid_accuracy: 0.88211 |  0:04:35s\n",
            "epoch 142| loss: 0.25713 | train_logloss: 0.23157 | train_accuracy: 0.90744 | valid_logloss: 0.33433 | valid_accuracy: 0.88156 |  0:04:37s\n",
            "epoch 143| loss: 0.24526 | train_logloss: 0.23555 | train_accuracy: 0.91078 | valid_logloss: 0.35055 | valid_accuracy: 0.88211 |  0:04:39s\n",
            "epoch 144| loss: 0.25204 | train_logloss: 0.23181 | train_accuracy: 0.90878 | valid_logloss: 0.35111 | valid_accuracy: 0.87911 |  0:04:41s\n",
            "epoch 145| loss: 0.24588 | train_logloss: 0.21977 | train_accuracy: 0.912   | valid_logloss: 0.34798 | valid_accuracy: 0.88211 |  0:04:43s\n",
            "epoch 146| loss: 0.24258 | train_logloss: 0.23671 | train_accuracy: 0.90711 | valid_logloss: 0.34667 | valid_accuracy: 0.88289 |  0:04:45s\n",
            "epoch 147| loss: 0.25519 | train_logloss: 0.23663 | train_accuracy: 0.90833 | valid_logloss: 0.34883 | valid_accuracy: 0.88244 |  0:04:47s\n",
            "epoch 148| loss: 0.24743 | train_logloss: 0.22495 | train_accuracy: 0.91289 | valid_logloss: 0.34854 | valid_accuracy: 0.88233 |  0:04:49s\n",
            "epoch 149| loss: 0.23847 | train_logloss: 0.21895 | train_accuracy: 0.91456 | valid_logloss: 0.35002 | valid_accuracy: 0.88433 |  0:04:51s\n",
            "epoch 150| loss: 0.2383  | train_logloss: 0.22346 | train_accuracy: 0.91033 | valid_logloss: 0.36177 | valid_accuracy: 0.87778 |  0:04:53s\n",
            "epoch 151| loss: 0.236   | train_logloss: 0.22308 | train_accuracy: 0.91322 | valid_logloss: 0.35694 | valid_accuracy: 0.88111 |  0:04:55s\n",
            "epoch 152| loss: 0.23722 | train_logloss: 0.22578 | train_accuracy: 0.912   | valid_logloss: 0.37041 | valid_accuracy: 0.88244 |  0:04:57s\n",
            "epoch 153| loss: 0.24339 | train_logloss: 0.23504 | train_accuracy: 0.90967 | valid_logloss: 0.34664 | valid_accuracy: 0.87944 |  0:04:59s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 103 and best_valid_accuracy = 0.88644\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORW5YG_knt2j",
        "outputId": "d121ee93-d861-4188-f1aa-1c0b9717d81e"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb10.pkl' 'tn10.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    255.4 MiB    255.4 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    259.8 MiB      4.4 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2319.0 MiB   2059.3 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhZ__ntF_hOu"
      },
      "source": [
        "  number_exp=11\n",
        "  Rows=30000\n",
        "  Nd=64\n",
        "  Na=64\t\n",
        "  B=512\n",
        "  BV=128\n",
        "  mB=0.7\t\n",
        "  λsparse=0.001\n",
        "  Nsteps=5\n",
        "  γ=1.5 \n",
        "  learning_rate=0.02\n",
        "  decay_rate=0.95\n",
        "  decay_iterations=200\t\n",
        "  shared=2\n",
        "  decision=2\n",
        "  mask_type='entmax'\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('number: '+str(11)+', ')\n",
        "  \n",
        "  #data\n",
        "  data_split = data_preparation(X, y, c=Rows//3, test_size=0.3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = Rows//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  #X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  #X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  #X_valid      = np.concatenate((X1_test[2*count : 3*count], X2_test[2*count : 3*count], X3_test[2*count : 3*count]))\n",
        "  #y_valid      = np.concatenate((y1_test[2*count : 3*count], y2_test[2*count : 3*count], y3_test[2*count : 3*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  #X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #classifier\n",
        "  tn = TabNetClassifier( n_d=Nd, n_a=Na,\n",
        "                          n_shared=shared, n_independent=decision,\n",
        "                          momentum=mB,\n",
        "                          optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=learning_rate),\n",
        "                          scheduler_params={\"step_size\":decay_iterations, # how to use learning rate scheduler\n",
        "                                            \"gamma\":decay_rate},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                          mask_type=mask_type,\n",
        "                          **{ 'gamma': γ,\n",
        "                              'lambda_sparse':λsparse,\n",
        "                              'n_steps': Nsteps}\n",
        "\n",
        "  )\n",
        "\n",
        "  max_epochs = 2000\n",
        "  t = time()\n",
        "  tn.fit(\n",
        "      X_train=X_train, y_train=y_train,\n",
        "      #X_valid=X_valid, y_valid=y_valid,\n",
        "      eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "      eval_name=['train', 'valid'],\n",
        "      eval_metric=['logloss','accuracy'],\n",
        "      max_epochs=max_epochs , patience=20,\n",
        "      batch_size=B, virtual_batch_size=BV,\n",
        "  ) \n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time TN: '+str(t)+', ')\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'colsample_bytree': 0.6437405148446416,\n",
        "    'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 100,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_test_norm, y_test)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('time GB: '+str(t)+', ')\n",
        "\n",
        "  #Accuracy\n",
        "  acc, err = bootstrap_accuracy(tn, X_test, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc TN: '+str(acc)+'+-'+str(err)+', ')\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(tn, 'TN', Rows)\n",
        "  feature_acc(gb, 'GB', Rows)\n",
        "\n",
        "  #save model\n",
        "\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb'+str(number_exp)+'.txt')\n",
        "  tn.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/tn'+str(number_exp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdmZ-Md1uTFA"
      },
      "source": [
        "ones(number_exp=12, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.5, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='sparsemax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldqGo0H1-Gq"
      },
      "source": [
        "ones(number_exp=13, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=128,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeKGjsIi6NLC",
        "outputId": "abe30477-390f-46c7-a3ac-cb43fd4f0389"
      },
      "source": [
        "memory(number_exp=14, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=16384,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.2, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.66566 | train_logloss: 14.13323| train_accuracy: 0.25928 | valid_logloss: 14.09257| valid_accuracy: 0.26009 |  0:00:11s\n",
            "epoch 1  | loss: 0.36818 | train_logloss: 8.37912 | train_accuracy: 0.3296  | valid_logloss: 8.38603 | valid_accuracy: 0.3297  |  0:00:22s\n",
            "epoch 2  | loss: 0.33743 | train_logloss: 2.09891 | train_accuracy: 0.33258 | valid_logloss: 2.09696 | valid_accuracy: 0.33267 |  0:00:34s\n",
            "epoch 3  | loss: 0.32329 | train_logloss: 1.56248 | train_accuracy: 0.33715 | valid_logloss: 1.5629  | valid_accuracy: 0.33734 |  0:00:45s\n",
            "epoch 4  | loss: 0.31404 | train_logloss: 3.23258 | train_accuracy: 0.31048 | valid_logloss: 3.2338  | valid_accuracy: 0.31048 |  0:00:57s\n",
            "epoch 5  | loss: 0.31225 | train_logloss: 2.10267 | train_accuracy: 0.38407 | valid_logloss: 2.10322 | valid_accuracy: 0.38382 |  0:01:08s\n",
            "epoch 6  | loss: 0.30942 | train_logloss: 1.47126 | train_accuracy: 0.34909 | valid_logloss: 1.4706  | valid_accuracy: 0.34812 |  0:01:20s\n",
            "epoch 7  | loss: 0.30211 | train_logloss: 1.22033 | train_accuracy: 0.41751 | valid_logloss: 1.22159 | valid_accuracy: 0.41664 |  0:01:31s\n",
            "epoch 8  | loss: 0.2998  | train_logloss: 1.08499 | train_accuracy: 0.46878 | valid_logloss: 1.08626 | valid_accuracy: 0.46655 |  0:01:43s\n",
            "epoch 9  | loss: 0.29796 | train_logloss: 1.05283 | train_accuracy: 0.52471 | valid_logloss: 1.05274 | valid_accuracy: 0.52406 |  0:01:54s\n",
            "epoch 10 | loss: 0.29522 | train_logloss: 0.88857 | train_accuracy: 0.58912 | valid_logloss: 0.88921 | valid_accuracy: 0.58893 |  0:02:05s\n",
            "epoch 11 | loss: 0.29085 | train_logloss: 0.70195 | train_accuracy: 0.69263 | valid_logloss: 0.70209 | valid_accuracy: 0.69258 |  0:02:17s\n",
            "epoch 12 | loss: 0.28768 | train_logloss: 0.59451 | train_accuracy: 0.76096 | valid_logloss: 0.59428 | valid_accuracy: 0.76161 |  0:02:28s\n",
            "epoch 13 | loss: 0.28968 | train_logloss: 0.49959 | train_accuracy: 0.80687 | valid_logloss: 0.50039 | valid_accuracy: 0.80681 |  0:02:40s\n",
            "epoch 14 | loss: 0.2869  | train_logloss: 0.42977 | train_accuracy: 0.84025 | valid_logloss: 0.43068 | valid_accuracy: 0.83958 |  0:02:51s\n",
            "epoch 15 | loss: 0.28329 | train_logloss: 0.38883 | train_accuracy: 0.85345 | valid_logloss: 0.39009 | valid_accuracy: 0.85306 |  0:03:03s\n",
            "epoch 16 | loss: 0.28058 | train_logloss: 0.33939 | train_accuracy: 0.87467 | valid_logloss: 0.34185 | valid_accuracy: 0.87454 |  0:03:14s\n",
            "epoch 17 | loss: 0.27929 | train_logloss: 0.32776 | train_accuracy: 0.87709 | valid_logloss: 0.3305  | valid_accuracy: 0.87641 |  0:03:26s\n",
            "epoch 18 | loss: 0.2793  | train_logloss: 0.30942 | train_accuracy: 0.8855  | valid_logloss: 0.31225 | valid_accuracy: 0.88522 |  0:03:37s\n",
            "epoch 19 | loss: 0.27865 | train_logloss: 0.2969  | train_accuracy: 0.88825 | valid_logloss: 0.30169 | valid_accuracy: 0.88756 |  0:03:48s\n",
            "epoch 20 | loss: 0.27624 | train_logloss: 0.28523 | train_accuracy: 0.89088 | valid_logloss: 0.28996 | valid_accuracy: 0.89002 |  0:04:00s\n",
            "epoch 21 | loss: 0.27554 | train_logloss: 0.28031 | train_accuracy: 0.8945  | valid_logloss: 0.28505 | valid_accuracy: 0.89358 |  0:04:11s\n",
            "epoch 22 | loss: 0.27511 | train_logloss: 0.28056 | train_accuracy: 0.89312 | valid_logloss: 0.28555 | valid_accuracy: 0.89186 |  0:04:22s\n",
            "epoch 23 | loss: 0.27669 | train_logloss: 0.27827 | train_accuracy: 0.89474 | valid_logloss: 0.28374 | valid_accuracy: 0.89384 |  0:04:34s\n",
            "epoch 24 | loss: 0.27651 | train_logloss: 0.27391 | train_accuracy: 0.89593 | valid_logloss: 0.27989 | valid_accuracy: 0.89454 |  0:04:45s\n",
            "epoch 25 | loss: 0.27597 | train_logloss: 0.27251 | train_accuracy: 0.89625 | valid_logloss: 0.27838 | valid_accuracy: 0.895   |  0:04:57s\n",
            "epoch 26 | loss: 0.27287 | train_logloss: 0.2673  | train_accuracy: 0.89859 | valid_logloss: 0.27425 | valid_accuracy: 0.89643 |  0:05:08s\n",
            "epoch 27 | loss: 0.27168 | train_logloss: 0.26818 | train_accuracy: 0.89822 | valid_logloss: 0.27493 | valid_accuracy: 0.89635 |  0:05:19s\n",
            "epoch 28 | loss: 0.27077 | train_logloss: 0.26704 | train_accuracy: 0.89876 | valid_logloss: 0.27445 | valid_accuracy: 0.89716 |  0:05:31s\n",
            "epoch 29 | loss: 0.27043 | train_logloss: 0.26679 | train_accuracy: 0.89859 | valid_logloss: 0.27285 | valid_accuracy: 0.89676 |  0:05:42s\n",
            "epoch 30 | loss: 0.27014 | train_logloss: 0.27427 | train_accuracy: 0.89506 | valid_logloss: 0.28082 | valid_accuracy: 0.8938  |  0:05:54s\n",
            "epoch 31 | loss: 0.27055 | train_logloss: 0.26624 | train_accuracy: 0.89829 | valid_logloss: 0.27346 | valid_accuracy: 0.89646 |  0:06:05s\n",
            "epoch 32 | loss: 0.26992 | train_logloss: 0.26485 | train_accuracy: 0.89891 | valid_logloss: 0.27246 | valid_accuracy: 0.89743 |  0:06:16s\n",
            "epoch 33 | loss: 0.26744 | train_logloss: 0.26478 | train_accuracy: 0.8992  | valid_logloss: 0.27229 | valid_accuracy: 0.89752 |  0:06:28s\n",
            "epoch 34 | loss: 0.26733 | train_logloss: 0.26383 | train_accuracy: 0.89953 | valid_logloss: 0.27106 | valid_accuracy: 0.89761 |  0:06:39s\n",
            "epoch 35 | loss: 0.26726 | train_logloss: 0.26247 | train_accuracy: 0.89928 | valid_logloss: 0.27058 | valid_accuracy: 0.89765 |  0:06:50s\n",
            "epoch 36 | loss: 0.26813 | train_logloss: 0.2639  | train_accuracy: 0.90003 | valid_logloss: 0.27056 | valid_accuracy: 0.89845 |  0:07:02s\n",
            "epoch 37 | loss: 0.26647 | train_logloss: 0.26172 | train_accuracy: 0.8994  | valid_logloss: 0.26946 | valid_accuracy: 0.89769 |  0:07:13s\n",
            "epoch 38 | loss: 0.26538 | train_logloss: 0.2645  | train_accuracy: 0.89894 | valid_logloss: 0.27262 | valid_accuracy: 0.89682 |  0:07:24s\n",
            "epoch 39 | loss: 0.26572 | train_logloss: 0.26109 | train_accuracy: 0.9005  | valid_logloss: 0.26919 | valid_accuracy: 0.89836 |  0:07:36s\n",
            "epoch 40 | loss: 0.26511 | train_logloss: 0.25939 | train_accuracy: 0.90065 | valid_logloss: 0.26782 | valid_accuracy: 0.89857 |  0:07:47s\n",
            "epoch 41 | loss: 0.26527 | train_logloss: 0.26251 | train_accuracy: 0.89953 | valid_logloss: 0.27027 | valid_accuracy: 0.89798 |  0:07:58s\n",
            "epoch 42 | loss: 0.26714 | train_logloss: 0.2625  | train_accuracy: 0.89985 | valid_logloss: 0.26996 | valid_accuracy: 0.8983  |  0:08:10s\n",
            "epoch 43 | loss: 0.2646  | train_logloss: 0.25885 | train_accuracy: 0.90108 | valid_logloss: 0.26706 | valid_accuracy: 0.89902 |  0:08:21s\n",
            "epoch 44 | loss: 0.26369 | train_logloss: 0.26049 | train_accuracy: 0.90093 | valid_logloss: 0.26912 | valid_accuracy: 0.89866 |  0:08:32s\n",
            "epoch 45 | loss: 0.26507 | train_logloss: 0.25996 | train_accuracy: 0.90071 | valid_logloss: 0.26769 | valid_accuracy: 0.8986  |  0:08:43s\n",
            "epoch 46 | loss: 0.26383 | train_logloss: 0.26037 | train_accuracy: 0.90066 | valid_logloss: 0.26919 | valid_accuracy: 0.89904 |  0:08:55s\n",
            "epoch 47 | loss: 0.26211 | train_logloss: 0.25739 | train_accuracy: 0.90132 | valid_logloss: 0.26644 | valid_accuracy: 0.89949 |  0:09:06s\n",
            "epoch 48 | loss: 0.26254 | train_logloss: 0.26186 | train_accuracy: 0.89979 | valid_logloss: 0.26971 | valid_accuracy: 0.89821 |  0:09:17s\n",
            "epoch 49 | loss: 0.26686 | train_logloss: 0.26045 | train_accuracy: 0.90064 | valid_logloss: 0.2686  | valid_accuracy: 0.89941 |  0:09:29s\n",
            "epoch 50 | loss: 0.26423 | train_logloss: 0.26047 | train_accuracy: 0.90065 | valid_logloss: 0.26865 | valid_accuracy: 0.89882 |  0:09:40s\n",
            "epoch 51 | loss: 0.26331 | train_logloss: 0.26172 | train_accuracy: 0.89962 | valid_logloss: 0.27016 | valid_accuracy: 0.89762 |  0:09:51s\n",
            "epoch 52 | loss: 0.26286 | train_logloss: 0.25828 | train_accuracy: 0.90082 | valid_logloss: 0.26672 | valid_accuracy: 0.89901 |  0:10:03s\n",
            "epoch 53 | loss: 0.26181 | train_logloss: 0.25587 | train_accuracy: 0.90218 | valid_logloss: 0.26509 | valid_accuracy: 0.89994 |  0:10:14s\n",
            "epoch 54 | loss: 0.2621  | train_logloss: 0.25897 | train_accuracy: 0.90006 | valid_logloss: 0.26878 | valid_accuracy: 0.89819 |  0:10:25s\n",
            "epoch 55 | loss: 0.26162 | train_logloss: 0.2599  | train_accuracy: 0.9009  | valid_logloss: 0.26962 | valid_accuracy: 0.89869 |  0:10:36s\n",
            "epoch 56 | loss: 0.26153 | train_logloss: 0.25649 | train_accuracy: 0.90141 | valid_logloss: 0.26635 | valid_accuracy: 0.89923 |  0:10:48s\n",
            "epoch 57 | loss: 0.26037 | train_logloss: 0.25846 | train_accuracy: 0.90132 | valid_logloss: 0.26778 | valid_accuracy: 0.89898 |  0:10:59s\n",
            "epoch 58 | loss: 0.26086 | train_logloss: 0.25756 | train_accuracy: 0.90112 | valid_logloss: 0.26664 | valid_accuracy: 0.89922 |  0:11:10s\n",
            "epoch 59 | loss: 0.2609  | train_logloss: 0.25586 | train_accuracy: 0.90232 | valid_logloss: 0.26558 | valid_accuracy: 0.90019 |  0:11:22s\n",
            "epoch 60 | loss: 0.26057 | train_logloss: 0.25647 | train_accuracy: 0.90162 | valid_logloss: 0.26647 | valid_accuracy: 0.89919 |  0:11:33s\n",
            "epoch 61 | loss: 0.25985 | train_logloss: 0.25818 | train_accuracy: 0.90157 | valid_logloss: 0.26832 | valid_accuracy: 0.8993  |  0:11:44s\n",
            "epoch 62 | loss: 0.25905 | train_logloss: 0.25693 | train_accuracy: 0.90146 | valid_logloss: 0.26802 | valid_accuracy: 0.8989  |  0:11:55s\n",
            "epoch 63 | loss: 0.25927 | train_logloss: 0.25466 | train_accuracy: 0.9025  | valid_logloss: 0.26558 | valid_accuracy: 0.89942 |  0:12:07s\n",
            "epoch 64 | loss: 0.25934 | train_logloss: 0.25803 | train_accuracy: 0.90114 | valid_logloss: 0.26753 | valid_accuracy: 0.89926 |  0:12:18s\n",
            "epoch 65 | loss: 0.26013 | train_logloss: 0.25655 | train_accuracy: 0.90162 | valid_logloss: 0.26678 | valid_accuracy: 0.89914 |  0:12:29s\n",
            "epoch 66 | loss: 0.2596  | train_logloss: 0.2544  | train_accuracy: 0.90276 | valid_logloss: 0.26422 | valid_accuracy: 0.90034 |  0:12:41s\n",
            "epoch 67 | loss: 0.25984 | train_logloss: 0.258   | train_accuracy: 0.90066 | valid_logloss: 0.26811 | valid_accuracy: 0.89802 |  0:12:52s\n",
            "epoch 68 | loss: 0.26044 | train_logloss: 0.25585 | train_accuracy: 0.90279 | valid_logloss: 0.26513 | valid_accuracy: 0.90016 |  0:13:03s\n",
            "epoch 69 | loss: 0.25826 | train_logloss: 0.25275 | train_accuracy: 0.90311 | valid_logloss: 0.26344 | valid_accuracy: 0.9004  |  0:13:15s\n",
            "epoch 70 | loss: 0.25767 | train_logloss: 0.25293 | train_accuracy: 0.90321 | valid_logloss: 0.26386 | valid_accuracy: 0.90001 |  0:13:26s\n",
            "epoch 71 | loss: 0.25782 | train_logloss: 0.25362 | train_accuracy: 0.90288 | valid_logloss: 0.26475 | valid_accuracy: 0.90018 |  0:13:37s\n",
            "epoch 72 | loss: 0.25879 | train_logloss: 0.25545 | train_accuracy: 0.90253 | valid_logloss: 0.26713 | valid_accuracy: 0.89943 |  0:13:48s\n",
            "epoch 73 | loss: 0.25862 | train_logloss: 0.25424 | train_accuracy: 0.90253 | valid_logloss: 0.26491 | valid_accuracy: 0.89935 |  0:13:59s\n",
            "epoch 74 | loss: 0.2581  | train_logloss: 0.25312 | train_accuracy: 0.90314 | valid_logloss: 0.26393 | valid_accuracy: 0.90036 |  0:14:10s\n",
            "epoch 75 | loss: 0.25755 | train_logloss: 0.25326 | train_accuracy: 0.90315 | valid_logloss: 0.26419 | valid_accuracy: 0.90024 |  0:14:21s\n",
            "epoch 76 | loss: 0.25727 | train_logloss: 0.25184 | train_accuracy: 0.90364 | valid_logloss: 0.2643  | valid_accuracy: 0.90052 |  0:14:32s\n",
            "epoch 77 | loss: 0.25696 | train_logloss: 0.25152 | train_accuracy: 0.90344 | valid_logloss: 0.26387 | valid_accuracy: 0.89993 |  0:14:43s\n",
            "epoch 78 | loss: 0.25562 | train_logloss: 0.25424 | train_accuracy: 0.90202 | valid_logloss: 0.26686 | valid_accuracy: 0.89931 |  0:14:54s\n",
            "epoch 79 | loss: 0.25546 | train_logloss: 0.25228 | train_accuracy: 0.90373 | valid_logloss: 0.26436 | valid_accuracy: 0.90031 |  0:15:05s\n",
            "epoch 80 | loss: 0.25612 | train_logloss: 0.25286 | train_accuracy: 0.90305 | valid_logloss: 0.26597 | valid_accuracy: 0.89972 |  0:15:16s\n",
            "epoch 81 | loss: 0.25607 | train_logloss: 0.25056 | train_accuracy: 0.9041  | valid_logloss: 0.26317 | valid_accuracy: 0.90089 |  0:15:27s\n",
            "epoch 82 | loss: 0.25593 | train_logloss: 0.25314 | train_accuracy: 0.90293 | valid_logloss: 0.26524 | valid_accuracy: 0.9002  |  0:15:38s\n",
            "epoch 83 | loss: 0.25552 | train_logloss: 0.25469 | train_accuracy: 0.90257 | valid_logloss: 0.26643 | valid_accuracy: 0.89947 |  0:15:49s\n",
            "epoch 84 | loss: 0.25623 | train_logloss: 0.25314 | train_accuracy: 0.90305 | valid_logloss: 0.26657 | valid_accuracy: 0.89939 |  0:16:00s\n",
            "epoch 85 | loss: 0.25554 | train_logloss: 0.25304 | train_accuracy: 0.90292 | valid_logloss: 0.26491 | valid_accuracy: 0.90019 |  0:16:12s\n",
            "epoch 86 | loss: 0.2555  | train_logloss: 0.25135 | train_accuracy: 0.90394 | valid_logloss: 0.26311 | valid_accuracy: 0.90126 |  0:16:23s\n",
            "epoch 87 | loss: 0.2567  | train_logloss: 0.25361 | train_accuracy: 0.9035  | valid_logloss: 0.26433 | valid_accuracy: 0.90043 |  0:16:35s\n",
            "epoch 88 | loss: 0.2566  | train_logloss: 0.25368 | train_accuracy: 0.90231 | valid_logloss: 0.26552 | valid_accuracy: 0.89972 |  0:16:46s\n",
            "epoch 89 | loss: 0.25696 | train_logloss: 0.25594 | train_accuracy: 0.90191 | valid_logloss: 0.26772 | valid_accuracy: 0.89906 |  0:16:58s\n",
            "epoch 90 | loss: 0.27703 | train_logloss: 0.27336 | train_accuracy: 0.89726 | valid_logloss: 0.27959 | valid_accuracy: 0.89565 |  0:17:09s\n",
            "epoch 91 | loss: 0.27068 | train_logloss: 0.26674 | train_accuracy: 0.89891 | valid_logloss: 0.27437 | valid_accuracy: 0.8967  |  0:17:21s\n",
            "epoch 92 | loss: 0.26573 | train_logloss: 0.25971 | train_accuracy: 0.90117 | valid_logloss: 0.26796 | valid_accuracy: 0.89968 |  0:17:33s\n",
            "epoch 93 | loss: 0.26383 | train_logloss: 0.26012 | train_accuracy: 0.90059 | valid_logloss: 0.26913 | valid_accuracy: 0.89862 |  0:17:44s\n",
            "epoch 94 | loss: 0.26279 | train_logloss: 0.25818 | train_accuracy: 0.90131 | valid_logloss: 0.26643 | valid_accuracy: 0.89963 |  0:17:56s\n",
            "epoch 95 | loss: 0.26038 | train_logloss: 0.256   | train_accuracy: 0.90236 | valid_logloss: 0.26547 | valid_accuracy: 0.89957 |  0:18:08s\n",
            "epoch 96 | loss: 0.2589  | train_logloss: 0.25381 | train_accuracy: 0.9028  | valid_logloss: 0.26405 | valid_accuracy: 0.90025 |  0:18:20s\n",
            "epoch 97 | loss: 0.25806 | train_logloss: 0.25508 | train_accuracy: 0.90263 | valid_logloss: 0.26524 | valid_accuracy: 0.90025 |  0:18:32s\n",
            "epoch 98 | loss: 0.25966 | train_logloss: 0.25551 | train_accuracy: 0.90238 | valid_logloss: 0.2658  | valid_accuracy: 0.90017 |  0:18:45s\n",
            "epoch 99 | loss: 0.25884 | train_logloss: 0.25461 | train_accuracy: 0.90274 | valid_logloss: 0.26507 | valid_accuracy: 0.89992 |  0:18:57s\n",
            "epoch 100| loss: 0.25716 | train_logloss: 0.25274 | train_accuracy: 0.9027  | valid_logloss: 0.26405 | valid_accuracy: 0.90004 |  0:19:08s\n",
            "epoch 101| loss: 0.25644 | train_logloss: 0.25114 | train_accuracy: 0.90383 | valid_logloss: 0.26285 | valid_accuracy: 0.90108 |  0:19:19s\n",
            "epoch 102| loss: 0.25594 | train_logloss: 0.25317 | train_accuracy: 0.90322 | valid_logloss: 0.26493 | valid_accuracy: 0.89996 |  0:19:31s\n",
            "epoch 103| loss: 0.25676 | train_logloss: 0.25321 | train_accuracy: 0.90216 | valid_logloss: 0.26385 | valid_accuracy: 0.89989 |  0:19:42s\n",
            "epoch 104| loss: 0.2559  | train_logloss: 0.25276 | train_accuracy: 0.90338 | valid_logloss: 0.26468 | valid_accuracy: 0.90035 |  0:19:53s\n",
            "epoch 105| loss: 0.25575 | train_logloss: 0.25024 | train_accuracy: 0.90434 | valid_logloss: 0.26217 | valid_accuracy: 0.90129 |  0:20:04s\n",
            "epoch 106| loss: 0.25434 | train_logloss: 0.24991 | train_accuracy: 0.90403 | valid_logloss: 0.26285 | valid_accuracy: 0.90099 |  0:20:15s\n",
            "epoch 107| loss: 0.25405 | train_logloss: 0.25339 | train_accuracy: 0.90258 | valid_logloss: 0.2658  | valid_accuracy: 0.8993  |  0:20:27s\n",
            "epoch 108| loss: 0.25506 | train_logloss: 0.25137 | train_accuracy: 0.90371 | valid_logloss: 0.2632  | valid_accuracy: 0.90093 |  0:20:38s\n",
            "epoch 109| loss: 0.25468 | train_logloss: 0.25106 | train_accuracy: 0.90347 | valid_logloss: 0.26259 | valid_accuracy: 0.90079 |  0:20:49s\n",
            "epoch 110| loss: 0.2549  | train_logloss: 0.25114 | train_accuracy: 0.90361 | valid_logloss: 0.26393 | valid_accuracy: 0.90041 |  0:21:00s\n",
            "epoch 111| loss: 0.25413 | train_logloss: 0.25166 | train_accuracy: 0.90317 | valid_logloss: 0.26406 | valid_accuracy: 0.9003  |  0:21:11s\n",
            "epoch 112| loss: 0.25467 | train_logloss: 0.25078 | train_accuracy: 0.90344 | valid_logloss: 0.26432 | valid_accuracy: 0.90016 |  0:21:23s\n",
            "epoch 113| loss: 0.25549 | train_logloss: 0.25556 | train_accuracy: 0.90206 | valid_logloss: 0.2679  | valid_accuracy: 0.89929 |  0:21:35s\n",
            "epoch 114| loss: 0.25599 | train_logloss: 0.25127 | train_accuracy: 0.90385 | valid_logloss: 0.26347 | valid_accuracy: 0.90105 |  0:21:47s\n",
            "epoch 115| loss: 0.25742 | train_logloss: 0.25339 | train_accuracy: 0.90309 | valid_logloss: 0.26503 | valid_accuracy: 0.90035 |  0:21:58s\n",
            "epoch 116| loss: 0.257   | train_logloss: 0.25644 | train_accuracy: 0.90175 | valid_logloss: 0.26723 | valid_accuracy: 0.89934 |  0:22:09s\n",
            "epoch 117| loss: 0.25794 | train_logloss: 0.25348 | train_accuracy: 0.90243 | valid_logloss: 0.26401 | valid_accuracy: 0.89998 |  0:22:21s\n",
            "epoch 118| loss: 0.25599 | train_logloss: 0.25083 | train_accuracy: 0.90354 | valid_logloss: 0.26262 | valid_accuracy: 0.90094 |  0:22:33s\n",
            "epoch 119| loss: 0.25802 | train_logloss: 0.25362 | train_accuracy: 0.90305 | valid_logloss: 0.26457 | valid_accuracy: 0.9004  |  0:22:44s\n",
            "epoch 120| loss: 0.25667 | train_logloss: 0.25272 | train_accuracy: 0.90372 | valid_logloss: 0.26426 | valid_accuracy: 0.9007  |  0:22:56s\n",
            "epoch 121| loss: 0.25553 | train_logloss: 0.25379 | train_accuracy: 0.90167 | valid_logloss: 0.26567 | valid_accuracy: 0.89864 |  0:23:08s\n",
            "epoch 122| loss: 0.25519 | train_logloss: 0.25122 | train_accuracy: 0.90377 | valid_logloss: 0.26443 | valid_accuracy: 0.90065 |  0:23:20s\n",
            "epoch 123| loss: 0.25347 | train_logloss: 0.2494  | train_accuracy: 0.90429 | valid_logloss: 0.26298 | valid_accuracy: 0.90072 |  0:23:32s\n",
            "epoch 124| loss: 0.2535  | train_logloss: 0.25147 | train_accuracy: 0.90387 | valid_logloss: 0.26418 | valid_accuracy: 0.90055 |  0:23:44s\n",
            "epoch 125| loss: 0.25417 | train_logloss: 0.24925 | train_accuracy: 0.904   | valid_logloss: 0.26228 | valid_accuracy: 0.9009  |  0:23:56s\n",
            "epoch 126| loss: 0.25389 | train_logloss: 0.25192 | train_accuracy: 0.90344 | valid_logloss: 0.26564 | valid_accuracy: 0.90015 |  0:24:08s\n",
            "epoch 127| loss: 0.25379 | train_logloss: 0.25011 | train_accuracy: 0.90398 | valid_logloss: 0.2635  | valid_accuracy: 0.90066 |  0:24:19s\n",
            "epoch 128| loss: 0.25312 | train_logloss: 0.24935 | train_accuracy: 0.90414 | valid_logloss: 0.26333 | valid_accuracy: 0.90038 |  0:24:31s\n",
            "epoch 129| loss: 0.25358 | train_logloss: 0.25138 | train_accuracy: 0.90322 | valid_logloss: 0.26544 | valid_accuracy: 0.90002 |  0:24:42s\n",
            "epoch 130| loss: 0.25389 | train_logloss: 0.25061 | train_accuracy: 0.9038  | valid_logloss: 0.26381 | valid_accuracy: 0.90015 |  0:24:53s\n",
            "epoch 131| loss: 0.25263 | train_logloss: 0.24931 | train_accuracy: 0.90411 | valid_logloss: 0.26298 | valid_accuracy: 0.9007  |  0:25:04s\n",
            "epoch 132| loss: 0.25265 | train_logloss: 0.24895 | train_accuracy: 0.90481 | valid_logloss: 0.2635  | valid_accuracy: 0.901   |  0:25:15s\n",
            "epoch 133| loss: 0.25251 | train_logloss: 0.24987 | train_accuracy: 0.90334 | valid_logloss: 0.26444 | valid_accuracy: 0.90043 |  0:25:26s\n",
            "epoch 134| loss: 0.25262 | train_logloss: 0.25133 | train_accuracy: 0.90346 | valid_logloss: 0.2644  | valid_accuracy: 0.90007 |  0:25:38s\n",
            "epoch 135| loss: 0.25251 | train_logloss: 0.24967 | train_accuracy: 0.90411 | valid_logloss: 0.26392 | valid_accuracy: 0.90078 |  0:25:49s\n",
            "epoch 136| loss: 0.25666 | train_logloss: 0.25893 | train_accuracy: 0.90118 | valid_logloss: 0.27088 | valid_accuracy: 0.89846 |  0:26:00s\n",
            "epoch 137| loss: 0.26248 | train_logloss: 0.25791 | train_accuracy: 0.90059 | valid_logloss: 0.26846 | valid_accuracy: 0.89808 |  0:26:11s\n",
            "epoch 138| loss: 0.25861 | train_logloss: 0.25422 | train_accuracy: 0.90269 | valid_logloss: 0.26546 | valid_accuracy: 0.89989 |  0:26:22s\n",
            "epoch 139| loss: 0.25576 | train_logloss: 0.25205 | train_accuracy: 0.90319 | valid_logloss: 0.26397 | valid_accuracy: 0.90043 |  0:26:34s\n",
            "epoch 140| loss: 0.25703 | train_logloss: 0.25289 | train_accuracy: 0.90273 | valid_logloss: 0.26519 | valid_accuracy: 0.90058 |  0:26:46s\n",
            "epoch 141| loss: 0.25575 | train_logloss: 0.25239 | train_accuracy: 0.90307 | valid_logloss: 0.26494 | valid_accuracy: 0.90036 |  0:26:59s\n",
            "epoch 142| loss: 0.25456 | train_logloss: 0.25011 | train_accuracy: 0.90371 | valid_logloss: 0.26305 | valid_accuracy: 0.90055 |  0:27:11s\n",
            "epoch 143| loss: 0.25398 | train_logloss: 0.25209 | train_accuracy: 0.90286 | valid_logloss: 0.26477 | valid_accuracy: 0.89962 |  0:27:22s\n",
            "epoch 144| loss: 0.25292 | train_logloss: 0.24739 | train_accuracy: 0.90455 | valid_logloss: 0.26177 | valid_accuracy: 0.90126 |  0:27:34s\n",
            "epoch 145| loss: 0.25309 | train_logloss: 0.25141 | train_accuracy: 0.90397 | valid_logloss: 0.26475 | valid_accuracy: 0.9002  |  0:27:45s\n",
            "epoch 146| loss: 0.25476 | train_logloss: 0.25135 | train_accuracy: 0.90341 | valid_logloss: 0.26501 | valid_accuracy: 0.90038 |  0:27:56s\n",
            "epoch 147| loss: 0.2539  | train_logloss: 0.25145 | train_accuracy: 0.90313 | valid_logloss: 0.26452 | valid_accuracy: 0.89993 |  0:28:07s\n",
            "epoch 148| loss: 0.25198 | train_logloss: 0.24924 | train_accuracy: 0.90449 | valid_logloss: 0.26376 | valid_accuracy: 0.90091 |  0:28:18s\n",
            "epoch 149| loss: 0.25206 | train_logloss: 0.24796 | train_accuracy: 0.90456 | valid_logloss: 0.2626  | valid_accuracy: 0.90112 |  0:28:30s\n",
            "epoch 150| loss: 0.25148 | train_logloss: 0.24877 | train_accuracy: 0.90429 | valid_logloss: 0.26367 | valid_accuracy: 0.90109 |  0:28:41s\n",
            "epoch 151| loss: 0.25196 | train_logloss: 0.24653 | train_accuracy: 0.90487 | valid_logloss: 0.2617  | valid_accuracy: 0.90143 |  0:28:52s\n",
            "epoch 152| loss: 0.25141 | train_logloss: 0.24678 | train_accuracy: 0.90477 | valid_logloss: 0.26193 | valid_accuracy: 0.90132 |  0:29:03s\n",
            "epoch 153| loss: 0.25305 | train_logloss: 0.24925 | train_accuracy: 0.90392 | valid_logloss: 0.26426 | valid_accuracy: 0.90055 |  0:29:14s\n",
            "epoch 154| loss: 0.25281 | train_logloss: 0.24877 | train_accuracy: 0.90434 | valid_logloss: 0.26406 | valid_accuracy: 0.90048 |  0:29:25s\n",
            "epoch 155| loss: 0.25146 | train_logloss: 0.24606 | train_accuracy: 0.90514 | valid_logloss: 0.26165 | valid_accuracy: 0.90144 |  0:29:36s\n",
            "epoch 156| loss: 0.2509  | train_logloss: 0.24699 | train_accuracy: 0.90508 | valid_logloss: 0.2623  | valid_accuracy: 0.90167 |  0:29:47s\n",
            "epoch 157| loss: 0.25089 | train_logloss: 0.24948 | train_accuracy: 0.90431 | valid_logloss: 0.26432 | valid_accuracy: 0.90047 |  0:29:58s\n",
            "epoch 158| loss: 0.25544 | train_logloss: 0.25424 | train_accuracy: 0.90263 | valid_logloss: 0.26634 | valid_accuracy: 0.89928 |  0:30:09s\n",
            "epoch 159| loss: 0.25513 | train_logloss: 0.25163 | train_accuracy: 0.90376 | valid_logloss: 0.26394 | valid_accuracy: 0.90066 |  0:30:20s\n",
            "epoch 160| loss: 0.2539  | train_logloss: 0.2486  | train_accuracy: 0.90465 | valid_logloss: 0.26194 | valid_accuracy: 0.90135 |  0:30:32s\n",
            "epoch 161| loss: 0.25286 | train_logloss: 0.24771 | train_accuracy: 0.9051  | valid_logloss: 0.26149 | valid_accuracy: 0.90158 |  0:30:43s\n",
            "epoch 162| loss: 0.25267 | train_logloss: 0.24865 | train_accuracy: 0.90502 | valid_logloss: 0.2623  | valid_accuracy: 0.90109 |  0:30:54s\n",
            "epoch 163| loss: 0.25102 | train_logloss: 0.24682 | train_accuracy: 0.90546 | valid_logloss: 0.2615  | valid_accuracy: 0.90126 |  0:31:06s\n",
            "epoch 164| loss: 0.25037 | train_logloss: 0.24821 | train_accuracy: 0.9046  | valid_logloss: 0.26352 | valid_accuracy: 0.90061 |  0:31:17s\n",
            "epoch 165| loss: 0.25283 | train_logloss: 0.24738 | train_accuracy: 0.905   | valid_logloss: 0.26229 | valid_accuracy: 0.9015  |  0:31:28s\n",
            "epoch 166| loss: 0.2509  | train_logloss: 0.24509 | train_accuracy: 0.90542 | valid_logloss: 0.26108 | valid_accuracy: 0.90143 |  0:31:39s\n",
            "epoch 167| loss: 0.25034 | train_logloss: 0.24725 | train_accuracy: 0.90486 | valid_logloss: 0.26292 | valid_accuracy: 0.90122 |  0:31:51s\n",
            "epoch 168| loss: 0.24998 | train_logloss: 0.24874 | train_accuracy: 0.90471 | valid_logloss: 0.26362 | valid_accuracy: 0.9009  |  0:32:02s\n",
            "epoch 169| loss: 0.24898 | train_logloss: 0.24742 | train_accuracy: 0.90455 | valid_logloss: 0.26412 | valid_accuracy: 0.90054 |  0:32:13s\n",
            "epoch 170| loss: 0.24934 | train_logloss: 0.24789 | train_accuracy: 0.90525 | valid_logloss: 0.26428 | valid_accuracy: 0.90082 |  0:32:24s\n",
            "epoch 171| loss: 0.24865 | train_logloss: 0.24504 | train_accuracy: 0.90505 | valid_logloss: 0.26278 | valid_accuracy: 0.9008  |  0:32:36s\n",
            "epoch 172| loss: 0.24946 | train_logloss: 0.2449  | train_accuracy: 0.90599 | valid_logloss: 0.26169 | valid_accuracy: 0.90141 |  0:32:47s\n",
            "epoch 173| loss: 0.24873 | train_logloss: 0.24473 | train_accuracy: 0.9062  | valid_logloss: 0.26136 | valid_accuracy: 0.90171 |  0:32:58s\n",
            "epoch 174| loss: 0.24887 | train_logloss: 0.24457 | train_accuracy: 0.90614 | valid_logloss: 0.26358 | valid_accuracy: 0.90136 |  0:33:10s\n",
            "epoch 175| loss: 0.25    | train_logloss: 0.24649 | train_accuracy: 0.90566 | valid_logloss: 0.26385 | valid_accuracy: 0.90097 |  0:33:21s\n",
            "epoch 176| loss: 0.2489  | train_logloss: 0.24331 | train_accuracy: 0.9066  | valid_logloss: 0.26208 | valid_accuracy: 0.90177 |  0:33:32s\n",
            "epoch 177| loss: 0.24827 | train_logloss: 0.243   | train_accuracy: 0.90658 | valid_logloss: 0.26179 | valid_accuracy: 0.90157 |  0:33:44s\n",
            "epoch 178| loss: 0.24844 | train_logloss: 0.24527 | train_accuracy: 0.9055  | valid_logloss: 0.26372 | valid_accuracy: 0.90127 |  0:33:55s\n",
            "epoch 179| loss: 0.24775 | train_logloss: 0.24662 | train_accuracy: 0.90503 | valid_logloss: 0.26466 | valid_accuracy: 0.89989 |  0:34:06s\n",
            "epoch 180| loss: 0.24815 | train_logloss: 0.24364 | train_accuracy: 0.90648 | valid_logloss: 0.26351 | valid_accuracy: 0.90114 |  0:34:18s\n",
            "epoch 181| loss: 0.24803 | train_logloss: 0.24515 | train_accuracy: 0.90626 | valid_logloss: 0.26423 | valid_accuracy: 0.90146 |  0:34:29s\n",
            "epoch 182| loss: 0.24949 | train_logloss: 0.24487 | train_accuracy: 0.90611 | valid_logloss: 0.26449 | valid_accuracy: 0.90127 |  0:34:41s\n",
            "epoch 183| loss: 0.24858 | train_logloss: 0.24383 | train_accuracy: 0.90657 | valid_logloss: 0.26249 | valid_accuracy: 0.9018  |  0:34:52s\n",
            "epoch 184| loss: 0.24727 | train_logloss: 0.24237 | train_accuracy: 0.90714 | valid_logloss: 0.26254 | valid_accuracy: 0.90148 |  0:35:03s\n",
            "epoch 185| loss: 0.24714 | train_logloss: 0.24294 | train_accuracy: 0.90692 | valid_logloss: 0.26255 | valid_accuracy: 0.90151 |  0:35:15s\n",
            "epoch 186| loss: 0.24691 | train_logloss: 0.24146 | train_accuracy: 0.90734 | valid_logloss: 0.26171 | valid_accuracy: 0.90199 |  0:35:26s\n",
            "epoch 187| loss: 0.2477  | train_logloss: 0.24425 | train_accuracy: 0.90575 | valid_logloss: 0.2643  | valid_accuracy: 0.90103 |  0:35:38s\n",
            "epoch 188| loss: 0.24713 | train_logloss: 0.24148 | train_accuracy: 0.9068  | valid_logloss: 0.26285 | valid_accuracy: 0.90144 |  0:35:49s\n",
            "epoch 189| loss: 0.24731 | train_logloss: 0.24263 | train_accuracy: 0.90638 | valid_logloss: 0.26292 | valid_accuracy: 0.90142 |  0:36:01s\n",
            "epoch 190| loss: 0.24751 | train_logloss: 0.24048 | train_accuracy: 0.90727 | valid_logloss: 0.26091 | valid_accuracy: 0.90197 |  0:36:12s\n",
            "epoch 191| loss: 0.24657 | train_logloss: 0.24333 | train_accuracy: 0.90642 | valid_logloss: 0.26332 | valid_accuracy: 0.90068 |  0:36:23s\n",
            "epoch 192| loss: 0.24673 | train_logloss: 0.24182 | train_accuracy: 0.90695 | valid_logloss: 0.26299 | valid_accuracy: 0.90167 |  0:36:35s\n",
            "epoch 193| loss: 0.24709 | train_logloss: 0.24166 | train_accuracy: 0.90731 | valid_logloss: 0.26174 | valid_accuracy: 0.90157 |  0:36:46s\n",
            "epoch 194| loss: 0.24614 | train_logloss: 0.24382 | train_accuracy: 0.90578 | valid_logloss: 0.26529 | valid_accuracy: 0.90041 |  0:36:57s\n",
            "epoch 195| loss: 0.24669 | train_logloss: 0.24245 | train_accuracy: 0.90704 | valid_logloss: 0.26231 | valid_accuracy: 0.9018  |  0:37:09s\n",
            "epoch 196| loss: 0.24587 | train_logloss: 0.24059 | train_accuracy: 0.90708 | valid_logloss: 0.2627  | valid_accuracy: 0.90125 |  0:37:20s\n",
            "epoch 197| loss: 0.24602 | train_logloss: 0.2454  | train_accuracy: 0.90524 | valid_logloss: 0.26642 | valid_accuracy: 0.89995 |  0:37:31s\n",
            "epoch 198| loss: 0.24651 | train_logloss: 0.24191 | train_accuracy: 0.90688 | valid_logloss: 0.26322 | valid_accuracy: 0.90151 |  0:37:43s\n",
            "epoch 199| loss: 0.24753 | train_logloss: 0.24371 | train_accuracy: 0.90655 | valid_logloss: 0.26388 | valid_accuracy: 0.90122 |  0:37:54s\n",
            "epoch 200| loss: 0.24816 | train_logloss: 0.24283 | train_accuracy: 0.90664 | valid_logloss: 0.26379 | valid_accuracy: 0.90128 |  0:38:06s\n",
            "epoch 201| loss: 0.24649 | train_logloss: 0.24133 | train_accuracy: 0.90705 | valid_logloss: 0.26255 | valid_accuracy: 0.90173 |  0:38:17s\n",
            "epoch 202| loss: 0.24519 | train_logloss: 0.24329 | train_accuracy: 0.90589 | valid_logloss: 0.26443 | valid_accuracy: 0.90046 |  0:38:29s\n",
            "epoch 203| loss: 0.24519 | train_logloss: 0.24109 | train_accuracy: 0.90739 | valid_logloss: 0.26395 | valid_accuracy: 0.90131 |  0:38:40s\n",
            "epoch 204| loss: 0.2459  | train_logloss: 0.24188 | train_accuracy: 0.90716 | valid_logloss: 0.26393 | valid_accuracy: 0.90155 |  0:38:52s\n",
            "epoch 205| loss: 0.24737 | train_logloss: 0.24354 | train_accuracy: 0.90603 | valid_logloss: 0.26494 | valid_accuracy: 0.90048 |  0:39:03s\n",
            "epoch 206| loss: 0.24635 | train_logloss: 0.24166 | train_accuracy: 0.90719 | valid_logloss: 0.26403 | valid_accuracy: 0.90137 |  0:39:15s\n",
            "epoch 207| loss: 0.24606 | train_logloss: 0.23987 | train_accuracy: 0.90734 | valid_logloss: 0.26274 | valid_accuracy: 0.90116 |  0:39:26s\n",
            "epoch 208| loss: 0.24491 | train_logloss: 0.23854 | train_accuracy: 0.90799 | valid_logloss: 0.26169 | valid_accuracy: 0.90202 |  0:39:37s\n",
            "epoch 209| loss: 0.24534 | train_logloss: 0.24124 | train_accuracy: 0.90699 | valid_logloss: 0.26269 | valid_accuracy: 0.90116 |  0:39:49s\n",
            "epoch 210| loss: 0.24705 | train_logloss: 0.24248 | train_accuracy: 0.90658 | valid_logloss: 0.26477 | valid_accuracy: 0.90018 |  0:40:00s\n",
            "epoch 211| loss: 0.2454  | train_logloss: 0.24105 | train_accuracy: 0.90722 | valid_logloss: 0.26333 | valid_accuracy: 0.90149 |  0:40:12s\n",
            "epoch 212| loss: 0.24534 | train_logloss: 0.24175 | train_accuracy: 0.90689 | valid_logloss: 0.2638  | valid_accuracy: 0.90112 |  0:40:23s\n",
            "epoch 213| loss: 0.24627 | train_logloss: 0.24224 | train_accuracy: 0.9062  | valid_logloss: 0.26482 | valid_accuracy: 0.9002  |  0:40:35s\n",
            "epoch 214| loss: 0.24663 | train_logloss: 0.24356 | train_accuracy: 0.9065  | valid_logloss: 0.26573 | valid_accuracy: 0.90035 |  0:40:46s\n",
            "epoch 215| loss: 0.24629 | train_logloss: 0.24076 | train_accuracy: 0.90744 | valid_logloss: 0.26206 | valid_accuracy: 0.9018  |  0:40:57s\n",
            "epoch 216| loss: 0.24873 | train_logloss: 0.24456 | train_accuracy: 0.90666 | valid_logloss: 0.26376 | valid_accuracy: 0.90114 |  0:41:09s\n",
            "epoch 217| loss: 0.25825 | train_logloss: 0.26122 | train_accuracy: 0.90082 | valid_logloss: 0.27325 | valid_accuracy: 0.89796 |  0:41:20s\n",
            "epoch 218| loss: 0.26291 | train_logloss: 0.25637 | train_accuracy: 0.90194 | valid_logloss: 0.26856 | valid_accuracy: 0.89922 |  0:41:32s\n",
            "epoch 219| loss: 0.26005 | train_logloss: 0.25271 | train_accuracy: 0.90292 | valid_logloss: 0.26583 | valid_accuracy: 0.8996  |  0:41:43s\n",
            "epoch 220| loss: 0.25779 | train_logloss: 0.25475 | train_accuracy: 0.9027  | valid_logloss: 0.26782 | valid_accuracy: 0.89962 |  0:41:54s\n",
            "epoch 221| loss: 0.25583 | train_logloss: 0.2502  | train_accuracy: 0.90369 | valid_logloss: 0.26556 | valid_accuracy: 0.90007 |  0:42:06s\n",
            "epoch 222| loss: 0.25284 | train_logloss: 0.24668 | train_accuracy: 0.90504 | valid_logloss: 0.2625  | valid_accuracy: 0.901   |  0:42:18s\n",
            "epoch 223| loss: 0.25181 | train_logloss: 0.24652 | train_accuracy: 0.90547 | valid_logloss: 0.26412 | valid_accuracy: 0.90077 |  0:42:29s\n",
            "epoch 224| loss: 0.25099 | train_logloss: 0.24655 | train_accuracy: 0.90514 | valid_logloss: 0.26446 | valid_accuracy: 0.90106 |  0:42:41s\n",
            "epoch 225| loss: 0.25145 | train_logloss: 0.24948 | train_accuracy: 0.90418 | valid_logloss: 0.26646 | valid_accuracy: 0.90004 |  0:42:52s\n",
            "epoch 226| loss: 0.25076 | train_logloss: 0.24716 | train_accuracy: 0.90476 | valid_logloss: 0.26518 | valid_accuracy: 0.90023 |  0:43:04s\n",
            "epoch 227| loss: 0.25052 | train_logloss: 0.2458  | train_accuracy: 0.90557 | valid_logloss: 0.26432 | valid_accuracy: 0.9006  |  0:43:15s\n",
            "epoch 228| loss: 0.25146 | train_logloss: 0.24769 | train_accuracy: 0.90529 | valid_logloss: 0.26592 | valid_accuracy: 0.90109 |  0:43:26s\n",
            "epoch 229| loss: 0.25172 | train_logloss: 0.25014 | train_accuracy: 0.90395 | valid_logloss: 0.26697 | valid_accuracy: 0.89974 |  0:43:38s\n",
            "epoch 230| loss: 0.25152 | train_logloss: 0.24753 | train_accuracy: 0.90485 | valid_logloss: 0.26505 | valid_accuracy: 0.90028 |  0:43:49s\n",
            "epoch 231| loss: 0.24992 | train_logloss: 0.24422 | train_accuracy: 0.90636 | valid_logloss: 0.26285 | valid_accuracy: 0.90108 |  0:44:01s\n",
            "epoch 232| loss: 0.2491  | train_logloss: 0.24471 | train_accuracy: 0.906   | valid_logloss: 0.26343 | valid_accuracy: 0.90137 |  0:44:12s\n",
            "epoch 233| loss: 0.24813 | train_logloss: 0.24511 | train_accuracy: 0.90613 | valid_logloss: 0.26381 | valid_accuracy: 0.90099 |  0:44:24s\n",
            "epoch 234| loss: 0.2521  | train_logloss: 0.25234 | train_accuracy: 0.90331 | valid_logloss: 0.26792 | valid_accuracy: 0.89984 |  0:44:35s\n",
            "epoch 235| loss: 0.25618 | train_logloss: 0.24888 | train_accuracy: 0.90437 | valid_logloss: 0.2652  | valid_accuracy: 0.90044 |  0:44:47s\n",
            "epoch 236| loss: 0.25214 | train_logloss: 0.24733 | train_accuracy: 0.90501 | valid_logloss: 0.26419 | valid_accuracy: 0.90105 |  0:44:58s\n",
            "epoch 237| loss: 0.2515  | train_logloss: 0.24552 | train_accuracy: 0.90598 | valid_logloss: 0.26294 | valid_accuracy: 0.90129 |  0:45:10s\n",
            "epoch 238| loss: 0.24943 | train_logloss: 0.2451  | train_accuracy: 0.90574 | valid_logloss: 0.26449 | valid_accuracy: 0.90038 |  0:45:21s\n",
            "epoch 239| loss: 0.24908 | train_logloss: 0.24413 | train_accuracy: 0.90637 | valid_logloss: 0.26333 | valid_accuracy: 0.9017  |  0:45:33s\n",
            "epoch 240| loss: 0.24757 | train_logloss: 0.24346 | train_accuracy: 0.9062  | valid_logloss: 0.26373 | valid_accuracy: 0.90078 |  0:45:44s\n",
            "epoch 241| loss: 0.24722 | train_logloss: 0.24419 | train_accuracy: 0.90624 | valid_logloss: 0.26334 | valid_accuracy: 0.90111 |  0:45:56s\n",
            "epoch 242| loss: 0.24809 | train_logloss: 0.24366 | train_accuracy: 0.90634 | valid_logloss: 0.26369 | valid_accuracy: 0.90086 |  0:46:08s\n",
            "epoch 243| loss: 0.24774 | train_logloss: 0.24581 | train_accuracy: 0.9059  | valid_logloss: 0.26459 | valid_accuracy: 0.90053 |  0:46:20s\n",
            "epoch 244| loss: 0.24665 | train_logloss: 0.24454 | train_accuracy: 0.90517 | valid_logloss: 0.26581 | valid_accuracy: 0.89989 |  0:46:32s\n",
            "epoch 245| loss: 0.24652 | train_logloss: 0.2415  | train_accuracy: 0.90705 | valid_logloss: 0.26276 | valid_accuracy: 0.90117 |  0:46:44s\n",
            "epoch 246| loss: 0.24677 | train_logloss: 0.24184 | train_accuracy: 0.90733 | valid_logloss: 0.26344 | valid_accuracy: 0.90123 |  0:46:56s\n",
            "epoch 247| loss: 0.24698 | train_logloss: 0.24455 | train_accuracy: 0.90622 | valid_logloss: 0.26537 | valid_accuracy: 0.90081 |  0:47:08s\n",
            "epoch 248| loss: 0.24776 | train_logloss: 0.24249 | train_accuracy: 0.90654 | valid_logloss: 0.26427 | valid_accuracy: 0.90129 |  0:47:19s\n",
            "epoch 249| loss: 0.24711 | train_logloss: 0.24274 | train_accuracy: 0.90648 | valid_logloss: 0.26435 | valid_accuracy: 0.90069 |  0:47:31s\n",
            "epoch 250| loss: 0.24619 | train_logloss: 0.24102 | train_accuracy: 0.90715 | valid_logloss: 0.26307 | valid_accuracy: 0.90101 |  0:47:42s\n",
            "epoch 251| loss: 0.24619 | train_logloss: 0.24185 | train_accuracy: 0.90708 | valid_logloss: 0.26425 | valid_accuracy: 0.90119 |  0:47:54s\n",
            "epoch 252| loss: 0.24588 | train_logloss: 0.24105 | train_accuracy: 0.90679 | valid_logloss: 0.26459 | valid_accuracy: 0.90083 |  0:48:06s\n",
            "epoch 253| loss: 0.24616 | train_logloss: 0.24037 | train_accuracy: 0.9074  | valid_logloss: 0.26315 | valid_accuracy: 0.90126 |  0:48:17s\n",
            "epoch 254| loss: 0.24526 | train_logloss: 0.24154 | train_accuracy: 0.90716 | valid_logloss: 0.26419 | valid_accuracy: 0.90132 |  0:48:29s\n",
            "epoch 255| loss: 0.24616 | train_logloss: 0.24145 | train_accuracy: 0.90697 | valid_logloss: 0.26531 | valid_accuracy: 0.90063 |  0:48:41s\n",
            "epoch 256| loss: 0.24571 | train_logloss: 0.24154 | train_accuracy: 0.90714 | valid_logloss: 0.26481 | valid_accuracy: 0.90105 |  0:48:52s\n",
            "epoch 257| loss: 0.24535 | train_logloss: 0.24082 | train_accuracy: 0.90714 | valid_logloss: 0.26459 | valid_accuracy: 0.90043 |  0:49:04s\n",
            "epoch 258| loss: 0.24562 | train_logloss: 0.24162 | train_accuracy: 0.90718 | valid_logloss: 0.26527 | valid_accuracy: 0.9011  |  0:49:15s\n",
            "\n",
            "Early stopping occurred at epoch 258 with best_epoch = 208 and best_valid_accuracy = 0.90202\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-mfBn97n9Jq",
        "outputId": "41d99273-6c9d-4cce-da97-8278bace67dd"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb14.pkl' 'tn14.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    252.0 MiB    252.0 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    270.2 MiB     18.2 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2315.6 MiB   2045.4 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoG5VV95VlNY"
      },
      "source": [
        "ones(number_exp=15, \n",
        "     Rows=300000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=16384,\tBV=1024,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltTzu3W1fva9",
        "outputId": "9b5a09cb-efcc-4155-c351-df14247544ca"
      },
      "source": [
        "memory(number_exp=20, \n",
        "     Rows=3000000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=30000,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=3,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 0.38582 | train_logloss: 5.25475 | train_accuracy: 0.13576 | valid_logloss: 4.21574 | valid_accuracy: 0.17169 |  0:01:05s\n",
            "epoch 1  | loss: 0.28353 | train_logloss: 1.27363 | train_accuracy: 0.30494 | valid_logloss: 1.20286 | valid_accuracy: 0.34235 |  0:02:10s\n",
            "epoch 2  | loss: 0.27088 | train_logloss: 0.72128 | train_accuracy: 0.72223 | valid_logloss: 0.58551 | valid_accuracy: 0.80174 |  0:03:14s\n",
            "epoch 3  | loss: 0.26378 | train_logloss: 0.38992 | train_accuracy: 0.85821 | valid_logloss: 0.32406 | valid_accuracy: 0.89515 |  0:04:19s\n",
            "epoch 4  | loss: 0.26047 | train_logloss: 0.28149 | train_accuracy: 0.89758 | valid_logloss: 0.24074 | valid_accuracy: 0.9191  |  0:05:24s\n",
            "epoch 5  | loss: 0.25833 | train_logloss: 0.26003 | train_accuracy: 0.905   | valid_logloss: 0.22064 | valid_accuracy: 0.92386 |  0:06:29s\n",
            "epoch 6  | loss: 0.25563 | train_logloss: 0.25203 | train_accuracy: 0.90783 | valid_logloss: 0.2188  | valid_accuracy: 0.92543 |  0:07:34s\n",
            "epoch 7  | loss: 0.25257 | train_logloss: 0.24885 | train_accuracy: 0.90921 | valid_logloss: 0.21741 | valid_accuracy: 0.92515 |  0:08:39s\n",
            "epoch 8  | loss: 0.24991 | train_logloss: 0.24599 | train_accuracy: 0.90989 | valid_logloss: 0.21593 | valid_accuracy: 0.92594 |  0:09:44s\n",
            "epoch 9  | loss: 0.24836 | train_logloss: 0.24645 | train_accuracy: 0.90969 | valid_logloss: 0.21619 | valid_accuracy: 0.92581 |  0:10:49s\n",
            "epoch 10 | loss: 0.24775 | train_logloss: 0.24681 | train_accuracy: 0.90873 | valid_logloss: 0.21975 | valid_accuracy: 0.92433 |  0:11:54s\n",
            "epoch 11 | loss: 0.24803 | train_logloss: 0.24327 | train_accuracy: 0.91071 | valid_logloss: 0.2127  | valid_accuracy: 0.92655 |  0:12:59s\n",
            "epoch 12 | loss: 0.24735 | train_logloss: 0.24531 | train_accuracy: 0.91018 | valid_logloss: 0.21809 | valid_accuracy: 0.9256  |  0:14:04s\n",
            "epoch 13 | loss: 0.24528 | train_logloss: 0.24114 | train_accuracy: 0.91173 | valid_logloss: 0.21532 | valid_accuracy: 0.92646 |  0:15:09s\n",
            "epoch 14 | loss: 0.24422 | train_logloss: 0.24019 | train_accuracy: 0.91209 | valid_logloss: 0.21086 | valid_accuracy: 0.92773 |  0:16:14s\n",
            "epoch 15 | loss: 0.24362 | train_logloss: 0.24089 | train_accuracy: 0.91174 | valid_logloss: 0.20943 | valid_accuracy: 0.92739 |  0:17:19s\n",
            "epoch 16 | loss: 0.24274 | train_logloss: 0.23909 | train_accuracy: 0.91234 | valid_logloss: 0.21266 | valid_accuracy: 0.9274  |  0:18:24s\n",
            "epoch 17 | loss: 0.24257 | train_logloss: 0.23809 | train_accuracy: 0.91247 | valid_logloss: 0.20961 | valid_accuracy: 0.92756 |  0:19:29s\n",
            "epoch 18 | loss: 0.24195 | train_logloss: 0.23779 | train_accuracy: 0.9127  | valid_logloss: 0.20695 | valid_accuracy: 0.92818 |  0:20:34s\n",
            "epoch 19 | loss: 0.24097 | train_logloss: 0.23681 | train_accuracy: 0.91325 | valid_logloss: 0.20893 | valid_accuracy: 0.92833 |  0:21:39s\n",
            "epoch 20 | loss: 0.24019 | train_logloss: 0.23625 | train_accuracy: 0.91345 | valid_logloss: 0.20909 | valid_accuracy: 0.92857 |  0:22:44s\n",
            "epoch 21 | loss: 0.24022 | train_logloss: 0.23686 | train_accuracy: 0.91319 | valid_logloss: 0.20958 | valid_accuracy: 0.92833 |  0:23:49s\n",
            "epoch 22 | loss: 0.23947 | train_logloss: 0.23716 | train_accuracy: 0.91309 | valid_logloss: 0.20681 | valid_accuracy: 0.9288  |  0:24:53s\n",
            "epoch 23 | loss: 0.23952 | train_logloss: 0.23627 | train_accuracy: 0.91336 | valid_logloss: 0.20817 | valid_accuracy: 0.92835 |  0:25:58s\n",
            "epoch 24 | loss: 0.2391  | train_logloss: 0.23675 | train_accuracy: 0.91294 | valid_logloss: 0.20947 | valid_accuracy: 0.92745 |  0:27:03s\n",
            "epoch 25 | loss: 0.23838 | train_logloss: 0.23544 | train_accuracy: 0.9139  | valid_logloss: 0.20488 | valid_accuracy: 0.92907 |  0:28:07s\n",
            "epoch 26 | loss: 0.23808 | train_logloss: 0.23614 | train_accuracy: 0.91349 | valid_logloss: 0.20997 | valid_accuracy: 0.92766 |  0:29:12s\n",
            "epoch 27 | loss: 0.23933 | train_logloss: 0.23756 | train_accuracy: 0.91296 | valid_logloss: 0.20789 | valid_accuracy: 0.92885 |  0:30:16s\n",
            "epoch 28 | loss: 0.23874 | train_logloss: 0.23567 | train_accuracy: 0.91356 | valid_logloss: 0.21002 | valid_accuracy: 0.92801 |  0:31:21s\n",
            "epoch 29 | loss: 0.2382  | train_logloss: 0.23522 | train_accuracy: 0.91363 | valid_logloss: 0.20886 | valid_accuracy: 0.92831 |  0:32:26s\n",
            "epoch 30 | loss: 0.23763 | train_logloss: 0.23656 | train_accuracy: 0.91333 | valid_logloss: 0.215   | valid_accuracy: 0.92663 |  0:33:30s\n",
            "epoch 31 | loss: 0.23741 | train_logloss: 0.23391 | train_accuracy: 0.9143  | valid_logloss: 0.20716 | valid_accuracy: 0.92897 |  0:34:35s\n",
            "epoch 32 | loss: 0.23668 | train_logloss: 0.23424 | train_accuracy: 0.91408 | valid_logloss: 0.20501 | valid_accuracy: 0.92919 |  0:35:40s\n",
            "epoch 33 | loss: 0.23698 | train_logloss: 0.23423 | train_accuracy: 0.91397 | valid_logloss: 0.2096  | valid_accuracy: 0.9284  |  0:36:44s\n",
            "epoch 34 | loss: 0.23654 | train_logloss: 0.23466 | train_accuracy: 0.9139  | valid_logloss: 0.20508 | valid_accuracy: 0.92918 |  0:37:49s\n",
            "epoch 35 | loss: 0.23634 | train_logloss: 0.23611 | train_accuracy: 0.91353 | valid_logloss: 0.21317 | valid_accuracy: 0.92692 |  0:38:54s\n",
            "epoch 36 | loss: 0.23579 | train_logloss: 0.23322 | train_accuracy: 0.91454 | valid_logloss: 0.20382 | valid_accuracy: 0.92946 |  0:39:59s\n",
            "epoch 37 | loss: 0.23609 | train_logloss: 0.23467 | train_accuracy: 0.91392 | valid_logloss: 0.20462 | valid_accuracy: 0.9295  |  0:41:03s\n",
            "epoch 38 | loss: 0.23695 | train_logloss: 0.23511 | train_accuracy: 0.91388 | valid_logloss: 0.2042  | valid_accuracy: 0.92992 |  0:42:08s\n",
            "epoch 39 | loss: 0.23674 | train_logloss: 0.23279 | train_accuracy: 0.91449 | valid_logloss: 0.20615 | valid_accuracy: 0.92936 |  0:43:13s\n",
            "epoch 40 | loss: 0.23574 | train_logloss: 0.23203 | train_accuracy: 0.91478 | valid_logloss: 0.20728 | valid_accuracy: 0.92856 |  0:44:18s\n",
            "epoch 41 | loss: 0.23523 | train_logloss: 0.23319 | train_accuracy: 0.91412 | valid_logloss: 0.20292 | valid_accuracy: 0.92962 |  0:45:22s\n",
            "epoch 42 | loss: 0.23549 | train_logloss: 0.23415 | train_accuracy: 0.91427 | valid_logloss: 0.20968 | valid_accuracy: 0.92841 |  0:46:27s\n",
            "epoch 43 | loss: 0.23508 | train_logloss: 0.23288 | train_accuracy: 0.91459 | valid_logloss: 0.20471 | valid_accuracy: 0.92928 |  0:47:32s\n",
            "epoch 44 | loss: 0.23539 | train_logloss: 0.23226 | train_accuracy: 0.91488 | valid_logloss: 0.20687 | valid_accuracy: 0.92919 |  0:48:37s\n",
            "epoch 45 | loss: 0.23473 | train_logloss: 0.2329  | train_accuracy: 0.91455 | valid_logloss: 0.20543 | valid_accuracy: 0.92955 |  0:49:42s\n",
            "epoch 46 | loss: 0.2347  | train_logloss: 0.23307 | train_accuracy: 0.914   | valid_logloss: 0.20572 | valid_accuracy: 0.92889 |  0:50:46s\n",
            "epoch 47 | loss: 0.23524 | train_logloss: 0.23211 | train_accuracy: 0.91478 | valid_logloss: 0.20598 | valid_accuracy: 0.92901 |  0:51:51s\n",
            "epoch 48 | loss: 0.23507 | train_logloss: 0.23147 | train_accuracy: 0.91509 | valid_logloss: 0.2025  | valid_accuracy: 0.93013 |  0:52:56s\n",
            "epoch 49 | loss: 0.23479 | train_logloss: 0.2341  | train_accuracy: 0.91412 | valid_logloss: 0.20845 | valid_accuracy: 0.92857 |  0:54:01s\n",
            "epoch 50 | loss: 0.23455 | train_logloss: 0.23398 | train_accuracy: 0.91425 | valid_logloss: 0.21211 | valid_accuracy: 0.92822 |  0:55:06s\n",
            "epoch 51 | loss: 0.23463 | train_logloss: 0.23277 | train_accuracy: 0.91444 | valid_logloss: 0.20422 | valid_accuracy: 0.92947 |  0:56:11s\n",
            "epoch 52 | loss: 0.23427 | train_logloss: 0.23116 | train_accuracy: 0.91503 | valid_logloss: 0.20399 | valid_accuracy: 0.92987 |  0:57:16s\n",
            "epoch 53 | loss: 0.23397 | train_logloss: 0.23197 | train_accuracy: 0.91505 | valid_logloss: 0.20701 | valid_accuracy: 0.92948 |  0:58:22s\n",
            "epoch 54 | loss: 0.23395 | train_logloss: 0.23078 | train_accuracy: 0.91523 | valid_logloss: 0.20646 | valid_accuracy: 0.92906 |  0:59:28s\n",
            "epoch 55 | loss: 0.23414 | train_logloss: 0.23136 | train_accuracy: 0.91495 | valid_logloss: 0.20371 | valid_accuracy: 0.92989 |  1:00:33s\n",
            "epoch 56 | loss: 0.23378 | train_logloss: 0.23131 | train_accuracy: 0.91507 | valid_logloss: 0.20146 | valid_accuracy: 0.93036 |  1:01:37s\n",
            "epoch 57 | loss: 0.23339 | train_logloss: 0.23186 | train_accuracy: 0.91491 | valid_logloss: 0.20479 | valid_accuracy: 0.92975 |  1:02:42s\n",
            "epoch 58 | loss: 0.23378 | train_logloss: 0.23174 | train_accuracy: 0.91503 | valid_logloss: 0.2038  | valid_accuracy: 0.92953 |  1:03:47s\n",
            "epoch 59 | loss: 0.23384 | train_logloss: 0.23282 | train_accuracy: 0.91453 | valid_logloss: 0.2082  | valid_accuracy: 0.92825 |  1:04:54s\n",
            "epoch 60 | loss: 0.23403 | train_logloss: 0.23282 | train_accuracy: 0.91444 | valid_logloss: 0.20834 | valid_accuracy: 0.92857 |  1:05:59s\n",
            "epoch 61 | loss: 0.23411 | train_logloss: 0.23253 | train_accuracy: 0.91484 | valid_logloss: 0.20964 | valid_accuracy: 0.92879 |  1:07:05s\n",
            "epoch 62 | loss: 0.23443 | train_logloss: 0.23151 | train_accuracy: 0.91505 | valid_logloss: 0.2096  | valid_accuracy: 0.92827 |  1:08:11s\n",
            "epoch 63 | loss: 0.23395 | train_logloss: 0.23242 | train_accuracy: 0.91468 | valid_logloss: 0.20181 | valid_accuracy: 0.93036 |  1:09:15s\n",
            "epoch 64 | loss: 0.23364 | train_logloss: 0.23193 | train_accuracy: 0.91483 | valid_logloss: 0.20702 | valid_accuracy: 0.9288  |  1:10:20s\n",
            "epoch 65 | loss: 0.23346 | train_logloss: 0.23106 | train_accuracy: 0.91508 | valid_logloss: 0.20725 | valid_accuracy: 0.92901 |  1:11:25s\n",
            "epoch 66 | loss: 0.23325 | train_logloss: 0.23125 | train_accuracy: 0.91494 | valid_logloss: 0.20854 | valid_accuracy: 0.92797 |  1:12:29s\n",
            "epoch 67 | loss: 0.233   | train_logloss: 0.23004 | train_accuracy: 0.91551 | valid_logloss: 0.20637 | valid_accuracy: 0.92956 |  1:13:34s\n",
            "epoch 68 | loss: 0.23304 | train_logloss: 0.23041 | train_accuracy: 0.91545 | valid_logloss: 0.20366 | valid_accuracy: 0.92971 |  1:14:38s\n",
            "epoch 69 | loss: 0.23274 | train_logloss: 0.22987 | train_accuracy: 0.9155  | valid_logloss: 0.2069  | valid_accuracy: 0.92896 |  1:15:43s\n",
            "epoch 70 | loss: 0.23282 | train_logloss: 0.22967 | train_accuracy: 0.91581 | valid_logloss: 0.20366 | valid_accuracy: 0.93025 |  1:16:47s\n",
            "epoch 71 | loss: 0.23267 | train_logloss: 0.23028 | train_accuracy: 0.91552 | valid_logloss: 0.20341 | valid_accuracy: 0.92997 |  1:17:53s\n",
            "epoch 72 | loss: 0.23301 | train_logloss: 0.22999 | train_accuracy: 0.91541 | valid_logloss: 0.20418 | valid_accuracy: 0.92995 |  1:18:58s\n",
            "epoch 73 | loss: 0.2343  | train_logloss: 0.23094 | train_accuracy: 0.91536 | valid_logloss: 0.20774 | valid_accuracy: 0.92917 |  1:20:03s\n",
            "epoch 74 | loss: 0.23314 | train_logloss: 0.23003 | train_accuracy: 0.91558 | valid_logloss: 0.2057  | valid_accuracy: 0.9296  |  1:21:08s\n",
            "epoch 75 | loss: 0.2324  | train_logloss: 0.2294  | train_accuracy: 0.91597 | valid_logloss: 0.20209 | valid_accuracy: 0.93024 |  1:22:13s\n",
            "epoch 76 | loss: 0.2325  | train_logloss: 0.22993 | train_accuracy: 0.9155  | valid_logloss: 0.20331 | valid_accuracy: 0.92992 |  1:23:18s\n",
            "epoch 77 | loss: 0.23262 | train_logloss: 0.22956 | train_accuracy: 0.91589 | valid_logloss: 0.20502 | valid_accuracy: 0.92975 |  1:24:23s\n",
            "epoch 78 | loss: 0.23221 | train_logloss: 0.23063 | train_accuracy: 0.91521 | valid_logloss: 0.2046  | valid_accuracy: 0.92963 |  1:25:28s\n",
            "epoch 79 | loss: 0.23207 | train_logloss: 0.23255 | train_accuracy: 0.91497 | valid_logloss: 0.20801 | valid_accuracy: 0.92858 |  1:26:32s\n",
            "epoch 80 | loss: 0.23433 | train_logloss: 0.23014 | train_accuracy: 0.91563 | valid_logloss: 0.20427 | valid_accuracy: 0.92943 |  1:27:37s\n",
            "epoch 81 | loss: 0.23284 | train_logloss: 0.23019 | train_accuracy: 0.91572 | valid_logloss: 0.20784 | valid_accuracy: 0.92923 |  1:28:42s\n",
            "epoch 82 | loss: 0.23233 | train_logloss: 0.22949 | train_accuracy: 0.91564 | valid_logloss: 0.2041  | valid_accuracy: 0.92977 |  1:29:47s\n",
            "epoch 83 | loss: 0.23195 | train_logloss: 0.22984 | train_accuracy: 0.91546 | valid_logloss: 0.20612 | valid_accuracy: 0.92916 |  1:30:51s\n",
            "epoch 84 | loss: 0.23248 | train_logloss: 0.23085 | train_accuracy: 0.91503 | valid_logloss: 0.20512 | valid_accuracy: 0.92949 |  1:31:56s\n",
            "epoch 85 | loss: 0.2323  | train_logloss: 0.22984 | train_accuracy: 0.91565 | valid_logloss: 0.20778 | valid_accuracy: 0.92898 |  1:33:01s\n",
            "epoch 86 | loss: 0.23187 | train_logloss: 0.22975 | train_accuracy: 0.91577 | valid_logloss: 0.20137 | valid_accuracy: 0.93049 |  1:34:05s\n",
            "epoch 87 | loss: 0.23184 | train_logloss: 0.22995 | train_accuracy: 0.9154  | valid_logloss: 0.2036  | valid_accuracy: 0.92994 |  1:35:10s\n",
            "epoch 88 | loss: 0.23284 | train_logloss: 0.23087 | train_accuracy: 0.91546 | valid_logloss: 0.2056  | valid_accuracy: 0.9299  |  1:36:15s\n",
            "epoch 89 | loss: 0.23283 | train_logloss: 0.23001 | train_accuracy: 0.91559 | valid_logloss: 0.20621 | valid_accuracy: 0.92957 |  1:37:19s\n",
            "epoch 90 | loss: 0.23252 | train_logloss: 0.22928 | train_accuracy: 0.91573 | valid_logloss: 0.20321 | valid_accuracy: 0.93    |  1:38:25s\n",
            "epoch 91 | loss: 0.23259 | train_logloss: 0.23124 | train_accuracy: 0.91512 | valid_logloss: 0.20596 | valid_accuracy: 0.92935 |  1:39:31s\n",
            "epoch 92 | loss: 0.23238 | train_logloss: 0.2288  | train_accuracy: 0.91599 | valid_logloss: 0.20346 | valid_accuracy: 0.93024 |  1:40:36s\n",
            "epoch 93 | loss: 0.23172 | train_logloss: 0.22895 | train_accuracy: 0.91584 | valid_logloss: 0.2041  | valid_accuracy: 0.92997 |  1:41:41s\n",
            "epoch 94 | loss: 0.2319  | train_logloss: 0.22852 | train_accuracy: 0.91614 | valid_logloss: 0.20397 | valid_accuracy: 0.93007 |  1:42:46s\n",
            "epoch 95 | loss: 0.23187 | train_logloss: 0.22931 | train_accuracy: 0.91578 | valid_logloss: 0.20727 | valid_accuracy: 0.92899 |  1:43:51s\n",
            "epoch 96 | loss: 0.23169 | train_logloss: 0.22959 | train_accuracy: 0.91553 | valid_logloss: 0.20335 | valid_accuracy: 0.9302  |  1:44:56s\n",
            "epoch 97 | loss: 0.2315  | train_logloss: 0.22888 | train_accuracy: 0.9158  | valid_logloss: 0.20357 | valid_accuracy: 0.92982 |  1:46:01s\n",
            "epoch 98 | loss: 0.23142 | train_logloss: 0.22892 | train_accuracy: 0.91595 | valid_logloss: 0.20593 | valid_accuracy: 0.92947 |  1:47:06s\n",
            "epoch 99 | loss: 0.23109 | train_logloss: 0.22832 | train_accuracy: 0.91637 | valid_logloss: 0.20207 | valid_accuracy: 0.93038 |  1:48:11s\n",
            "epoch 100| loss: 0.23159 | train_logloss: 0.22996 | train_accuracy: 0.91551 | valid_logloss: 0.20294 | valid_accuracy: 0.93035 |  1:49:16s\n",
            "epoch 101| loss: 0.23441 | train_logloss: 0.23169 | train_accuracy: 0.91483 | valid_logloss: 0.20661 | valid_accuracy: 0.92932 |  1:50:20s\n",
            "epoch 102| loss: 0.23303 | train_logloss: 0.23931 | train_accuracy: 0.91268 | valid_logloss: 0.21168 | valid_accuracy: 0.9276  |  1:51:25s\n",
            "epoch 103| loss: 0.23643 | train_logloss: 0.23198 | train_accuracy: 0.91489 | valid_logloss: 0.20615 | valid_accuracy: 0.92939 |  1:52:30s\n",
            "epoch 104| loss: 0.23371 | train_logloss: 0.23007 | train_accuracy: 0.91573 | valid_logloss: 0.20334 | valid_accuracy: 0.92987 |  1:53:35s\n",
            "epoch 105| loss: 0.23323 | train_logloss: 0.23024 | train_accuracy: 0.91544 | valid_logloss: 0.20491 | valid_accuracy: 0.92952 |  1:54:40s\n",
            "epoch 106| loss: 0.23238 | train_logloss: 0.229   | train_accuracy: 0.91595 | valid_logloss: 0.20254 | valid_accuracy: 0.93055 |  1:55:45s\n",
            "epoch 107| loss: 0.23264 | train_logloss: 0.22957 | train_accuracy: 0.91566 | valid_logloss: 0.20449 | valid_accuracy: 0.92964 |  1:56:49s\n",
            "epoch 108| loss: 0.23203 | train_logloss: 0.22972 | train_accuracy: 0.91572 | valid_logloss: 0.20464 | valid_accuracy: 0.92974 |  1:57:55s\n",
            "epoch 109| loss: 0.23161 | train_logloss: 0.22869 | train_accuracy: 0.91608 | valid_logloss: 0.20318 | valid_accuracy: 0.93042 |  1:58:59s\n",
            "epoch 110| loss: 0.23132 | train_logloss: 0.22865 | train_accuracy: 0.91615 | valid_logloss: 0.20235 | valid_accuracy: 0.93054 |  2:00:04s\n",
            "epoch 111| loss: 0.23113 | train_logloss: 0.22855 | train_accuracy: 0.91622 | valid_logloss: 0.20352 | valid_accuracy: 0.93005 |  2:01:09s\n",
            "epoch 112| loss: 0.23153 | train_logloss: 0.22849 | train_accuracy: 0.91604 | valid_logloss: 0.20241 | valid_accuracy: 0.93014 |  2:02:14s\n",
            "epoch 113| loss: 0.23119 | train_logloss: 0.22826 | train_accuracy: 0.91609 | valid_logloss: 0.20532 | valid_accuracy: 0.92956 |  2:03:18s\n",
            "epoch 114| loss: 0.23148 | train_logloss: 0.22917 | train_accuracy: 0.91603 | valid_logloss: 0.20377 | valid_accuracy: 0.93007 |  2:04:23s\n",
            "epoch 115| loss: 0.23119 | train_logloss: 0.22949 | train_accuracy: 0.91546 | valid_logloss: 0.20413 | valid_accuracy: 0.92967 |  2:05:28s\n",
            "epoch 116| loss: 0.23099 | train_logloss: 0.22861 | train_accuracy: 0.91607 | valid_logloss: 0.20378 | valid_accuracy: 0.93037 |  2:06:32s\n",
            "epoch 117| loss: 0.23062 | train_logloss: 0.22821 | train_accuracy: 0.91616 | valid_logloss: 0.20131 | valid_accuracy: 0.93078 |  2:07:37s\n",
            "epoch 118| loss: 0.23089 | train_logloss: 0.22872 | train_accuracy: 0.91592 | valid_logloss: 0.20199 | valid_accuracy: 0.93027 |  2:08:42s\n",
            "epoch 119| loss: 0.23052 | train_logloss: 0.22819 | train_accuracy: 0.91617 | valid_logloss: 0.20306 | valid_accuracy: 0.92999 |  2:09:46s\n",
            "epoch 120| loss: 0.23075 | train_logloss: 0.22832 | train_accuracy: 0.91616 | valid_logloss: 0.20577 | valid_accuracy: 0.92921 |  2:10:50s\n",
            "epoch 121| loss: 0.23323 | train_logloss: 0.23028 | train_accuracy: 0.91537 | valid_logloss: 0.20696 | valid_accuracy: 0.92925 |  2:11:54s\n",
            "epoch 122| loss: 0.23225 | train_logloss: 0.22857 | train_accuracy: 0.91605 | valid_logloss: 0.20231 | valid_accuracy: 0.93018 |  2:12:59s\n",
            "epoch 123| loss: 0.23123 | train_logloss: 0.22876 | train_accuracy: 0.91599 | valid_logloss: 0.206   | valid_accuracy: 0.92964 |  2:14:03s\n",
            "epoch 124| loss: 0.23153 | train_logloss: 0.22931 | train_accuracy: 0.91578 | valid_logloss: 0.20635 | valid_accuracy: 0.92958 |  2:15:07s\n",
            "epoch 125| loss: 0.23121 | train_logloss: 0.22874 | train_accuracy: 0.91599 | valid_logloss: 0.20621 | valid_accuracy: 0.92972 |  2:16:11s\n",
            "epoch 126| loss: 0.23109 | train_logloss: 0.22952 | train_accuracy: 0.91566 | valid_logloss: 0.20679 | valid_accuracy: 0.92888 |  2:17:16s\n",
            "epoch 127| loss: 0.23067 | train_logloss: 0.22835 | train_accuracy: 0.91627 | valid_logloss: 0.20126 | valid_accuracy: 0.93085 |  2:18:20s\n",
            "epoch 128| loss: 0.23099 | train_logloss: 0.22778 | train_accuracy: 0.91624 | valid_logloss: 0.20141 | valid_accuracy: 0.93071 |  2:19:24s\n",
            "epoch 129| loss: 0.23056 | train_logloss: 0.22791 | train_accuracy: 0.91632 | valid_logloss: 0.20328 | valid_accuracy: 0.9302  |  2:20:29s\n",
            "epoch 130| loss: 0.23039 | train_logloss: 0.2289  | train_accuracy: 0.91599 | valid_logloss: 0.2081  | valid_accuracy: 0.92925 |  2:21:33s\n",
            "epoch 131| loss: 0.23047 | train_logloss: 0.22824 | train_accuracy: 0.91616 | valid_logloss: 0.20254 | valid_accuracy: 0.93034 |  2:22:37s\n",
            "epoch 132| loss: 0.23167 | train_logloss: 0.23089 | train_accuracy: 0.91529 | valid_logloss: 0.20266 | valid_accuracy: 0.93022 |  2:23:42s\n",
            "epoch 133| loss: 0.23128 | train_logloss: 0.22805 | train_accuracy: 0.91583 | valid_logloss: 0.20418 | valid_accuracy: 0.92985 |  2:24:46s\n",
            "epoch 134| loss: 0.23074 | train_logloss: 0.22767 | train_accuracy: 0.9163  | valid_logloss: 0.20411 | valid_accuracy: 0.92968 |  2:25:51s\n",
            "epoch 135| loss: 0.23062 | train_logloss: 0.22792 | train_accuracy: 0.91628 | valid_logloss: 0.20474 | valid_accuracy: 0.93014 |  2:26:55s\n",
            "epoch 136| loss: 0.23018 | train_logloss: 0.22839 | train_accuracy: 0.9162  | valid_logloss: 0.20275 | valid_accuracy: 0.9304  |  2:27:59s\n",
            "epoch 137| loss: 0.22999 | train_logloss: 0.22763 | train_accuracy: 0.91652 | valid_logloss: 0.20244 | valid_accuracy: 0.93057 |  2:29:04s\n",
            "epoch 138| loss: 0.23005 | train_logloss: 0.22773 | train_accuracy: 0.91646 | valid_logloss: 0.20697 | valid_accuracy: 0.92957 |  2:30:08s\n",
            "epoch 139| loss: 0.23002 | train_logloss: 0.22854 | train_accuracy: 0.91614 | valid_logloss: 0.203   | valid_accuracy: 0.93008 |  2:31:12s\n",
            "epoch 140| loss: 0.23046 | train_logloss: 0.22759 | train_accuracy: 0.91638 | valid_logloss: 0.20576 | valid_accuracy: 0.92949 |  2:32:16s\n",
            "epoch 141| loss: 0.23037 | train_logloss: 0.22825 | train_accuracy: 0.91609 | valid_logloss: 0.20332 | valid_accuracy: 0.9299  |  2:33:21s\n",
            "epoch 142| loss: 0.23063 | train_logloss: 0.22784 | train_accuracy: 0.91636 | valid_logloss: 0.20744 | valid_accuracy: 0.92938 |  2:34:25s\n",
            "epoch 143| loss: 0.23023 | train_logloss: 0.22716 | train_accuracy: 0.9166  | valid_logloss: 0.20237 | valid_accuracy: 0.93038 |  2:35:30s\n",
            "epoch 144| loss: 0.23016 | train_logloss: 0.22648 | train_accuracy: 0.91674 | valid_logloss: 0.20285 | valid_accuracy: 0.93019 |  2:36:34s\n",
            "epoch 145| loss: 0.2297  | train_logloss: 0.22681 | train_accuracy: 0.91662 | valid_logloss: 0.20274 | valid_accuracy: 0.93003 |  2:37:38s\n",
            "epoch 146| loss: 0.2299  | train_logloss: 0.22727 | train_accuracy: 0.91651 | valid_logloss: 0.20361 | valid_accuracy: 0.93007 |  2:38:42s\n",
            "epoch 147| loss: 0.2296  | train_logloss: 0.22677 | train_accuracy: 0.91677 | valid_logloss: 0.20242 | valid_accuracy: 0.93054 |  2:39:47s\n",
            "epoch 148| loss: 0.22973 | train_logloss: 0.22673 | train_accuracy: 0.91666 | valid_logloss: 0.20213 | valid_accuracy: 0.93064 |  2:40:51s\n",
            "epoch 149| loss: 0.23021 | train_logloss: 0.22761 | train_accuracy: 0.91646 | valid_logloss: 0.20827 | valid_accuracy: 0.92924 |  2:41:55s\n",
            "epoch 150| loss: 0.22971 | train_logloss: 0.22734 | train_accuracy: 0.91638 | valid_logloss: 0.20268 | valid_accuracy: 0.93027 |  2:42:59s\n",
            "epoch 151| loss: 0.22936 | train_logloss: 0.22719 | train_accuracy: 0.91659 | valid_logloss: 0.20631 | valid_accuracy: 0.92939 |  2:44:04s\n",
            "epoch 152| loss: 0.22969 | train_logloss: 0.22911 | train_accuracy: 0.91588 | valid_logloss: 0.2111  | valid_accuracy: 0.92839 |  2:45:08s\n",
            "epoch 153| loss: 0.22988 | train_logloss: 0.22771 | train_accuracy: 0.91635 | valid_logloss: 0.20172 | valid_accuracy: 0.93064 |  2:46:12s\n",
            "epoch 154| loss: 0.22989 | train_logloss: 0.22761 | train_accuracy: 0.91624 | valid_logloss: 0.20418 | valid_accuracy: 0.92958 |  2:47:17s\n",
            "epoch 155| loss: 0.22981 | train_logloss: 0.22752 | train_accuracy: 0.91641 | valid_logloss: 0.20632 | valid_accuracy: 0.92911 |  2:48:21s\n",
            "epoch 156| loss: 0.22969 | train_logloss: 0.22788 | train_accuracy: 0.91621 | valid_logloss: 0.20314 | valid_accuracy: 0.92987 |  2:49:25s\n",
            "epoch 157| loss: 0.22962 | train_logloss: 0.22647 | train_accuracy: 0.91688 | valid_logloss: 0.20214 | valid_accuracy: 0.93055 |  2:50:31s\n",
            "epoch 158| loss: 0.2298  | train_logloss: 0.22668 | train_accuracy: 0.91669 | valid_logloss: 0.20202 | valid_accuracy: 0.93054 |  2:51:36s\n",
            "epoch 159| loss: 0.22952 | train_logloss: 0.22642 | train_accuracy: 0.91672 | valid_logloss: 0.20192 | valid_accuracy: 0.93037 |  2:52:40s\n",
            "epoch 160| loss: 0.22946 | train_logloss: 0.22787 | train_accuracy: 0.9162  | valid_logloss: 0.20374 | valid_accuracy: 0.92983 |  2:53:44s\n",
            "epoch 161| loss: 0.22941 | train_logloss: 0.22728 | train_accuracy: 0.91651 | valid_logloss: 0.20555 | valid_accuracy: 0.9298  |  2:54:48s\n",
            "epoch 162| loss: 0.23003 | train_logloss: 0.22797 | train_accuracy: 0.91632 | valid_logloss: 0.20092 | valid_accuracy: 0.93088 |  2:55:53s\n",
            "epoch 163| loss: 0.22967 | train_logloss: 0.22849 | train_accuracy: 0.91598 | valid_logloss: 0.20133 | valid_accuracy: 0.93019 |  2:56:57s\n",
            "epoch 164| loss: 0.22956 | train_logloss: 0.22678 | train_accuracy: 0.91676 | valid_logloss: 0.20297 | valid_accuracy: 0.93059 |  2:58:01s\n",
            "epoch 165| loss: 0.22968 | train_logloss: 0.22834 | train_accuracy: 0.91627 | valid_logloss: 0.20061 | valid_accuracy: 0.93063 |  2:59:06s\n",
            "epoch 166| loss: 0.22975 | train_logloss: 0.22771 | train_accuracy: 0.91635 | valid_logloss: 0.20531 | valid_accuracy: 0.92966 |  3:00:10s\n",
            "epoch 167| loss: 0.2294  | train_logloss: 0.22626 | train_accuracy: 0.9169  | valid_logloss: 0.2025  | valid_accuracy: 0.93049 |  3:01:14s\n",
            "epoch 168| loss: 0.22933 | train_logloss: 0.22805 | train_accuracy: 0.91623 | valid_logloss: 0.20687 | valid_accuracy: 0.92943 |  3:02:18s\n",
            "epoch 169| loss: 0.22952 | train_logloss: 0.22637 | train_accuracy: 0.91684 | valid_logloss: 0.20446 | valid_accuracy: 0.92999 |  3:03:23s\n",
            "epoch 170| loss: 0.22929 | train_logloss: 0.22853 | train_accuracy: 0.91605 | valid_logloss: 0.20912 | valid_accuracy: 0.92879 |  3:04:27s\n",
            "epoch 171| loss: 0.22979 | train_logloss: 0.23046 | train_accuracy: 0.91574 | valid_logloss: 0.20384 | valid_accuracy: 0.9299  |  3:05:31s\n",
            "epoch 172| loss: 0.23089 | train_logloss: 0.22699 | train_accuracy: 0.91665 | valid_logloss: 0.2047  | valid_accuracy: 0.93023 |  3:06:36s\n",
            "epoch 173| loss: 0.2309  | train_logloss: 0.23009 | train_accuracy: 0.91554 | valid_logloss: 0.20589 | valid_accuracy: 0.92942 |  3:07:40s\n",
            "epoch 174| loss: 0.23055 | train_logloss: 0.22733 | train_accuracy: 0.91649 | valid_logloss: 0.20301 | valid_accuracy: 0.93036 |  3:08:45s\n",
            "epoch 175| loss: 0.23019 | train_logloss: 0.2275  | train_accuracy: 0.91651 | valid_logloss: 0.20341 | valid_accuracy: 0.93016 |  3:09:49s\n",
            "epoch 176| loss: 0.22978 | train_logloss: 0.22657 | train_accuracy: 0.91665 | valid_logloss: 0.20192 | valid_accuracy: 0.93042 |  3:10:54s\n",
            "epoch 177| loss: 0.22924 | train_logloss: 0.22827 | train_accuracy: 0.91625 | valid_logloss: 0.20479 | valid_accuracy: 0.93003 |  3:11:58s\n",
            "epoch 178| loss: 0.22938 | train_logloss: 0.22676 | train_accuracy: 0.91655 | valid_logloss: 0.20124 | valid_accuracy: 0.93089 |  3:13:03s\n",
            "epoch 179| loss: 0.22914 | train_logloss: 0.22668 | train_accuracy: 0.91684 | valid_logloss: 0.20174 | valid_accuracy: 0.93057 |  3:14:08s\n",
            "epoch 180| loss: 0.22889 | train_logloss: 0.22677 | train_accuracy: 0.91686 | valid_logloss: 0.20366 | valid_accuracy: 0.93009 |  3:15:13s\n",
            "epoch 181| loss: 0.22933 | train_logloss: 0.22638 | train_accuracy: 0.91698 | valid_logloss: 0.20244 | valid_accuracy: 0.93032 |  3:16:17s\n",
            "epoch 182| loss: 0.22892 | train_logloss: 0.22667 | train_accuracy: 0.91666 | valid_logloss: 0.20162 | valid_accuracy: 0.93063 |  3:17:22s\n",
            "epoch 183| loss: 0.22879 | train_logloss: 0.22661 | train_accuracy: 0.9169  | valid_logloss: 0.20462 | valid_accuracy: 0.92987 |  3:18:26s\n",
            "epoch 184| loss: 0.22855 | train_logloss: 0.22606 | train_accuracy: 0.91694 | valid_logloss: 0.2047  | valid_accuracy: 0.92978 |  3:19:31s\n",
            "epoch 185| loss: 0.22899 | train_logloss: 0.22776 | train_accuracy: 0.91609 | valid_logloss: 0.20603 | valid_accuracy: 0.92925 |  3:20:35s\n",
            "epoch 186| loss: 0.22903 | train_logloss: 0.22663 | train_accuracy: 0.91677 | valid_logloss: 0.20372 | valid_accuracy: 0.9302  |  3:21:40s\n",
            "epoch 187| loss: 0.22938 | train_logloss: 0.22671 | train_accuracy: 0.9167  | valid_logloss: 0.20481 | valid_accuracy: 0.92987 |  3:22:44s\n",
            "epoch 188| loss: 0.2291  | train_logloss: 0.22603 | train_accuracy: 0.91718 | valid_logloss: 0.20094 | valid_accuracy: 0.93103 |  3:23:49s\n",
            "epoch 189| loss: 0.22859 | train_logloss: 0.22509 | train_accuracy: 0.91734 | valid_logloss: 0.20255 | valid_accuracy: 0.93053 |  3:24:53s\n",
            "epoch 190| loss: 0.22862 | train_logloss: 0.22635 | train_accuracy: 0.91702 | valid_logloss: 0.20306 | valid_accuracy: 0.93026 |  3:25:57s\n",
            "epoch 191| loss: 0.22877 | train_logloss: 0.22575 | train_accuracy: 0.91714 | valid_logloss: 0.20251 | valid_accuracy: 0.93047 |  3:27:02s\n",
            "epoch 192| loss: 0.22876 | train_logloss: 0.22611 | train_accuracy: 0.91682 | valid_logloss: 0.20077 | valid_accuracy: 0.9307  |  3:28:06s\n",
            "epoch 193| loss: 0.22863 | train_logloss: 0.22626 | train_accuracy: 0.91694 | valid_logloss: 0.20282 | valid_accuracy: 0.93052 |  3:29:11s\n",
            "epoch 194| loss: 0.22882 | train_logloss: 0.22629 | train_accuracy: 0.91709 | valid_logloss: 0.20425 | valid_accuracy: 0.93013 |  3:30:16s\n",
            "epoch 195| loss: 0.2287  | train_logloss: 0.22736 | train_accuracy: 0.91666 | valid_logloss: 0.19866 | valid_accuracy: 0.93129 |  3:31:20s\n",
            "epoch 196| loss: 0.22843 | train_logloss: 0.22657 | train_accuracy: 0.91691 | valid_logloss: 0.20379 | valid_accuracy: 0.93017 |  3:32:25s\n",
            "epoch 197| loss: 0.2282  | train_logloss: 0.22675 | train_accuracy: 0.91674 | valid_logloss: 0.2061  | valid_accuracy: 0.92929 |  3:33:29s\n",
            "epoch 198| loss: 0.22877 | train_logloss: 0.22873 | train_accuracy: 0.91609 | valid_logloss: 0.20491 | valid_accuracy: 0.92927 |  3:34:34s\n",
            "epoch 199| loss: 0.22968 | train_logloss: 0.2298  | train_accuracy: 0.91589 | valid_logloss: 0.20402 | valid_accuracy: 0.92971 |  3:35:38s\n",
            "epoch 200| loss: 0.23055 | train_logloss: 0.22893 | train_accuracy: 0.91588 | valid_logloss: 0.20555 | valid_accuracy: 0.92956 |  3:36:43s\n",
            "epoch 201| loss: 0.22996 | train_logloss: 0.22696 | train_accuracy: 0.91664 | valid_logloss: 0.20218 | valid_accuracy: 0.9303  |  3:37:47s\n",
            "epoch 202| loss: 0.22944 | train_logloss: 0.22609 | train_accuracy: 0.91697 | valid_logloss: 0.20478 | valid_accuracy: 0.92977 |  3:38:52s\n",
            "epoch 203| loss: 0.22883 | train_logloss: 0.22604 | train_accuracy: 0.91703 | valid_logloss: 0.2038  | valid_accuracy: 0.93026 |  3:39:57s\n",
            "epoch 204| loss: 0.22873 | train_logloss: 0.22627 | train_accuracy: 0.91679 | valid_logloss: 0.20504 | valid_accuracy: 0.92963 |  3:41:01s\n",
            "epoch 205| loss: 0.22863 | train_logloss: 0.22532 | train_accuracy: 0.91714 | valid_logloss: 0.20386 | valid_accuracy: 0.9302  |  3:42:06s\n",
            "epoch 206| loss: 0.22849 | train_logloss: 0.22664 | train_accuracy: 0.91678 | valid_logloss: 0.20313 | valid_accuracy: 0.92993 |  3:43:10s\n",
            "epoch 207| loss: 0.22819 | train_logloss: 0.22586 | train_accuracy: 0.91688 | valid_logloss: 0.20014 | valid_accuracy: 0.931   |  3:44:15s\n",
            "epoch 208| loss: 0.2281  | train_logloss: 0.22669 | train_accuracy: 0.91691 | valid_logloss: 0.20659 | valid_accuracy: 0.92984 |  3:45:20s\n",
            "epoch 209| loss: 0.22846 | train_logloss: 0.22641 | train_accuracy: 0.9168  | valid_logloss: 0.20226 | valid_accuracy: 0.93064 |  3:46:24s\n",
            "epoch 210| loss: 0.23013 | train_logloss: 0.22728 | train_accuracy: 0.9167  | valid_logloss: 0.20521 | valid_accuracy: 0.92972 |  3:47:29s\n",
            "epoch 211| loss: 0.22958 | train_logloss: 0.22696 | train_accuracy: 0.91659 | valid_logloss: 0.20136 | valid_accuracy: 0.9307  |  3:48:33s\n",
            "epoch 212| loss: 0.22898 | train_logloss: 0.22662 | train_accuracy: 0.91677 | valid_logloss: 0.20222 | valid_accuracy: 0.93036 |  3:49:38s\n",
            "epoch 213| loss: 0.2288  | train_logloss: 0.22605 | train_accuracy: 0.91689 | valid_logloss: 0.20666 | valid_accuracy: 0.92916 |  3:50:43s\n",
            "epoch 214| loss: 0.22847 | train_logloss: 0.2266  | train_accuracy: 0.9169  | valid_logloss: 0.20024 | valid_accuracy: 0.93105 |  3:51:47s\n",
            "epoch 215| loss: 0.22856 | train_logloss: 0.22663 | train_accuracy: 0.91679 | valid_logloss: 0.20541 | valid_accuracy: 0.92954 |  3:52:52s\n",
            "epoch 216| loss: 0.22854 | train_logloss: 0.22804 | train_accuracy: 0.91605 | valid_logloss: 0.20748 | valid_accuracy: 0.9288  |  3:53:56s\n",
            "epoch 217| loss: 0.22892 | train_logloss: 0.22604 | train_accuracy: 0.91703 | valid_logloss: 0.2019  | valid_accuracy: 0.9306  |  3:55:01s\n",
            "epoch 218| loss: 0.2289  | train_logloss: 0.22612 | train_accuracy: 0.91691 | valid_logloss: 0.20239 | valid_accuracy: 0.93062 |  3:56:06s\n",
            "epoch 219| loss: 0.22839 | train_logloss: 0.22642 | train_accuracy: 0.91694 | valid_logloss: 0.20635 | valid_accuracy: 0.92977 |  3:57:10s\n",
            "epoch 220| loss: 0.22822 | train_logloss: 0.22534 | train_accuracy: 0.91726 | valid_logloss: 0.2046  | valid_accuracy: 0.93013 |  3:58:15s\n",
            "epoch 221| loss: 0.22839 | train_logloss: 0.2281  | train_accuracy: 0.91619 | valid_logloss: 0.20156 | valid_accuracy: 0.93076 |  3:59:20s\n",
            "epoch 222| loss: 0.22832 | train_logloss: 0.22553 | train_accuracy: 0.91715 | valid_logloss: 0.20442 | valid_accuracy: 0.92993 |  4:00:25s\n",
            "epoch 223| loss: 0.22806 | train_logloss: 0.22594 | train_accuracy: 0.917   | valid_logloss: 0.20396 | valid_accuracy: 0.93008 |  4:01:29s\n",
            "epoch 224| loss: 0.22831 | train_logloss: 0.22645 | train_accuracy: 0.91697 | valid_logloss: 0.20724 | valid_accuracy: 0.92972 |  4:02:34s\n",
            "epoch 225| loss: 0.22821 | train_logloss: 0.22563 | train_accuracy: 0.91712 | valid_logloss: 0.20145 | valid_accuracy: 0.93067 |  4:03:39s\n",
            "epoch 226| loss: 0.22793 | train_logloss: 0.22656 | train_accuracy: 0.9172  | valid_logloss: 0.21066 | valid_accuracy: 0.92912 |  4:04:43s\n",
            "epoch 227| loss: 0.22817 | train_logloss: 0.22531 | train_accuracy: 0.91721 | valid_logloss: 0.20462 | valid_accuracy: 0.93035 |  4:05:48s\n",
            "epoch 228| loss: 0.22822 | train_logloss: 0.22603 | train_accuracy: 0.91699 | valid_logloss: 0.20051 | valid_accuracy: 0.9312  |  4:06:53s\n",
            "epoch 229| loss: 0.22897 | train_logloss: 0.22799 | train_accuracy: 0.91658 | valid_logloss: 0.20614 | valid_accuracy: 0.92976 |  4:07:58s\n",
            "epoch 230| loss: 0.22897 | train_logloss: 0.22562 | train_accuracy: 0.91721 | valid_logloss: 0.20336 | valid_accuracy: 0.93033 |  4:09:02s\n",
            "epoch 231| loss: 0.22857 | train_logloss: 0.2262  | train_accuracy: 0.91691 | valid_logloss: 0.20205 | valid_accuracy: 0.93044 |  4:10:07s\n",
            "epoch 232| loss: 0.22847 | train_logloss: 0.22525 | train_accuracy: 0.91723 | valid_logloss: 0.20518 | valid_accuracy: 0.92975 |  4:11:12s\n",
            "epoch 233| loss: 0.2281  | train_logloss: 0.22673 | train_accuracy: 0.91689 | valid_logloss: 0.20675 | valid_accuracy: 0.92937 |  4:12:16s\n",
            "epoch 234| loss: 0.2282  | train_logloss: 0.22611 | train_accuracy: 0.91694 | valid_logloss: 0.20524 | valid_accuracy: 0.92988 |  4:13:21s\n",
            "epoch 235| loss: 0.22806 | train_logloss: 0.22557 | train_accuracy: 0.9171  | valid_logloss: 0.20479 | valid_accuracy: 0.92989 |  4:14:26s\n",
            "epoch 236| loss: 0.22846 | train_logloss: 0.22495 | train_accuracy: 0.91742 | valid_logloss: 0.20549 | valid_accuracy: 0.92976 |  4:15:30s\n",
            "epoch 237| loss: 0.22818 | train_logloss: 0.22523 | train_accuracy: 0.91735 | valid_logloss: 0.20589 | valid_accuracy: 0.92996 |  4:16:35s\n",
            "epoch 238| loss: 0.23119 | train_logloss: 0.23507 | train_accuracy: 0.91429 | valid_logloss: 0.21152 | valid_accuracy: 0.92808 |  4:17:40s\n",
            "epoch 239| loss: 0.23325 | train_logloss: 0.22849 | train_accuracy: 0.91623 | valid_logloss: 0.20296 | valid_accuracy: 0.93016 |  4:18:45s\n",
            "epoch 240| loss: 0.23048 | train_logloss: 0.22762 | train_accuracy: 0.91654 | valid_logloss: 0.20325 | valid_accuracy: 0.93022 |  4:19:49s\n",
            "epoch 241| loss: 0.22964 | train_logloss: 0.22786 | train_accuracy: 0.91638 | valid_logloss: 0.20474 | valid_accuracy: 0.92972 |  4:20:54s\n",
            "epoch 242| loss: 0.2291  | train_logloss: 0.22682 | train_accuracy: 0.91691 | valid_logloss: 0.20802 | valid_accuracy: 0.92971 |  4:21:59s\n",
            "epoch 243| loss: 0.22888 | train_logloss: 0.22577 | train_accuracy: 0.91714 | valid_logloss: 0.2039  | valid_accuracy: 0.93033 |  4:23:04s\n",
            "epoch 244| loss: 0.22853 | train_logloss: 0.22576 | train_accuracy: 0.9169  | valid_logloss: 0.20362 | valid_accuracy: 0.93003 |  4:24:08s\n",
            "epoch 245| loss: 0.2284  | train_logloss: 0.22543 | train_accuracy: 0.91706 | valid_logloss: 0.20345 | valid_accuracy: 0.92998 |  4:25:13s\n",
            "\n",
            "Early stopping occurred at epoch 245 with best_epoch = 195 and best_valid_accuracy = 0.93129\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea_KEeoBoFtq",
        "outputId": "142ee504-ffb7-47ec-c667-19f38bdd0023"
      },
      "source": [
        "!python -m memory_profiler '/content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py' 'gb20.pkl' 'tn20.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: /content/drive/MyDrive/Научная работа/Data/hyper/X-r/gb.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurences   Line Contents\n",
            "============================================================\n",
            "     7    257.2 MiB    257.2 MiB           1   @profile\n",
            "     8                                         def main_func(a, b):\n",
            "     9                                             #tn = TabNetClassifier()\n",
            "    10                                             #tn.load_model('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "    11                                             #gb = lgb.Booster(model_file='/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    12    302.7 MiB     45.5 MiB           1       gb = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+a)\n",
            "    13   2342.0 MiB   2039.3 MiB           1       tn = joblib.load('/content/drive/MyDrive/Научная работа/Data/hyper/'+b)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RezXgsvA_0Te"
      },
      "source": [
        "ones(number_exp=16, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkvhQ62bxm6d"
      },
      "source": [
        "ones(number_exp=17, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.9,\t\n",
        "     λsparse=0.001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRBx3JDt-Veh",
        "outputId": "68751604-cf81-4204-d315-2d0aeb61cfe6"
      },
      "source": [
        "ones(number_exp=18, \n",
        "     Rows=30000, \n",
        "     Nd=16,\tNa=16,\t\n",
        "     B=2048,\tBV=512,\tmB=0.7,\t\n",
        "     λsparse=0.0001,\tNsteps=5,\tγ=1.7, \n",
        "     learning_rate=0.02,\tdecay_rate=0.95,\tdecay_iterations=200,\t\n",
        "     shared=2, decision=2, \n",
        "     mask_type='entmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 1.0721  | train_logloss: 20.48473| train_accuracy: 0.3116  | valid_logloss: 20.52434| valid_accuracy: 0.31487 |  0:00:02s\n",
            "epoch 1  | loss: 0.62702 | train_logloss: 7.26988 | train_accuracy: 0.28423 | valid_logloss: 7.35665 | valid_accuracy: 0.28297 |  0:00:04s\n",
            "epoch 2  | loss: 0.4926  | train_logloss: 4.78337 | train_accuracy: 0.3647  | valid_logloss: 4.88082 | valid_accuracy: 0.36347 |  0:00:06s\n",
            "epoch 3  | loss: 0.44313 | train_logloss: 3.25561 | train_accuracy: 0.29043 | valid_logloss: 3.28029 | valid_accuracy: 0.29257 |  0:00:08s\n",
            "epoch 4  | loss: 0.4382  | train_logloss: 5.81552 | train_accuracy: 0.35523 | valid_logloss: 5.85558 | valid_accuracy: 0.35263 |  0:00:10s\n",
            "epoch 5  | loss: 0.42124 | train_logloss: 3.80157 | train_accuracy: 0.3198  | valid_logloss: 3.82596 | valid_accuracy: 0.32247 |  0:00:12s\n",
            "epoch 6  | loss: 0.40068 | train_logloss: 2.142   | train_accuracy: 0.3114  | valid_logloss: 2.15032 | valid_accuracy: 0.31263 |  0:00:14s\n",
            "epoch 7  | loss: 0.39334 | train_logloss: 1.34959 | train_accuracy: 0.4055  | valid_logloss: 1.36478 | valid_accuracy: 0.40283 |  0:00:16s\n",
            "epoch 8  | loss: 0.38421 | train_logloss: 2.04112 | train_accuracy: 0.2778  | valid_logloss: 2.0746  | valid_accuracy: 0.27353 |  0:00:18s\n",
            "epoch 9  | loss: 0.36923 | train_logloss: 1.39782 | train_accuracy: 0.4167  | valid_logloss: 1.41947 | valid_accuracy: 0.40707 |  0:00:20s\n",
            "epoch 10 | loss: 0.35991 | train_logloss: 0.93261 | train_accuracy: 0.6835  | valid_logloss: 0.94757 | valid_accuracy: 0.67823 |  0:00:22s\n",
            "epoch 11 | loss: 0.36143 | train_logloss: 0.92531 | train_accuracy: 0.67113 | valid_logloss: 0.94228 | valid_accuracy: 0.66313 |  0:00:24s\n",
            "epoch 12 | loss: 0.36208 | train_logloss: 0.75854 | train_accuracy: 0.63137 | valid_logloss: 0.77162 | valid_accuracy: 0.62067 |  0:00:26s\n",
            "epoch 13 | loss: 0.35072 | train_logloss: 0.65422 | train_accuracy: 0.72803 | valid_logloss: 0.66262 | valid_accuracy: 0.72177 |  0:00:28s\n",
            "epoch 14 | loss: 0.34479 | train_logloss: 0.64139 | train_accuracy: 0.77137 | valid_logloss: 0.65007 | valid_accuracy: 0.76677 |  0:00:30s\n",
            "epoch 15 | loss: 0.33878 | train_logloss: 0.54607 | train_accuracy: 0.8126  | valid_logloss: 0.55488 | valid_accuracy: 0.80833 |  0:00:33s\n",
            "epoch 16 | loss: 0.33383 | train_logloss: 0.49983 | train_accuracy: 0.8259  | valid_logloss: 0.50804 | valid_accuracy: 0.82147 |  0:00:35s\n",
            "epoch 17 | loss: 0.33658 | train_logloss: 0.39857 | train_accuracy: 0.85717 | valid_logloss: 0.40424 | valid_accuracy: 0.8565  |  0:00:37s\n",
            "epoch 18 | loss: 0.34441 | train_logloss: 0.39084 | train_accuracy: 0.8579  | valid_logloss: 0.39665 | valid_accuracy: 0.8559  |  0:00:39s\n",
            "epoch 19 | loss: 0.33534 | train_logloss: 0.39496 | train_accuracy: 0.86057 | valid_logloss: 0.39884 | valid_accuracy: 0.8598  |  0:00:41s\n",
            "epoch 20 | loss: 0.34243 | train_logloss: 0.37095 | train_accuracy: 0.86187 | valid_logloss: 0.37499 | valid_accuracy: 0.85943 |  0:00:43s\n",
            "epoch 21 | loss: 0.35844 | train_logloss: 0.36074 | train_accuracy: 0.8628  | valid_logloss: 0.36884 | valid_accuracy: 0.861   |  0:00:45s\n",
            "epoch 22 | loss: 0.34144 | train_logloss: 0.35265 | train_accuracy: 0.8701  | valid_logloss: 0.36195 | valid_accuracy: 0.86793 |  0:00:47s\n",
            "epoch 23 | loss: 0.33422 | train_logloss: 0.34395 | train_accuracy: 0.8682  | valid_logloss: 0.35279 | valid_accuracy: 0.86477 |  0:00:49s\n",
            "epoch 24 | loss: 0.32953 | train_logloss: 0.3424  | train_accuracy: 0.87227 | valid_logloss: 0.35381 | valid_accuracy: 0.86943 |  0:00:51s\n",
            "epoch 25 | loss: 0.33326 | train_logloss: 0.34047 | train_accuracy: 0.871   | valid_logloss: 0.35438 | valid_accuracy: 0.86863 |  0:00:53s\n",
            "epoch 26 | loss: 0.32762 | train_logloss: 0.33234 | train_accuracy: 0.87097 | valid_logloss: 0.34278 | valid_accuracy: 0.86967 |  0:00:55s\n",
            "epoch 27 | loss: 0.32341 | train_logloss: 0.32039 | train_accuracy: 0.87537 | valid_logloss: 0.33346 | valid_accuracy: 0.8721  |  0:00:57s\n",
            "epoch 28 | loss: 0.32219 | train_logloss: 0.32146 | train_accuracy: 0.87773 | valid_logloss: 0.33367 | valid_accuracy: 0.87467 |  0:00:59s\n",
            "epoch 29 | loss: 0.32317 | train_logloss: 0.31209 | train_accuracy: 0.8778  | valid_logloss: 0.32736 | valid_accuracy: 0.87483 |  0:01:02s\n",
            "epoch 30 | loss: 0.31925 | train_logloss: 0.31908 | train_accuracy: 0.87643 | valid_logloss: 0.33484 | valid_accuracy: 0.8716  |  0:01:04s\n",
            "epoch 31 | loss: 0.31853 | train_logloss: 0.31679 | train_accuracy: 0.87993 | valid_logloss: 0.33483 | valid_accuracy: 0.8722  |  0:01:06s\n",
            "epoch 32 | loss: 0.32118 | train_logloss: 0.32433 | train_accuracy: 0.87767 | valid_logloss: 0.33863 | valid_accuracy: 0.8734  |  0:01:08s\n",
            "epoch 33 | loss: 0.31992 | train_logloss: 0.31434 | train_accuracy: 0.88153 | valid_logloss: 0.32799 | valid_accuracy: 0.874   |  0:01:10s\n",
            "epoch 34 | loss: 0.3181  | train_logloss: 0.30971 | train_accuracy: 0.88043 | valid_logloss: 0.32648 | valid_accuracy: 0.8765  |  0:01:12s\n",
            "epoch 35 | loss: 0.31753 | train_logloss: 0.31938 | train_accuracy: 0.8773  | valid_logloss: 0.33488 | valid_accuracy: 0.87233 |  0:01:14s\n",
            "epoch 36 | loss: 0.32233 | train_logloss: 0.31166 | train_accuracy: 0.8807  | valid_logloss: 0.32691 | valid_accuracy: 0.8747  |  0:01:16s\n",
            "epoch 37 | loss: 0.31679 | train_logloss: 0.30835 | train_accuracy: 0.88147 | valid_logloss: 0.3208  | valid_accuracy: 0.87877 |  0:01:18s\n",
            "epoch 38 | loss: 0.31735 | train_logloss: 0.32477 | train_accuracy: 0.87737 | valid_logloss: 0.33791 | valid_accuracy: 0.87397 |  0:01:20s\n",
            "epoch 39 | loss: 0.3302  | train_logloss: 0.3245  | train_accuracy: 0.8773  | valid_logloss: 0.33604 | valid_accuracy: 0.873   |  0:01:22s\n",
            "epoch 40 | loss: 0.32972 | train_logloss: 0.3186  | train_accuracy: 0.8783  | valid_logloss: 0.32822 | valid_accuracy: 0.87497 |  0:01:24s\n",
            "epoch 41 | loss: 0.33287 | train_logloss: 0.33    | train_accuracy: 0.87767 | valid_logloss: 0.33803 | valid_accuracy: 0.87453 |  0:01:27s\n",
            "epoch 42 | loss: 0.3288  | train_logloss: 0.31949 | train_accuracy: 0.87983 | valid_logloss: 0.32887 | valid_accuracy: 0.87647 |  0:01:29s\n",
            "epoch 43 | loss: 0.33366 | train_logloss: 0.36999 | train_accuracy: 0.8547  | valid_logloss: 0.38024 | valid_accuracy: 0.8549  |  0:01:31s\n",
            "epoch 44 | loss: 0.36122 | train_logloss: 0.35163 | train_accuracy: 0.8677  | valid_logloss: 0.36271 | valid_accuracy: 0.86447 |  0:01:33s\n",
            "epoch 45 | loss: 0.36099 | train_logloss: 0.35869 | train_accuracy: 0.8667  | valid_logloss: 0.36364 | valid_accuracy: 0.8671  |  0:01:35s\n",
            "epoch 46 | loss: 0.36181 | train_logloss: 0.35149 | train_accuracy: 0.86917 | valid_logloss: 0.36073 | valid_accuracy: 0.86683 |  0:01:37s\n",
            "epoch 47 | loss: 0.34776 | train_logloss: 0.34358 | train_accuracy: 0.87183 | valid_logloss: 0.35279 | valid_accuracy: 0.8703  |  0:01:39s\n",
            "epoch 48 | loss: 0.34301 | train_logloss: 0.34459 | train_accuracy: 0.87073 | valid_logloss: 0.35203 | valid_accuracy: 0.8676  |  0:01:41s\n",
            "epoch 49 | loss: 0.34192 | train_logloss: 0.33294 | train_accuracy: 0.87473 | valid_logloss: 0.33943 | valid_accuracy: 0.8718  |  0:01:43s\n",
            "epoch 50 | loss: 0.34117 | train_logloss: 0.34001 | train_accuracy: 0.8708  | valid_logloss: 0.34509 | valid_accuracy: 0.86953 |  0:01:45s\n",
            "epoch 51 | loss: 0.33987 | train_logloss: 0.34442 | train_accuracy: 0.8704  | valid_logloss: 0.35111 | valid_accuracy: 0.86927 |  0:01:47s\n",
            "epoch 52 | loss: 0.34994 | train_logloss: 0.34002 | train_accuracy: 0.8726  | valid_logloss: 0.34634 | valid_accuracy: 0.87077 |  0:01:49s\n",
            "epoch 53 | loss: 0.3369  | train_logloss: 0.32543 | train_accuracy: 0.87647 | valid_logloss: 0.33239 | valid_accuracy: 0.87377 |  0:01:51s\n",
            "epoch 54 | loss: 0.33345 | train_logloss: 0.34132 | train_accuracy: 0.87113 | valid_logloss: 0.34995 | valid_accuracy: 0.86843 |  0:01:53s\n",
            "epoch 55 | loss: 0.33686 | train_logloss: 0.3314  | train_accuracy: 0.87517 | valid_logloss: 0.34065 | valid_accuracy: 0.8706  |  0:01:56s\n",
            "epoch 56 | loss: 0.32852 | train_logloss: 0.32283 | train_accuracy: 0.8765  | valid_logloss: 0.33137 | valid_accuracy: 0.87413 |  0:01:58s\n",
            "epoch 57 | loss: 0.32582 | train_logloss: 0.31916 | train_accuracy: 0.87847 | valid_logloss: 0.3293  | valid_accuracy: 0.87507 |  0:02:00s\n",
            "epoch 58 | loss: 0.32467 | train_logloss: 0.33451 | train_accuracy: 0.87117 | valid_logloss: 0.34933 | valid_accuracy: 0.86673 |  0:02:02s\n",
            "epoch 59 | loss: 0.33102 | train_logloss: 0.32348 | train_accuracy: 0.87717 | valid_logloss: 0.33785 | valid_accuracy: 0.873   |  0:02:04s\n",
            "epoch 60 | loss: 0.32731 | train_logloss: 0.32795 | train_accuracy: 0.87493 | valid_logloss: 0.33929 | valid_accuracy: 0.8715  |  0:02:06s\n",
            "epoch 61 | loss: 0.33617 | train_logloss: 0.32658 | train_accuracy: 0.877   | valid_logloss: 0.33882 | valid_accuracy: 0.87263 |  0:02:08s\n",
            "epoch 62 | loss: 0.32664 | train_logloss: 0.31847 | train_accuracy: 0.87857 | valid_logloss: 0.33201 | valid_accuracy: 0.87457 |  0:02:10s\n",
            "epoch 63 | loss: 0.32063 | train_logloss: 0.31143 | train_accuracy: 0.8811  | valid_logloss: 0.32449 | valid_accuracy: 0.87733 |  0:02:12s\n",
            "epoch 64 | loss: 0.32056 | train_logloss: 0.30953 | train_accuracy: 0.88197 | valid_logloss: 0.32281 | valid_accuracy: 0.8778  |  0:02:14s\n",
            "epoch 65 | loss: 0.31719 | train_logloss: 0.31554 | train_accuracy: 0.87957 | valid_logloss: 0.32795 | valid_accuracy: 0.87697 |  0:02:16s\n",
            "epoch 66 | loss: 0.32752 | train_logloss: 0.36298 | train_accuracy: 0.86153 | valid_logloss: 0.36992 | valid_accuracy: 0.86037 |  0:02:18s\n",
            "epoch 67 | loss: 0.34035 | train_logloss: 0.32633 | train_accuracy: 0.8742  | valid_logloss: 0.33685 | valid_accuracy: 0.87483 |  0:02:20s\n",
            "epoch 68 | loss: 0.32298 | train_logloss: 0.32032 | train_accuracy: 0.87837 | valid_logloss: 0.33164 | valid_accuracy: 0.8775  |  0:02:22s\n",
            "epoch 69 | loss: 0.32361 | train_logloss: 0.31667 | train_accuracy: 0.88057 | valid_logloss: 0.32589 | valid_accuracy: 0.87807 |  0:02:24s\n",
            "epoch 70 | loss: 0.31694 | train_logloss: 0.30773 | train_accuracy: 0.88093 | valid_logloss: 0.31684 | valid_accuracy: 0.88077 |  0:02:26s\n",
            "epoch 71 | loss: 0.31268 | train_logloss: 0.30908 | train_accuracy: 0.8823  | valid_logloss: 0.31715 | valid_accuracy: 0.8799  |  0:02:28s\n",
            "epoch 72 | loss: 0.31259 | train_logloss: 0.30612 | train_accuracy: 0.88053 | valid_logloss: 0.31917 | valid_accuracy: 0.87793 |  0:02:30s\n",
            "epoch 73 | loss: 0.30778 | train_logloss: 0.3011  | train_accuracy: 0.88453 | valid_logloss: 0.31364 | valid_accuracy: 0.8811  |  0:02:32s\n",
            "epoch 74 | loss: 0.30228 | train_logloss: 0.29713 | train_accuracy: 0.88537 | valid_logloss: 0.30912 | valid_accuracy: 0.88193 |  0:02:34s\n",
            "epoch 75 | loss: 0.30166 | train_logloss: 0.29897 | train_accuracy: 0.8848  | valid_logloss: 0.3109  | valid_accuracy: 0.88263 |  0:02:36s\n",
            "epoch 76 | loss: 0.30514 | train_logloss: 0.30238 | train_accuracy: 0.88397 | valid_logloss: 0.31618 | valid_accuracy: 0.88063 |  0:02:38s\n",
            "epoch 77 | loss: 0.30687 | train_logloss: 0.30146 | train_accuracy: 0.88403 | valid_logloss: 0.31432 | valid_accuracy: 0.87963 |  0:02:41s\n",
            "epoch 78 | loss: 0.30118 | train_logloss: 0.294   | train_accuracy: 0.88757 | valid_logloss: 0.30804 | valid_accuracy: 0.88373 |  0:02:42s\n",
            "epoch 79 | loss: 0.30063 | train_logloss: 0.29697 | train_accuracy: 0.88603 | valid_logloss: 0.31109 | valid_accuracy: 0.88293 |  0:02:45s\n",
            "epoch 80 | loss: 0.29783 | train_logloss: 0.29716 | train_accuracy: 0.8841  | valid_logloss: 0.31014 | valid_accuracy: 0.8827  |  0:02:47s\n",
            "epoch 81 | loss: 0.29859 | train_logloss: 0.29616 | train_accuracy: 0.88483 | valid_logloss: 0.31123 | valid_accuracy: 0.88183 |  0:02:49s\n",
            "epoch 82 | loss: 0.29977 | train_logloss: 0.29965 | train_accuracy: 0.88437 | valid_logloss: 0.31709 | valid_accuracy: 0.88147 |  0:02:51s\n",
            "epoch 83 | loss: 0.30154 | train_logloss: 0.2972  | train_accuracy: 0.88537 | valid_logloss: 0.31163 | valid_accuracy: 0.88233 |  0:02:53s\n",
            "epoch 84 | loss: 0.30465 | train_logloss: 0.29776 | train_accuracy: 0.88693 | valid_logloss: 0.31187 | valid_accuracy: 0.8831  |  0:02:55s\n",
            "epoch 85 | loss: 0.29823 | train_logloss: 0.30347 | train_accuracy: 0.8849  | valid_logloss: 0.31903 | valid_accuracy: 0.88023 |  0:02:57s\n",
            "epoch 86 | loss: 0.31783 | train_logloss: 0.31415 | train_accuracy: 0.8813  | valid_logloss: 0.32569 | valid_accuracy: 0.87683 |  0:02:59s\n",
            "epoch 87 | loss: 0.3147  | train_logloss: 0.31408 | train_accuracy: 0.878   | valid_logloss: 0.32479 | valid_accuracy: 0.87543 |  0:03:01s\n",
            "epoch 88 | loss: 0.30986 | train_logloss: 0.30321 | train_accuracy: 0.88353 | valid_logloss: 0.31575 | valid_accuracy: 0.87987 |  0:03:03s\n",
            "epoch 89 | loss: 0.30653 | train_logloss: 0.29653 | train_accuracy: 0.8853  | valid_logloss: 0.3098  | valid_accuracy: 0.88097 |  0:03:05s\n",
            "epoch 90 | loss: 0.29841 | train_logloss: 0.30086 | train_accuracy: 0.8864  | valid_logloss: 0.31523 | valid_accuracy: 0.8836  |  0:03:07s\n",
            "epoch 91 | loss: 0.30355 | train_logloss: 0.29664 | train_accuracy: 0.88593 | valid_logloss: 0.31054 | valid_accuracy: 0.88127 |  0:03:09s\n",
            "epoch 92 | loss: 0.29839 | train_logloss: 0.29472 | train_accuracy: 0.88503 | valid_logloss: 0.30962 | valid_accuracy: 0.88027 |  0:03:11s\n",
            "epoch 93 | loss: 0.29636 | train_logloss: 0.29552 | train_accuracy: 0.88707 | valid_logloss: 0.31089 | valid_accuracy: 0.88197 |  0:03:13s\n",
            "epoch 94 | loss: 0.29494 | train_logloss: 0.28877 | train_accuracy: 0.88867 | valid_logloss: 0.30618 | valid_accuracy: 0.88393 |  0:03:15s\n",
            "epoch 95 | loss: 0.29462 | train_logloss: 0.29225 | train_accuracy: 0.88807 | valid_logloss: 0.3087  | valid_accuracy: 0.88513 |  0:03:17s\n",
            "epoch 96 | loss: 0.29659 | train_logloss: 0.29097 | train_accuracy: 0.88827 | valid_logloss: 0.30521 | valid_accuracy: 0.885   |  0:03:19s\n",
            "epoch 97 | loss: 0.2951  | train_logloss: 0.28936 | train_accuracy: 0.8885  | valid_logloss: 0.30325 | valid_accuracy: 0.8861  |  0:03:21s\n",
            "epoch 98 | loss: 0.2923  | train_logloss: 0.28761 | train_accuracy: 0.88833 | valid_logloss: 0.30374 | valid_accuracy: 0.88543 |  0:03:23s\n",
            "epoch 99 | loss: 0.29853 | train_logloss: 0.29645 | train_accuracy: 0.88343 | valid_logloss: 0.30816 | valid_accuracy: 0.8824  |  0:03:25s\n",
            "epoch 100| loss: 0.29969 | train_logloss: 0.28781 | train_accuracy: 0.88757 | valid_logloss: 0.30166 | valid_accuracy: 0.8847  |  0:03:27s\n",
            "epoch 101| loss: 0.29417 | train_logloss: 0.29253 | train_accuracy: 0.88803 | valid_logloss: 0.30483 | valid_accuracy: 0.8845  |  0:03:29s\n",
            "epoch 102| loss: 0.2961  | train_logloss: 0.2885  | train_accuracy: 0.88847 | valid_logloss: 0.30063 | valid_accuracy: 0.88543 |  0:03:31s\n",
            "epoch 103| loss: 0.2927  | train_logloss: 0.28873 | train_accuracy: 0.88853 | valid_logloss: 0.30264 | valid_accuracy: 0.88657 |  0:03:33s\n",
            "epoch 104| loss: 0.28977 | train_logloss: 0.28468 | train_accuracy: 0.88973 | valid_logloss: 0.30263 | valid_accuracy: 0.88433 |  0:03:35s\n",
            "epoch 105| loss: 0.28876 | train_logloss: 0.28477 | train_accuracy: 0.88967 | valid_logloss: 0.30109 | valid_accuracy: 0.88523 |  0:03:37s\n",
            "epoch 106| loss: 0.28888 | train_logloss: 0.28333 | train_accuracy: 0.89023 | valid_logloss: 0.30089 | valid_accuracy: 0.8855  |  0:03:39s\n",
            "epoch 107| loss: 0.28863 | train_logloss: 0.28617 | train_accuracy: 0.8891  | valid_logloss: 0.30199 | valid_accuracy: 0.8842  |  0:03:41s\n",
            "epoch 108| loss: 0.29166 | train_logloss: 0.28963 | train_accuracy: 0.89037 | valid_logloss: 0.31013 | valid_accuracy: 0.88483 |  0:03:43s\n",
            "epoch 109| loss: 0.29072 | train_logloss: 0.28823 | train_accuracy: 0.89087 | valid_logloss: 0.30807 | valid_accuracy: 0.886   |  0:03:45s\n",
            "epoch 110| loss: 0.2899  | train_logloss: 0.29326 | train_accuracy: 0.88747 | valid_logloss: 0.3089  | valid_accuracy: 0.88283 |  0:03:47s\n",
            "epoch 111| loss: 0.29537 | train_logloss: 0.28919 | train_accuracy: 0.89003 | valid_logloss: 0.30315 | valid_accuracy: 0.88627 |  0:03:49s\n",
            "epoch 112| loss: 0.29401 | train_logloss: 0.29495 | train_accuracy: 0.88617 | valid_logloss: 0.30834 | valid_accuracy: 0.88313 |  0:03:51s\n",
            "epoch 113| loss: 0.2947  | train_logloss: 0.28594 | train_accuracy: 0.89043 | valid_logloss: 0.29893 | valid_accuracy: 0.8872  |  0:03:53s\n",
            "epoch 114| loss: 0.29162 | train_logloss: 0.2856  | train_accuracy: 0.88983 | valid_logloss: 0.30138 | valid_accuracy: 0.88673 |  0:03:55s\n",
            "epoch 115| loss: 0.29783 | train_logloss: 0.29854 | train_accuracy: 0.88473 | valid_logloss: 0.31239 | valid_accuracy: 0.88163 |  0:03:57s\n",
            "epoch 116| loss: 0.30822 | train_logloss: 0.30237 | train_accuracy: 0.88513 | valid_logloss: 0.31138 | valid_accuracy: 0.88223 |  0:03:59s\n",
            "epoch 117| loss: 0.30385 | train_logloss: 0.29642 | train_accuracy: 0.88597 | valid_logloss: 0.31085 | valid_accuracy: 0.8825  |  0:04:01s\n",
            "epoch 118| loss: 0.29666 | train_logloss: 0.29031 | train_accuracy: 0.88833 | valid_logloss: 0.3052  | valid_accuracy: 0.884   |  0:04:03s\n",
            "epoch 119| loss: 0.29436 | train_logloss: 0.28863 | train_accuracy: 0.89103 | valid_logloss: 0.30378 | valid_accuracy: 0.88607 |  0:04:05s\n",
            "epoch 120| loss: 0.29266 | train_logloss: 0.28664 | train_accuracy: 0.8898  | valid_logloss: 0.30107 | valid_accuracy: 0.88673 |  0:04:07s\n",
            "epoch 121| loss: 0.29158 | train_logloss: 0.2887  | train_accuracy: 0.88823 | valid_logloss: 0.30416 | valid_accuracy: 0.88443 |  0:04:09s\n",
            "epoch 122| loss: 0.28949 | train_logloss: 0.28323 | train_accuracy: 0.8927  | valid_logloss: 0.30021 | valid_accuracy: 0.88617 |  0:04:11s\n",
            "epoch 123| loss: 0.29078 | train_logloss: 0.28858 | train_accuracy: 0.89043 | valid_logloss: 0.30491 | valid_accuracy: 0.88483 |  0:04:13s\n",
            "epoch 124| loss: 0.28991 | train_logloss: 0.30816 | train_accuracy: 0.88403 | valid_logloss: 0.32276 | valid_accuracy: 0.88    |  0:04:15s\n",
            "epoch 125| loss: 0.29713 | train_logloss: 0.28751 | train_accuracy: 0.88977 | valid_logloss: 0.30317 | valid_accuracy: 0.8841  |  0:04:17s\n",
            "epoch 126| loss: 0.2915  | train_logloss: 0.28604 | train_accuracy: 0.8908  | valid_logloss: 0.30158 | valid_accuracy: 0.88463 |  0:04:19s\n",
            "epoch 127| loss: 0.2886  | train_logloss: 0.28589 | train_accuracy: 0.88953 | valid_logloss: 0.30317 | valid_accuracy: 0.88403 |  0:04:21s\n",
            "epoch 128| loss: 0.28848 | train_logloss: 0.2851  | train_accuracy: 0.89143 | valid_logloss: 0.30311 | valid_accuracy: 0.8843  |  0:04:23s\n",
            "epoch 129| loss: 0.2903  | train_logloss: 0.28506 | train_accuracy: 0.88873 | valid_logloss: 0.30325 | valid_accuracy: 0.88343 |  0:04:25s\n",
            "epoch 130| loss: 0.2894  | train_logloss: 0.28965 | train_accuracy: 0.89057 | valid_logloss: 0.30727 | valid_accuracy: 0.8842  |  0:04:27s\n",
            "epoch 131| loss: 0.28762 | train_logloss: 0.28461 | train_accuracy: 0.89223 | valid_logloss: 0.30297 | valid_accuracy: 0.8849  |  0:04:29s\n",
            "epoch 132| loss: 0.285   | train_logloss: 0.27961 | train_accuracy: 0.89263 | valid_logloss: 0.29898 | valid_accuracy: 0.88577 |  0:04:31s\n",
            "epoch 133| loss: 0.2856  | train_logloss: 0.28006 | train_accuracy: 0.89123 | valid_logloss: 0.29784 | valid_accuracy: 0.88593 |  0:04:33s\n",
            "epoch 134| loss: 0.28631 | train_logloss: 0.27796 | train_accuracy: 0.8936  | valid_logloss: 0.29582 | valid_accuracy: 0.88547 |  0:04:35s\n",
            "epoch 135| loss: 0.28502 | train_logloss: 0.28171 | train_accuracy: 0.89147 | valid_logloss: 0.30053 | valid_accuracy: 0.8871  |  0:04:37s\n",
            "epoch 136| loss: 0.28233 | train_logloss: 0.28323 | train_accuracy: 0.8912  | valid_logloss: 0.29975 | valid_accuracy: 0.88393 |  0:04:39s\n",
            "epoch 137| loss: 0.28947 | train_logloss: 0.28355 | train_accuracy: 0.89027 | valid_logloss: 0.30281 | valid_accuracy: 0.88617 |  0:04:41s\n",
            "epoch 138| loss: 0.29231 | train_logloss: 0.29651 | train_accuracy: 0.88693 | valid_logloss: 0.31217 | valid_accuracy: 0.8844  |  0:04:43s\n",
            "epoch 139| loss: 0.29671 | train_logloss: 0.29588 | train_accuracy: 0.88577 | valid_logloss: 0.31043 | valid_accuracy: 0.88387 |  0:04:45s\n",
            "epoch 140| loss: 0.29229 | train_logloss: 0.28555 | train_accuracy: 0.88843 | valid_logloss: 0.30152 | valid_accuracy: 0.8868  |  0:04:47s\n",
            "epoch 141| loss: 0.28667 | train_logloss: 0.28253 | train_accuracy: 0.8922  | valid_logloss: 0.30055 | valid_accuracy: 0.88567 |  0:04:49s\n",
            "epoch 142| loss: 0.2838  | train_logloss: 0.2798  | train_accuracy: 0.89207 | valid_logloss: 0.29761 | valid_accuracy: 0.88663 |  0:04:51s\n",
            "epoch 143| loss: 0.28263 | train_logloss: 0.27713 | train_accuracy: 0.8931  | valid_logloss: 0.29762 | valid_accuracy: 0.88773 |  0:04:53s\n",
            "epoch 144| loss: 0.29312 | train_logloss: 0.29438 | train_accuracy: 0.88807 | valid_logloss: 0.30979 | valid_accuracy: 0.88223 |  0:04:55s\n",
            "epoch 145| loss: 0.29435 | train_logloss: 0.29418 | train_accuracy: 0.88613 | valid_logloss: 0.31456 | valid_accuracy: 0.87967 |  0:04:57s\n",
            "epoch 146| loss: 0.3019  | train_logloss: 0.29062 | train_accuracy: 0.88933 | valid_logloss: 0.30791 | valid_accuracy: 0.8841  |  0:04:59s\n",
            "epoch 147| loss: 0.29219 | train_logloss: 0.28273 | train_accuracy: 0.89143 | valid_logloss: 0.30019 | valid_accuracy: 0.88597 |  0:05:01s\n",
            "epoch 148| loss: 0.28887 | train_logloss: 0.27913 | train_accuracy: 0.8936  | valid_logloss: 0.29889 | valid_accuracy: 0.88717 |  0:05:03s\n",
            "epoch 149| loss: 0.28628 | train_logloss: 0.2815  | train_accuracy: 0.8928  | valid_logloss: 0.30365 | valid_accuracy: 0.88627 |  0:05:05s\n",
            "epoch 150| loss: 0.29148 | train_logloss: 0.29523 | train_accuracy: 0.8854  | valid_logloss: 0.31148 | valid_accuracy: 0.88047 |  0:05:07s\n",
            "epoch 151| loss: 0.29156 | train_logloss: 0.28685 | train_accuracy: 0.88903 | valid_logloss: 0.30559 | valid_accuracy: 0.88477 |  0:05:09s\n",
            "epoch 152| loss: 0.28895 | train_logloss: 0.28363 | train_accuracy: 0.8912  | valid_logloss: 0.30186 | valid_accuracy: 0.88377 |  0:05:11s\n",
            "epoch 153| loss: 0.28358 | train_logloss: 0.27792 | train_accuracy: 0.89273 | valid_logloss: 0.29818 | valid_accuracy: 0.8862  |  0:05:13s\n",
            "epoch 154| loss: 0.28502 | train_logloss: 0.28191 | train_accuracy: 0.89177 | valid_logloss: 0.30061 | valid_accuracy: 0.88553 |  0:05:15s\n",
            "epoch 155| loss: 0.2873  | train_logloss: 0.28285 | train_accuracy: 0.89017 | valid_logloss: 0.30119 | valid_accuracy: 0.886   |  0:05:17s\n",
            "epoch 156| loss: 0.2916  | train_logloss: 0.28573 | train_accuracy: 0.89197 | valid_logloss: 0.30249 | valid_accuracy: 0.886   |  0:05:19s\n",
            "epoch 157| loss: 0.29098 | train_logloss: 0.2845  | train_accuracy: 0.89053 | valid_logloss: 0.30209 | valid_accuracy: 0.8862  |  0:05:21s\n",
            "epoch 158| loss: 0.28928 | train_logloss: 0.28452 | train_accuracy: 0.8921  | valid_logloss: 0.30025 | valid_accuracy: 0.8885  |  0:05:23s\n",
            "epoch 159| loss: 0.28896 | train_logloss: 0.28423 | train_accuracy: 0.89053 | valid_logloss: 0.30158 | valid_accuracy: 0.8856  |  0:05:25s\n",
            "epoch 160| loss: 0.28667 | train_logloss: 0.27949 | train_accuracy: 0.89277 | valid_logloss: 0.29901 | valid_accuracy: 0.88813 |  0:05:27s\n",
            "epoch 161| loss: 0.28757 | train_logloss: 0.2874  | train_accuracy: 0.88827 | valid_logloss: 0.30536 | valid_accuracy: 0.88547 |  0:05:29s\n",
            "epoch 162| loss: 0.29313 | train_logloss: 0.29471 | train_accuracy: 0.8865  | valid_logloss: 0.31204 | valid_accuracy: 0.88117 |  0:05:31s\n",
            "epoch 163| loss: 0.29558 | train_logloss: 0.28574 | train_accuracy: 0.89093 | valid_logloss: 0.30555 | valid_accuracy: 0.8851  |  0:05:33s\n",
            "epoch 164| loss: 0.28722 | train_logloss: 0.27993 | train_accuracy: 0.892   | valid_logloss: 0.30142 | valid_accuracy: 0.88607 |  0:05:35s\n",
            "epoch 165| loss: 0.28414 | train_logloss: 0.27833 | train_accuracy: 0.8926  | valid_logloss: 0.29729 | valid_accuracy: 0.88927 |  0:05:37s\n",
            "epoch 166| loss: 0.28333 | train_logloss: 0.27734 | train_accuracy: 0.89387 | valid_logloss: 0.30068 | valid_accuracy: 0.88723 |  0:05:39s\n",
            "epoch 167| loss: 0.28187 | train_logloss: 0.2782  | train_accuracy: 0.89287 | valid_logloss: 0.30147 | valid_accuracy: 0.88587 |  0:05:41s\n",
            "epoch 168| loss: 0.28429 | train_logloss: 0.28651 | train_accuracy: 0.89063 | valid_logloss: 0.30885 | valid_accuracy: 0.88433 |  0:05:43s\n",
            "epoch 169| loss: 0.29058 | train_logloss: 0.2813  | train_accuracy: 0.8926  | valid_logloss: 0.30253 | valid_accuracy: 0.8862  |  0:05:45s\n",
            "epoch 170| loss: 0.28234 | train_logloss: 0.27975 | train_accuracy: 0.8912  | valid_logloss: 0.29938 | valid_accuracy: 0.8866  |  0:05:47s\n",
            "epoch 171| loss: 0.28526 | train_logloss: 0.28394 | train_accuracy: 0.89347 | valid_logloss: 0.29788 | valid_accuracy: 0.88793 |  0:05:49s\n",
            "epoch 172| loss: 0.2855  | train_logloss: 0.28728 | train_accuracy: 0.8917  | valid_logloss: 0.30292 | valid_accuracy: 0.8862  |  0:05:51s\n",
            "epoch 173| loss: 0.28274 | train_logloss: 0.27901 | train_accuracy: 0.8943  | valid_logloss: 0.29868 | valid_accuracy: 0.8879  |  0:05:53s\n",
            "epoch 174| loss: 0.28248 | train_logloss: 0.27409 | train_accuracy: 0.89443 | valid_logloss: 0.29338 | valid_accuracy: 0.8893  |  0:05:55s\n",
            "epoch 175| loss: 0.28451 | train_logloss: 0.28231 | train_accuracy: 0.89313 | valid_logloss: 0.30192 | valid_accuracy: 0.88703 |  0:05:57s\n",
            "epoch 176| loss: 0.28208 | train_logloss: 0.27588 | train_accuracy: 0.89387 | valid_logloss: 0.29491 | valid_accuracy: 0.88873 |  0:05:59s\n",
            "epoch 177| loss: 0.28024 | train_logloss: 0.27356 | train_accuracy: 0.89443 | valid_logloss: 0.29585 | valid_accuracy: 0.8881  |  0:06:01s\n",
            "epoch 178| loss: 0.27697 | train_logloss: 0.27657 | train_accuracy: 0.89183 | valid_logloss: 0.29854 | valid_accuracy: 0.88857 |  0:06:03s\n",
            "epoch 179| loss: 0.27632 | train_logloss: 0.27165 | train_accuracy: 0.89497 | valid_logloss: 0.29381 | valid_accuracy: 0.8889  |  0:06:05s\n",
            "epoch 180| loss: 0.27514 | train_logloss: 0.27138 | train_accuracy: 0.89543 | valid_logloss: 0.29585 | valid_accuracy: 0.88807 |  0:06:07s\n",
            "epoch 181| loss: 0.27933 | train_logloss: 0.28085 | train_accuracy: 0.8904  | valid_logloss: 0.30109 | valid_accuracy: 0.88693 |  0:06:09s\n",
            "epoch 182| loss: 0.28048 | train_logloss: 0.2739  | train_accuracy: 0.89457 | valid_logloss: 0.29348 | valid_accuracy: 0.8901  |  0:06:11s\n",
            "epoch 183| loss: 0.27598 | train_logloss: 0.27392 | train_accuracy: 0.89427 | valid_logloss: 0.29632 | valid_accuracy: 0.88907 |  0:06:13s\n",
            "epoch 184| loss: 0.27808 | train_logloss: 0.27132 | train_accuracy: 0.89533 | valid_logloss: 0.29501 | valid_accuracy: 0.8883  |  0:06:15s\n",
            "epoch 185| loss: 0.27451 | train_logloss: 0.27087 | train_accuracy: 0.8971  | valid_logloss: 0.29424 | valid_accuracy: 0.89063 |  0:06:17s\n",
            "epoch 186| loss: 0.27738 | train_logloss: 0.26597 | train_accuracy: 0.8973  | valid_logloss: 0.29087 | valid_accuracy: 0.89043 |  0:06:19s\n",
            "epoch 187| loss: 0.27284 | train_logloss: 0.26481 | train_accuracy: 0.89733 | valid_logloss: 0.29195 | valid_accuracy: 0.8901  |  0:06:21s\n",
            "epoch 188| loss: 0.27198 | train_logloss: 0.26528 | train_accuracy: 0.89777 | valid_logloss: 0.29029 | valid_accuracy: 0.89147 |  0:06:23s\n",
            "epoch 189| loss: 0.26994 | train_logloss: 0.27022 | train_accuracy: 0.8957  | valid_logloss: 0.29573 | valid_accuracy: 0.8893  |  0:06:25s\n",
            "epoch 190| loss: 0.27016 | train_logloss: 0.26559 | train_accuracy: 0.8973  | valid_logloss: 0.29195 | valid_accuracy: 0.89007 |  0:06:27s\n",
            "epoch 191| loss: 0.27049 | train_logloss: 0.26866 | train_accuracy: 0.89663 | valid_logloss: 0.29771 | valid_accuracy: 0.88677 |  0:06:29s\n",
            "epoch 192| loss: 0.27184 | train_logloss: 0.27726 | train_accuracy: 0.89497 | valid_logloss: 0.3085  | valid_accuracy: 0.88393 |  0:06:31s\n",
            "epoch 193| loss: 0.28232 | train_logloss: 0.27191 | train_accuracy: 0.8955  | valid_logloss: 0.29736 | valid_accuracy: 0.88873 |  0:06:33s\n",
            "epoch 194| loss: 0.27718 | train_logloss: 0.26705 | train_accuracy: 0.89727 | valid_logloss: 0.29394 | valid_accuracy: 0.88967 |  0:06:35s\n",
            "epoch 195| loss: 0.2862  | train_logloss: 0.32688 | train_accuracy: 0.88123 | valid_logloss: 0.33832 | valid_accuracy: 0.87627 |  0:06:37s\n",
            "epoch 196| loss: 0.30374 | train_logloss: 0.28732 | train_accuracy: 0.89123 | valid_logloss: 0.30627 | valid_accuracy: 0.88523 |  0:06:39s\n",
            "epoch 197| loss: 0.28761 | train_logloss: 0.28678 | train_accuracy: 0.89087 | valid_logloss: 0.30569 | valid_accuracy: 0.8858  |  0:06:41s\n",
            "epoch 198| loss: 0.28241 | train_logloss: 0.27526 | train_accuracy: 0.8935  | valid_logloss: 0.2954  | valid_accuracy: 0.88847 |  0:06:43s\n",
            "epoch 199| loss: 0.28075 | train_logloss: 0.2743  | train_accuracy: 0.89553 | valid_logloss: 0.29774 | valid_accuracy: 0.88843 |  0:06:45s\n",
            "epoch 200| loss: 0.2769  | train_logloss: 0.27187 | train_accuracy: 0.8954  | valid_logloss: 0.29708 | valid_accuracy: 0.88913 |  0:06:47s\n",
            "epoch 201| loss: 0.27565 | train_logloss: 0.27795 | train_accuracy: 0.8929  | valid_logloss: 0.30414 | valid_accuracy: 0.88657 |  0:06:49s\n",
            "epoch 202| loss: 0.28035 | train_logloss: 0.27685 | train_accuracy: 0.8936  | valid_logloss: 0.29509 | valid_accuracy: 0.8874  |  0:06:51s\n",
            "epoch 203| loss: 0.27828 | train_logloss: 0.26967 | train_accuracy: 0.89667 | valid_logloss: 0.293   | valid_accuracy: 0.88973 |  0:06:53s\n",
            "epoch 204| loss: 0.27842 | train_logloss: 0.27066 | train_accuracy: 0.8955  | valid_logloss: 0.29366 | valid_accuracy: 0.8893  |  0:06:55s\n",
            "epoch 205| loss: 0.27532 | train_logloss: 0.269   | train_accuracy: 0.89553 | valid_logloss: 0.29426 | valid_accuracy: 0.88893 |  0:06:57s\n",
            "epoch 206| loss: 0.2722  | train_logloss: 0.27194 | train_accuracy: 0.89317 | valid_logloss: 0.29736 | valid_accuracy: 0.88687 |  0:06:59s\n",
            "epoch 207| loss: 0.27339 | train_logloss: 0.26651 | train_accuracy: 0.89797 | valid_logloss: 0.29347 | valid_accuracy: 0.8882  |  0:07:01s\n",
            "epoch 208| loss: 0.27339 | train_logloss: 0.26685 | train_accuracy: 0.8977  | valid_logloss: 0.29235 | valid_accuracy: 0.89    |  0:07:03s\n",
            "epoch 209| loss: 0.27029 | train_logloss: 0.2641  | train_accuracy: 0.89917 | valid_logloss: 0.29264 | valid_accuracy: 0.88963 |  0:07:05s\n",
            "epoch 210| loss: 0.26937 | train_logloss: 0.26392 | train_accuracy: 0.89783 | valid_logloss: 0.29663 | valid_accuracy: 0.8887  |  0:07:07s\n",
            "epoch 211| loss: 0.26854 | train_logloss: 0.26281 | train_accuracy: 0.89843 | valid_logloss: 0.29376 | valid_accuracy: 0.88967 |  0:07:09s\n",
            "epoch 212| loss: 0.27452 | train_logloss: 0.27711 | train_accuracy: 0.89377 | valid_logloss: 0.30342 | valid_accuracy: 0.88673 |  0:07:11s\n",
            "epoch 213| loss: 0.27671 | train_logloss: 0.26866 | train_accuracy: 0.89647 | valid_logloss: 0.29479 | valid_accuracy: 0.8882  |  0:07:13s\n",
            "epoch 214| loss: 0.27195 | train_logloss: 0.26481 | train_accuracy: 0.8975  | valid_logloss: 0.29176 | valid_accuracy: 0.88987 |  0:07:16s\n",
            "epoch 215| loss: 0.26989 | train_logloss: 0.26513 | train_accuracy: 0.8972  | valid_logloss: 0.29397 | valid_accuracy: 0.89093 |  0:07:17s\n",
            "epoch 216| loss: 0.27027 | train_logloss: 0.26362 | train_accuracy: 0.898   | valid_logloss: 0.29274 | valid_accuracy: 0.88963 |  0:07:20s\n",
            "epoch 217| loss: 0.26623 | train_logloss: 0.26164 | train_accuracy: 0.9001  | valid_logloss: 0.29202 | valid_accuracy: 0.89037 |  0:07:22s\n",
            "epoch 218| loss: 0.26571 | train_logloss: 0.26241 | train_accuracy: 0.8995  | valid_logloss: 0.29477 | valid_accuracy: 0.8896  |  0:07:24s\n",
            "epoch 219| loss: 0.26664 | train_logloss: 0.25976 | train_accuracy: 0.8995  | valid_logloss: 0.29294 | valid_accuracy: 0.88937 |  0:07:26s\n",
            "epoch 220| loss: 0.26711 | train_logloss: 0.26537 | train_accuracy: 0.89807 | valid_logloss: 0.29732 | valid_accuracy: 0.88917 |  0:07:28s\n",
            "epoch 221| loss: 0.26658 | train_logloss: 0.25995 | train_accuracy: 0.89917 | valid_logloss: 0.2916  | valid_accuracy: 0.89007 |  0:07:30s\n",
            "epoch 222| loss: 0.2673  | train_logloss: 0.26162 | train_accuracy: 0.89893 | valid_logloss: 0.2925  | valid_accuracy: 0.8905  |  0:07:32s\n",
            "epoch 223| loss: 0.26618 | train_logloss: 0.26324 | train_accuracy: 0.89887 | valid_logloss: 0.29778 | valid_accuracy: 0.88977 |  0:07:34s\n",
            "epoch 224| loss: 0.2652  | train_logloss: 0.26261 | train_accuracy: 0.8977  | valid_logloss: 0.29519 | valid_accuracy: 0.89053 |  0:07:36s\n",
            "epoch 225| loss: 0.26423 | train_logloss: 0.25686 | train_accuracy: 0.9009  | valid_logloss: 0.2933  | valid_accuracy: 0.8913  |  0:07:38s\n",
            "epoch 226| loss: 0.26273 | train_logloss: 0.25545 | train_accuracy: 0.90063 | valid_logloss: 0.29231 | valid_accuracy: 0.89043 |  0:07:40s\n",
            "epoch 227| loss: 0.26255 | train_logloss: 0.25584 | train_accuracy: 0.9005  | valid_logloss: 0.29217 | valid_accuracy: 0.89143 |  0:07:42s\n",
            "epoch 228| loss: 0.25956 | train_logloss: 0.25575 | train_accuracy: 0.9006  | valid_logloss: 0.28865 | valid_accuracy: 0.89107 |  0:07:44s\n",
            "epoch 229| loss: 0.25952 | train_logloss: 0.25504 | train_accuracy: 0.90107 | valid_logloss: 0.29357 | valid_accuracy: 0.89147 |  0:07:46s\n",
            "epoch 230| loss: 0.27308 | train_logloss: 0.27744 | train_accuracy: 0.8939  | valid_logloss: 0.30525 | valid_accuracy: 0.88583 |  0:07:48s\n",
            "epoch 231| loss: 0.2756  | train_logloss: 0.2654  | train_accuracy: 0.8977  | valid_logloss: 0.2954  | valid_accuracy: 0.8893  |  0:07:50s\n",
            "epoch 232| loss: 0.27109 | train_logloss: 0.26601 | train_accuracy: 0.89613 | valid_logloss: 0.2968  | valid_accuracy: 0.88777 |  0:07:52s\n",
            "epoch 233| loss: 0.26694 | train_logloss: 0.26158 | train_accuracy: 0.89923 | valid_logloss: 0.29497 | valid_accuracy: 0.89023 |  0:07:54s\n",
            "epoch 234| loss: 0.26703 | train_logloss: 0.26414 | train_accuracy: 0.89757 | valid_logloss: 0.29627 | valid_accuracy: 0.88967 |  0:07:56s\n",
            "epoch 235| loss: 0.26891 | train_logloss: 0.26247 | train_accuracy: 0.89753 | valid_logloss: 0.29674 | valid_accuracy: 0.88933 |  0:07:58s\n",
            "epoch 236| loss: 0.2648  | train_logloss: 0.25947 | train_accuracy: 0.89883 | valid_logloss: 0.2955  | valid_accuracy: 0.89013 |  0:08:00s\n",
            "epoch 237| loss: 0.26411 | train_logloss: 0.25461 | train_accuracy: 0.9011  | valid_logloss: 0.29302 | valid_accuracy: 0.892   |  0:08:02s\n",
            "epoch 238| loss: 0.26003 | train_logloss: 0.25766 | train_accuracy: 0.89867 | valid_logloss: 0.29744 | valid_accuracy: 0.88993 |  0:08:04s\n",
            "epoch 239| loss: 0.2601  | train_logloss: 0.25471 | train_accuracy: 0.9016  | valid_logloss: 0.29461 | valid_accuracy: 0.89117 |  0:08:06s\n",
            "epoch 240| loss: 0.26437 | train_logloss: 0.26948 | train_accuracy: 0.89597 | valid_logloss: 0.30102 | valid_accuracy: 0.8889  |  0:08:08s\n",
            "epoch 241| loss: 0.27138 | train_logloss: 0.2643  | train_accuracy: 0.8975  | valid_logloss: 0.2999  | valid_accuracy: 0.8884  |  0:08:10s\n",
            "epoch 242| loss: 0.26557 | train_logloss: 0.25649 | train_accuracy: 0.90027 | valid_logloss: 0.29486 | valid_accuracy: 0.8906  |  0:08:12s\n",
            "epoch 243| loss: 0.26536 | train_logloss: 0.25769 | train_accuracy: 0.89917 | valid_logloss: 0.29773 | valid_accuracy: 0.89033 |  0:08:14s\n",
            "epoch 244| loss: 0.26142 | train_logloss: 0.25467 | train_accuracy: 0.9007  | valid_logloss: 0.29352 | valid_accuracy: 0.8916  |  0:08:16s\n",
            "epoch 245| loss: 0.25737 | train_logloss: 0.25418 | train_accuracy: 0.90203 | valid_logloss: 0.29455 | valid_accuracy: 0.89157 |  0:08:18s\n",
            "epoch 246| loss: 0.25858 | train_logloss: 0.25286 | train_accuracy: 0.9009  | valid_logloss: 0.2943  | valid_accuracy: 0.89083 |  0:08:20s\n",
            "epoch 247| loss: 0.25952 | train_logloss: 0.25374 | train_accuracy: 0.90157 | valid_logloss: 0.29939 | valid_accuracy: 0.8915  |  0:08:22s\n",
            "epoch 248| loss: 0.25929 | train_logloss: 0.25598 | train_accuracy: 0.90067 | valid_logloss: 0.3003  | valid_accuracy: 0.88927 |  0:08:24s\n",
            "epoch 249| loss: 0.26206 | train_logloss: 0.25927 | train_accuracy: 0.89893 | valid_logloss: 0.30242 | valid_accuracy: 0.88867 |  0:08:26s\n",
            "epoch 250| loss: 0.25907 | train_logloss: 0.25544 | train_accuracy: 0.90027 | valid_logloss: 0.30002 | valid_accuracy: 0.88907 |  0:08:28s\n",
            "epoch 251| loss: 0.26633 | train_logloss: 0.27912 | train_accuracy: 0.89017 | valid_logloss: 0.31513 | valid_accuracy: 0.88313 |  0:08:30s\n",
            "epoch 252| loss: 0.28278 | train_logloss: 0.274   | train_accuracy: 0.89303 | valid_logloss: 0.30406 | valid_accuracy: 0.88817 |  0:08:32s\n",
            "epoch 253| loss: 0.28233 | train_logloss: 0.27218 | train_accuracy: 0.8955  | valid_logloss: 0.30228 | valid_accuracy: 0.88747 |  0:08:34s\n",
            "epoch 254| loss: 0.28067 | train_logloss: 0.27421 | train_accuracy: 0.89563 | valid_logloss: 0.29829 | valid_accuracy: 0.88707 |  0:08:36s\n",
            "epoch 255| loss: 0.27548 | train_logloss: 0.26786 | train_accuracy: 0.8971  | valid_logloss: 0.29646 | valid_accuracy: 0.88763 |  0:08:38s\n",
            "epoch 256| loss: 0.27237 | train_logloss: 0.26337 | train_accuracy: 0.8992  | valid_logloss: 0.2942  | valid_accuracy: 0.8889  |  0:08:40s\n",
            "epoch 257| loss: 0.26585 | train_logloss: 0.25792 | train_accuracy: 0.9008  | valid_logloss: 0.29311 | valid_accuracy: 0.89023 |  0:08:42s\n",
            "epoch 258| loss: 0.27488 | train_logloss: 0.28174 | train_accuracy: 0.8917  | valid_logloss: 0.30313 | valid_accuracy: 0.88617 |  0:08:44s\n",
            "epoch 259| loss: 0.27983 | train_logloss: 0.27409 | train_accuracy: 0.89557 | valid_logloss: 0.3007  | valid_accuracy: 0.88927 |  0:08:46s\n",
            "epoch 260| loss: 0.27451 | train_logloss: 0.27247 | train_accuracy: 0.89607 | valid_logloss: 0.29911 | valid_accuracy: 0.88793 |  0:08:48s\n",
            "epoch 261| loss: 0.27055 | train_logloss: 0.25954 | train_accuracy: 0.89933 | valid_logloss: 0.28966 | valid_accuracy: 0.8903  |  0:08:50s\n",
            "epoch 262| loss: 0.26646 | train_logloss: 0.26116 | train_accuracy: 0.8992  | valid_logloss: 0.29387 | valid_accuracy: 0.8899  |  0:08:52s\n",
            "epoch 263| loss: 0.26555 | train_logloss: 0.26226 | train_accuracy: 0.8992  | valid_logloss: 0.2944  | valid_accuracy: 0.89    |  0:08:54s\n",
            "epoch 264| loss: 0.26853 | train_logloss: 0.27316 | train_accuracy: 0.8965  | valid_logloss: 0.30094 | valid_accuracy: 0.8901  |  0:08:56s\n",
            "epoch 265| loss: 0.27209 | train_logloss: 0.26304 | train_accuracy: 0.8982  | valid_logloss: 0.29688 | valid_accuracy: 0.88857 |  0:08:58s\n",
            "epoch 266| loss: 0.26892 | train_logloss: 0.25988 | train_accuracy: 0.90077 | valid_logloss: 0.29097 | valid_accuracy: 0.89077 |  0:09:00s\n",
            "epoch 267| loss: 0.26533 | train_logloss: 0.25597 | train_accuracy: 0.90103 | valid_logloss: 0.29024 | valid_accuracy: 0.8907  |  0:09:02s\n",
            "epoch 268| loss: 0.2619  | train_logloss: 0.26325 | train_accuracy: 0.8982  | valid_logloss: 0.29668 | valid_accuracy: 0.88817 |  0:09:04s\n",
            "epoch 269| loss: 0.26179 | train_logloss: 0.2524  | train_accuracy: 0.9017  | valid_logloss: 0.28855 | valid_accuracy: 0.89113 |  0:09:06s\n",
            "epoch 270| loss: 0.25819 | train_logloss: 0.25315 | train_accuracy: 0.9014  | valid_logloss: 0.29278 | valid_accuracy: 0.89097 |  0:09:08s\n",
            "epoch 271| loss: 0.25938 | train_logloss: 0.25187 | train_accuracy: 0.9022  | valid_logloss: 0.29391 | valid_accuracy: 0.88997 |  0:09:10s\n",
            "epoch 272| loss: 0.2604  | train_logloss: 0.25152 | train_accuracy: 0.90283 | valid_logloss: 0.29234 | valid_accuracy: 0.89007 |  0:09:12s\n",
            "epoch 273| loss: 0.25584 | train_logloss: 0.25066 | train_accuracy: 0.9026  | valid_logloss: 0.28853 | valid_accuracy: 0.89167 |  0:09:14s\n",
            "epoch 274| loss: 0.25739 | train_logloss: 0.25001 | train_accuracy: 0.903   | valid_logloss: 0.2938  | valid_accuracy: 0.88987 |  0:09:16s\n",
            "epoch 275| loss: 0.25625 | train_logloss: 0.26212 | train_accuracy: 0.89917 | valid_logloss: 0.30536 | valid_accuracy: 0.88603 |  0:09:18s\n",
            "epoch 276| loss: 0.26663 | train_logloss: 0.26069 | train_accuracy: 0.8976  | valid_logloss: 0.30017 | valid_accuracy: 0.88707 |  0:09:20s\n",
            "epoch 277| loss: 0.26376 | train_logloss: 0.25722 | train_accuracy: 0.8994  | valid_logloss: 0.29478 | valid_accuracy: 0.88853 |  0:09:22s\n",
            "epoch 278| loss: 0.25891 | train_logloss: 0.25476 | train_accuracy: 0.90173 | valid_logloss: 0.29633 | valid_accuracy: 0.8892  |  0:09:24s\n",
            "epoch 279| loss: 0.25522 | train_logloss: 0.24765 | train_accuracy: 0.90377 | valid_logloss: 0.29184 | valid_accuracy: 0.88967 |  0:09:26s\n",
            "epoch 280| loss: 0.26783 | train_logloss: 0.27098 | train_accuracy: 0.8977  | valid_logloss: 0.30806 | valid_accuracy: 0.88457 |  0:09:28s\n",
            "epoch 281| loss: 0.2784  | train_logloss: 0.2828  | train_accuracy: 0.89387 | valid_logloss: 0.3106  | valid_accuracy: 0.8829  |  0:09:30s\n",
            "epoch 282| loss: 0.2904  | train_logloss: 0.27913 | train_accuracy: 0.895   | valid_logloss: 0.30259 | valid_accuracy: 0.88593 |  0:09:32s\n",
            "epoch 283| loss: 0.27844 | train_logloss: 0.27004 | train_accuracy: 0.8978  | valid_logloss: 0.29571 | valid_accuracy: 0.88807 |  0:09:34s\n",
            "epoch 284| loss: 0.27037 | train_logloss: 0.26244 | train_accuracy: 0.90007 | valid_logloss: 0.29262 | valid_accuracy: 0.88967 |  0:09:36s\n",
            "epoch 285| loss: 0.2664  | train_logloss: 0.25863 | train_accuracy: 0.90033 | valid_logloss: 0.2929  | valid_accuracy: 0.8898  |  0:09:38s\n",
            "epoch 286| loss: 0.26153 | train_logloss: 0.25482 | train_accuracy: 0.9013  | valid_logloss: 0.28829 | valid_accuracy: 0.8906  |  0:09:40s\n",
            "epoch 287| loss: 0.25943 | train_logloss: 0.25334 | train_accuracy: 0.90313 | valid_logloss: 0.2894  | valid_accuracy: 0.89153 |  0:09:42s\n",
            "\n",
            "Early stopping occurred at epoch 287 with best_epoch = 237 and best_valid_accuracy = 0.892\n",
            "Best weights from best epoch are automatically used!\n",
            "TN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz8TXSqYuxWr"
      },
      "source": [
        "  c1 = 9000\n",
        "  data_split = data_preparation(X, y, c=c1//3)\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = c1//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "  gb = lgb.LGBMClassifier(  #ЛУЧШАЯ МОДЕЛЬ\n",
        "    **{'colsample_bytree': 0.6437405148446416,\n",
        "    'learning_rate': 0.0741521019613115,\n",
        "    'min_child_samples': 9+1,\n",
        "    'min_child_weight': 0.43858057836890685,\n",
        "    'n_estimators': 10000,\n",
        "    'num_leaves': 59+10}\n",
        "  )\n",
        "\n",
        "  t = time()\n",
        "  gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_valid_norm, y_valid)],  **lgb_fit_params)\n",
        "  t = time()-t\n",
        "  print(t)\n",
        "\n",
        "  data_split = data_preparation(X, y, c=30000//3)\n",
        "\n",
        "\n",
        "  X_train, X_test = data_split[:2]\n",
        "  y_train, y_test = data_split[2:4]\n",
        "\n",
        "  train = np.concatenate((X_train, y_train.reshape((len(y_train), 1))), axis=1)\n",
        "  np.random.shuffle(train)\n",
        "  X_train, y_train = train[:,:-1], train[:,-1].astype('int')\n",
        "\n",
        "  count = 30000//3\n",
        "\n",
        "  X1_train, X1_test = data_split[4:6]\n",
        "  y1_train, y1_test = data_split[6:8]\n",
        "  X2_train, X2_test = data_split[8:10] \n",
        "  y2_train, y2_test = data_split[10:12]\n",
        "  X3_train, X3_test = data_split[12:14]\n",
        "  y3_train, y3_test = data_split[14:16]\n",
        "\n",
        "  X_train_pred = np.concatenate((X1_test[2*count : 4*count], X2_test[2*count : 4*count], X3_test[2*count : 4*count])) ###############\n",
        "  X_val_pred   = np.concatenate((X1_test[count : 2*count], X2_test[count : 2*count], X3_test[count : 2*count]))\n",
        "  np.random.shuffle(X_train_pred)\n",
        "  np.random.shuffle(X_val_pred)\n",
        "\n",
        "  X_valid      = np.concatenate((X1_test[4*count : 5*count], X2_test[4*count : 5*count], X3_test[4*count : 5*count]))\n",
        "  y_valid      = np.concatenate((y1_test[4*count : 5*count], y2_test[4*count : 5*count], y3_test[4*count : 5*count]))\n",
        "\n",
        "  robust = RobustScaler()\n",
        "\n",
        "  X_train_norm = robust.fit_transform(X_train)\n",
        "  X_test_norm = robust.transform(X_test)\n",
        "  X_valid_norm = robust.transform(X_valid)\n",
        "\n",
        "\n",
        "  #Accuracy\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('GB '+str(c1)+'   ')\n",
        "\n",
        "  acc, err = bootstrap_accuracy(gb, X_test_norm, y_test)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('acc GB: '+str(acc)+'+-'+str(err)+', ')\n",
        "\n",
        "  #Feature importance\n",
        "  feature_acc(gb, 'GB', Rows)\n",
        "  with open('/content/drive/MyDrive/Научная работа/Data/hyperparametrs.txt', 'a') as f:\n",
        "    f.write('\\n')\n",
        "  gb.booster_.save_model('/content/drive/MyDrive/Научная работа/Data/hyper/gb_size'+str(c1)+'.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}