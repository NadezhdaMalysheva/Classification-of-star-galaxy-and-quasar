{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Эксперименты по отбору признаков",
      "provenance": [],
      "collapsed_sections": [
        "m0kPZJ5yOVbo",
        "N-hlXdD5xbL8"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadezhdaMalysheva/projects/blob/main/X-r_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXFJs19Ks2y",
        "outputId": "e773fd53-845b-4c6a-ae03-8f11b38696e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0kPZJ5yOVbo"
      },
      "source": [
        "# Подгружаем необходимые библиотеки "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtFLkOBsdnJR",
        "outputId": "c7a61afa-e878-4eee-84eb-acb6b5e79145"
      },
      "source": [
        "pip install pytorch-tabnet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading https://files.pythonhosted.org/packages/94/e5/2a808d611a5d44e3c997c0d07362c04a56c70002208e00aec9eee3d923b5/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLtYHFqwmod"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B2lA0dgTwmib",
        "outputId": "20243706-b681-418f-b03e-29fefe42d471"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.offline as py\n",
        "color = sns.color_palette()\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.tools as tls\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVxxcXZGwmdb"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score \n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "from hyperopt import hp\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
        "\n",
        "from time import time\n",
        "import joblib"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuBGRaaiDoJ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hlXdD5xbL8"
      },
      "source": [
        "#Загружаем данные, параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yer4R5LMyP0m"
      },
      "source": [
        "def data_preparation(X, y, c=10000, test_size = 0.8):\n",
        "\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(X[y==1], y[y==1], test_size=test_size, random_state = 43)\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(X[y==2], y[y==2], test_size=test_size, random_state = 43)\n",
        "    X3_train, X3_test, y3_train, y3_test = train_test_split(X[y==3], y[y==3], test_size=test_size, random_state = 43)\n",
        "    \n",
        "    count = c\n",
        "    count1 = c\n",
        "\n",
        "    X_train, X_test = np.concatenate((X1_train[:count], X2_train[:count], X3_train[:count])), np.concatenate((X1_test[:count1], X2_test[:count1], X3_test[:count1]))\n",
        "    y_train, y_test = np.concatenate((y1_train[:count], y2_train[:count], y3_train[:count])), np.concatenate((y1_test[:count1], y2_test[:count1], y3_test[:count1]))\n",
        "    \n",
        "\n",
        "    return [X_train, X_test, y_train, y_test, X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test, X3_train, X3_test, y3_train, y3_test]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dG50Njl5ktB"
      },
      "source": [
        "# Разделение данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycZUuKOxL47B"
      },
      "source": [
        "df_path = '/content/drive/MyDrive/Научная работа/Data/X-ray/x-ray_data_gz.uu'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COI0WXc4sXJ9"
      },
      "source": [
        "#with gzip.open(add_data_path, 'rb') as fd:\n",
        "df = pd.read_pickle(df_path, compression='gzip')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bs3b50BH_Qq"
      },
      "source": [
        "features_path = '/content/drive/MyDrive/Научная работа/Data/features_sdssdr16+psdr2+all_deacls8tr_QSO+GALAXY_20201212133711.pkl'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSG1zcS5B89x"
      },
      "source": [
        "features = pd.read_pickle(features_path)+ ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGhqoP3A69i4"
      },
      "source": [
        "sdss = [i for i in features if 'sdss' in i and 'decals' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxm3-QXp7Lsa"
      },
      "source": [
        "decals = [i for i in features if 'decals' in i and 'sdss' not in i and 'psdr' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKv8oquU7xbr"
      },
      "source": [
        "ps = [i for i in features if 'psdr' in i and 'decals' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SLvzPZd8Eih",
        "outputId": "ddeede6d-98fb-4e9d-9693-e8717e87944c"
      },
      "source": [
        "sdss, decals, ps"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['sdssdr16_u_psf',\n",
              "  'sdssdr16_g_psf',\n",
              "  'sdssdr16_r_psf',\n",
              "  'sdssdr16_i_psf',\n",
              "  'sdssdr16_z_psf',\n",
              "  'sdssdr16_u_cmodel',\n",
              "  'sdssdr16_i_cmodel',\n",
              "  'sdssdr16_u-g_psf',\n",
              "  'sdssdr16_u-r_psf',\n",
              "  'sdssdr16_u-i_psf',\n",
              "  'sdssdr16_u-z_psf',\n",
              "  'sdssdr16_u_psf-cmodel',\n",
              "  'sdssdr16_g-i_psf',\n",
              "  'sdssdr16_g_psf-cmodel',\n",
              "  'sdssdr16_r-i_psf',\n",
              "  'sdssdr16_i-z_psf',\n",
              "  'sdssdr16_i_psf-cmodel',\n",
              "  'LabelQ',\n",
              "  'LabelG',\n",
              "  'LabelS',\n",
              "  'Label'],\n",
              " ['decals8tr_Lw1-Lw2',\n",
              "  'decals8tr_Lw1',\n",
              "  'decals8tr_Lw2',\n",
              "  'decals8tr_g',\n",
              "  'decals8tr_r',\n",
              "  'decals8tr_z',\n",
              "  'decals8tr_g-r',\n",
              "  'decals8tr_g-z',\n",
              "  'decals8tr_r-z',\n",
              "  'LabelQ',\n",
              "  'LabelG',\n",
              "  'LabelS',\n",
              "  'Label'],\n",
              " ['psdr2_i_kron',\n",
              "  'psdr2_y_kron',\n",
              "  'psdr2_g_psf',\n",
              "  'psdr2_r_psf',\n",
              "  'psdr2_i_psf',\n",
              "  'psdr2_z_psf',\n",
              "  'psdr2_y_psf',\n",
              "  'psdr2_g-i_psf',\n",
              "  'psdr2_g-y_psf',\n",
              "  'psdr2_r-i_psf',\n",
              "  'psdr2_r-y_psf',\n",
              "  'psdr2_i-z_psf',\n",
              "  'psdr2_i-y_psf',\n",
              "  'psdr2_i_psf-kron',\n",
              "  'psdr2_z-y_psf',\n",
              "  'psdr2_y_psf-kron',\n",
              "  'LabelQ',\n",
              "  'LabelG',\n",
              "  'LabelS',\n",
              "  'Label'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSYNr0lW9OBd"
      },
      "source": [
        "df = df[features].drop_duplicates()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "nPS0IQn94na-",
        "outputId": "944ec1d1-72d0-4ca6-c836-c99faa51fe41"
      },
      "source": [
        "df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sdssdr16_u_psf</th>\n",
              "      <th>sdssdr16_g_psf</th>\n",
              "      <th>sdssdr16_r_psf</th>\n",
              "      <th>sdssdr16_i_psf</th>\n",
              "      <th>sdssdr16_z_psf</th>\n",
              "      <th>sdssdr16_u_cmodel</th>\n",
              "      <th>sdssdr16_i_cmodel</th>\n",
              "      <th>sdssdr16_u-g_psf</th>\n",
              "      <th>sdssdr16_u-r_psf</th>\n",
              "      <th>sdssdr16_u-i_psf</th>\n",
              "      <th>sdssdr16_u-z_psf</th>\n",
              "      <th>sdssdr16_u_psf-cmodel</th>\n",
              "      <th>sdssdr16_g-i_psf</th>\n",
              "      <th>sdssdr16_g_psf-cmodel</th>\n",
              "      <th>sdssdr16_r-i_psf</th>\n",
              "      <th>sdssdr16_i-z_psf</th>\n",
              "      <th>sdssdr16_i_psf-cmodel</th>\n",
              "      <th>sdssdr16_u_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_u_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_i_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_i_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_Lw2</th>\n",
              "      <th>psdr2_i_kron</th>\n",
              "      <th>psdr2_y_kron</th>\n",
              "      <th>psdr2_g_psf</th>\n",
              "      <th>psdr2_r_psf</th>\n",
              "      <th>psdr2_i_psf</th>\n",
              "      <th>psdr2_z_psf</th>\n",
              "      <th>psdr2_y_psf</th>\n",
              "      <th>psdr2_g-i_psf</th>\n",
              "      <th>psdr2_g-y_psf</th>\n",
              "      <th>psdr2_r-i_psf</th>\n",
              "      <th>psdr2_r-y_psf</th>\n",
              "      <th>psdr2_i-z_psf</th>\n",
              "      <th>psdr2_i-y_psf</th>\n",
              "      <th>psdr2_i_psf-kron</th>\n",
              "      <th>psdr2_z-y_psf</th>\n",
              "      <th>psdr2_y_psf-kron</th>\n",
              "      <th>psdr2_g_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_g_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_r_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_r_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_i_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_i_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_z_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_z_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_y_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_y_kron-decals8tr_Lw2</th>\n",
              "      <th>decals8tr_Lw1-Lw2</th>\n",
              "      <th>decals8tr_Lw1</th>\n",
              "      <th>decals8tr_Lw2</th>\n",
              "      <th>decals8tr_g</th>\n",
              "      <th>decals8tr_r</th>\n",
              "      <th>decals8tr_z</th>\n",
              "      <th>decals8tr_g-r</th>\n",
              "      <th>decals8tr_g-z</th>\n",
              "      <th>decals8tr_r-z</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_g</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_r</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_z</th>\n",
              "      <th>LabelQ</th>\n",
              "      <th>LabelG</th>\n",
              "      <th>LabelS</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.920023</td>\n",
              "      <td>21.565505</td>\n",
              "      <td>21.621283</td>\n",
              "      <td>21.359891</td>\n",
              "      <td>20.538294</td>\n",
              "      <td>22.769656</td>\n",
              "      <td>21.003623</td>\n",
              "      <td>1.354518</td>\n",
              "      <td>1.298741</td>\n",
              "      <td>1.560132</td>\n",
              "      <td>2.381730</td>\n",
              "      <td>0.150367</td>\n",
              "      <td>0.205614</td>\n",
              "      <td>-0.005931</td>\n",
              "      <td>0.261391</td>\n",
              "      <td>0.821598</td>\n",
              "      <td>0.356269</td>\n",
              "      <td>3.785637</td>\n",
              "      <td>3.685080</td>\n",
              "      <td>2.587416</td>\n",
              "      <td>2.486860</td>\n",
              "      <td>2.554807</td>\n",
              "      <td>2.454250</td>\n",
              "      <td>2.019603</td>\n",
              "      <td>1.919047</td>\n",
              "      <td>1.291036</td>\n",
              "      <td>1.190479</td>\n",
              "      <td>21.665634</td>\n",
              "      <td>21.402220</td>\n",
              "      <td>22.190093</td>\n",
              "      <td>22.087713</td>\n",
              "      <td>21.634099</td>\n",
              "      <td>21.289692</td>\n",
              "      <td>21.402220</td>\n",
              "      <td>0.555995</td>\n",
              "      <td>0.787873</td>\n",
              "      <td>0.453615</td>\n",
              "      <td>0.685493</td>\n",
              "      <td>0.344407</td>\n",
              "      <td>0.231878</td>\n",
              "      <td>-0.031535</td>\n",
              "      <td>-0.112529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.063683</td>\n",
              "      <td>2.963126</td>\n",
              "      <td>3.354069</td>\n",
              "      <td>3.253512</td>\n",
              "      <td>2.681614</td>\n",
              "      <td>2.581058</td>\n",
              "      <td>2.016970</td>\n",
              "      <td>1.916414</td>\n",
              "      <td>2.418201</td>\n",
              "      <td>2.317644</td>\n",
              "      <td>-0.100556</td>\n",
              "      <td>18.984020</td>\n",
              "      <td>19.084576</td>\n",
              "      <td>22.001940</td>\n",
              "      <td>21.912679</td>\n",
              "      <td>20.819145</td>\n",
              "      <td>0.089261</td>\n",
              "      <td>1.182795</td>\n",
              "      <td>1.093534</td>\n",
              "      <td>-0.430505</td>\n",
              "      <td>-0.373853</td>\n",
              "      <td>-0.544090</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19.933288</td>\n",
              "      <td>19.661708</td>\n",
              "      <td>19.533851</td>\n",
              "      <td>19.275093</td>\n",
              "      <td>19.199960</td>\n",
              "      <td>19.896294</td>\n",
              "      <td>19.258232</td>\n",
              "      <td>0.271581</td>\n",
              "      <td>0.399437</td>\n",
              "      <td>0.658195</td>\n",
              "      <td>0.733328</td>\n",
              "      <td>0.036994</td>\n",
              "      <td>0.386615</td>\n",
              "      <td>0.016985</td>\n",
              "      <td>0.258758</td>\n",
              "      <td>0.075133</td>\n",
              "      <td>0.016861</td>\n",
              "      <td>1.332332</td>\n",
              "      <td>1.991179</td>\n",
              "      <td>1.080761</td>\n",
              "      <td>1.739607</td>\n",
              "      <td>0.975765</td>\n",
              "      <td>1.634612</td>\n",
              "      <td>0.694270</td>\n",
              "      <td>1.353117</td>\n",
              "      <td>0.598125</td>\n",
              "      <td>1.256971</td>\n",
              "      <td>19.445711</td>\n",
              "      <td>19.468038</td>\n",
              "      <td>19.991779</td>\n",
              "      <td>19.853252</td>\n",
              "      <td>19.411217</td>\n",
              "      <td>19.369294</td>\n",
              "      <td>19.387387</td>\n",
              "      <td>0.580562</td>\n",
              "      <td>0.604392</td>\n",
              "      <td>0.442035</td>\n",
              "      <td>0.465865</td>\n",
              "      <td>0.041924</td>\n",
              "      <td>0.023830</td>\n",
              "      <td>-0.034493</td>\n",
              "      <td>-0.018094</td>\n",
              "      <td>-0.080650</td>\n",
              "      <td>1.546244</td>\n",
              "      <td>2.205090</td>\n",
              "      <td>1.343668</td>\n",
              "      <td>2.002514</td>\n",
              "      <td>0.881749</td>\n",
              "      <td>1.540595</td>\n",
              "      <td>0.936519</td>\n",
              "      <td>1.595366</td>\n",
              "      <td>0.904076</td>\n",
              "      <td>1.562922</td>\n",
              "      <td>0.658846</td>\n",
              "      <td>18.563962</td>\n",
              "      <td>17.905116</td>\n",
              "      <td>19.511440</td>\n",
              "      <td>19.379499</td>\n",
              "      <td>19.041582</td>\n",
              "      <td>0.131941</td>\n",
              "      <td>0.469858</td>\n",
              "      <td>0.337916</td>\n",
              "      <td>0.133283</td>\n",
              "      <td>0.160229</td>\n",
              "      <td>0.120505</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.169854</td>\n",
              "      <td>19.856397</td>\n",
              "      <td>19.653011</td>\n",
              "      <td>19.657884</td>\n",
              "      <td>19.544269</td>\n",
              "      <td>21.155460</td>\n",
              "      <td>19.626169</td>\n",
              "      <td>1.313457</td>\n",
              "      <td>1.516843</td>\n",
              "      <td>1.511970</td>\n",
              "      <td>1.625585</td>\n",
              "      <td>0.014394</td>\n",
              "      <td>0.198513</td>\n",
              "      <td>0.015617</td>\n",
              "      <td>-0.004873</td>\n",
              "      <td>0.113615</td>\n",
              "      <td>0.031715</td>\n",
              "      <td>0.910512</td>\n",
              "      <td>1.261518</td>\n",
              "      <td>-0.404168</td>\n",
              "      <td>-0.053162</td>\n",
              "      <td>-0.601723</td>\n",
              "      <td>-0.250717</td>\n",
              "      <td>-0.618778</td>\n",
              "      <td>-0.267773</td>\n",
              "      <td>-0.761571</td>\n",
              "      <td>-0.410565</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.351006</td>\n",
              "      <td>20.244948</td>\n",
              "      <td>19.893942</td>\n",
              "      <td>20.246877</td>\n",
              "      <td>20.167877</td>\n",
              "      <td>20.141632</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>0.105245</td>\n",
              "      <td>0.026245</td>\n",
              "      <td>-0.406098</td>\n",
              "      <td>-0.524652</td>\n",
              "      <td>-0.658255</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21.462910</td>\n",
              "      <td>20.896202</td>\n",
              "      <td>20.697560</td>\n",
              "      <td>20.624691</td>\n",
              "      <td>20.230447</td>\n",
              "      <td>21.543505</td>\n",
              "      <td>20.629416</td>\n",
              "      <td>0.566709</td>\n",
              "      <td>0.765350</td>\n",
              "      <td>0.838219</td>\n",
              "      <td>1.232464</td>\n",
              "      <td>-0.080595</td>\n",
              "      <td>0.271510</td>\n",
              "      <td>-0.013927</td>\n",
              "      <td>0.072869</td>\n",
              "      <td>0.394245</td>\n",
              "      <td>-0.004725</td>\n",
              "      <td>0.952090</td>\n",
              "      <td>1.367123</td>\n",
              "      <td>0.318714</td>\n",
              "      <td>0.733747</td>\n",
              "      <td>0.103659</td>\n",
              "      <td>0.518692</td>\n",
              "      <td>0.038001</td>\n",
              "      <td>0.453034</td>\n",
              "      <td>-0.469909</td>\n",
              "      <td>-0.054876</td>\n",
              "      <td>21.142477</td>\n",
              "      <td>20.489200</td>\n",
              "      <td>21.200060</td>\n",
              "      <td>21.016467</td>\n",
              "      <td>21.097208</td>\n",
              "      <td>20.800597</td>\n",
              "      <td>20.638847</td>\n",
              "      <td>0.102852</td>\n",
              "      <td>0.561213</td>\n",
              "      <td>-0.080741</td>\n",
              "      <td>0.377620</td>\n",
              "      <td>0.296612</td>\n",
              "      <td>0.458361</td>\n",
              "      <td>-0.045269</td>\n",
              "      <td>0.161749</td>\n",
              "      <td>0.149648</td>\n",
              "      <td>0.562941</td>\n",
              "      <td>0.977974</td>\n",
              "      <td>0.525012</td>\n",
              "      <td>0.940045</td>\n",
              "      <td>0.551062</td>\n",
              "      <td>0.966095</td>\n",
              "      <td>0.411343</td>\n",
              "      <td>0.826376</td>\n",
              "      <td>-0.102215</td>\n",
              "      <td>0.312818</td>\n",
              "      <td>0.415033</td>\n",
              "      <td>20.591415</td>\n",
              "      <td>20.176382</td>\n",
              "      <td>21.039786</td>\n",
              "      <td>20.928928</td>\n",
              "      <td>20.674966</td>\n",
              "      <td>0.110858</td>\n",
              "      <td>0.364820</td>\n",
              "      <td>0.253962</td>\n",
              "      <td>-0.129658</td>\n",
              "      <td>-0.233854</td>\n",
              "      <td>-0.553460</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.807372</td>\n",
              "      <td>21.333804</td>\n",
              "      <td>21.339532</td>\n",
              "      <td>21.396991</td>\n",
              "      <td>20.997808</td>\n",
              "      <td>21.747126</td>\n",
              "      <td>21.271441</td>\n",
              "      <td>0.473568</td>\n",
              "      <td>0.467840</td>\n",
              "      <td>0.410380</td>\n",
              "      <td>0.809564</td>\n",
              "      <td>0.060246</td>\n",
              "      <td>-0.063188</td>\n",
              "      <td>0.020335</td>\n",
              "      <td>-0.057459</td>\n",
              "      <td>0.399184</td>\n",
              "      <td>0.125550</td>\n",
              "      <td>1.675917</td>\n",
              "      <td>2.307175</td>\n",
              "      <td>1.242260</td>\n",
              "      <td>1.873518</td>\n",
              "      <td>1.243229</td>\n",
              "      <td>1.874486</td>\n",
              "      <td>1.200232</td>\n",
              "      <td>1.831490</td>\n",
              "      <td>0.755095</td>\n",
              "      <td>1.386353</td>\n",
              "      <td>21.795659</td>\n",
              "      <td>20.986745</td>\n",
              "      <td>21.709883</td>\n",
              "      <td>21.645918</td>\n",
              "      <td>21.475944</td>\n",
              "      <td>20.975349</td>\n",
              "      <td>20.924649</td>\n",
              "      <td>0.233939</td>\n",
              "      <td>0.785234</td>\n",
              "      <td>0.169974</td>\n",
              "      <td>0.721269</td>\n",
              "      <td>0.500595</td>\n",
              "      <td>0.551295</td>\n",
              "      <td>-0.319715</td>\n",
              "      <td>0.050700</td>\n",
              "      <td>-0.062096</td>\n",
              "      <td>1.646190</td>\n",
              "      <td>2.277448</td>\n",
              "      <td>1.919331</td>\n",
              "      <td>2.550589</td>\n",
              "      <td>1.724450</td>\n",
              "      <td>2.355707</td>\n",
              "      <td>0.935480</td>\n",
              "      <td>1.566738</td>\n",
              "      <td>0.915536</td>\n",
              "      <td>1.546794</td>\n",
              "      <td>0.631258</td>\n",
              "      <td>20.071209</td>\n",
              "      <td>19.439951</td>\n",
              "      <td>22.010142</td>\n",
              "      <td>21.805173</td>\n",
              "      <td>21.275584</td>\n",
              "      <td>0.204969</td>\n",
              "      <td>0.734557</td>\n",
              "      <td>0.529589</td>\n",
              "      <td>-0.696673</td>\n",
              "      <td>-0.490735</td>\n",
              "      <td>-0.449280</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>20.661284</td>\n",
              "      <td>19.255890</td>\n",
              "      <td>18.623087</td>\n",
              "      <td>18.411583</td>\n",
              "      <td>18.284448</td>\n",
              "      <td>20.482987</td>\n",
              "      <td>18.381478</td>\n",
              "      <td>1.405394</td>\n",
              "      <td>2.038197</td>\n",
              "      <td>2.249701</td>\n",
              "      <td>2.376836</td>\n",
              "      <td>0.178297</td>\n",
              "      <td>0.844307</td>\n",
              "      <td>0.039244</td>\n",
              "      <td>0.211504</td>\n",
              "      <td>0.127135</td>\n",
              "      <td>0.030105</td>\n",
              "      <td>1.667076</td>\n",
              "      <td>1.503909</td>\n",
              "      <td>0.400734</td>\n",
              "      <td>0.237567</td>\n",
              "      <td>-0.230477</td>\n",
              "      <td>-0.393644</td>\n",
              "      <td>-0.434433</td>\n",
              "      <td>-0.597601</td>\n",
              "      <td>-0.609287</td>\n",
              "      <td>-0.772454</td>\n",
              "      <td>18.435874</td>\n",
              "      <td>18.337636</td>\n",
              "      <td>19.185897</td>\n",
              "      <td>18.584284</td>\n",
              "      <td>18.409641</td>\n",
              "      <td>18.333290</td>\n",
              "      <td>18.317061</td>\n",
              "      <td>0.776256</td>\n",
              "      <td>0.868836</td>\n",
              "      <td>0.174643</td>\n",
              "      <td>0.267223</td>\n",
              "      <td>0.076351</td>\n",
              "      <td>0.092580</td>\n",
              "      <td>-0.026233</td>\n",
              "      <td>0.016229</td>\n",
              "      <td>-0.020575</td>\n",
              "      <td>0.381438</td>\n",
              "      <td>0.218271</td>\n",
              "      <td>-0.158372</td>\n",
              "      <td>-0.321539</td>\n",
              "      <td>-0.380038</td>\n",
              "      <td>-0.543205</td>\n",
              "      <td>-0.483440</td>\n",
              "      <td>-0.646607</td>\n",
              "      <td>-0.478276</td>\n",
              "      <td>-0.641443</td>\n",
              "      <td>-0.163167</td>\n",
              "      <td>18.815911</td>\n",
              "      <td>18.979078</td>\n",
              "      <td>19.113936</td>\n",
              "      <td>18.539375</td>\n",
              "      <td>18.278859</td>\n",
              "      <td>0.574561</td>\n",
              "      <td>0.835076</td>\n",
              "      <td>0.260515</td>\n",
              "      <td>0.102710</td>\n",
              "      <td>0.046059</td>\n",
              "      <td>-0.072235</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000</th>\n",
              "      <td>13.312305</td>\n",
              "      <td>11.962711</td>\n",
              "      <td>11.560782</td>\n",
              "      <td>11.436245</td>\n",
              "      <td>11.592154</td>\n",
              "      <td>13.995124</td>\n",
              "      <td>14.431538</td>\n",
              "      <td>1.349594</td>\n",
              "      <td>1.751522</td>\n",
              "      <td>1.876060</td>\n",
              "      <td>1.720150</td>\n",
              "      <td>-0.682820</td>\n",
              "      <td>0.526466</td>\n",
              "      <td>-3.094369</td>\n",
              "      <td>0.124537</td>\n",
              "      <td>-0.155909</td>\n",
              "      <td>-2.995293</td>\n",
              "      <td>1.147789</td>\n",
              "      <td>0.485336</td>\n",
              "      <td>2.209744</td>\n",
              "      <td>1.547292</td>\n",
              "      <td>1.862622</td>\n",
              "      <td>1.200169</td>\n",
              "      <td>1.584202</td>\n",
              "      <td>0.921750</td>\n",
              "      <td>0.609923</td>\n",
              "      <td>-0.052529</td>\n",
              "      <td>13.691715</td>\n",
              "      <td>11.542591</td>\n",
              "      <td>12.762819</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.707123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.915187</td>\n",
              "      <td>0.055696</td>\n",
              "      <td>0.847632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.791936</td>\n",
              "      <td>-0.984592</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.372596</td>\n",
              "      <td>1.132858</td>\n",
              "      <td>0.470406</td>\n",
              "      <td>-1.198279</td>\n",
              "      <td>-1.860731</td>\n",
              "      <td>0.844379</td>\n",
              "      <td>0.181927</td>\n",
              "      <td>-0.189074</td>\n",
              "      <td>-0.851526</td>\n",
              "      <td>-1.304745</td>\n",
              "      <td>-1.967197</td>\n",
              "      <td>-0.662452</td>\n",
              "      <td>12.847336</td>\n",
              "      <td>13.509788</td>\n",
              "      <td>12.494762</td>\n",
              "      <td>12.603138</td>\n",
              "      <td>12.784739</td>\n",
              "      <td>-0.108376</td>\n",
              "      <td>-0.289977</td>\n",
              "      <td>-0.181601</td>\n",
              "      <td>2.562318</td>\n",
              "      <td>2.106819</td>\n",
              "      <td>0.672520</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20001</th>\n",
              "      <td>23.303352</td>\n",
              "      <td>20.578231</td>\n",
              "      <td>19.062118</td>\n",
              "      <td>17.481073</td>\n",
              "      <td>16.599224</td>\n",
              "      <td>21.505317</td>\n",
              "      <td>17.486812</td>\n",
              "      <td>2.725120</td>\n",
              "      <td>4.241234</td>\n",
              "      <td>5.822279</td>\n",
              "      <td>6.704128</td>\n",
              "      <td>1.798035</td>\n",
              "      <td>3.097158</td>\n",
              "      <td>0.029285</td>\n",
              "      <td>1.581045</td>\n",
              "      <td>0.881850</td>\n",
              "      <td>-0.005739</td>\n",
              "      <td>4.709437</td>\n",
              "      <td>4.241491</td>\n",
              "      <td>3.753066</td>\n",
              "      <td>3.285120</td>\n",
              "      <td>2.271511</td>\n",
              "      <td>1.803565</td>\n",
              "      <td>0.690932</td>\n",
              "      <td>0.222986</td>\n",
              "      <td>-0.194418</td>\n",
              "      <td>-0.662365</td>\n",
              "      <td>17.566609</td>\n",
              "      <td>16.433256</td>\n",
              "      <td>20.270035</td>\n",
              "      <td>19.036616</td>\n",
              "      <td>17.437841</td>\n",
              "      <td>16.582640</td>\n",
              "      <td>16.385155</td>\n",
              "      <td>2.832194</td>\n",
              "      <td>3.884880</td>\n",
              "      <td>1.598775</td>\n",
              "      <td>2.651461</td>\n",
              "      <td>0.855201</td>\n",
              "      <td>1.052686</td>\n",
              "      <td>-0.128767</td>\n",
              "      <td>0.197485</td>\n",
              "      <td>-0.048101</td>\n",
              "      <td>3.541963</td>\n",
              "      <td>3.074017</td>\n",
              "      <td>2.357511</td>\n",
              "      <td>1.889564</td>\n",
              "      <td>0.770729</td>\n",
              "      <td>0.302783</td>\n",
              "      <td>-0.122753</td>\n",
              "      <td>-0.590700</td>\n",
              "      <td>-0.362624</td>\n",
              "      <td>-0.830570</td>\n",
              "      <td>-0.467946</td>\n",
              "      <td>16.795880</td>\n",
              "      <td>17.263826</td>\n",
              "      <td>20.480267</td>\n",
              "      <td>18.895657</td>\n",
              "      <td>16.544754</td>\n",
              "      <td>1.584610</td>\n",
              "      <td>3.935514</td>\n",
              "      <td>2.350903</td>\n",
              "      <td>0.068679</td>\n",
              "      <td>0.171734</td>\n",
              "      <td>0.056708</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20010</th>\n",
              "      <td>12.033379</td>\n",
              "      <td>10.839125</td>\n",
              "      <td>9.676886</td>\n",
              "      <td>15.717350</td>\n",
              "      <td>7.958119</td>\n",
              "      <td>14.109184</td>\n",
              "      <td>11.643447</td>\n",
              "      <td>1.194254</td>\n",
              "      <td>2.356493</td>\n",
              "      <td>-3.683971</td>\n",
              "      <td>4.075260</td>\n",
              "      <td>-2.075805</td>\n",
              "      <td>-4.878225</td>\n",
              "      <td>-3.254005</td>\n",
              "      <td>-6.040463</td>\n",
              "      <td>7.759231</td>\n",
              "      <td>4.073902</td>\n",
              "      <td>5.488688</td>\n",
              "      <td>5.529773</td>\n",
              "      <td>5.472634</td>\n",
              "      <td>5.513719</td>\n",
              "      <td>4.671113</td>\n",
              "      <td>4.712198</td>\n",
              "      <td>3.022951</td>\n",
              "      <td>3.064036</td>\n",
              "      <td>1.916298</td>\n",
              "      <td>1.957383</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.041085</td>\n",
              "      <td>8.620496</td>\n",
              "      <td>8.579411</td>\n",
              "      <td>12.236090</td>\n",
              "      <td>12.009304</td>\n",
              "      <td>14.070101</td>\n",
              "      <td>0.226786</td>\n",
              "      <td>-1.834011</td>\n",
              "      <td>-2.060797</td>\n",
              "      <td>1.857040</td>\n",
              "      <td>1.282305</td>\n",
              "      <td>-3.533307</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20011</th>\n",
              "      <td>18.825795</td>\n",
              "      <td>16.930775</td>\n",
              "      <td>16.009830</td>\n",
              "      <td>15.568099</td>\n",
              "      <td>15.335913</td>\n",
              "      <td>18.814910</td>\n",
              "      <td>15.567361</td>\n",
              "      <td>1.895021</td>\n",
              "      <td>2.815965</td>\n",
              "      <td>3.257696</td>\n",
              "      <td>3.489883</td>\n",
              "      <td>0.010886</td>\n",
              "      <td>1.362675</td>\n",
              "      <td>0.012904</td>\n",
              "      <td>0.441731</td>\n",
              "      <td>0.232187</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>2.700888</td>\n",
              "      <td>2.077104</td>\n",
              "      <td>0.803849</td>\n",
              "      <td>0.180066</td>\n",
              "      <td>-0.115503</td>\n",
              "      <td>-0.739286</td>\n",
              "      <td>-0.546661</td>\n",
              "      <td>-1.170445</td>\n",
              "      <td>-0.847446</td>\n",
              "      <td>-1.471230</td>\n",
              "      <td>15.652003</td>\n",
              "      <td>15.257767</td>\n",
              "      <td>16.863090</td>\n",
              "      <td>16.099855</td>\n",
              "      <td>15.645479</td>\n",
              "      <td>15.331374</td>\n",
              "      <td>15.280365</td>\n",
              "      <td>1.217611</td>\n",
              "      <td>1.582725</td>\n",
              "      <td>0.454376</td>\n",
              "      <td>0.819491</td>\n",
              "      <td>0.314105</td>\n",
              "      <td>0.365114</td>\n",
              "      <td>-0.006524</td>\n",
              "      <td>0.051009</td>\n",
              "      <td>0.022598</td>\n",
              "      <td>0.818291</td>\n",
              "      <td>0.194508</td>\n",
              "      <td>-0.008586</td>\n",
              "      <td>-0.632370</td>\n",
              "      <td>-0.462019</td>\n",
              "      <td>-1.085803</td>\n",
              "      <td>-0.705682</td>\n",
              "      <td>-1.329466</td>\n",
              "      <td>-0.856255</td>\n",
              "      <td>-1.480038</td>\n",
              "      <td>-0.623784</td>\n",
              "      <td>16.114022</td>\n",
              "      <td>16.737805</td>\n",
              "      <td>16.960602</td>\n",
              "      <td>16.039827</td>\n",
              "      <td>15.399585</td>\n",
              "      <td>0.920774</td>\n",
              "      <td>1.561016</td>\n",
              "      <td>0.640242</td>\n",
              "      <td>-0.042731</td>\n",
              "      <td>-0.041308</td>\n",
              "      <td>-0.133010</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17405 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sdssdr16_u_psf  sdssdr16_g_psf  sdssdr16_r_psf  ...  LabelG  LabelS  Label\n",
              "0           22.920023       21.565505       21.621283  ...       0       0      1\n",
              "1           19.933288       19.661708       19.533851  ...       0       0      1\n",
              "2           21.169854       19.856397       19.653011  ...       0       0      1\n",
              "3           21.462910       20.896202       20.697560  ...       0       0      1\n",
              "4           21.807372       21.333804       21.339532  ...       0       0      1\n",
              "...               ...             ...             ...  ...     ...     ...    ...\n",
              "19999       20.661284       19.255890       18.623087  ...       0       2      0\n",
              "20000       13.312305       11.962711       11.560782  ...       0       2      0\n",
              "20001       23.303352       20.578231       19.062118  ...       0       2      0\n",
              "20010       12.033379       10.839125        9.676886  ...       0       2      0\n",
              "20011       18.825795       16.930775       16.009830  ...       0       2      0\n",
              "\n",
              "[17405 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "XyZFojW8rHNk",
        "outputId": "59493d72-34bd-43ee-f33d-e180e5528f7c"
      },
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize = (24, 6))\n",
        "sns.histplot(df[df['Label']==0]['sdssdr16_r_cmodel'], ax = ax1, bins = 30, color = 'g', element=\"step\")\n",
        "sns.histplot(df[df['Label']==2]['sdssdr16_r_cmodel'], ax = ax2, bins = 30, color = 'r', element=\"step\")\n",
        "sns.histplot(df[df['Label']==1]['sdssdr16_r_cmodel'], ax = ax3, bins = 30, color = 'b', element=\"step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f93a3901bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXFYvowD2w_M"
      },
      "source": [
        "df_s = df[df.Label==0]\n",
        "df_q = df[df.Label==1]\n",
        "df_g = df[df.Label==2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "T9UlYPAE5dPF",
        "outputId": "bb3248ee-1add-4c3e-ee23-be1d1a4d0bdc"
      },
      "source": [
        "df_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sdssdr16_u_psf</th>\n",
              "      <th>sdssdr16_g_psf</th>\n",
              "      <th>sdssdr16_r_psf</th>\n",
              "      <th>sdssdr16_i_psf</th>\n",
              "      <th>sdssdr16_z_psf</th>\n",
              "      <th>sdssdr16_u_cmodel</th>\n",
              "      <th>sdssdr16_i_cmodel</th>\n",
              "      <th>sdssdr16_u-g_psf</th>\n",
              "      <th>sdssdr16_u-r_psf</th>\n",
              "      <th>sdssdr16_u-i_psf</th>\n",
              "      <th>sdssdr16_u-z_psf</th>\n",
              "      <th>sdssdr16_u_psf-cmodel</th>\n",
              "      <th>sdssdr16_g-i_psf</th>\n",
              "      <th>sdssdr16_g_psf-cmodel</th>\n",
              "      <th>sdssdr16_r-i_psf</th>\n",
              "      <th>sdssdr16_i-z_psf</th>\n",
              "      <th>sdssdr16_i_psf-cmodel</th>\n",
              "      <th>sdssdr16_u_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_u_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_i_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_i_cmodel-decals8tr_Lw2</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_Lw1</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_Lw2</th>\n",
              "      <th>psdr2_i_kron</th>\n",
              "      <th>psdr2_y_kron</th>\n",
              "      <th>psdr2_g_psf</th>\n",
              "      <th>psdr2_r_psf</th>\n",
              "      <th>psdr2_i_psf</th>\n",
              "      <th>psdr2_z_psf</th>\n",
              "      <th>psdr2_y_psf</th>\n",
              "      <th>psdr2_g-i_psf</th>\n",
              "      <th>psdr2_g-y_psf</th>\n",
              "      <th>psdr2_r-i_psf</th>\n",
              "      <th>psdr2_r-y_psf</th>\n",
              "      <th>psdr2_i-z_psf</th>\n",
              "      <th>psdr2_i-y_psf</th>\n",
              "      <th>psdr2_i_psf-kron</th>\n",
              "      <th>psdr2_z-y_psf</th>\n",
              "      <th>psdr2_y_psf-kron</th>\n",
              "      <th>psdr2_g_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_g_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_r_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_r_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_i_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_i_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_z_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_z_kron-decals8tr_Lw2</th>\n",
              "      <th>psdr2_y_kron-decals8tr_Lw1</th>\n",
              "      <th>psdr2_y_kron-decals8tr_Lw2</th>\n",
              "      <th>decals8tr_Lw1-Lw2</th>\n",
              "      <th>decals8tr_Lw1</th>\n",
              "      <th>decals8tr_Lw2</th>\n",
              "      <th>decals8tr_g</th>\n",
              "      <th>decals8tr_r</th>\n",
              "      <th>decals8tr_z</th>\n",
              "      <th>decals8tr_g-r</th>\n",
              "      <th>decals8tr_g-z</th>\n",
              "      <th>decals8tr_r-z</th>\n",
              "      <th>sdssdr16_g_cmodel-decals8tr_g</th>\n",
              "      <th>sdssdr16_r_cmodel-decals8tr_r</th>\n",
              "      <th>sdssdr16_z_cmodel-decals8tr_z</th>\n",
              "      <th>LabelQ</th>\n",
              "      <th>LabelG</th>\n",
              "      <th>LabelS</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14095</th>\n",
              "      <td>22.205799</td>\n",
              "      <td>20.147787</td>\n",
              "      <td>19.289909</td>\n",
              "      <td>18.977191</td>\n",
              "      <td>18.667902</td>\n",
              "      <td>20.895187</td>\n",
              "      <td>18.822989</td>\n",
              "      <td>2.058012</td>\n",
              "      <td>2.915890</td>\n",
              "      <td>3.228608</td>\n",
              "      <td>3.537898</td>\n",
              "      <td>1.310613</td>\n",
              "      <td>1.170596</td>\n",
              "      <td>0.061094</td>\n",
              "      <td>0.312718</td>\n",
              "      <td>0.309289</td>\n",
              "      <td>0.154202</td>\n",
              "      <td>1.974550</td>\n",
              "      <td>1.960860</td>\n",
              "      <td>1.166056</td>\n",
              "      <td>1.152367</td>\n",
              "      <td>0.305213</td>\n",
              "      <td>0.291523</td>\n",
              "      <td>-0.097648</td>\n",
              "      <td>-0.111338</td>\n",
              "      <td>-0.425530</td>\n",
              "      <td>-0.439219</td>\n",
              "      <td>18.920448</td>\n",
              "      <td>18.628564</td>\n",
              "      <td>20.006031</td>\n",
              "      <td>19.303136</td>\n",
              "      <td>18.975600</td>\n",
              "      <td>18.842633</td>\n",
              "      <td>18.759075</td>\n",
              "      <td>1.030431</td>\n",
              "      <td>1.246956</td>\n",
              "      <td>0.327537</td>\n",
              "      <td>0.544061</td>\n",
              "      <td>0.132967</td>\n",
              "      <td>0.216524</td>\n",
              "      <td>0.055151</td>\n",
              "      <td>0.083557</td>\n",
              "      <td>0.130512</td>\n",
              "      <td>1.094189</td>\n",
              "      <td>1.080499</td>\n",
              "      <td>0.366355</td>\n",
              "      <td>0.352666</td>\n",
              "      <td>-0.000189</td>\n",
              "      <td>-0.013878</td>\n",
              "      <td>-0.220647</td>\n",
              "      <td>-0.234337</td>\n",
              "      <td>-0.292073</td>\n",
              "      <td>-0.305763</td>\n",
              "      <td>-0.013689</td>\n",
              "      <td>18.920637</td>\n",
              "      <td>18.934326</td>\n",
              "      <td>20.042944</td>\n",
              "      <td>19.211602</td>\n",
              "      <td>18.759660</td>\n",
              "      <td>0.831342</td>\n",
              "      <td>1.283285</td>\n",
              "      <td>0.451942</td>\n",
              "      <td>0.043749</td>\n",
              "      <td>0.014248</td>\n",
              "      <td>-0.264553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14096</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14097</th>\n",
              "      <td>19.140804</td>\n",
              "      <td>17.888408</td>\n",
              "      <td>17.442821</td>\n",
              "      <td>17.247753</td>\n",
              "      <td>17.171994</td>\n",
              "      <td>19.147438</td>\n",
              "      <td>17.253295</td>\n",
              "      <td>1.252396</td>\n",
              "      <td>1.697983</td>\n",
              "      <td>1.893051</td>\n",
              "      <td>1.968810</td>\n",
              "      <td>-0.006634</td>\n",
              "      <td>0.640655</td>\n",
              "      <td>0.004969</td>\n",
              "      <td>0.195068</td>\n",
              "      <td>0.075759</td>\n",
              "      <td>-0.005543</td>\n",
              "      <td>0.812691</td>\n",
              "      <td>0.243831</td>\n",
              "      <td>-0.451308</td>\n",
              "      <td>-1.020168</td>\n",
              "      <td>-0.904510</td>\n",
              "      <td>-1.473370</td>\n",
              "      <td>-1.081452</td>\n",
              "      <td>-1.650312</td>\n",
              "      <td>-1.157631</td>\n",
              "      <td>-1.726491</td>\n",
              "      <td>17.361251</td>\n",
              "      <td>17.204722</td>\n",
              "      <td>17.837975</td>\n",
              "      <td>17.269408</td>\n",
              "      <td>17.273035</td>\n",
              "      <td>17.220647</td>\n",
              "      <td>17.180256</td>\n",
              "      <td>0.564940</td>\n",
              "      <td>0.657720</td>\n",
              "      <td>-0.003628</td>\n",
              "      <td>0.089152</td>\n",
              "      <td>0.052388</td>\n",
              "      <td>0.092780</td>\n",
              "      <td>-0.088216</td>\n",
              "      <td>0.040392</td>\n",
              "      <td>-0.024467</td>\n",
              "      <td>-0.439107</td>\n",
              "      <td>-1.007967</td>\n",
              "      <td>-0.855828</td>\n",
              "      <td>-1.424688</td>\n",
              "      <td>-0.973496</td>\n",
              "      <td>-1.542356</td>\n",
              "      <td>-1.032787</td>\n",
              "      <td>-1.601647</td>\n",
              "      <td>-1.130025</td>\n",
              "      <td>-1.698885</td>\n",
              "      <td>-0.568860</td>\n",
              "      <td>18.334747</td>\n",
              "      <td>18.903607</td>\n",
              "      <td>17.828840</td>\n",
              "      <td>17.366467</td>\n",
              "      <td>17.176663</td>\n",
              "      <td>0.462373</td>\n",
              "      <td>0.652177</td>\n",
              "      <td>0.189804</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.063770</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14098</th>\n",
              "      <td>19.882465</td>\n",
              "      <td>18.527261</td>\n",
              "      <td>17.841124</td>\n",
              "      <td>17.628888</td>\n",
              "      <td>17.428524</td>\n",
              "      <td>19.862783</td>\n",
              "      <td>17.609165</td>\n",
              "      <td>1.355204</td>\n",
              "      <td>2.041341</td>\n",
              "      <td>2.253578</td>\n",
              "      <td>2.453942</td>\n",
              "      <td>0.019683</td>\n",
              "      <td>0.898374</td>\n",
              "      <td>0.021378</td>\n",
              "      <td>0.212236</td>\n",
              "      <td>0.200364</td>\n",
              "      <td>0.019722</td>\n",
              "      <td>1.320175</td>\n",
              "      <td>0.703110</td>\n",
              "      <td>-0.036724</td>\n",
              "      <td>-0.653789</td>\n",
              "      <td>-0.703643</td>\n",
              "      <td>-1.320707</td>\n",
              "      <td>-0.933443</td>\n",
              "      <td>-1.550507</td>\n",
              "      <td>-1.096180</td>\n",
              "      <td>-1.713245</td>\n",
              "      <td>17.709008</td>\n",
              "      <td>17.603952</td>\n",
              "      <td>18.415109</td>\n",
              "      <td>17.840860</td>\n",
              "      <td>17.702549</td>\n",
              "      <td>17.514029</td>\n",
              "      <td>17.457911</td>\n",
              "      <td>0.712560</td>\n",
              "      <td>0.957198</td>\n",
              "      <td>0.138311</td>\n",
              "      <td>0.382949</td>\n",
              "      <td>0.188521</td>\n",
              "      <td>0.244638</td>\n",
              "      <td>-0.006458</td>\n",
              "      <td>0.056118</td>\n",
              "      <td>-0.146041</td>\n",
              "      <td>-0.078471</td>\n",
              "      <td>-0.695536</td>\n",
              "      <td>-0.648537</td>\n",
              "      <td>-1.265601</td>\n",
              "      <td>-0.833600</td>\n",
              "      <td>-1.450665</td>\n",
              "      <td>-0.923954</td>\n",
              "      <td>-1.541018</td>\n",
              "      <td>-0.938655</td>\n",
              "      <td>-1.555720</td>\n",
              "      <td>-0.617064</td>\n",
              "      <td>18.542608</td>\n",
              "      <td>19.159672</td>\n",
              "      <td>18.420719</td>\n",
              "      <td>17.778269</td>\n",
              "      <td>17.500613</td>\n",
              "      <td>0.642449</td>\n",
              "      <td>0.920105</td>\n",
              "      <td>0.277656</td>\n",
              "      <td>0.085165</td>\n",
              "      <td>0.060696</td>\n",
              "      <td>-0.054186</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14099</th>\n",
              "      <td>20.980263</td>\n",
              "      <td>18.559502</td>\n",
              "      <td>17.500689</td>\n",
              "      <td>17.085194</td>\n",
              "      <td>16.828653</td>\n",
              "      <td>20.862707</td>\n",
              "      <td>17.077459</td>\n",
              "      <td>2.420761</td>\n",
              "      <td>3.479574</td>\n",
              "      <td>3.895069</td>\n",
              "      <td>4.151611</td>\n",
              "      <td>0.117557</td>\n",
              "      <td>1.474308</td>\n",
              "      <td>-0.001674</td>\n",
              "      <td>0.415495</td>\n",
              "      <td>0.256542</td>\n",
              "      <td>0.007735</td>\n",
              "      <td>3.275773</td>\n",
              "      <td>2.688127</td>\n",
              "      <td>0.974243</td>\n",
              "      <td>0.386598</td>\n",
              "      <td>-0.101355</td>\n",
              "      <td>-0.689001</td>\n",
              "      <td>-0.509474</td>\n",
              "      <td>-1.097120</td>\n",
              "      <td>-0.745986</td>\n",
              "      <td>-1.333632</td>\n",
              "      <td>17.155033</td>\n",
              "      <td>16.890206</td>\n",
              "      <td>18.422798</td>\n",
              "      <td>17.450494</td>\n",
              "      <td>17.080503</td>\n",
              "      <td>16.921963</td>\n",
              "      <td>16.856364</td>\n",
              "      <td>1.342295</td>\n",
              "      <td>1.566434</td>\n",
              "      <td>0.369991</td>\n",
              "      <td>0.594130</td>\n",
              "      <td>0.158539</td>\n",
              "      <td>0.224139</td>\n",
              "      <td>-0.074530</td>\n",
              "      <td>0.065600</td>\n",
              "      <td>-0.033843</td>\n",
              "      <td>0.907575</td>\n",
              "      <td>0.319929</td>\n",
              "      <td>-0.051811</td>\n",
              "      <td>-0.639456</td>\n",
              "      <td>-0.431901</td>\n",
              "      <td>-1.019546</td>\n",
              "      <td>-0.606877</td>\n",
              "      <td>-1.194523</td>\n",
              "      <td>-0.696727</td>\n",
              "      <td>-1.284373</td>\n",
              "      <td>-0.587646</td>\n",
              "      <td>17.586934</td>\n",
              "      <td>18.174579</td>\n",
              "      <td>18.473763</td>\n",
              "      <td>17.378221</td>\n",
              "      <td>16.844015</td>\n",
              "      <td>1.095543</td>\n",
              "      <td>1.629748</td>\n",
              "      <td>0.534205</td>\n",
              "      <td>0.087414</td>\n",
              "      <td>0.107358</td>\n",
              "      <td>-0.003068</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>20.661284</td>\n",
              "      <td>19.255890</td>\n",
              "      <td>18.623087</td>\n",
              "      <td>18.411583</td>\n",
              "      <td>18.284448</td>\n",
              "      <td>20.482987</td>\n",
              "      <td>18.381478</td>\n",
              "      <td>1.405394</td>\n",
              "      <td>2.038197</td>\n",
              "      <td>2.249701</td>\n",
              "      <td>2.376836</td>\n",
              "      <td>0.178297</td>\n",
              "      <td>0.844307</td>\n",
              "      <td>0.039244</td>\n",
              "      <td>0.211504</td>\n",
              "      <td>0.127135</td>\n",
              "      <td>0.030105</td>\n",
              "      <td>1.667076</td>\n",
              "      <td>1.503909</td>\n",
              "      <td>0.400734</td>\n",
              "      <td>0.237567</td>\n",
              "      <td>-0.230477</td>\n",
              "      <td>-0.393644</td>\n",
              "      <td>-0.434433</td>\n",
              "      <td>-0.597601</td>\n",
              "      <td>-0.609287</td>\n",
              "      <td>-0.772454</td>\n",
              "      <td>18.435874</td>\n",
              "      <td>18.337636</td>\n",
              "      <td>19.185897</td>\n",
              "      <td>18.584284</td>\n",
              "      <td>18.409641</td>\n",
              "      <td>18.333290</td>\n",
              "      <td>18.317061</td>\n",
              "      <td>0.776256</td>\n",
              "      <td>0.868836</td>\n",
              "      <td>0.174643</td>\n",
              "      <td>0.267223</td>\n",
              "      <td>0.076351</td>\n",
              "      <td>0.092580</td>\n",
              "      <td>-0.026233</td>\n",
              "      <td>0.016229</td>\n",
              "      <td>-0.020575</td>\n",
              "      <td>0.381438</td>\n",
              "      <td>0.218271</td>\n",
              "      <td>-0.158372</td>\n",
              "      <td>-0.321539</td>\n",
              "      <td>-0.380038</td>\n",
              "      <td>-0.543205</td>\n",
              "      <td>-0.483440</td>\n",
              "      <td>-0.646607</td>\n",
              "      <td>-0.478276</td>\n",
              "      <td>-0.641443</td>\n",
              "      <td>-0.163167</td>\n",
              "      <td>18.815911</td>\n",
              "      <td>18.979078</td>\n",
              "      <td>19.113936</td>\n",
              "      <td>18.539375</td>\n",
              "      <td>18.278859</td>\n",
              "      <td>0.574561</td>\n",
              "      <td>0.835076</td>\n",
              "      <td>0.260515</td>\n",
              "      <td>0.102710</td>\n",
              "      <td>0.046059</td>\n",
              "      <td>-0.072235</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000</th>\n",
              "      <td>13.312305</td>\n",
              "      <td>11.962711</td>\n",
              "      <td>11.560782</td>\n",
              "      <td>11.436245</td>\n",
              "      <td>11.592154</td>\n",
              "      <td>13.995124</td>\n",
              "      <td>14.431538</td>\n",
              "      <td>1.349594</td>\n",
              "      <td>1.751522</td>\n",
              "      <td>1.876060</td>\n",
              "      <td>1.720150</td>\n",
              "      <td>-0.682820</td>\n",
              "      <td>0.526466</td>\n",
              "      <td>-3.094369</td>\n",
              "      <td>0.124537</td>\n",
              "      <td>-0.155909</td>\n",
              "      <td>-2.995293</td>\n",
              "      <td>1.147789</td>\n",
              "      <td>0.485336</td>\n",
              "      <td>2.209744</td>\n",
              "      <td>1.547292</td>\n",
              "      <td>1.862622</td>\n",
              "      <td>1.200169</td>\n",
              "      <td>1.584202</td>\n",
              "      <td>0.921750</td>\n",
              "      <td>0.609923</td>\n",
              "      <td>-0.052529</td>\n",
              "      <td>13.691715</td>\n",
              "      <td>11.542591</td>\n",
              "      <td>12.762819</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.707123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.915187</td>\n",
              "      <td>0.055696</td>\n",
              "      <td>0.847632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.791936</td>\n",
              "      <td>-0.984592</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.372596</td>\n",
              "      <td>1.132858</td>\n",
              "      <td>0.470406</td>\n",
              "      <td>-1.198279</td>\n",
              "      <td>-1.860731</td>\n",
              "      <td>0.844379</td>\n",
              "      <td>0.181927</td>\n",
              "      <td>-0.189074</td>\n",
              "      <td>-0.851526</td>\n",
              "      <td>-1.304745</td>\n",
              "      <td>-1.967197</td>\n",
              "      <td>-0.662452</td>\n",
              "      <td>12.847336</td>\n",
              "      <td>13.509788</td>\n",
              "      <td>12.494762</td>\n",
              "      <td>12.603138</td>\n",
              "      <td>12.784739</td>\n",
              "      <td>-0.108376</td>\n",
              "      <td>-0.289977</td>\n",
              "      <td>-0.181601</td>\n",
              "      <td>2.562318</td>\n",
              "      <td>2.106819</td>\n",
              "      <td>0.672520</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20001</th>\n",
              "      <td>23.303352</td>\n",
              "      <td>20.578231</td>\n",
              "      <td>19.062118</td>\n",
              "      <td>17.481073</td>\n",
              "      <td>16.599224</td>\n",
              "      <td>21.505317</td>\n",
              "      <td>17.486812</td>\n",
              "      <td>2.725120</td>\n",
              "      <td>4.241234</td>\n",
              "      <td>5.822279</td>\n",
              "      <td>6.704128</td>\n",
              "      <td>1.798035</td>\n",
              "      <td>3.097158</td>\n",
              "      <td>0.029285</td>\n",
              "      <td>1.581045</td>\n",
              "      <td>0.881850</td>\n",
              "      <td>-0.005739</td>\n",
              "      <td>4.709437</td>\n",
              "      <td>4.241491</td>\n",
              "      <td>3.753066</td>\n",
              "      <td>3.285120</td>\n",
              "      <td>2.271511</td>\n",
              "      <td>1.803565</td>\n",
              "      <td>0.690932</td>\n",
              "      <td>0.222986</td>\n",
              "      <td>-0.194418</td>\n",
              "      <td>-0.662365</td>\n",
              "      <td>17.566609</td>\n",
              "      <td>16.433256</td>\n",
              "      <td>20.270035</td>\n",
              "      <td>19.036616</td>\n",
              "      <td>17.437841</td>\n",
              "      <td>16.582640</td>\n",
              "      <td>16.385155</td>\n",
              "      <td>2.832194</td>\n",
              "      <td>3.884880</td>\n",
              "      <td>1.598775</td>\n",
              "      <td>2.651461</td>\n",
              "      <td>0.855201</td>\n",
              "      <td>1.052686</td>\n",
              "      <td>-0.128767</td>\n",
              "      <td>0.197485</td>\n",
              "      <td>-0.048101</td>\n",
              "      <td>3.541963</td>\n",
              "      <td>3.074017</td>\n",
              "      <td>2.357511</td>\n",
              "      <td>1.889564</td>\n",
              "      <td>0.770729</td>\n",
              "      <td>0.302783</td>\n",
              "      <td>-0.122753</td>\n",
              "      <td>-0.590700</td>\n",
              "      <td>-0.362624</td>\n",
              "      <td>-0.830570</td>\n",
              "      <td>-0.467946</td>\n",
              "      <td>16.795880</td>\n",
              "      <td>17.263826</td>\n",
              "      <td>20.480267</td>\n",
              "      <td>18.895657</td>\n",
              "      <td>16.544754</td>\n",
              "      <td>1.584610</td>\n",
              "      <td>3.935514</td>\n",
              "      <td>2.350903</td>\n",
              "      <td>0.068679</td>\n",
              "      <td>0.171734</td>\n",
              "      <td>0.056708</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20010</th>\n",
              "      <td>12.033379</td>\n",
              "      <td>10.839125</td>\n",
              "      <td>9.676886</td>\n",
              "      <td>15.717350</td>\n",
              "      <td>7.958119</td>\n",
              "      <td>14.109184</td>\n",
              "      <td>11.643447</td>\n",
              "      <td>1.194254</td>\n",
              "      <td>2.356493</td>\n",
              "      <td>-3.683971</td>\n",
              "      <td>4.075260</td>\n",
              "      <td>-2.075805</td>\n",
              "      <td>-4.878225</td>\n",
              "      <td>-3.254005</td>\n",
              "      <td>-6.040463</td>\n",
              "      <td>7.759231</td>\n",
              "      <td>4.073902</td>\n",
              "      <td>5.488688</td>\n",
              "      <td>5.529773</td>\n",
              "      <td>5.472634</td>\n",
              "      <td>5.513719</td>\n",
              "      <td>4.671113</td>\n",
              "      <td>4.712198</td>\n",
              "      <td>3.022951</td>\n",
              "      <td>3.064036</td>\n",
              "      <td>1.916298</td>\n",
              "      <td>1.957383</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.041085</td>\n",
              "      <td>8.620496</td>\n",
              "      <td>8.579411</td>\n",
              "      <td>12.236090</td>\n",
              "      <td>12.009304</td>\n",
              "      <td>14.070101</td>\n",
              "      <td>0.226786</td>\n",
              "      <td>-1.834011</td>\n",
              "      <td>-2.060797</td>\n",
              "      <td>1.857040</td>\n",
              "      <td>1.282305</td>\n",
              "      <td>-3.533307</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20011</th>\n",
              "      <td>18.825795</td>\n",
              "      <td>16.930775</td>\n",
              "      <td>16.009830</td>\n",
              "      <td>15.568099</td>\n",
              "      <td>15.335913</td>\n",
              "      <td>18.814910</td>\n",
              "      <td>15.567361</td>\n",
              "      <td>1.895021</td>\n",
              "      <td>2.815965</td>\n",
              "      <td>3.257696</td>\n",
              "      <td>3.489883</td>\n",
              "      <td>0.010886</td>\n",
              "      <td>1.362675</td>\n",
              "      <td>0.012904</td>\n",
              "      <td>0.441731</td>\n",
              "      <td>0.232187</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>2.700888</td>\n",
              "      <td>2.077104</td>\n",
              "      <td>0.803849</td>\n",
              "      <td>0.180066</td>\n",
              "      <td>-0.115503</td>\n",
              "      <td>-0.739286</td>\n",
              "      <td>-0.546661</td>\n",
              "      <td>-1.170445</td>\n",
              "      <td>-0.847446</td>\n",
              "      <td>-1.471230</td>\n",
              "      <td>15.652003</td>\n",
              "      <td>15.257767</td>\n",
              "      <td>16.863090</td>\n",
              "      <td>16.099855</td>\n",
              "      <td>15.645479</td>\n",
              "      <td>15.331374</td>\n",
              "      <td>15.280365</td>\n",
              "      <td>1.217611</td>\n",
              "      <td>1.582725</td>\n",
              "      <td>0.454376</td>\n",
              "      <td>0.819491</td>\n",
              "      <td>0.314105</td>\n",
              "      <td>0.365114</td>\n",
              "      <td>-0.006524</td>\n",
              "      <td>0.051009</td>\n",
              "      <td>0.022598</td>\n",
              "      <td>0.818291</td>\n",
              "      <td>0.194508</td>\n",
              "      <td>-0.008586</td>\n",
              "      <td>-0.632370</td>\n",
              "      <td>-0.462019</td>\n",
              "      <td>-1.085803</td>\n",
              "      <td>-0.705682</td>\n",
              "      <td>-1.329466</td>\n",
              "      <td>-0.856255</td>\n",
              "      <td>-1.480038</td>\n",
              "      <td>-0.623784</td>\n",
              "      <td>16.114022</td>\n",
              "      <td>16.737805</td>\n",
              "      <td>16.960602</td>\n",
              "      <td>16.039827</td>\n",
              "      <td>15.399585</td>\n",
              "      <td>0.920774</td>\n",
              "      <td>1.561016</td>\n",
              "      <td>0.640242</td>\n",
              "      <td>-0.042731</td>\n",
              "      <td>-0.041308</td>\n",
              "      <td>-0.133010</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3462 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sdssdr16_u_psf  sdssdr16_g_psf  sdssdr16_r_psf  ...  LabelG  LabelS  Label\n",
              "14095       22.205799       20.147787       19.289909  ...       0       1      0\n",
              "14096             NaN             NaN             NaN  ...       0       1      0\n",
              "14097       19.140804       17.888408       17.442821  ...       0       1      0\n",
              "14098       19.882465       18.527261       17.841124  ...       0       1      0\n",
              "14099       20.980263       18.559502       17.500689  ...       0       1      0\n",
              "...               ...             ...             ...  ...     ...     ...    ...\n",
              "19999       20.661284       19.255890       18.623087  ...       0       2      0\n",
              "20000       13.312305       11.962711       11.560782  ...       0       2      0\n",
              "20001       23.303352       20.578231       19.062118  ...       0       2      0\n",
              "20010       12.033379       10.839125        9.676886  ...       0       2      0\n",
              "20011       18.825795       16.930775       16.009830  ...       0       2      0\n",
              "\n",
              "[3462 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vGzO_BtL5FdD",
        "outputId": "cf68aa3e-87f0-425c-8bce-30c62d11e524"
      },
      "source": [
        "sns.displot(df_s.sdssdr16_r_psf)\n",
        "sns.displot(df_q.sdssdr16_r_psf)\n",
        "sns.displot(df_g.sdssdr16_r_psf)\n",
        "print(df_s.shape, df_q.shape, df_g.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3462, 69) (9442, 69) (4501, 69)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX0ElEQVR4nO3dfbBkdX3n8fdH0WwiJEAYZgYYghKii3FRdvA5CQJRZF3RlE60UopEd+IGguyqK2pVMlu1pjBGExNTuKgE3OAD+LCSiCgKxo2lwDjhGZURYZ3ZucygM+JDooH73T/6TGyH27fvnbndv773vl9VXX36dx76e3sOH07/+pzfSVUhSRq/h7UuQJKWKwNYkhoxgCWpEQNYkhoxgCWpkf1aF7AvTj311LrqqqtalyFJw2SmxkV9BHzfffe1LkGS9tqiDmBJWswMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYW9XCU0nwdd/wJbJuaGjh/9apV3LTphjFWpOXMANaysm1qipM2XDZw/jUb1o2xGi13dkFIUiMeAUt9du7axaGHrZl1GbsptFAMYKnP9PT0rF0UYDeFFo5dEJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY14IYaWjGED7UDvSjdpUhjAWjKGDbQDcPk5p4ypGmk4uyAkqREDWJIaMYAlqREDWJIaMYAlqRHPgpDmadig7Q7YrrkygKV5GjZouwO2a67sgpCkRkYWwEnWJLk2ye1Jbkvymq59Q5KtSW7sHqf1rfPGJJuTfC3Jc0ZVmyRNglF2QTwAvLaqNiU5APhKkqu7eX9WVX/av3CSY4GXAI8HDgM+m+RXqurBEdYoSc2M7Ai4qrZV1aZu+nvAHcDhs6xyOvChqvpRVX0T2Aw8eVT1SVJrY+kDTnIU8CTguq7p7CQ3J7koyUFd2+HAt/pW28LsgS1Ji9rIAzjJ/sBHgXOr6n7gAuBo4InANuDt89ze+iQbk2zcsWPHgtcrSeMy0gBO8gh64XtpVX0MoKruraoHq2oaeA8/6WbYCvSfXHlE1/ZTqurCqlpbVWtXrFgxyvIlaaRGeRZEgPcBd1TVO/raV/ct9kLg1m76CuAlSX4myaOBY4DrR1WfJLU2yrMgngG8DLglyY1d25uAlyZ5IlDA3cDvAVTVbUkuA26ndwbFWZ4BIWkpG1kAV9U/AJlh1pWzrPMW4C2jqkmSJolXwklSIwawJDViAEtSIwawJDViAEtSIwawJDViAEtSI94RQ1pgw25ZBN62SD0GsLTAht2yCLxtkXoMYC0axx1/AtumpgbO37lr1xirkfadAaxFY9vU1KxHlpefc8oYq5H2nT/CSVIjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1IjIwvgJGuSXJvk9iS3JXlN135wkquT3Nk9H9S1J8lfJNmc5OYkx4+qNkmaBKM8An4AeG1VHQs8FTgrybHAecDnquoY4HPda4DnAsd0j/XABSOsTZKaG1kAV9W2qtrUTX8PuAM4HDgduKRb7BLgBd306cD7q+fLwIFJVo+qPklqbSx9wEmOAp4EXAesrKpt3awpYGU3fTjwrb7VtnRtkrQkjTyAk+wPfBQ4t6ru759XVQXUPLe3PsnGJBt37NixgJVK0niNNICTPIJe+F5aVR/rmu/d3bXQPW/v2rcCa/pWP6Jr+ylVdWFVra2qtStWrBhd8ZI0YqM8CyLA+4A7quodfbOuAM7ops8APtHX/vLubIinAt/t66qQpCVnvxFu+xnAy4BbktzYtb0JOB+4LMkrgXuAdd28K4HTgM3AD4EzR1ibJDU3sgCuqn8AMmD2yTMsX8BZo6pHkiaNV8JJUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1MsrzgCUNsHPXLg49bM3A+atXreKmTTeMsSK1YABLDUxPT3PShssGzr9mw7qB87R02AUhSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY3s17oACeC4409g29TUrMvs3LVrTNVI42EAayJsm5ripA2XzbrM5eecMqZqpPGwC0KSGjGAJakRA1iSGjGAJakRA1iSGjGAJamROQVwkmfMpU2SNHdzPQL+yzm2SZLmaNYLMZI8DXg6sCLJf+2b9fPAw0dZmCQtdcOOgB8J7E8vqA/oe9wPvGi2FZNclGR7klv72jYk2Zrkxu5xWt+8NybZnORrSZ6zt3+QJC0Wsx4BV9XfA3+f5OKqumee274YeBfw/j3a/6yq/rS/IcmxwEuAxwOHAZ9N8itV9eA831OSFo25jgXxM0kuBI7qX6eqThq0QlV9IclRc9z+6cCHqupHwDeTbAaeDHxpjutL0qIz1wC+HHg38F5gX49Kz07ycmAj8Nqq2gkcDny5b5ktXZskLVlzPQvigaq6oKqur6qv7H7sxftdABwNPBHYBrx9vhtIsj7JxiQbd+zYsRclSNJkmGsA/22S30+yOsnBux/zfbOqureqHqyqaeA99LoZALYCa/oWPaJrm2kbF1bV2qpau2LFivmWIEkTY65dEGd0z6/vayvgMfN5sySrq2pb9/KFwO4zJK4APpDkHfR+hDsGuH4+25akxWZOAVxVj57vhpN8EDgROCTJFuCPgBOTPJFeeN8N/F63/duSXAbcDjwAnOUZEFrOdu7axaGHrZl1mdWrVnHTphvGVJFGYU4B3P1o9hBVtecpZv3zXjpD8/tmWf4twFvmUo8Wn2G3HPJ2Qz9tenp66B1CrtmwbkzVaFTm2gVxQt/0vwFOBjbx0HN8pRkNu+WQtxvScjTXLog/6H+d5EDgQyOpSAtqLje79Kus1Mbe3pTzB8C8+4U1fnO52aVfZaU25toH/Lf0fjiD3iA8/xaY/b9qSdKs5noE3D92wwPAPVW1ZQT1SNKyMacLMbpBeb5KbyS0g4Afj7IoSVoO5npHjHX0Lox4MbAOuC7JrMNRSpJmN9cuiDcDJ1TVdoAkK4DPAh8ZVWGStNTNdSyIh+0O386357GuJGkGcz0CvirJp4EPdq9/G7hyNCVJ0vIw7J5wvwysrKrXJ/kt4JndrC8Bl466OElayoYdAf858EaAqvoY8DGAJE/o5v3HkVYnSUvYsH7clVV1y56NXdtRI6lIkpaJYQF84CzzfnYhC5Gk5WZYF8TGJP+pqt7T35jkVcDe3JJIi5AD+kijMSyAzwU+nuR3+EngrgUeSe+OFloGHNBHGo1ZA7iq7gWenuRZwK92zZ+sqmtGXpkWlWF3cHDAdemh5joe8LXAtSOuRYvYsDs4OOC69FBezSZJjeztgOxaQuw+kNowgGX3gdSIXRCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IhjQUww70QhLW0G8ATzThTS0mYXhCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1MrKxIJJcBDwP2F5Vv9q1HQx8GDgKuBtYV1U7kwR4J3Aa8EPgFVW1aVS1LSU7d+3i0MPWzDpf0mQa5WA8FwPvAt7f13Ye8LmqOj/Jed3rNwDPBY7pHk8BLuieNcT09PSsA/Zcfs4pY6xG0nyMrAuiqr4AfGeP5tOBS7rpS4AX9LW/v3q+DByYZPWoapOkSTDu4ShXVtW2bnoKWNlNHw58q2+5LV3bNvaQZD2wHuDII48cXaXShBvW/eRY0ZOv2XjAVVVJai/WuxC4EGDt2rXzXl9aKoZ1PzlW9OQb91kQ9+7uWuiet3ftW4H+/5Uf0bVJ0pI17gC+Ajijmz4D+ERf+8vT81Tgu31dFZK0JI3yNLQPAicChyTZAvwRcD5wWZJXAvcAu78jXUnvFLTN9E5DO3NUdUnLxbA+YrCfuLWRBXBVvXTArJNnWLaAs0ZVi7QcDesjBvuJW/NKOElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqxACWpEYMYElqZL/WBSxnxx1/AtumpgbO37lr1xirkTRuBnBD26amOGnDZQPnX37OKWOsRtK42QUhSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY0YwJLUiAEsSY00GQ0tyd3A94AHgQeqam2Sg4EPA0cBdwPrqmpni/okaRxaDkf5rKq6r+/1ecDnqur8JOd1r9/QprR9N2ysX3C8X2m5m6TxgE8HTuymLwE+zyIO4GFj/YLj/WryzeVAYvWqVdy06YYxVbS0tArgAj6TpID/WVUXAiurals3fwpYOdOKSdYD6wGOPPLIcdQqLVtzOZC4ZsO6MVWz9LQK4GdW1dYkhwJXJ/lq/8yqqi6cH6IL6wsB1q5dO+MykrQYNDkLoqq2ds/bgY8DTwbuTbIaoHve3qI2SRqXsQdwkkclOWD3NPBs4FbgCuCMbrEzgE+MuzZJGqcWXRArgY8n2f3+H6iqq5LcAFyW5JXAPYAdS5KWtLEHcFXdBRw3Q/u3gZPHXY+0nO3ctYtDD1sz63yNziSdhiZpzKanp2c9y8FTJUfLS5ElqREDWJIaMYAlqREDWJIaMYAlqREDWJIa8TQ0Sftk2LnEjpY2mAEsaZ8MO5fY0dIGswtCkhoxgCWpEQNYkhoxgCWpEQNYkhoxgCWpEQNYkhrxPOC9NOx23Q5kLWkYA3gvDbtdtwNZSxrGLghJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasSxIGYwbKAdcLAdSfvOAJ7BsIF2wMF2JO07uyAkqREDWJIaMYAlqREDWJIaMYAlqRHPgpA0Ujt37eLQw9bMuszqVau4adMNY6pochjAkkZqenp66Gmd12xYN6ZqJotdEJLUiAEsSY0YwJLUyLLsAx421oPjPEgah2UZwMPGenCcB0njsCwDWNJkGXaq2ve//33233//WbexGE9lm7gATnIq8E7g4cB7q+r8xiVJGrFhp6pdfs4pPH8Jnso2UQGc5OHAXwG/CWwBbkhyRVXd3rYySZNu2FH0JB4hT1QAA08GNlfVXQBJPgScDhjAkmY17Cj6o+c+e+gVecO6OhY6xFNVC7axfZXkRcCpVfWq7vXLgKdU1dl9y6wH1ncvHwt8bYHe/hDgvgXa1kKb1Nqsa34mtS6Y3NqWSl33VdWpezZO2hHwUFV1IXDhQm83ycaqWrvQ210Ik1qbdc3PpNYFk1vbUq9r0i7E2Ar0f0c4omuTpCVn0gL4BuCYJI9O8kjgJcAVjWuSpJGYqC6IqnogydnAp+mdhnZRVd02prdf8G6NBTSptVnX/ExqXTC5tS3puibqRzhJWk4mrQtCkpYNA1iSGllWAZzksUlu7Hvcn+TcPZY5Mcl3+5b5wxHWc1GS7Ulu7Ws7OMnVSe7sng8asO4Z3TJ3JjljDHW9LclXk9yc5ONJDhyw7t1Jbuk+u41jqGtDkq19/16nDVj31CRfS7I5yXljqOvDfTXdneTGAeuO8vNak+TaJLcnuS3Ja7r2pvvYLHVNwj42qLbR7GdVtSwf9H7kmwJ+aY/2E4G/G1MNvw4cD9za1/YnwHnd9HnAW2dY72Dgru75oG76oBHX9Wxgv276rTPV1c27GzhkjJ/XBuB1c/i3/gbwGOCRwE3AsaOsa4/5bwf+sMHntRo4vps+APg6cGzrfWyWuiZhHxtU20j2s2V1BLyHk4FvVNU9rQqoqi8A39mj+XTgkm76EuAFM6z6HODqqvpOVe0ErgYecpXNQtZVVZ+pqge6l1+md472WA34vObiXy9xr6ofA7svcR95XUkCrAM+uFDvN1dVta2qNnXT3wPuAA6n8T42qK4J2ccGfWZzMe/9bDkH8EsY/B/F05LclORTSR4/zqKAlVW1rZueAlbOsMzhwLf6Xm9h7jvJQvhd4FMD5hXwmSRf6S4bH4ezu6+tFw34Ot3y8/o14N6qunPA/LF8XkmOAp4EXMcE7WN71NWv+T42Q20Lvp8tywDuLvJ4PnD5DLM30euWOA74S+B/j7O2ftX7XjNR5wkmeTPwAHDpgEWeWVXHA88Fzkry6yMu6QLgaOCJwDZ6X/cnyUuZ/eh35J9Xkv2BjwLnVtX9/fNa7mOD6pqEfWyG2kayny3LAKb3D7epqu7dc0ZV3V9V3++mrwQekeSQMdZ2b5LVAN3z9hmWaXLJdpJXAM8Dfqf7D/chqmpr97wd+Di9r2UjU1X3VtWDVTUNvGfA+7X6vPYDfgv48KBlRv15JXkEvSC5tKo+1jU338cG1DUR+9hMtY1qP1uuATzwqCTJqq7fjiRPpvcZfXuMtV0B7P7F+QzgEzMs82ng2UkO6r4KPbtrG5n0Bsr/b8Dzq+qHA5Z5VJIDdk93dd0607ILWNfqvpcvHPB+rS5xPwX4alVtmWnmqD+vbj9+H3BHVb2jb1bTfWxQXZOwj81S22j2s1H8kjjJD+BR9AL1F/raXg28ups+G7iN3i+YXwaePsJaPkjv68y/0OsveiXwi8DngDuBzwIHd8uupXeHkN3r/i6wuXucOYa6NtPr37qxe7y7W/Yw4Mpu+jHd53ZT9xm+eQx1/S/gFuDmbmdfvWdd3evT6P2i/Y1x1NW1X7x7v+pbdpyf1zPpdS/c3PfvdlrrfWyWuiZhHxtU20j2My9FlqRGlmsXhCQ1ZwBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCZaesOD/t0+buMVSd41YN7jknwpyY+SvG6PeQcm+Ug3ROIdSZ62L3XMV5JzuvcddEmuFrmJuiecNE7dpcLfAc5h5hHB3glcVVUv6q5s+rm5brd+MqrXvvh94JQacCWdFj+PgNVUd2npJ7vR525N8tvdoNZfTbKJ3lgKu5f9jb4Bsf8xyQFJVif5Qtd2a5Jf65Y9M8nXk1wPPKNvGxcneXeS64A/qartVXUDvavY+uv6BXrj/L4PoKp+XFW7Zvk7Pp/kz9MbIPw1A5bZ/d4bu9qe17U/Psn13d9wc5Jjkryb3lVfn0ryX/bqw9XE8whYrZ0K/L+q+g/wr8F3K3ASvUtT+weyeR1wVlV9sRut6p+B9cCnq+otSR4O/Fx33f5/B/498F3gWuAf+7ZzBL1LzB+cpa5HAzuAv05yHPAV4DVV9YNZ1nlkVa0d8vceRW8gl6OBa5P8Mr1L4d9ZVZd2R9oPr6pXd2MjPKuq7huyTS1SHgGrtVuA30zy1u7o9dHAN6vqzupdJ/83fct+EXhHknOAA7uv+TcAZybZADyheoNoPwX4fFXtqN7A2HuORnb5kPCF3sHJ8cAFVfUk4Af07h4xm4GjnvW5rKqmqzc+8F3A44AvAW9K8gZ6Q6H+0xy2oyXAAFZTVfV1ekF3C/A/6I3TPGjZ84FXAT8LfDHJ46p3N4pfpzfs38VJXj6Ht53tKHa3LcCWqto9GPdHujr3dbt7Dr5SVfUBen/3PwFXJjlpDtvREmAAq6kkhwE/rKq/Ad4GPB04KsnR3SIv7Vv26Kq6pareSu/I93FJfoneHSfeA7yXXkheB/xGkl/sxnZ98Xzrqqop4FtJHts1nQzcvnd/5U95cZKHdX/fY4CvJXkMcFdV/QW9oSH/3QK8jxYB+4DV2hOAtyWZpvdD2H8GDgE+meSHwP+hd3NEgHOTPAuYpjcU4afojbn6+iT/AnwfeHlVbeu6JL4E7KI3pOCMkqwCNgI/D0ynd5fsY6t3F4Q/AHb3y94FnLkAf+//Ba7v3u/VVfXPSdYBL+v+hingjxfgfbQIOBylNCZJLqZ3x+2PtK5Fk8EuCElqxC4IaR6S/BV95xV33llVf923zJt5aL/z5VX1ihGXp0XGLghJasQuCElqxACWpEYMYElqxACWpEb+P3JahLyceW2VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySCjr2-g6H34"
      },
      "source": [
        "def data_split(df_all, column='sdssdr16_r_psf', label='Label'):\n",
        "  df = df_all.sort_values(column)\n",
        "  def split(df_loc, c=9000):\n",
        "    data = df_loc.drop(['LabelQ', 'LabelG', 'LabelS', 'Label'], axis = 1).values\n",
        "    data1 = data[::2]\n",
        "    data2 = data[1::2]\n",
        "    print(data, data1, data2)\n",
        "    np.random.shuffle(data1)\n",
        "    np.random.shuffle(data2)\n",
        "    return data1[:c//2], data2[:c//2]\n",
        "\n",
        "  def train_test(X1, X2, X3, test_size=0.1):\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(X1, 1*np.ones([len(X1), 1]), test_size=test_size, random_state = 43)\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, 2*np.ones([len(X2), 1]), test_size=test_size, random_state = 43)\n",
        "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3, 3*np.ones([len(X3), 1]), test_size=test_size, random_state = 43)\n",
        "\n",
        "    X_train, X_test = np.concatenate((X1_train, X2_train, X3_train)), np.concatenate((X1_test, X2_test, X3_test))\n",
        "    y_train, y_test = np.concatenate((y1_train, y2_train, y3_train)), np.concatenate((y1_test, y2_test, y3_test))\n",
        "        \n",
        "    train = np.concatenate((X_train, y_train.rashape((len(X_train), 1))))\n",
        "    test = np.concatenate((X_test, y_test.rashape((len(X_test), 1))))\n",
        "\n",
        "    return train, test\n",
        "    \n",
        "  df_s = df[df[label]==0]\n",
        "  df_q = df[df[label]==1]\n",
        "  df_g = df[df[label]==2]\n",
        "\n",
        "  c = np.min((len(df_s), len(df_q), len(df_g)))\n",
        "\n",
        "\n",
        "  X1_s , X2_s = split(df_s)\n",
        "  X1_q , X2_q = split(df_q)\n",
        "  X1_g , X2_g = split(df_g)\n",
        "\n",
        "\n",
        "  X1 = np.concatenate((np.concatenate((X1_s, 1*np.ones([len(X1_s), 1])), axis=1),\n",
        "                       np.concatenate((X1_q, 2*np.ones([len(X1_q), 1])), axis=1),\n",
        "                       np.concatenate((X1_g, 3*np.ones([len(X1_g), 1])), axis=1)),\n",
        "                      axis=0)\n",
        "  X2 = np.concatenate((np.concatenate((X2_s, 1*np.ones([len(X2_s), 1])), axis=1),\n",
        "                       np.concatenate((X2_q, 2*np.ones([len(X2_q), 1])), axis=1),\n",
        "                       np.concatenate((X2_g, 3*np.ones([len(X2_g), 1])), axis=1)),\n",
        "                      axis=0)\n",
        "  \n",
        "  np.random.shuffle(X1)\n",
        "  np.random.shuffle(X2)\n",
        "\n",
        "  return X1, X2#train1, train2, test1, test2"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQX2MYFQ5Ebl"
      },
      "source": [
        "def scor(y_test, y_pred):\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "lgb_reg_params = {\n",
        "    'min_child_samples':hp.randint('min_child_samples', 50)+1,\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.9),\n",
        "    'num_leaves' :      hp.randint('num_leaves', 100)+10,\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 0.001, 0.99),\n",
        "    'n_estimators':     1000\n",
        "}\n",
        "lgb_fit_params = {\n",
        "    'early_stopping_rounds': 20,\n",
        "    'verbose': False\n",
        "}\n",
        "lgb_para = dict()\n",
        "lgb_para['reg_params'] = lgb_reg_params\n",
        "lgb_para['fit_params'] = lgb_fit_params\n",
        "lgb_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "\n",
        "rf_reg_params = {\n",
        "    'min_samples_leaf': hp.randint('min_samples_leaf', 20)+1,\n",
        "    'min_samples_split':hp.uniform('min_samples_split', 0.001, 0.1),\n",
        "    #'max_features':     hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
        "    #'learning_rate':    hp.uniform('learning_rate', 0.001, 0.1),\n",
        "    'n_estimators':     hp.randint('n_estimators', 800)+100\n",
        "}\n",
        "rf_fit_params = {\n",
        "}\n",
        "rf_para = dict()\n",
        "rf_para['reg_params'] = rf_reg_params\n",
        "rf_para['fit_params'] = rf_fit_params\n",
        "rf_para['score'] = lambda y, pred: -accuracy_score(y, pred)\n",
        "\n",
        "tabnet_reg_params = {\n",
        "    'n_d' :              64,\n",
        "    'n_a' :              64,\n",
        "    'n_steps' :          hp.randint('n_steps', 10-3)+3,\n",
        "    'gamma' :            hp.uniform('gamma', 1.0, 3.0),\n",
        "    'lambda_sparse' :    hp.uniform('lambda_sparse', 0.0, 0.01),\n",
        "    'momentum' :         0.3, \n",
        "    'clip_value' :       2.,\n",
        "    'optimizer_params' : dict(lr=2e-2),\n",
        "    'scheduler_params' : {\"step_size\":50, \"gamma\":0.9},\n",
        "    'scheduler_fn' :     torch.optim.lr_scheduler.StepLR,\n",
        "    'mask_type' :       'entmax'\n",
        "}\n",
        "\n",
        "tabnet_fit_params = {\n",
        "    'max_epochs' : 100, \n",
        "    'patience' : 15,\n",
        "    'batch_size' : 512,\n",
        "    'virtual_batch_size' : 128,\n",
        "    'num_workers' : 0,\n",
        "    'weights' : 1,\n",
        "    'drop_last' : False,\n",
        "    #'from_unsupervised' : unsupervised_model\n",
        "}\n",
        "tabnet_para = dict()\n",
        "tabnet_para['reg_params'] = tabnet_reg_params\n",
        "tabnet_para['fit_params'] = tabnet_fit_params\n",
        "tabnet_para['score'] = lambda y, pred: -accuracy_score(y, pred)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWMNPpeokzaC"
      },
      "source": [
        "class HPOpt(object):\n",
        "\n",
        "    def __init__(self, X, y, cv=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.cv = cv\n",
        "\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
        "        except Exception as e:\n",
        "            print({'status': STATUS_FAIL,\n",
        "                    'exception': str(e)})\n",
        "            return {'status': STATUS_FAIL,\n",
        "                    'exception': str(e)}\n",
        "        return result, trials\n",
        "\n",
        "    def rf_reg(self, para):\n",
        "        reg = RandomForestClassifier(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def lgb_reg(self, para):\n",
        "        reg = lgb.LGBMClassifier(**para['reg_params'])\n",
        "        if self.cv:\n",
        "          return self.train_cv(reg, para)\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def tabnet_reg(self, para):\n",
        "        reg = TabNetClassifier(**para['reg_params'])\n",
        "        return self.train_cv(reg, para)\n",
        "\n",
        "\n",
        "    def train_reg(self, reg, para):\n",
        "        if len(para['fit_params'])>0:\n",
        "            reg.fit(self.X, self.y,\n",
        "                  eval_set=[(self.X, self.y), (self.X, self.t)],\n",
        "                  **para['fit_params'])\n",
        "        else:\n",
        "            reg.fit(self.X, self.y)\n",
        "        pred = reg.predict(self.X)\n",
        "        loss = para['score'](self.y, pred)\n",
        "        return {'loss': loss, 'status': STATUS_OK}\n",
        "\n",
        "    def train_cv(self, reg, para):\n",
        "        kf = KFold(n_splits=2, shuffle=False)\n",
        "        loss = 0 \n",
        "        for train, test in kf.split(self.X):\n",
        "            if len(para['fit_params'])>0:\n",
        "                reg.fit(self.X[train], self.y[train],\n",
        "                      eval_set=[(self.X[train], self.y[train]), (self.X[test], self.y[test])],\n",
        "                      **para['fit_params'])\n",
        "            else:\n",
        "                reg.fit(self.X[train], self.y[train])\n",
        "            pred = reg.predict(self.X[test])\n",
        "            score = para['score'](self.y[test], pred)\n",
        "            loss += score\n",
        "\n",
        "        loss=loss/2\n",
        "        return {'loss': loss, 'status': STATUS_OK}\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abHhCsDBkka"
      },
      "source": [
        "acc = pd.DataFrame(index=['acc'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "lhjPrmfzGQDo",
        "outputId": "08b81700-5d1a-447a-ad41-9a73dffeff59"
      },
      "source": [
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sdss</th>\n",
              "      <th>decals</th>\n",
              "      <th>ps</th>\n",
              "      <th>sdss+decals</th>\n",
              "      <th>sdss+ps</th>\n",
              "      <th>ps+decals</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>acc</th>\n",
              "      <td>0.964092</td>\n",
              "      <td>0.958982</td>\n",
              "      <td>0.958806</td>\n",
              "      <td>0.972359</td>\n",
              "      <td>0.96782</td>\n",
              "      <td>0.968395</td>\n",
              "      <td>0.973226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sdss    decals        ps  sdss+decals  sdss+ps  ps+decals       all\n",
              "acc  0.964092  0.958982  0.958806     0.972359  0.96782   0.968395  0.973226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7J8yw478tg5"
      },
      "source": [
        "# Обучаем только на SDSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjPpqwaEB8NR"
      },
      "source": [
        "overview = 'sdss'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25GmHQB-heVc",
        "outputId": "5b8d1aa1-aa20-4362-cd91-837d777e1402"
      },
      "source": [
        "len(sdss)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T40qa4wfCLb9"
      },
      "source": [
        "data1, data2 = data_split(df[sdss].dropna())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpVNxZwxDxav"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvBz-4GXRQ8Q",
        "outputId": "24118dba-ab6b-4103-f796-716afc1b9669"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8480, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkWDnHfHEtjW"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKs6Hmrb9o57"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFYsXOSm5vk2"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_tEPPk05vWk"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f31np6j66Eqb"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TFNfyZe9fNg",
        "outputId": "116a041b-97c7-46f5-c701-fa52a436f3d5"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.3752270436877269,\n",
        "  'min_child_samples': 40+1,\n",
        "  'min_child_weight': 0.7184095532059656,\n",
        "  'num_leaves': 50+10,\n",
        "  'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.4844889640808105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDoquWIU9ivN",
        "outputId": "c13f9213-034a-418a-d127-73392532c610"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.96      0.97      1730\n",
            "           2       0.96      0.97      0.97      4500\n",
            "           3       0.95      0.94      0.95      2250\n",
            "\n",
            "    accuracy                           0.96      8480\n",
            "   macro avg       0.96      0.96      0.96      8480\n",
            "weighted avg       0.96      0.96      0.96      8480\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1669   45   16]\n",
            " [  33 4373   94]\n",
            " [   3  125 2122]]\n",
            "Training Score:  0.9964622641509434\n",
            "Training Score:  0.9627358490566038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC9QgzBeAzg9"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0p3SLB4AzhG"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-wvS7U5AzhG"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy1CuocDAzhH"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMIOVAnZAzhH"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnEA4i5zAzhH",
        "outputId": "8a043676-6d50-48ef-cfb1-61b927c9428a"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.7394978740595625,\n",
        "      'min_child_samples': 16+1,\n",
        "      'min_child_weight': 0.1265642873892295,\n",
        "      'num_leaves': 94+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.820367097854614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5TCFF-dAzhH",
        "outputId": "503da0de-9673-4491-cba1-ec89e757f79c"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.96      0.98      1730\n",
            "           2       0.97      0.97      0.97      4500\n",
            "           3       0.94      0.95      0.95      2250\n",
            "\n",
            "    accuracy                           0.97      8480\n",
            "   macro avg       0.97      0.96      0.96      8480\n",
            "weighted avg       0.97      0.97      0.97      8480\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1668   47   15]\n",
            " [  16 4372  112]\n",
            " [   7   96 2147]]\n",
            "Training Score:  0.9974056603773584\n",
            "Training Score:  0.9654481132075472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4wXAxAlCiAv"
      },
      "source": [
        "# Обучаем только на decals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z02t2GboCiFS"
      },
      "source": [
        "overview = 'decals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF3aQP1-hkvb",
        "outputId": "249a0f83-a02d-433d-f15d-9bb4396badb7"
      },
      "source": [
        "len(decals)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxJdNOz-CiFl",
        "outputId": "ed21734b-6dd7-46bd-8b78-820bd3fff647"
      },
      "source": [
        "data1, data2 = data_split(df[decals].dropna(), 'decals8tr_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.26217823   9.41464717   9.15246895 ...  -4.84004766 -13.43904955\n",
            "   -8.5990019 ]\n",
            " [  0.17745963   9.34961965   9.17216002 ...   1.60803036  -6.41288316\n",
            "   -8.02091352]\n",
            " [  0.153528     9.12074033   8.96721234 ...   1.8224283   -4.22339327\n",
            "   -6.04582157]\n",
            " ...\n",
            " [ -0.60255292  10.47927689  11.0818298  ... -11.54840686   4.08179483\n",
            "   15.63020168]\n",
            " [ -0.65411008  11.63292086  12.28703094 ...  -0.64045917  14.61866631\n",
            "   15.25912548]\n",
            " [ -0.6000965   10.58879719  11.18889369 ... -22.0848624   -2.86427814\n",
            "   19.22058426]] [[  0.26217823   9.41464717   9.15246895 ...  -4.84004766 -13.43904955\n",
            "   -8.5990019 ]\n",
            " [  0.153528     9.12074033   8.96721234 ...   1.8224283   -4.22339327\n",
            "   -6.04582157]\n",
            " [ -0.49179392  10.05505866  10.54685258 ...  22.54012283  15.21954755\n",
            "   -7.32057528]\n",
            " ...\n",
            " [ -0.54181862  10.4282365   10.97005512 ...   0.37824479  15.004679\n",
            "   14.62643421]\n",
            " [ -0.60255292  10.47927689  11.0818298  ... -11.54840686   4.08179483\n",
            "   15.63020168]\n",
            " [ -0.6000965   10.58879719  11.18889369 ... -22.0848624   -2.86427814\n",
            "   19.22058426]] [[  0.17745963   9.34961965   9.17216002 ...   1.60803036  -6.41288316\n",
            "   -8.02091352]\n",
            " [ -0.48936313  10.21879087  10.708154   ...   8.64670513   0.63334649\n",
            "   -8.01335864]\n",
            " [ -0.48728143  10.41792793  10.90520936 ...  -0.05944887  -3.03712994\n",
            "   -2.97768107]\n",
            " ...\n",
            " [ -0.64977873  12.56707953  13.21685826 ... -13.78796381  -1.1176173\n",
            "   12.67034651]\n",
            " [ -0.5481915   10.44872295  10.99691446 ... -21.81575406  -5.31582712\n",
            "   16.49992693]\n",
            " [ -0.65411008  11.63292086  12.28703094 ...  -0.64045917  14.61866631\n",
            "   15.25912548]]\n",
            "[[ 3.83671753e-01  1.27148382e+01  1.23311664e+01 ... -2.18267906e-04\n",
            "   8.11211420e-02  8.13394099e-02]\n",
            " [ 2.66451273e-01  1.36290556e+01  1.33626043e+01 ...  3.59097435e-01\n",
            "   4.80847849e-01  1.21750414e-01]\n",
            " [ 4.40271352e-01  1.24958101e+01  1.20555387e+01 ...  7.00845084e-01\n",
            "   1.16353207e+00  4.62686990e-01]\n",
            " ...\n",
            " [-1.38736189e+00  2.07338343e+01  2.21211962e+01 ...  1.68025100e+00\n",
            "   4.75940878e+00  3.07915778e+00]\n",
            " [ 2.31479147e+00  2.87850199e+01  2.64702284e+01 ... -1.61428436e+00\n",
            "  -4.50475185e+00 -2.89046749e+00]\n",
            " [ 5.00107051e-02  1.87776735e+01  1.87276628e+01 ...  8.59441464e-02\n",
            "   5.40704583e+00  5.32110168e+00]] [[ 3.83671753e-01  1.27148382e+01  1.23311664e+01 ... -2.18267906e-04\n",
            "   8.11211420e-02  8.13394099e-02]\n",
            " [ 4.40271352e-01  1.24958101e+01  1.20555387e+01 ...  7.00845084e-01\n",
            "   1.16353207e+00  4.62686990e-01]\n",
            " [ 3.48759633e-01  1.30589976e+01  1.27102379e+01 ...  1.60182220e+00\n",
            "   1.39803593e+00 -2.03786267e-01]\n",
            " ...\n",
            " [ 1.63064163e-01  2.04668906e+01  2.03038264e+01 ...  1.35054595e+00\n",
            "   3.73802793e+00  2.38748198e+00]\n",
            " [ 4.18938569e-01  2.05867896e+01  2.01678511e+01 ... -1.93839114e-01\n",
            "   1.31762230e+00  1.51146142e+00]\n",
            " [ 2.31479147e+00  2.87850199e+01  2.64702284e+01 ... -1.61428436e+00\n",
            "  -4.50475185e+00 -2.89046749e+00]] [[ 0.26645127 13.6290556  13.36260433 ...  0.35909743  0.48084785\n",
            "   0.12175041]\n",
            " [ 0.47374284 13.07122399 12.59748115 ...  0.30809054  0.63477041\n",
            "   0.32667987]\n",
            " [ 0.40570453 13.9853951  13.57969057 ...  0.08573102  0.05134437\n",
            "  -0.03438665]\n",
            " ...\n",
            " [ 1.98238135 23.4487382  21.46635685 ...  1.42188666  3.89548805\n",
            "   2.47360139]\n",
            " [-1.38736189 20.73383434 22.12119623 ...  1.680251    4.75940878\n",
            "   3.07915778]\n",
            " [ 0.05001071 18.77767355 18.72766284 ...  0.08594415  5.40704583\n",
            "   5.32110168]]\n",
            "[[-0.52450357 10.56632752 11.09083109 ...  0.75484114  1.34622697\n",
            "   0.59138584]\n",
            " [-0.69694899 10.98793795 11.68488694 ...  0.73620844  1.30571224\n",
            "   0.5695038 ]\n",
            " [-0.60748514 11.03833226 11.6458174  ...  0.65771223  1.15435361\n",
            "   0.49664139]\n",
            " ...\n",
            " [ 0.53225472 20.83994854 20.30769382 ...  0.2546214   1.29415748\n",
            "   1.03953608]\n",
            " [-0.19252608 18.55766236 18.75018844 ...  1.55509132  3.48168692\n",
            "   1.9265956 ]\n",
            " [ 0.20435914 19.29498495 19.09062581 ...  0.62765378  1.68730874\n",
            "   1.05965496]] [[-0.52450357 10.56632752 11.09083109 ...  0.75484114  1.34622697\n",
            "   0.59138584]\n",
            " [-0.60748514 11.03833226 11.6458174  ...  0.65771223  1.15435361\n",
            "   0.49664139]\n",
            " [-0.39381953 10.96935839 11.36317792 ...  0.66751937  1.09786972\n",
            "   0.43035036]\n",
            " ...\n",
            " [-0.34294393 19.02557979 19.36852372 ...  1.37601305  2.95471123\n",
            "   1.57869818]\n",
            " [-0.84496363 20.20827426 21.05323789 ...  1.55903793  3.9503291\n",
            "   2.39129117]\n",
            " [-0.19252608 18.55766236 18.75018844 ...  1.55509132  3.48168692\n",
            "   1.9265956 ]] [[-0.69694899 10.98793795 11.68488694 ...  0.73620844  1.30571224\n",
            "   0.5695038 ]\n",
            " [ 0.36800392 10.16918882  9.80118491 ...  0.80815347  1.49934175\n",
            "   0.69118829]\n",
            " [-0.53449807 10.08313851 10.61763658 ...  0.95554065  1.75672789\n",
            "   0.80118723]\n",
            " ...\n",
            " [ 0.21844719 19.37308364 19.15463644 ...  1.14264712  2.76280454\n",
            "   1.62015742]\n",
            " [ 0.53225472 20.83994854 20.30769382 ...  0.2546214   1.29415748\n",
            "   1.03953608]\n",
            " [ 0.20435914 19.29498495 19.09062581 ...  0.62765378  1.68730874\n",
            "   1.05965496]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--ZCaEbCiFt"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG_mktnLCiFz",
        "outputId": "7e1e614b-7e31-4b33-8566-9c58eef41a63"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8448, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Ssnl0DCiGO"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxRr9B8jCiGV"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFhAg6eWCiGc"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ-t9nOZCiGi"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGk8xhGkCiGp"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpp7pWgVCiGx",
        "outputId": "6ba97530-5fec-43a3-b8f7-fdaaf1923729"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.8104497809038045,\n",
        "  'min_child_samples': 5+1,\n",
        "  'min_child_weight': 0.9758726061228604,\n",
        "  'num_leaves': 65+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.200746059417725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgMb0VLxCiG5",
        "outputId": "4917351a-2d98-4eb3-945b-01f753b92eb8"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.96      0.97      1706\n",
            "           2       0.97      0.97      0.97      4500\n",
            "           3       0.93      0.94      0.94      2241\n",
            "\n",
            "    accuracy                           0.96      8447\n",
            "   macro avg       0.96      0.96      0.96      8447\n",
            "weighted avg       0.96      0.96      0.96      8447\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1640   30   36]\n",
            " [  14 4369  117]\n",
            " [  29  103 2109]]\n",
            "Training Score:  0.9951467803030303\n",
            "Training Score:  0.9610512608026518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI6b2MgrCiHA"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlkGsWSQCiHE"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgISz8dWCiHK"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioy7pd2zCiHO"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGMNS4QCiHU"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPdIrk8WCiHb",
        "outputId": "eabd8c09-3fee-430d-edf4-a06a946a55d2"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.4068541706198642,\n",
        "  'min_child_samples': 11+1,\n",
        "  'min_child_weight': 0.06272672350564332,\n",
        "  'num_leaves': 1+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.792611837387085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn6wHbbKCiHi",
        "outputId": "3bfa8710-2665-4ad5-f60b-86fe44e6b6ab"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.95      0.96      1707\n",
            "           2       0.97      0.97      0.97      4500\n",
            "           3       0.93      0.94      0.93      2241\n",
            "\n",
            "    accuracy                           0.96      8448\n",
            "   macro avg       0.95      0.95      0.95      8448\n",
            "weighted avg       0.96      0.96      0.96      8448\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1624   34   49]\n",
            " [  20 4362  118]\n",
            " [  32  111 2098]]\n",
            "Training Score:  0.9955013614300935\n",
            "Training Score:  0.9569128787878788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P6ZXyDfC7gB"
      },
      "source": [
        "# Обучаем только на PanStars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacNIWKhC7gE"
      },
      "source": [
        "overview = 'ps'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iczSvwRfhp8Y",
        "outputId": "7cb9f0eb-5525-440c-f2ae-b895fe172e04"
      },
      "source": [
        "len(ps)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U5zuxt3C7gE",
        "outputId": "adecc6e6-cfa0-4c3e-c0a3-d634e77f6ce1"
      },
      "source": [
        "data1, data2 = data_split(df[ps].dropna(), 'psdr2_r_psf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12.17900645 11.16607677 12.347154   ...  0.         -0.17530019\n",
            "   0.44061258]\n",
            " [12.92595218 10.481565   12.77834023 ... -1.78081752  1.80119602\n",
            "  -0.79847213]\n",
            " [11.03919022 10.88226782 11.23045471 ...  0.         -0.59540646\n",
            "   0.49064453]\n",
            " ...\n",
            " [13.56105427 11.63453763 11.92288652 ... 13.21145735  0.68786227\n",
            "   0.15804971]\n",
            " [14.84441555 11.92918359 12.86315534 ...  0.          1.40868434\n",
            "   0.0656633 ]\n",
            " [12.67790691 12.50346556 13.57709838 ...  0.48292688  0.31582203\n",
            "   0.08938451]] [[12.17900645 11.16607677 12.347154   ...  0.         -0.17530019\n",
            "   0.44061258]\n",
            " [11.03919022 10.88226782 11.23045471 ...  0.         -0.59540646\n",
            "   0.49064453]\n",
            " [11.23227248 10.79290003 12.16429912 ...  0.          1.24353279\n",
            "   0.21495824]\n",
            " ...\n",
            " [12.85087038 12.04507608 26.85110731 ... 13.79971166  0.0489412\n",
            "   0.20595001]\n",
            " [13.06877627 11.89068047 13.03883522 ... -0.89973457 -1.18879058\n",
            "   0.84074796]\n",
            " [14.84441555 11.92918359 12.86315534 ...  0.          1.40868434\n",
            "   0.0656633 ]] [[12.92595218 10.481565   12.77834023 ... -1.78081752  1.80119602\n",
            "  -0.79847213]\n",
            " [12.71560386 10.06096486 10.6607943  ... -1.59644941  0.91483124\n",
            "   0.43368565]\n",
            " [12.69883676 10.99734143 14.28317194 ...  0.         -0.89704774\n",
            "   0.70632839]\n",
            " ...\n",
            " [12.49836917 11.96591485 13.25412882 ...  0.         -0.27960442\n",
            "   0.18037713]\n",
            " [13.56105427 11.63453763 11.92288652 ... 13.21145735  0.68786227\n",
            "   0.15804971]\n",
            " [12.67790691 12.50346556 13.57709838 ...  0.48292688  0.31582203\n",
            "   0.08938451]]\n",
            "[[ 1.52972874e+01  1.51942274e+01  1.55184930e+01 ... -8.63652134e-02\n",
            "  -3.87464250e-01  1.29927577e-03]\n",
            " [ 1.54051347e+01  1.53316282e+01  1.54404479e+01 ... -5.11614963e-02\n",
            "   2.64917465e-01 -5.38679959e-02]\n",
            " [ 1.51640849e+01  1.51521476e+01  1.53592533e+01 ...  1.16058776e-02\n",
            "   1.37452696e-01  1.59772780e-01]\n",
            " ...\n",
            " [ 2.29491610e+01  2.12689099e+01  2.46645262e+01 ...  0.00000000e+00\n",
            "   4.84904291e-01  0.00000000e+00]\n",
            " [ 2.28047718e+01  1.93387102e+01  2.57081468e+01 ... -6.82717816e-02\n",
            "   1.49416018e+00 -2.28104713e-01]\n",
            " [ 2.25974418e+01  2.10115341e+01  2.67166851e+01 ...  0.00000000e+00\n",
            "   7.86152417e-01 -1.24427349e-01]] [[ 1.52972874e+01  1.51942274e+01  1.55184930e+01 ... -8.63652134e-02\n",
            "  -3.87464250e-01  1.29927577e-03]\n",
            " [ 1.51640849e+01  1.51521476e+01  1.53592533e+01 ...  1.16058776e-02\n",
            "   1.37452696e-01  1.59772780e-01]\n",
            " [ 1.52225343e+01  1.54086155e+01  1.56276194e+01 ...  1.37861318e-02\n",
            "   2.20873318e-01 -7.47339731e-03]\n",
            " ...\n",
            " [ 2.40720599e+01  2.25210616e+01  2.33348676e+01 ...  0.00000000e+00\n",
            "  -4.71910134e-02  0.00000000e+00]\n",
            " [ 2.29491610e+01  2.12689099e+01  2.46645262e+01 ...  0.00000000e+00\n",
            "   4.84904291e-01  0.00000000e+00]\n",
            " [ 2.25974418e+01  2.10115341e+01  2.67166851e+01 ...  0.00000000e+00\n",
            "   7.86152417e-01 -1.24427349e-01]] [[ 1.54051347e+01  1.53316282e+01  1.54404479e+01 ... -5.11614963e-02\n",
            "   2.64917465e-01 -5.38679959e-02]\n",
            " [ 1.55864316e+01  1.52545494e+01  1.55039393e+01 ... -5.14805393e-02\n",
            "   4.81919772e-01 -3.86590699e-02]\n",
            " [ 1.57976839e+01  1.57108621e+01  1.56357634e+01 ... -1.46228086e-02\n",
            "  -3.88614446e-01  6.42014021e-02]\n",
            " ...\n",
            " [ 2.23549824e+01  2.19589506e+01  2.31559202e+01 ...  1.30548214e-01\n",
            "   3.86168281e-01  0.00000000e+00]\n",
            " [ 2.32533625e+01  2.19883644e+01  2.50574297e+01 ...  0.00000000e+00\n",
            "   1.00170449e-01  0.00000000e+00]\n",
            " [ 2.28047718e+01  1.93387102e+01  2.57081468e+01 ... -6.82717816e-02\n",
            "   1.49416018e+00 -2.28104713e-01]]\n",
            "[[11.86256353 12.76279674 15.39720747 ...  2.94390737  0.20910886\n",
            "   1.59719498]\n",
            " [11.52273413 11.11973033 15.81049862 ...  3.15808604  0.29302445\n",
            "   3.12999183]\n",
            " [14.56277425 14.32156895 15.52635224 ...  0.11774631  0.09779623\n",
            "   0.11512981]\n",
            " ...\n",
            " [21.06504815 19.6356839  23.07385315 ...  0.56940027  0.53735083\n",
            "   0.95554706]\n",
            " [21.40085391 19.91362069 23.88447606 ...  0.43420557  0.42207884\n",
            "   0.62909862]\n",
            " [21.15979997 19.84731698 24.59178115 ...  0.09749734  0.61998827\n",
            "   0.31121761]] [[11.86256353 12.76279674 15.39720747 ...  2.94390737  0.20910886\n",
            "   1.59719498]\n",
            " [14.56277425 14.32156895 15.52635224 ...  0.11774631  0.09779623\n",
            "   0.11512981]\n",
            " [12.6692976  12.67582803 15.62044817 ...  2.13874498 -0.34043628\n",
            "   2.35063388]\n",
            " ...\n",
            " [21.1986744  20.73532542 23.70519699 ...  0.43607636  0.07629236\n",
            "   0.17563525]\n",
            " [21.04508421 19.99333914 24.35362065 ...  0.43801896  0.2827584\n",
            "   0.537085  ]\n",
            " [21.40085391 19.91362069 23.88447606 ...  0.43420557  0.42207884\n",
            "   0.62909862]] [[11.52273413 11.11973033 15.81049862 ...  3.15808604  0.29302445\n",
            "   3.12999183]\n",
            " [12.69688132 12.20462185 15.77623574 ...  1.99069286  0.21179506\n",
            "   2.11738912]\n",
            " [13.50852617 13.05846603 15.6480839  ...  1.6044123   0.31913439\n",
            "   1.52121201]\n",
            " ...\n",
            " [20.94531371 20.45855907 23.81975336 ...  0.64093458  0.39300954\n",
            "   0.34213358]\n",
            " [21.06504815 19.6356839  23.07385315 ...  0.56940027  0.53735083\n",
            "   0.95554706]\n",
            " [21.15979997 19.84731698 24.59178115 ...  0.09749734  0.61998827\n",
            "   0.31121761]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5DgQWyC7gF"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSTSLAs9C7gF",
        "outputId": "acd9ec66-9cad-47fe-c2a6-f58735c73e22"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7101, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLTYK6-bC7gF"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTABhaXqC7gF"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeL1-1KcC7gF"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y07wND9GC7gG"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IslsQv4C7gG"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE5kpoLEC7gG",
        "outputId": "a34ccda0-d55c-4b9a-fb37-1e589bb27db0"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.6642887258022818,\n",
        "  'min_child_samples': 4+1,\n",
        "  'min_child_weight': 0.775262820204908,\n",
        "  'num_leaves': 6+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.849103927612305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPSpGS3uC7gH",
        "outputId": "f5b06029-f7e1-4838-a06e-3bda39c1ef78"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.94      0.96      1167\n",
            "           2       0.96      0.97      0.96      4027\n",
            "           3       0.94      0.94      0.94      1906\n",
            "\n",
            "    accuracy                           0.96      7100\n",
            "   macro avg       0.96      0.95      0.95      7100\n",
            "weighted avg       0.96      0.96      0.96      7100\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1101   55   11]\n",
            " [  27 3896  104]\n",
            " [   5  101 1800]]\n",
            "Training Score:  0.9957752429235319\n",
            "Training Score:  0.9573239436619718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tYYtFMuC7gH"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aQ9WPS1C7gH"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD-MdfpNC7gH"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIWMEYwkC7gH"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODZF_VSLC7gH"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7mwDs_6C7gH",
        "outputId": "f2561516-5c7b-4777-dae0-28e9523b059c"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.44232732453787127,\n",
        "  'min_child_samples': 39+1,\n",
        "  'min_child_weight': 0.15636662933973536,\n",
        "  'num_leaves': 25+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.247951984405518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q28CAHqC7gH",
        "outputId": "5e175068-91d4-4719-e0e1-41299966eeba"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.94      0.96      1167\n",
            "           2       0.97      0.97      0.97      4028\n",
            "           3       0.94      0.95      0.95      1906\n",
            "\n",
            "    accuracy                           0.96      7101\n",
            "   macro avg       0.96      0.95      0.96      7101\n",
            "weighted avg       0.96      0.96      0.96      7101\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1100   52   15]\n",
            " [  31 3902   95]\n",
            " [   5   84 1817]]\n",
            "Training Score:  0.995774647887324\n",
            "Training Score:  0.9602872834811998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzmVeJh4DFow"
      },
      "source": [
        "# Обучаем на SDSS and decals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGMS43_DFow"
      },
      "source": [
        "overview = 'sdss+decals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FTPSQR8GiZf"
      },
      "source": [
        "sdss_decals = [i for i in features if ('sdss' in i or 'decals' in i) and 'psdr' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lYmGo53hvDs",
        "outputId": "bfdd9b81-71cb-432e-abd8-2e494622c0ed"
      },
      "source": [
        "len(sdss_decals)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbR-BrSRDFow",
        "outputId": "8b3012e3-2fe5-45dd-8562-94ff2828819c"
      },
      "source": [
        "data1, data2 = data_split(df[sdss_decals].dropna())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 8.32313612  7.06674872  7.14253691 ...  3.29560597  2.38805577\n",
            "  -0.15290435]\n",
            " [14.34546057 13.89415478  7.29767599 ...  0.62771844  1.51317155\n",
            "  -2.76776875]\n",
            " [ 8.8665732  11.04006502  7.34243742 ...  3.23131392  1.0449481\n",
            "  -0.78204208]\n",
            " ...\n",
            " [23.11332269 24.02218114 23.64032341 ...  0.64568178  1.42733941\n",
            "   0.09861513]\n",
            " [24.36290307 15.82685278 23.92909239 ... -1.90784587 -1.30835829\n",
            "  -1.39195694]\n",
            " [24.18340755 25.12869598 24.85982292 ...  2.83996985  0.82611757\n",
            "  -0.23275616]] [[ 8.32313612  7.06674872  7.14253691 ...  3.29560597  2.38805577\n",
            "  -0.15290435]\n",
            " [ 8.8665732  11.04006502  7.34243742 ...  3.23131392  1.0449481\n",
            "  -0.78204208]\n",
            " [ 8.85890032  8.16954477  7.63161607 ...  1.45292819  0.50861688\n",
            "  -0.92139865]\n",
            " ...\n",
            " [23.10549057 23.87371022 22.91696185 ...  3.21728682 -0.49271541\n",
            "   0.18148377]\n",
            " [23.11332269 24.02218114 23.64032341 ...  0.64568178  1.42733941\n",
            "   0.09861513]\n",
            " [24.18340755 25.12869598 24.85982292 ...  2.83996985  0.82611757\n",
            "  -0.23275616]] [[14.34546057 13.89415478  7.29767599 ...  0.62771844  1.51317155\n",
            "  -2.76776875]\n",
            " [10.79950946  8.30401138  7.4645746  ... -0.63612839 -0.43505784\n",
            "  -1.41535773]\n",
            " [10.33929271  8.38008284  7.64578625 ...  2.38075869  0.23130931\n",
            "  -1.59269683]\n",
            " ...\n",
            " [23.44140945 23.96219269 22.86958722 ... 11.17806572 11.78787589\n",
            "   5.42307188]\n",
            " [23.58409707 24.98982939 23.53254382 ...  1.30199583  3.21635345\n",
            "   0.08307366]\n",
            " [24.36290307 15.82685278 23.92909239 ... -1.90784587 -1.30835829\n",
            "  -1.39195694]]\n",
            "[[14.24714651 14.38851327 14.45906141 ... -0.32117306 -0.21492619\n",
            "  -0.2459053 ]\n",
            " [15.15536432 15.06108459 15.04715259 ... -1.78587989 -0.25460064\n",
            "  -0.90463925]\n",
            " [15.66798544 15.44707046 15.23454548 ... -0.22202483 -0.13692316\n",
            "  -0.34673668]\n",
            " ...\n",
            " [23.63837953 25.19444462 24.08667653 ... -0.40013377  0.05371384\n",
            "   0.15200719]\n",
            " [23.94281424 25.02906542 24.42240893 ...  0.19850883 -1.85172845\n",
            "   0.41514116]\n",
            " [21.12293314 21.34357527 24.54789956 ...  0.60856793  3.36935966\n",
            "  -0.82894079]] [[14.24714651 14.38851327 14.45906141 ... -0.32117306 -0.21492619\n",
            "  -0.2459053 ]\n",
            " [15.66798544 15.44707046 15.23454548 ... -0.22202483 -0.13692316\n",
            "  -0.34673668]\n",
            " [15.95232941 15.5569868  15.40525714 ...  0.21788923  0.332229\n",
            "   0.23542821]\n",
            " ...\n",
            " [24.56061282 24.71605445 23.95746879 ... -1.29324234 -1.86968578\n",
            "   0.16158338]\n",
            " [23.63837953 25.19444462 24.08667653 ... -0.40013377  0.05371384\n",
            "   0.15200719]\n",
            " [21.12293314 21.34357527 24.54789956 ...  0.60856793  3.36935966\n",
            "  -0.82894079]] [[15.15536432 15.06108459 15.04715259 ... -1.78587989 -0.25460064\n",
            "  -0.90463925]\n",
            " [16.17368929 15.65380194 15.31668988 ...  0.19578116  0.17592854\n",
            "   0.12100394]\n",
            " [16.17768933 15.70705016 15.41347949 ... -2.67611393 -2.69723756\n",
            "  -2.63263854]\n",
            " ...\n",
            " [24.55375891 24.80933316 23.80697299 ... -0.50274787 -0.08241791\n",
            "   0.40262851]\n",
            " [23.43079627 24.55534032 24.00315191 ... -2.95288786  0.03721881\n",
            "   0.15909636]\n",
            " [23.94281424 25.02906542 24.42240893 ...  0.19850883 -1.85172845\n",
            "   0.41514116]]\n",
            "[[16.10025066 15.495036   14.92068378 ... -2.38960418  0.14601106\n",
            "  -3.48808031]\n",
            " [17.34812672 15.68192223 14.9676701  ...  0.24046805  0.23092393\n",
            "   0.10514708]\n",
            " [17.69518216 15.86661033 15.01468035 ... -0.02871844  0.05501345\n",
            "   0.02779263]\n",
            " ...\n",
            " [23.14813991 23.82309712 23.18545634 ...  2.69329511 -0.30356771\n",
            "  -0.21053733]\n",
            " [23.57875417 24.04322178 23.25238554 ... -1.63804921 -0.59395077\n",
            "  -0.75384057]\n",
            " [23.75388413 25.37837256 23.27950818 ...  1.7759648  -0.51827757\n",
            "   0.06120135]] [[ 1.61002507e+01  1.54950360e+01  1.49206838e+01 ... -2.38960418e+00\n",
            "   1.46011056e-01 -3.48808031e+00]\n",
            " [ 1.76951822e+01  1.58666103e+01  1.50146803e+01 ... -2.87184394e-02\n",
            "   5.50134457e-02  2.77926339e-02]\n",
            " [ 1.77163441e+01  1.58891988e+01  1.51397370e+01 ...  2.97351400e-02\n",
            "  -1.54457477e-02 -5.36075089e-02]\n",
            " ...\n",
            " [ 2.24392656e+01  2.20959988e+01  2.29784703e+01 ...  6.84478908e-01\n",
            "  -1.46338635e-01 -9.61606815e-01]\n",
            " [ 2.37487829e+01  2.30916837e+01  2.31405331e+01 ...  2.67466359e-01\n",
            "   3.17919888e-01 -3.17025312e-01]\n",
            " [ 2.35787542e+01  2.40432218e+01  2.32523855e+01 ... -1.63804921e+00\n",
            "  -5.93950766e-01 -7.53840574e-01]] [[ 1.73481267e+01  1.56819222e+01  1.49676701e+01 ...  2.40468048e-01\n",
            "   2.30923931e-01  1.05147085e-01]\n",
            " [ 1.80711456e+01  1.59260552e+01  1.50581701e+01 ...  5.81297848e-01\n",
            "   5.10554001e-01  5.13075431e-01]\n",
            " [ 1.74571994e+01  1.59196433e+01  1.51645674e+01 ...  5.41426434e-04\n",
            "  -2.22211930e-03 -1.57404525e-01]\n",
            " ...\n",
            " [ 2.48382309e+01  2.46432646e+01  2.30695978e+01 ...  2.36541617e-01\n",
            "   9.76192702e-01  5.68705273e-01]\n",
            " [ 2.31481399e+01  2.38230971e+01  2.31854563e+01 ...  2.69329511e+00\n",
            "  -3.03567714e-01 -2.10537334e-01]\n",
            " [ 2.37538841e+01  2.53783726e+01  2.32795082e+01 ...  1.77596480e+00\n",
            "  -5.18277568e-01  6.12013500e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mh-lexnDFow"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOlOx_L5DFow",
        "outputId": "9489fb4a-45d2-47ee-9a25-1f66ef858808"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8448, 39)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4EHkQBXDFow"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgAArx1uDFoy"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxzZ2RB_DFoy"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh44kDHCDFoy"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTXzoluNDFoy"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5DeRQL8DFoy",
        "outputId": "ee641483-d04b-4e24-c3fb-344e710c6c82"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.5048269884336619,\n",
        "  'min_child_samples': 49+1,\n",
        "  'min_child_weight': 0.4809285563025559,\n",
        "  'num_leaves': 26+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.429488658905029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ur_0vj1DFoy",
        "outputId": "9c095863-d742-40ad-9d8d-202bcce11489"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.97      0.98      1706\n",
            "           2       0.98      0.98      0.98      4500\n",
            "           3       0.96      0.96      0.96      2241\n",
            "\n",
            "    accuracy                           0.97      8447\n",
            "   macro avg       0.97      0.97      0.97      8447\n",
            "weighted avg       0.97      0.97      0.97      8447\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1663   27   16]\n",
            " [  17 4401   82]\n",
            " [   5   84 2152]]\n",
            "Training Score:  0.9968039772727273\n",
            "Training Score:  0.9726530129039895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF8qMhqUDFoy"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR8-C3dnDFoy"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6cRkGgVDFoy"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKYjiChVDFoz"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdSmsPODFoz"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iatConAPDFoz",
        "outputId": "c77aefa7-c3f1-479d-cea5-c53ac0371b8a"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.8753852156471218,\n",
        "  'min_child_samples': 32+1,\n",
        "  'min_child_weight': 0.9825613994422406,\n",
        "  'num_leaves': 65+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.455375671386719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrwJUMwgDFoz",
        "outputId": "10a7f4d2-5b8c-48fb-a32d-97efb2d63234"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.98      0.98      1707\n",
            "           2       0.98      0.97      0.98      4500\n",
            "           3       0.95      0.96      0.95      2241\n",
            "\n",
            "    accuracy                           0.97      8448\n",
            "   macro avg       0.97      0.97      0.97      8448\n",
            "weighted avg       0.97      0.97      0.97      8448\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1679   14   14]\n",
            " [  19 4386   95]\n",
            " [   6   88 2147]]\n",
            "Training Score:  0.9976322954895229\n",
            "Training Score:  0.9720643939393939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JleLOYUPDcyl"
      },
      "source": [
        "# Обучаем на SDSS and PanStars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCVIoN_SDcyt"
      },
      "source": [
        "overview = 'sdss+ps'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqkcyLf0Gus4"
      },
      "source": [
        "sdss_ps = [i for i in features if ('sdss' in i or 'psdr' in i) and 'decals' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQNLHVCAh1gx",
        "outputId": "d513d311-bb46-495f-eaf6-63269cfe33eb"
      },
      "source": [
        "len(sdss_ps)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06yOmsqpDcyt",
        "outputId": "b9c89f9f-f986-4997-9499-db0332a53395"
      },
      "source": [
        "data1, data2 = data_split(df[sdss_ps].dropna())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.43454606e+01  1.38941548e+01  7.29767599e+00 ...  8.10714351e-01\n",
            "   2.75247402e-01 -1.98030407e-02]\n",
            " [ 1.13680256e+01  1.01497681e+01  9.68987750e+00 ...  1.27953714e+01\n",
            "   1.57979427e+00 -5.98191351e-01]\n",
            " [ 1.11709129e+01  1.00061718e+01  9.75755261e+00 ... -2.71915287e-01\n",
            "   1.06058999e+01  0.00000000e+00]\n",
            " ...\n",
            " [ 2.35840971e+01  2.49898294e+01  2.35325438e+01 ... -6.08689677e-02\n",
            "   4.50813531e-01 -1.51341389e-02]\n",
            " [ 2.31133227e+01  2.40221811e+01  2.36403234e+01 ... -6.76038425e-02\n",
            "   5.74629863e-01 -7.89346538e-02]\n",
            " [ 2.41834075e+01  2.51286960e+01  2.48598229e+01 ...  5.61210198e-01\n",
            "   2.12632938e-01  4.63558781e-01]] [[ 1.43454606e+01  1.38941548e+01  7.29767599e+00 ...  8.10714351e-01\n",
            "   2.75247402e-01 -1.98030407e-02]\n",
            " [ 1.11709129e+01  1.00061718e+01  9.75755261e+00 ... -2.71915287e-01\n",
            "   1.06058999e+01  0.00000000e+00]\n",
            " [ 1.22559279e+01  1.06389340e+01  1.01326665e+01 ...  0.00000000e+00\n",
            "   4.91487918e+00  0.00000000e+00]\n",
            " ...\n",
            " [ 2.32742668e+01  2.37942077e+01  2.26553168e+01 ... -2.67598295e-01\n",
            "   3.44703114e-01 -5.87556522e-01]\n",
            " [ 2.31054906e+01  2.38737102e+01  2.29169618e+01 ... -4.07882762e-02\n",
            "   1.70315671e-01 -9.91552800e-02]\n",
            " [ 2.31133227e+01  2.40221811e+01  2.36403234e+01 ... -6.76038425e-02\n",
            "   5.74629863e-01 -7.89346538e-02]] [[ 1.13680256e+01  1.01497681e+01  9.68987750e+00 ...  1.27953714e+01\n",
            "   1.57979427e+00 -5.98191351e-01]\n",
            " [ 1.15352307e+01  1.01565918e+01  9.92304101e+00 ... -1.59644941e+00\n",
            "   9.14831236e-01  4.33685647e-01]\n",
            " [ 1.16011723e+01  1.03864360e+01  1.01914261e+01 ...  0.00000000e+00\n",
            "   3.14258933e+00  0.00000000e+00]\n",
            " ...\n",
            " [ 2.32264642e+01  2.37382765e+01  2.26720501e+01 ... -9.28199181e-02\n",
            "   6.96496124e-01 -1.33044080e-01]\n",
            " [ 2.35840971e+01  2.49898294e+01  2.35325438e+01 ... -6.08689677e-02\n",
            "   4.50813531e-01 -1.51341389e-02]\n",
            " [ 2.41834075e+01  2.51286960e+01  2.48598229e+01 ...  5.61210198e-01\n",
            "   2.12632938e-01  4.63558781e-01]]\n",
            "[[ 1.51553643e+01  1.50610846e+01  1.50471526e+01 ...  2.93255998e-01\n",
            "   1.74190353e-02  5.44635757e-01]\n",
            " [ 1.56679854e+01  1.54470705e+01  1.52345455e+01 ... -5.11614963e-02\n",
            "   2.64917465e-01 -5.38679959e-02]\n",
            " [ 1.61736893e+01  1.56538019e+01  1.53166899e+01 ... -8.63652134e-02\n",
            "  -3.87464250e-01  1.29927577e-03]\n",
            " ...\n",
            " [ 2.36383795e+01  2.51944446e+01  2.40866765e+01 ...  0.00000000e+00\n",
            "   2.66146790e-03 -9.05950480e-01]\n",
            " [ 2.39428142e+01  2.50290654e+01  2.44224089e+01 ... -6.82717816e-02\n",
            "   1.49416018e+00 -2.28104713e-01]\n",
            " [ 2.11229331e+01  2.13435753e+01  2.45478996e+01 ...  6.65117455e-01\n",
            "  -1.52748635e-01 -1.65514308e-01]] [[ 1.51553643e+01  1.50610846e+01  1.50471526e+01 ...  2.93255998e-01\n",
            "   1.74190353e-02  5.44635757e-01]\n",
            " [ 1.61736893e+01  1.56538019e+01  1.53166899e+01 ... -8.63652134e-02\n",
            "  -3.87464250e-01  1.29927577e-03]\n",
            " [ 1.55615629e+01  1.56118234e+01  1.55525012e+01 ... -1.20740752e-01\n",
            "  -9.69122806e-02  1.65983962e-02]\n",
            " ...\n",
            " [ 2.45606128e+01  2.47160544e+01  2.39574688e+01 ...  0.00000000e+00\n",
            "   1.93998248e-01  0.00000000e+00]\n",
            " [ 2.36383795e+01  2.51944446e+01  2.40866765e+01 ...  0.00000000e+00\n",
            "   2.66146790e-03 -9.05950480e-01]\n",
            " [ 2.11229331e+01  2.13435753e+01  2.45478996e+01 ...  6.65117455e-01\n",
            "  -1.52748635e-01 -1.65514308e-01]] [[ 1.56679854e+01  1.54470705e+01  1.52345455e+01 ... -5.11614963e-02\n",
            "   2.64917465e-01 -5.38679959e-02]\n",
            " [ 1.54659036e+01  1.53736189e+01  1.54698168e+01 ...  1.16058776e-02\n",
            "   1.37452696e-01  1.59772780e-01]\n",
            " [ 1.55425042e+01  1.56673333e+01  1.56290105e+01 ...  1.37861318e-02\n",
            "   2.20873318e-01 -7.47339731e-03]\n",
            " ...\n",
            " [ 2.45537589e+01  2.48093332e+01  2.38069730e+01 ...  1.71428844e-01\n",
            "   5.33386888e-01  0.00000000e+00]\n",
            " [ 2.34307963e+01  2.45553403e+01  2.40031519e+01 ...  0.00000000e+00\n",
            "   9.56940495e-01 -2.96356717e-03]\n",
            " [ 2.39428142e+01  2.50290654e+01  2.44224089e+01 ... -6.82717816e-02\n",
            "   1.49416018e+00 -2.28104713e-01]]\n",
            "[[16.10025066 15.495036   14.92068378 ...  2.94390737  0.20910886\n",
            "   1.59719498]\n",
            " [17.34812672 15.68192223 14.9676701  ...  0.11774631  0.09779623\n",
            "   0.11512981]\n",
            " [17.69518216 15.86661033 15.01468035 ...  1.99069286  0.21179506\n",
            "   2.11738912]\n",
            " ...\n",
            " [23.74878292 23.09168367 23.14053311 ... -0.05971311  1.269525\n",
            "   0.        ]\n",
            " [23.57875417 24.04322178 23.25238554 ...  0.28646284  0.03580417\n",
            "   0.12857911]\n",
            " [23.75388413 25.37837256 23.27950818 ...  0.43420557  0.42207884\n",
            "   0.62909862]] [[16.10025066 15.495036   14.92068378 ...  2.94390737  0.20910886\n",
            "   1.59719498]\n",
            " [17.69518216 15.86661033 15.01468035 ...  1.99069286  0.21179506\n",
            "   2.11738912]\n",
            " [17.71634412 15.88919876 15.13973705 ...  2.88033428  0.5717428\n",
            "   2.77979062]\n",
            " ...\n",
            " [21.12950343 23.53090039 22.91321577 ...  0.31216922  0.2245103\n",
            "   0.17796975]\n",
            " [24.83823088 24.64326456 23.06959781 ...  0.64093458  0.39300954\n",
            "   0.34213358]\n",
            " [23.57875417 24.04322178 23.25238554 ...  0.28646284  0.03580417\n",
            "   0.12857911]] [[17.34812672 15.68192223 14.9676701  ...  0.11774631  0.09779623\n",
            "   0.11512981]\n",
            " [18.07114556 15.92605521 15.05817014 ...  3.15808604  0.29302445\n",
            "   3.12999183]\n",
            " [17.97231913 16.04078064 15.17006206 ...  1.80040387  0.28309282\n",
            "   2.22011842]\n",
            " ...\n",
            " [23.45466326 24.17550071 22.93107311 ...  0.43607636  0.07629236\n",
            "   0.17563525]\n",
            " [23.74878292 23.09168367 23.14053311 ... -0.05971311  1.269525\n",
            "   0.        ]\n",
            " [23.75388413 25.37837256 23.27950818 ...  0.43420557  0.42207884\n",
            "   0.62909862]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0x4W516Dcyu"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kFNMukTDcyu",
        "outputId": "36b4daf0-87ff-4b74-c141-0124ebe5ea86"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7070, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9h_C2HSDcyu"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoUxd8paDcyu"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjXeu2pyDcyu"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZABXYF7Dcyu"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCiJf_MDcyu"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHnMzpRjDcyv",
        "outputId": "2c370d79-cd81-49ca-921d-a4f903ce1ff0"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.6756414264650209,\n",
        "  'min_child_samples': 16+1,\n",
        "  'min_child_weight': 0.011169721239062961,\n",
        "  'num_leaves': 38+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.22081208229065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQKWOPtVDcyv",
        "outputId": "caa1b3e6-5341-4768-f33c-6cb8cb4b8ccc"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.96      0.97      1167\n",
            "           2       0.98      0.97      0.98      3996\n",
            "           3       0.95      0.96      0.96      1906\n",
            "\n",
            "    accuracy                           0.97      7069\n",
            "   macro avg       0.97      0.97      0.97      7069\n",
            "weighted avg       0.97      0.97      0.97      7069\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1125   33    9]\n",
            " [  23 3893   80]\n",
            " [   4   63 1839]]\n",
            "Training Score:  0.9963224893917964\n",
            "Training Score:  0.97000990239072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4jaTyODcyv"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYNeOzacDcyv"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyOuS1T3Dcyv"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXkFRuCrDcyv"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RedhBRPKDcyv"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN6qQ67lDcyv",
        "outputId": "b90d4876-ee24-4908-8ee3-3ee635f76b41"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.7097663865005799,\n",
        "  'min_child_samples': 40+1,\n",
        "  'min_child_weight': 0.09319093708977999,\n",
        "  'num_leaves': 65+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.605796813964844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcelH_GTDcyv",
        "outputId": "66a1c712-67c3-47b8-cb07-77f52464439a"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.95      0.97      1167\n",
            "           2       0.97      0.97      0.97      3997\n",
            "           3       0.94      0.96      0.95      1906\n",
            "\n",
            "    accuracy                           0.97      7070\n",
            "   macro avg       0.97      0.96      0.96      7070\n",
            "weighted avg       0.97      0.97      0.97      7070\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1109   41   17]\n",
            " [  17 3888   92]\n",
            " [   1   75 1830]]\n",
            "Training Score:  0.9958975809874098\n",
            "Training Score:  0.9656294200848656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEmzErN8D7l2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc-vfPONE1Af"
      },
      "source": [
        "# Обучаем на decals and PanStars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8za6huzE1Gv"
      },
      "source": [
        "overview = 'ps+decals'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix84jAGOHCHp"
      },
      "source": [
        "ps_decals = [i for i in features if ('decals' in i or 'psdr' in i) and 'sdss' not in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obdbzW-ph6sc",
        "outputId": "407b3af0-dbc9-4a50-fac9-ac10044554cf"
      },
      "source": [
        "len(ps_decals)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwbODAFoE1HP"
      },
      "source": [
        "data1, data2 = data_split(df[ps_decals].dropna(), 'psdr2_r_psf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrr4a8ZGE1Hg"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NRhb_MJE1H0",
        "outputId": "b6c2ab9d-72c6-4091-ffad-48eccb17d688"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7072, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U9st_JYE1IA"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q8bp0j-E1IP"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X1)\n",
        "X_test_norm = robust.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFt0h9c1E1Id"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWr0-nW2E1Ir"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTix00OKE1I5"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8SyhhCgE1JF",
        "outputId": "11eb56c9-5c88-4383-bf7d-1b7124894c28"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.4287682144495232,\n",
        "  'min_child_samples': 41+1,\n",
        "  'min_child_weight': 0.8439517049649784,\n",
        "  'num_leaves': 37+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.7620885372161865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq2E6eCvE1JP",
        "outputId": "9c728e41-e11b-4b70-bf18-48a75fc55b33"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.97      0.98      1152\n",
            "           2       0.98      0.97      0.98      4020\n",
            "           3       0.95      0.97      0.96      1899\n",
            "\n",
            "    accuracy                           0.97      7071\n",
            "   macro avg       0.97      0.97      0.97      7071\n",
            "weighted avg       0.97      0.97      0.97      7071\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1123   20    9]\n",
            " [  21 3917   82]\n",
            " [   7   54 1838]]\n",
            "Training Score:  0.9958993212669683\n",
            "Training Score:  0.9727054164898883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HASN3_4SE1JV"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghHUeRe-E1Jg"
      },
      "source": [
        "robust = RobustScaler()\n",
        "\n",
        "X_train_norm = robust.fit_transform(X2)\n",
        "X_test_norm = robust.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6B3GZ8E1Jp"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM5P26SuE1Jy"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf7yOPcuE1J5"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAk0jFr1E1KE",
        "outputId": "7847ca1d-ee87-4594-cb06-878ed4dbd350"
      },
      "source": [
        "gb = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.8979758467441785,\n",
        "  'min_child_samples': 20,\n",
        "  'min_child_weight': 0.6165560587018821,\n",
        "  'num_leaves': 52+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.9347662925720215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZtBm6iDE1KN",
        "outputId": "ea1744dd-98d1-4f32-8bd8-28677e3d1cc4"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.96      0.97      1152\n",
            "           2       0.97      0.97      0.97      4020\n",
            "           3       0.94      0.95      0.95      1900\n",
            "\n",
            "    accuracy                           0.96      7072\n",
            "   macro avg       0.96      0.96      0.96      7072\n",
            "weighted avg       0.96      0.96      0.96      7072\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1111   28   13]\n",
            " [  19 3906   95]\n",
            " [   7   92 1801]]\n",
            "Training Score:  0.9967472776127846\n",
            "Training Score:  0.9640837104072398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOZtu9HE1KW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOMR32CyFV3y"
      },
      "source": [
        "# Обучаем на всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsWqbp8vHNMs"
      },
      "source": [
        "overview = 'all'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWgyx24h_8i",
        "outputId": "86c93f9a-43fb-49a6-a2fc-1a151808028d"
      },
      "source": [
        "len(features)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWGzAP2JFV38",
        "outputId": "c35e8b6e-aed6-4aca-e03d-804a24ff2a62"
      },
      "source": [
        "data1, data2 = data_split(df[features].dropna())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14.34546057 13.89415478  7.29767599 ...  0.62771844  1.51317155\n",
            "  -2.76776875]\n",
            " [11.36802556 10.14976812  9.6898775  ...  1.54674132  1.87814739\n",
            "  -0.2598255 ]\n",
            " [11.17091291 10.00617184  9.75755261 ...  3.0413092   0.57088776\n",
            "  -1.22056085]\n",
            " ...\n",
            " [23.58409707 24.98982939 23.53254382 ...  1.30199583  3.21635345\n",
            "   0.08307366]\n",
            " [23.11332269 24.02218114 23.64032341 ...  0.64568178  1.42733941\n",
            "   0.09861513]\n",
            " [24.18340755 25.12869598 24.85982292 ...  2.83996985  0.82611757\n",
            "  -0.23275616]] [[14.34546057 13.89415478  7.29767599 ...  0.62771844  1.51317155\n",
            "  -2.76776875]\n",
            " [11.17091291 10.00617184  9.75755261 ...  3.0413092   0.57088776\n",
            "  -1.22056085]\n",
            " [12.25592793 10.63893401 10.13266654 ...  1.23259915  1.62019655\n",
            "   1.1198265 ]\n",
            " ...\n",
            " [23.27426677 23.79420769 22.65531679 ... -1.88550701 -0.28586571\n",
            "  -0.10112914]\n",
            " [23.10549057 23.87371022 22.91696185 ...  3.21728682 -0.49271541\n",
            "   0.18148377]\n",
            " [23.11332269 24.02218114 23.64032341 ...  0.64568178  1.42733941\n",
            "   0.09861513]] [[ 11.36802556  10.14976812   9.6898775  ...   1.54674132   1.87814739\n",
            "   -0.2598255 ]\n",
            " [ 11.53523071  10.15659183   9.92304101 ... -16.27114543 -17.40387224\n",
            "   -2.78789719]\n",
            " [ 11.60117227  10.38643604  10.19142614 ...  -0.52422022  -0.49681008\n",
            "   -0.99581489]\n",
            " ...\n",
            " [ 23.22646422  23.73827654  22.67205007 ...  -2.90918899   0.36191019\n",
            "    0.13499076]\n",
            " [ 23.58409707  24.98982939  23.53254382 ...   1.30199583   3.21635345\n",
            "    0.08307366]\n",
            " [ 24.18340755  25.12869598  24.85982292 ...   2.83996985   0.82611757\n",
            "   -0.23275616]]\n",
            "[[15.15536432 15.06108459 15.04715259 ... -1.78587989 -0.25460064\n",
            "  -0.90463925]\n",
            " [15.66798544 15.44707046 15.23454548 ... -0.22202483 -0.13692316\n",
            "  -0.34673668]\n",
            " [16.17368929 15.65380194 15.31668988 ...  0.19578116  0.17592854\n",
            "   0.12100394]\n",
            " ...\n",
            " [23.63837953 25.19444462 24.08667653 ... -0.40013377  0.05371384\n",
            "   0.15200719]\n",
            " [23.94281424 25.02906542 24.42240893 ...  0.19850883 -1.85172845\n",
            "   0.41514116]\n",
            " [21.12293314 21.34357527 24.54789956 ...  0.60856793  3.36935966\n",
            "  -0.82894079]] [[ 1.51553643e+01  1.50610846e+01  1.50471526e+01 ... -1.78587989e+00\n",
            "  -2.54600636e-01 -9.04639253e-01]\n",
            " [ 1.61736893e+01  1.56538019e+01  1.53166899e+01 ...  1.95781164e-01\n",
            "   1.75928536e-01  1.21003944e-01]\n",
            " [ 1.55615629e+01  1.56118234e+01  1.55525012e+01 ... -4.12539275e-02\n",
            "   1.09618430e-02  8.64002682e-03]\n",
            " ...\n",
            " [ 2.45537589e+01  2.48093332e+01  2.38069730e+01 ... -5.02747870e-01\n",
            "  -8.24179103e-02  4.02628510e-01]\n",
            " [ 2.34307963e+01  2.45553403e+01  2.40031519e+01 ... -2.95288786e+00\n",
            "   3.72188075e-02  1.59096361e-01]\n",
            " [ 2.39428142e+01  2.50290654e+01  2.44224089e+01 ...  1.98508827e-01\n",
            "  -1.85172845e+00  4.15141164e-01]] [[15.66798544 15.44707046 15.23454548 ... -0.22202483 -0.13692316\n",
            "  -0.34673668]\n",
            " [15.4659036  15.37361893 15.46981679 ... -0.05875839  0.04339169\n",
            "  -0.13771201]\n",
            " [15.54250418 15.66733332 15.62901047 ... -0.29014466 -0.26489833\n",
            "  -0.21095011]\n",
            " ...\n",
            " [24.56061282 24.71605445 23.95746879 ... -1.29324234 -1.86968578\n",
            "   0.16158338]\n",
            " [23.63837953 25.19444462 24.08667653 ... -0.40013377  0.05371384\n",
            "   0.15200719]\n",
            " [21.12293314 21.34357527 24.54789956 ...  0.60856793  3.36935966\n",
            "  -0.82894079]]\n",
            "[[16.10025066 15.495036   14.92068378 ... -2.38960418  0.14601106\n",
            "  -3.48808031]\n",
            " [17.34812672 15.68192223 14.9676701  ...  0.24046805  0.23092393\n",
            "   0.10514708]\n",
            " [17.69518216 15.86661033 15.01468035 ... -0.02871844  0.05501345\n",
            "   0.02779263]\n",
            " ...\n",
            " [23.74878292 23.09168367 23.14053311 ...  0.26746636  0.31791989\n",
            "  -0.31702531]\n",
            " [23.57875417 24.04322178 23.25238554 ... -1.63804921 -0.59395077\n",
            "  -0.75384057]\n",
            " [23.75388413 25.37837256 23.27950818 ...  1.7759648  -0.51827757\n",
            "   0.06120135]] [[ 1.61002507e+01  1.54950360e+01  1.49206838e+01 ... -2.38960418e+00\n",
            "   1.46011056e-01 -3.48808031e+00]\n",
            " [ 1.76951822e+01  1.58666103e+01  1.50146803e+01 ... -2.87184394e-02\n",
            "   5.50134457e-02  2.77926339e-02]\n",
            " [ 1.77163441e+01  1.58891988e+01  1.51397370e+01 ...  2.97351400e-02\n",
            "  -1.54457477e-02 -5.36075089e-02]\n",
            " ...\n",
            " [ 2.34546633e+01  2.41755007e+01  2.29310731e+01 ... -2.77029559e+00\n",
            "   3.64226416e-01 -4.68834936e-01]\n",
            " [ 2.37487829e+01  2.30916837e+01  2.31405331e+01 ...  2.67466359e-01\n",
            "   3.17919888e-01 -3.17025312e-01]\n",
            " [ 2.37538841e+01  2.53783726e+01  2.32795082e+01 ...  1.77596480e+00\n",
            "  -5.18277568e-01  6.12013500e-02]] [[17.34812672 15.68192223 14.9676701  ...  0.24046805  0.23092393\n",
            "   0.10514708]\n",
            " [18.07114556 15.92605521 15.05817014 ...  0.58129785  0.510554\n",
            "   0.51307543]\n",
            " [17.97231913 16.04078064 15.17006206 ...  0.03281123  0.06694797\n",
            "   0.10806962]\n",
            " ...\n",
            " [21.12950343 23.53090039 22.91321577 ... -0.92485556  0.34081933\n",
            "   0.43933684]\n",
            " [24.83823088 24.64326456 23.06959781 ...  0.23654162  0.9761927\n",
            "   0.56870527]\n",
            " [23.57875417 24.04322178 23.25238554 ... -1.63804921 -0.59395077\n",
            "  -0.75384057]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncLGHKMuFV39"
      },
      "source": [
        "X1, y1 = data1[:,:-1], data1[:,-1].astype('int')\n",
        "X2, y2 = data2[:,:-1], data2[:,-1].astype('int')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a38xTQBFV3-",
        "outputId": "a72847c8-d10a-4207-9320-d08a02a47cc9"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7041, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPtBjVg1FV3-"
      },
      "source": [
        "##Обучаем на первой выборке, проверяем на второй"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P14I-2NjFV3-"
      },
      "source": [
        "robust1 = RobustScaler()\n",
        "\n",
        "X_train_norm = robust1.fit_transform(X1)\n",
        "X_test_norm = robust1.transform(X2)\n",
        "y_train = y1\n",
        "y_test = y2"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdhdWVx7FV3-"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_SyeDZNFV3-"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "645oXHZbFV3-"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb_QYr7eFV3_",
        "outputId": "355ff213-1652-45fb-add5-d4a1cbbcb9b3"
      },
      "source": [
        "gb1 = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.8211965075024374,\n",
        "  'min_child_samples': 33+1,\n",
        "  'min_child_weight': 0.9636584831554481,\n",
        "  'num_leaves': 67+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb1.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.17336893081665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU4mQIK4FV3_",
        "outputId": "20b7aa78-6394-423e-d7e0-253c73e423d4"
      },
      "source": [
        "pred = gb1.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb1.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "#acc[overview] = gb_test_acc\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.97      0.98      1152\n",
            "           2       0.98      0.98      0.98      3989\n",
            "           3       0.96      0.97      0.96      1899\n",
            "\n",
            "    accuracy                           0.97      7040\n",
            "   macro avg       0.97      0.97      0.97      7040\n",
            "weighted avg       0.97      0.97      0.97      7040\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1121   20   11]\n",
            " [  13 3901   75]\n",
            " [   3   57 1839]]\n",
            "Training Score:  0.9960232921460019\n",
            "Training Score:  0.9745738636363637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcAWxvNAFV3_"
      },
      "source": [
        "##Обучаем на второй выборке, проверяем на первой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-gYP2DlFV3_"
      },
      "source": [
        "robust2 = RobustScaler()\n",
        "\n",
        "X_train_norm = robust2.fit_transform(X2)\n",
        "X_test_norm = robust2.transform(X1)\n",
        "y_train = y2\n",
        "y_test = y1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyMwy44QFV3_"
      },
      "source": [
        "#obj = HPOpt(X_train_norm, y_train)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7i6t4CfFV3_"
      },
      "source": [
        "#lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Q2UPR5FV3_"
      },
      "source": [
        "#lgb_opt"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLjFI8vnFV3_",
        "outputId": "7b22d541-4b7e-4ae2-e08e-b8cf31bfdcb5"
      },
      "source": [
        "gb2 = lgb.LGBMClassifier(\n",
        "    **{'colsample_bytree': 0.7122774534428687,\n",
        "  'min_child_samples': 12,\n",
        "  'min_child_weight': 0.5902465770913051,\n",
        "  'num_leaves': 0+10,\n",
        "      'n_estimators': 1000}\n",
        ")\n",
        "\n",
        "\n",
        "t = time()\n",
        "gb2.fit(X_train_norm[:-500], y_train[:-500], eval_set=[(X_train_norm[:-500], y_train[:-500]), (X_train_norm[:-500], y_train[:-500])],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.7135820388793945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tQg0OKcFV4A",
        "outputId": "78c6ba5c-c22b-4369-afe5-c18f5c81c71f"
      },
      "source": [
        "pred = gb2.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb2.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y_test, pred)\n",
        "#acc[overview] = (acc[overview] + gb_test_acc)/2\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.98      0.98      1152\n",
            "           2       0.98      0.98      0.98      3989\n",
            "           3       0.95      0.96      0.95      1900\n",
            "\n",
            "    accuracy                           0.97      7041\n",
            "   macro avg       0.97      0.97      0.97      7041\n",
            "weighted avg       0.97      0.97      0.97      7041\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1124   19    9]\n",
            " [  18 3892   79]\n",
            " [   4   80 1816]]\n",
            "Training Score:  0.9961647727272728\n",
            "Training Score:  0.9703167163755149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDoUCvlGpqG"
      },
      "source": [
        "# Построение графиков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyOmkFD0Ifm8",
        "outputId": "98081714-0967-418a-c229-4344fd5727c7"
      },
      "source": [
        "a = np.array([np.arange(10),np.arange(10)]) \n",
        "print(a)\n",
        "a[[True]*2]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
              "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F_UpxbmGuV-"
      },
      "source": [
        "def acc_for_features(X1, y1, X2, y2, model1, model2, n_split, n_features, robust1, robust2):\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  def get_pred(X, y, model, r1, r2, robust):\n",
        "    ind = (X[:, n_features]>r1) & (X[:, n_features]< r2)\n",
        "    print(np.sum(ind), np.sum([X[:, n_features]>r1]), np.sum([X[:, n_features]< r2]))\n",
        "    X = X[ind]\n",
        "    y = y[ind]\n",
        "    \n",
        "    print(r1, r2)\n",
        "    print((np.sum(y==1), np.sum(y==2), np.sum(y==3)))\n",
        "    size = np.min((np.sum(y==1), np.sum(y==3), np.sum(y==2))) #для уравновешивания классов\n",
        "    print(size)\n",
        "    if size < 50:\n",
        "      return [0], [0]\n",
        "    test = np.concatenate((X[y==1][:size], X[y==2][:size], X[y==3][:size]))\n",
        "    yt = np.concatenate((y[y==1][:size], y[y==2][:size], y[y==3][:size]))\n",
        "\n",
        "    return yt, model.predict(robust.transform(test))#_proba\n",
        "\n",
        "  q = np.linspace(0, 1, num=n_split+1)\n",
        "  feature_q = np.quantile(np.concatenate((X1, X2))[:, n_features], q)\n",
        "\n",
        "  Acc = []\n",
        "  feature = []\n",
        "  r0 = feature_q[0]\n",
        "\n",
        "  for i in range(n_split):\n",
        "\n",
        "    r1 = feature_q[i+1]\n",
        "\n",
        "    yt1, yp1 = get_pred(X1, y1, model1, r0, r1, robust1)\n",
        "    yt2, yp2 = get_pred(X2, y2, model2, r0, r1, robust2)\n",
        "    if (len(yt1) == 1 or len(yt2) == 1):\n",
        "      continue\n",
        "     \n",
        "\n",
        "    y_true = np.concatenate((yt1, yt2))\n",
        "    y_pred = np.concatenate((yp1, yp2))\n",
        "\n",
        "    Acc.append(f1_score(y_true, y_pred, average=None))\n",
        "    feature.append(np.mean((r0, r1)))\n",
        "    r0 = r1\n",
        "    print('not cont')\n",
        "\n",
        "    #ind1 = tuple([X_true[:,n_features]>feature_q[i]] and [X_true[:,n_features]<feature_q[i+1]])\n",
        "    #ind2 = tuple([X_false[:,n_features]>feature_q[i]] and [X_false[:,n_features]<feature_q[i+1]])\n",
        "    #test = np.concatenate((X_true[ind1], np.random.permutation(X_false[ind2])))\n",
        "    #print(len(test))\n",
        "    #yt = np.concatenate((y_true[ind1], y_false[ind2]))\n",
        "    #yp = model.predict_proba(test)\n",
        "    \n",
        "   \n",
        "  return Acc, feature"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXrjaJL7696e"
      },
      "source": [
        "\n",
        "def auc(y, pred, pos_label):\n",
        "  import numpy as np\n",
        "  from sklearn import metrics\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=pos_label)\n",
        "  return metrics.auc(fpr, tpr)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMaqHnsLRDPA"
      },
      "source": [
        "acc, r = acc_for_features(X2, y2, X1, y1, gb1, gb2, 5, 2, robust1, robust2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9r8aGnlVESi"
      },
      "source": [
        "acc = np.array(acc).T"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "MNjg8_cYWupQ",
        "outputId": "22ad55a4-6bec-4df9-cbe9-928d7b99ab6a"
      },
      "source": [
        "    plt.figure(figsize=(10, 10))\n",
        "    lab =['stars', 'qwazars', 'galactics']\n",
        "    lines = []\n",
        "    labels = []\n",
        "    clr = ['skyblue', 'red', 'greenyellow', 'orangered', 'chocolate', 'darkorange', 'orange', 'gold', 'yellow', 'lime', 'springgreen', 'aquamarine', 'aqua',  'lightskyblue']\n",
        "    for j in range(3):\n",
        "      l, = plt.plot(r, acc[j], color=clr[j], lw=2)\n",
        "      lines.append(l)\n",
        "      labels.append(lab[j])\n",
        "\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    fig.subplots_adjust(bottom=0.25)\n",
        "\n",
        "    plt.xlabel('sdssdr16_r_psf')\n",
        "    plt.ylabel('F1')\n",
        "    plt.title('F1')\n",
        "    plt.legend(lines, labels, loc=(0, -0.35), prop=dict(size=14))\n",
        "    plt.show()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stVRLTB3-XDc"
      },
      "source": [
        "def prec_rec(ml, X1_test, y1_test, X2_test, y2_test, X3_test, y3_test, true_rt=False, rt = None):\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "    feature = 2\n",
        "    \n",
        "    X_test_not_scaling = X1_test[X1_test[:,feature].argsort()]\n",
        "    X_test = X1_test[X1_test[:,feature].argsort()]#################\n",
        "    if true_rt:\n",
        "      X_test = rt.transform(X_test)\n",
        "    y_test = y1_test[X1_test[:,feature].argsort()]\n",
        "    \n",
        "    #shift = 1000\n",
        "    #n = len(y_test)//shift\n",
        "    n = 2\n",
        "    shift = len(y_test)//n\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    lines = []\n",
        "    labels = []\n",
        "    AUC = []\n",
        "    r = []\n",
        "    clr = ['red', 'orangered', 'chocolate', 'darkorange', 'orange', 'gold', 'yellow', 'greenyellow', 'lime', 'springgreen', 'aquamarine', 'aqua', 'skyblue', 'lightskyblue']\n",
        "   \n",
        "    #y_pred_all = ml.predict(X_test)\n",
        "    y_pred_all_proba = ml.predict_proba(X_test)\n",
        "    j = 0\n",
        "\n",
        "    for i in range(0, n):\n",
        "            r1 = X_test_not_scaling[i*shift][feature]\n",
        "            r2 = X_test_not_scaling[(i+1) * shift - 1][feature]\n",
        "\n",
        "            X2 = X2_test[(X2_test[:, feature] >= r1) & (X2_test[:, feature] <= r2)]\n",
        "            y2 = y2_test[(X2_test[:, feature] >= r1) & (X2_test[:, feature] <= r2)]\n",
        "            ind2 = np.random.choice(len(y2), size=min(shift, len(y2)), replace=False)\n",
        "\n",
        "            X3 = X3_test[(X3_test[:, feature] >= r1) & (X3_test[:, feature] <= r2)]\n",
        "            y3 = y3_test[(X3_test[:, feature] >= r1) & (X3_test[:, feature] <= r2)]\n",
        "            ind3 = np.random.choice(len(y3), size=min(shift, len(y3)), replace=False)\n",
        "\n",
        "            X = np.concatenate((X2[ind2], X3[ind3]))\n",
        "            if true_rt:\n",
        "              X = rt.transform(X)\n",
        "            y = np.concatenate((y2[ind2], y3[ind3]))\n",
        "\n",
        "            print(i, shift, len(y2), len(y3))\n",
        "\n",
        "            yt = np.concatenate((y_test[i*shift : (i+1) * shift], y))\n",
        "            yt = label_binarize(yt, classes=[1, 2, 3])\n",
        "            yp = np.concatenate((y_pred_all_proba[i*shift : (i+1) * shift], ml.predict_proba(X)))###################\n",
        "\n",
        "            f, t, thresholds = roc_curve(yt.ravel(), yp.ravel())\n",
        "            l, = plt.plot(f, t, color=clr[j], lw=2)\n",
        "            j+=1\n",
        "            lines.append(l)\n",
        "            r1 = X_test_not_scaling[i*shift : (i+1) * shift][:,feature].mean()\n",
        "            labels.append('r = ' + str(r1))\n",
        "            r.append(r1)\n",
        "            AUC.append(auc(f, t))\n",
        "            \n",
        "    fig = plt.gcf()\n",
        "    fig.subplots_adjust(bottom=0.25)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC multi-class')\n",
        "    plt.legend(lines, labels, loc=(0, -0.75), prop=dict(size=14))\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('AUC')\n",
        "    plt.plot(r, AUC, 'b')\n",
        "    plt.plot([0,1],[0,1],'r--')\n",
        "    plt.ylim([0.75, 1])\n",
        "    plt.xlim([22,16])\n",
        "    plt.ylabel('AUC')\n",
        "    plt.xlabel('R')\n",
        "    plt.show()"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np1-_8cg-jxP"
      },
      "source": [
        "prec_rec(gb, X2[y2==1], y2[y2==1], X2[y2==2], y2[y2==2], X2[y2==3], y2[y2==3], True, robust)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgg5m0b8_kyz"
      },
      "source": [
        " X2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkh_VkrgFV4A"
      },
      "source": [
        "acc = lambda X, y : accuracy_score(y, gb.predict(robust.transform(X).astype('float')))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7enfzF1oCYk6"
      },
      "source": [
        "\n",
        "#Далее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MBT4JQt_Cxv",
        "outputId": "7646bd4a-905c-4bf2-f37d-ec1e303fc4c6"
      },
      "source": [
        "X1.shape, X2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8480, 17), (8480, 17))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBesGweYnJzA",
        "outputId": "194c04a2-d214-41fd-9833-c007f0bbba29"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6336, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3hffW6r7G9A",
        "outputId": "55fe4d30-3a52-4ae7-9822-7c8e73cde745"
      },
      "source": [
        "gb = lgb.LGBMClassifier()\n",
        "t = time()\n",
        "gb.fit(X_train_norm, y_train, eval_set=[(X_train_norm, y_train), (X_test_norm, y_test)],  **lgb_fit_params)\n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1678180694580078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABqdNAgqyk3m",
        "outputId": "b026c00d-7061-4cd7-8b58-081bf0e365c6"
      },
      "source": [
        "pred = gb.predict(X_test_norm)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc =  accuracy_score(y_test, pred)\n",
        "print('Testing Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.96      0.96       116\n",
            "           2       0.98      0.98      0.98       399\n",
            "           3       0.98      0.96      0.97       190\n",
            "\n",
            "    accuracy                           0.97       705\n",
            "   macro avg       0.97      0.97      0.97       705\n",
            "weighted avg       0.97      0.97      0.97       705\n",
            "\n",
            "Confusion Matrix: \n",
            " [[111   4   1]\n",
            " [  3 393   3]\n",
            " [  1   6 183]]\n",
            "Training Score:  0.9970012626262627\n",
            "Testing Score:  0.9744680851063829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkS2XnGfy6vt",
        "outputId": "6b8ca5db-73e0-424d-a0a4-2b4d75d47d55"
      },
      "source": [
        "pred = gb.predict(X2_norm)\n",
        "print('Classification Report: \\n', classification_report(y2, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y2, pred))\n",
        "gb_train_acc = accuracy_score(y_train, gb.predict(X_train_norm.astype('float')))\n",
        "print('Training Score: ', gb_train_acc)\n",
        "gb_test_acc = accuracy_score(y2, pred)\n",
        "print('Training Score: ', gb_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99      1152\n",
            "           2       1.00      1.00      1.00      3989\n",
            "           3       0.99      0.99      0.99      1900\n",
            "\n",
            "    accuracy                           0.99      7041\n",
            "   macro avg       0.99      0.99      0.99      7041\n",
            "weighted avg       0.99      0.99      0.99      7041\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1142    7    3]\n",
            " [   5 3974   10]\n",
            " [   1   11 1888]]\n",
            "Training Score:  0.9970012626262627\n",
            "Training Score:  0.9947450646215026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A96i6oXXRtWN",
        "outputId": "99592e12-8c9f-49b1-ef2a-cf3290e1ad9a"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6340, 65), (706, 65))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPW8o-hvuYjf",
        "outputId": "ddcd571d-1505-446c-c0c8-ae73887d4190"
      },
      "source": [
        "clf = TabNetClassifier(n_d=32, n_a=32,\n",
        "                       momentum=0.7,\n",
        "                       optimizer_fn=torch.optim.Adam,\n",
        "                       optimizer_params=dict(lr=2e-2),\n",
        "                       scheduler_params={\"step_size\":20, # how to use learning rate scheduler\n",
        "                                         \"gamma\":0.95},\n",
        "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "\n",
        "                       mask_type='entmax',\n",
        "                       \n",
        "                       **{'gamma': 1.7,\n",
        "                          'lambda_sparse': 0.001,\n",
        "                          'n_steps': 5}\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1c7joHKuYjg"
      },
      "source": [
        "max_epochs = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5LMIziKL9TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888847be-8f15-411b-c850-7c215759182b"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6336, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 346
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTPwIwdnGA5f",
        "outputId": "9cb7d799-33e2-4c0c-ab56-8023e0ab01b0"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(705, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "id": "ElyLpi8awsis",
        "outputId": "e87f45e4-9b6b-4a5c-c1ee-e39c7a1a2c9a"
      },
      "source": [
        "t = time()\n",
        "clf.fit(\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    #X_valid=X_valid, y_valid=y_valid,\n",
        "    eval_set=[(X_train, y_train), (X_train, y_train)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=['logloss','accuracy'],\n",
        "    max_epochs=max_epochs , patience=50,\n",
        "    batch_size=512, virtual_batch_size=128,\n",
        "    #from_unsupervised=unsupervised_model\n",
        ") \n",
        "print(time()-t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.96442 | train_logloss: 5.35252 | train_accuracy: 0.42551 | valid_logloss: 5.35252 | valid_accuracy: 0.42551 |  0:00:00s\n",
            "epoch 1  | loss: 0.32988 | train_logloss: 2.74    | train_accuracy: 0.64378 | valid_logloss: 2.74    | valid_accuracy: 0.64378 |  0:00:01s\n",
            "epoch 2  | loss: 0.27342 | train_logloss: 3.26327 | train_accuracy: 0.41525 | valid_logloss: 3.26327 | valid_accuracy: 0.41525 |  0:00:02s\n",
            "epoch 3  | loss: 0.2023  | train_logloss: 9.63578 | train_accuracy: 0.34501 | valid_logloss: 9.63578 | valid_accuracy: 0.34501 |  0:00:03s\n",
            "epoch 4  | loss: 0.19852 | train_logloss: 3.86725 | train_accuracy: 0.41177 | valid_logloss: 3.86725 | valid_accuracy: 0.41177 |  0:00:04s\n",
            "epoch 5  | loss: 0.16243 | train_logloss: 2.8799  | train_accuracy: 0.65025 | valid_logloss: 2.8799  | valid_accuracy: 0.65025 |  0:00:05s\n",
            "epoch 6  | loss: 0.18096 | train_logloss: 0.91737 | train_accuracy: 0.79419 | valid_logloss: 0.91737 | valid_accuracy: 0.79419 |  0:00:06s\n",
            "epoch 7  | loss: 0.15446 | train_logloss: 0.95715 | train_accuracy: 0.71165 | valid_logloss: 0.95715 | valid_accuracy: 0.71165 |  0:00:07s\n",
            "epoch 8  | loss: 0.13625 | train_logloss: 1.72362 | train_accuracy: 0.68939 | valid_logloss: 1.72362 | valid_accuracy: 0.68939 |  0:00:08s\n",
            "epoch 9  | loss: 0.15086 | train_logloss: 0.47804 | train_accuracy: 0.88431 | valid_logloss: 0.47804 | valid_accuracy: 0.88431 |  0:00:09s\n",
            "epoch 10 | loss: 0.13513 | train_logloss: 0.37232 | train_accuracy: 0.88668 | valid_logloss: 0.37232 | valid_accuracy: 0.88668 |  0:00:09s\n",
            "epoch 11 | loss: 0.12589 | train_logloss: 0.26089 | train_accuracy: 0.92219 | valid_logloss: 0.26089 | valid_accuracy: 0.92219 |  0:00:10s\n",
            "epoch 12 | loss: 0.12215 | train_logloss: 0.17243 | train_accuracy: 0.94539 | valid_logloss: 0.17243 | valid_accuracy: 0.94539 |  0:00:11s\n",
            "epoch 13 | loss: 0.12241 | train_logloss: 0.37877 | train_accuracy: 0.90372 | valid_logloss: 0.37877 | valid_accuracy: 0.90372 |  0:00:12s\n",
            "epoch 14 | loss: 0.13469 | train_logloss: 0.17441 | train_accuracy: 0.94634 | valid_logloss: 0.17441 | valid_accuracy: 0.94634 |  0:00:13s\n",
            "epoch 15 | loss: 0.12968 | train_logloss: 0.17967 | train_accuracy: 0.94302 | valid_logloss: 0.17967 | valid_accuracy: 0.94302 |  0:00:14s\n",
            "epoch 16 | loss: 0.1187  | train_logloss: 0.18812 | train_accuracy: 0.93292 | valid_logloss: 0.18812 | valid_accuracy: 0.93292 |  0:00:15s\n",
            "epoch 17 | loss: 0.1133  | train_logloss: 0.15479 | train_accuracy: 0.9536  | valid_logloss: 0.15479 | valid_accuracy: 0.9536  |  0:00:16s\n",
            "epoch 18 | loss: 0.10411 | train_logloss: 0.13271 | train_accuracy: 0.95597 | valid_logloss: 0.13271 | valid_accuracy: 0.95597 |  0:00:17s\n",
            "epoch 19 | loss: 0.1047  | train_logloss: 0.12383 | train_accuracy: 0.95991 | valid_logloss: 0.12383 | valid_accuracy: 0.95991 |  0:00:18s\n",
            "epoch 20 | loss: 0.10477 | train_logloss: 0.11906 | train_accuracy: 0.9596  | valid_logloss: 0.11906 | valid_accuracy: 0.9596  |  0:00:19s\n",
            "epoch 21 | loss: 0.10117 | train_logloss: 0.11133 | train_accuracy: 0.96165 | valid_logloss: 0.11133 | valid_accuracy: 0.96165 |  0:00:20s\n",
            "epoch 22 | loss: 0.10161 | train_logloss: 0.09761 | train_accuracy: 0.96749 | valid_logloss: 0.09761 | valid_accuracy: 0.96749 |  0:00:21s\n",
            "epoch 23 | loss: 0.09922 | train_logloss: 0.11562 | train_accuracy: 0.95944 | valid_logloss: 0.11562 | valid_accuracy: 0.95944 |  0:00:22s\n",
            "epoch 24 | loss: 0.10518 | train_logloss: 0.11269 | train_accuracy: 0.96591 | valid_logloss: 0.11269 | valid_accuracy: 0.96591 |  0:00:23s\n",
            "epoch 25 | loss: 0.10674 | train_logloss: 0.12541 | train_accuracy: 0.96023 | valid_logloss: 0.12541 | valid_accuracy: 0.96023 |  0:00:23s\n",
            "epoch 26 | loss: 0.10439 | train_logloss: 0.10081 | train_accuracy: 0.96701 | valid_logloss: 0.10081 | valid_accuracy: 0.96701 |  0:00:24s\n",
            "epoch 27 | loss: 0.10458 | train_logloss: 0.10999 | train_accuracy: 0.96607 | valid_logloss: 0.10999 | valid_accuracy: 0.96607 |  0:00:25s\n",
            "epoch 28 | loss: 0.09647 | train_logloss: 0.10402 | train_accuracy: 0.96796 | valid_logloss: 0.10402 | valid_accuracy: 0.96796 |  0:00:26s\n",
            "epoch 29 | loss: 0.09703 | train_logloss: 0.09506 | train_accuracy: 0.9678  | valid_logloss: 0.09506 | valid_accuracy: 0.9678  |  0:00:27s\n",
            "epoch 30 | loss: 0.09057 | train_logloss: 0.0839  | train_accuracy: 0.97222 | valid_logloss: 0.0839  | valid_accuracy: 0.97222 |  0:00:28s\n",
            "epoch 31 | loss: 0.09577 | train_logloss: 0.08776 | train_accuracy: 0.97143 | valid_logloss: 0.08776 | valid_accuracy: 0.97143 |  0:00:29s\n",
            "epoch 32 | loss: 0.09321 | train_logloss: 0.08967 | train_accuracy: 0.97049 | valid_logloss: 0.08967 | valid_accuracy: 0.97049 |  0:00:30s\n",
            "epoch 33 | loss: 0.09243 | train_logloss: 0.0991  | train_accuracy: 0.97049 | valid_logloss: 0.0991  | valid_accuracy: 0.97049 |  0:00:31s\n",
            "epoch 34 | loss: 0.09183 | train_logloss: 0.09672 | train_accuracy: 0.97159 | valid_logloss: 0.09672 | valid_accuracy: 0.97159 |  0:00:32s\n",
            "epoch 35 | loss: 0.08737 | train_logloss: 0.08561 | train_accuracy: 0.97064 | valid_logloss: 0.08561 | valid_accuracy: 0.97064 |  0:00:33s\n",
            "epoch 36 | loss: 0.08808 | train_logloss: 0.08851 | train_accuracy: 0.97017 | valid_logloss: 0.08851 | valid_accuracy: 0.97017 |  0:00:34s\n",
            "epoch 37 | loss: 0.08501 | train_logloss: 0.07707 | train_accuracy: 0.97475 | valid_logloss: 0.07707 | valid_accuracy: 0.97475 |  0:00:34s\n",
            "epoch 38 | loss: 0.08383 | train_logloss: 0.0752  | train_accuracy: 0.97333 | valid_logloss: 0.0752  | valid_accuracy: 0.97333 |  0:00:35s\n",
            "epoch 39 | loss: 0.08212 | train_logloss: 0.0748  | train_accuracy: 0.97396 | valid_logloss: 0.0748  | valid_accuracy: 0.97396 |  0:00:36s\n",
            "epoch 40 | loss: 0.08599 | train_logloss: 0.08452 | train_accuracy: 0.97112 | valid_logloss: 0.08452 | valid_accuracy: 0.97112 |  0:00:37s\n",
            "epoch 41 | loss: 0.08718 | train_logloss: 0.08447 | train_accuracy: 0.97238 | valid_logloss: 0.08447 | valid_accuracy: 0.97238 |  0:00:38s\n",
            "epoch 42 | loss: 0.09487 | train_logloss: 0.08318 | train_accuracy: 0.97064 | valid_logloss: 0.08318 | valid_accuracy: 0.97064 |  0:00:39s\n",
            "epoch 43 | loss: 0.09296 | train_logloss: 0.0889  | train_accuracy: 0.96859 | valid_logloss: 0.0889  | valid_accuracy: 0.96859 |  0:00:40s\n",
            "epoch 44 | loss: 0.09523 | train_logloss: 0.08859 | train_accuracy: 0.96985 | valid_logloss: 0.08859 | valid_accuracy: 0.96985 |  0:00:41s\n",
            "epoch 45 | loss: 0.10093 | train_logloss: 0.0927  | train_accuracy: 0.96891 | valid_logloss: 0.0927  | valid_accuracy: 0.96891 |  0:00:42s\n",
            "epoch 46 | loss: 0.10009 | train_logloss: 0.08488 | train_accuracy: 0.97049 | valid_logloss: 0.08488 | valid_accuracy: 0.97049 |  0:00:43s\n",
            "epoch 47 | loss: 0.09356 | train_logloss: 0.08773 | train_accuracy: 0.96985 | valid_logloss: 0.08773 | valid_accuracy: 0.96985 |  0:00:44s\n",
            "epoch 48 | loss: 0.09651 | train_logloss: 0.08605 | train_accuracy: 0.97128 | valid_logloss: 0.08605 | valid_accuracy: 0.97128 |  0:00:44s\n",
            "epoch 49 | loss: 0.11837 | train_logloss: 0.14963 | train_accuracy: 0.94555 | valid_logloss: 0.14963 | valid_accuracy: 0.94555 |  0:00:45s\n",
            "epoch 50 | loss: 0.12439 | train_logloss: 0.11495 | train_accuracy: 0.96607 | valid_logloss: 0.11495 | valid_accuracy: 0.96607 |  0:00:46s\n",
            "epoch 51 | loss: 0.1101  | train_logloss: 0.0912  | train_accuracy: 0.96828 | valid_logloss: 0.0912  | valid_accuracy: 0.96828 |  0:00:47s\n",
            "epoch 52 | loss: 0.09832 | train_logloss: 0.08895 | train_accuracy: 0.96828 | valid_logloss: 0.08895 | valid_accuracy: 0.96828 |  0:00:48s\n",
            "epoch 53 | loss: 0.09697 | train_logloss: 0.08768 | train_accuracy: 0.96843 | valid_logloss: 0.08768 | valid_accuracy: 0.96843 |  0:00:49s\n",
            "epoch 54 | loss: 0.09242 | train_logloss: 0.07873 | train_accuracy: 0.9738  | valid_logloss: 0.07873 | valid_accuracy: 0.9738  |  0:00:50s\n",
            "epoch 55 | loss: 0.09052 | train_logloss: 0.0824  | train_accuracy: 0.97333 | valid_logloss: 0.0824  | valid_accuracy: 0.97333 |  0:00:51s\n",
            "epoch 56 | loss: 0.08828 | train_logloss: 0.08885 | train_accuracy: 0.97064 | valid_logloss: 0.08885 | valid_accuracy: 0.97064 |  0:00:52s\n",
            "epoch 57 | loss: 0.09075 | train_logloss: 0.08568 | train_accuracy: 0.9708  | valid_logloss: 0.08568 | valid_accuracy: 0.9708  |  0:00:53s\n",
            "epoch 58 | loss: 0.08734 | train_logloss: 0.08764 | train_accuracy: 0.97033 | valid_logloss: 0.08764 | valid_accuracy: 0.97033 |  0:00:53s\n",
            "epoch 59 | loss: 0.08719 | train_logloss: 0.07737 | train_accuracy: 0.97285 | valid_logloss: 0.07737 | valid_accuracy: 0.97285 |  0:00:54s\n",
            "epoch 60 | loss: 0.08135 | train_logloss: 0.07796 | train_accuracy: 0.97348 | valid_logloss: 0.07796 | valid_accuracy: 0.97348 |  0:00:55s\n",
            "epoch 61 | loss: 0.08394 | train_logloss: 0.07715 | train_accuracy: 0.97222 | valid_logloss: 0.07715 | valid_accuracy: 0.97222 |  0:00:56s\n",
            "epoch 62 | loss: 0.08714 | train_logloss: 0.08017 | train_accuracy: 0.97159 | valid_logloss: 0.08017 | valid_accuracy: 0.97159 |  0:00:57s\n",
            "epoch 63 | loss: 0.08392 | train_logloss: 0.0782  | train_accuracy: 0.97191 | valid_logloss: 0.0782  | valid_accuracy: 0.97191 |  0:00:58s\n",
            "epoch 64 | loss: 0.08209 | train_logloss: 0.07151 | train_accuracy: 0.97412 | valid_logloss: 0.07151 | valid_accuracy: 0.97412 |  0:00:59s\n",
            "epoch 65 | loss: 0.07934 | train_logloss: 0.07307 | train_accuracy: 0.97348 | valid_logloss: 0.07307 | valid_accuracy: 0.97348 |  0:01:00s\n",
            "epoch 66 | loss: 0.08017 | train_logloss: 0.06961 | train_accuracy: 0.97443 | valid_logloss: 0.06961 | valid_accuracy: 0.97443 |  0:01:01s\n",
            "epoch 67 | loss: 0.07375 | train_logloss: 0.07008 | train_accuracy: 0.97475 | valid_logloss: 0.07008 | valid_accuracy: 0.97475 |  0:01:02s\n",
            "epoch 68 | loss: 0.07494 | train_logloss: 0.07047 | train_accuracy: 0.97348 | valid_logloss: 0.07047 | valid_accuracy: 0.97348 |  0:01:03s\n",
            "epoch 69 | loss: 0.07356 | train_logloss: 0.07186 | train_accuracy: 0.97412 | valid_logloss: 0.07186 | valid_accuracy: 0.97412 |  0:01:03s\n",
            "epoch 70 | loss: 0.07739 | train_logloss: 0.06693 | train_accuracy: 0.97475 | valid_logloss: 0.06693 | valid_accuracy: 0.97475 |  0:01:04s\n",
            "epoch 71 | loss: 0.07609 | train_logloss: 0.06984 | train_accuracy: 0.97443 | valid_logloss: 0.06984 | valid_accuracy: 0.97443 |  0:01:05s\n",
            "epoch 72 | loss: 0.07776 | train_logloss: 0.06639 | train_accuracy: 0.97601 | valid_logloss: 0.06639 | valid_accuracy: 0.97601 |  0:01:06s\n",
            "epoch 73 | loss: 0.07657 | train_logloss: 0.06845 | train_accuracy: 0.97617 | valid_logloss: 0.06845 | valid_accuracy: 0.97617 |  0:01:07s\n",
            "epoch 74 | loss: 0.07465 | train_logloss: 0.07296 | train_accuracy: 0.97206 | valid_logloss: 0.07296 | valid_accuracy: 0.97206 |  0:01:08s\n",
            "epoch 75 | loss: 0.0778  | train_logloss: 0.07289 | train_accuracy: 0.9738  | valid_logloss: 0.07289 | valid_accuracy: 0.9738  |  0:01:09s\n",
            "epoch 76 | loss: 0.07651 | train_logloss: 0.07426 | train_accuracy: 0.97617 | valid_logloss: 0.07426 | valid_accuracy: 0.97617 |  0:01:10s\n",
            "epoch 77 | loss: 0.07439 | train_logloss: 0.07206 | train_accuracy: 0.97459 | valid_logloss: 0.07206 | valid_accuracy: 0.97459 |  0:01:11s\n",
            "epoch 78 | loss: 0.07756 | train_logloss: 0.06727 | train_accuracy: 0.97396 | valid_logloss: 0.06727 | valid_accuracy: 0.97396 |  0:01:12s\n",
            "epoch 79 | loss: 0.0766  | train_logloss: 0.07127 | train_accuracy: 0.97475 | valid_logloss: 0.07127 | valid_accuracy: 0.97475 |  0:01:13s\n",
            "epoch 80 | loss: 0.07307 | train_logloss: 0.06421 | train_accuracy: 0.97491 | valid_logloss: 0.06421 | valid_accuracy: 0.97491 |  0:01:14s\n",
            "epoch 81 | loss: 0.07059 | train_logloss: 0.06541 | train_accuracy: 0.97538 | valid_logloss: 0.06541 | valid_accuracy: 0.97538 |  0:01:14s\n",
            "epoch 82 | loss: 0.07076 | train_logloss: 0.06453 | train_accuracy: 0.97601 | valid_logloss: 0.06453 | valid_accuracy: 0.97601 |  0:01:15s\n",
            "epoch 83 | loss: 0.07127 | train_logloss: 0.06276 | train_accuracy: 0.97617 | valid_logloss: 0.06276 | valid_accuracy: 0.97617 |  0:01:16s\n",
            "epoch 84 | loss: 0.0683  | train_logloss: 0.0664  | train_accuracy: 0.97427 | valid_logloss: 0.0664  | valid_accuracy: 0.97427 |  0:01:17s\n",
            "epoch 85 | loss: 0.07971 | train_logloss: 0.06843 | train_accuracy: 0.97427 | valid_logloss: 0.06843 | valid_accuracy: 0.97427 |  0:01:18s\n",
            "epoch 86 | loss: 0.07617 | train_logloss: 0.06645 | train_accuracy: 0.97427 | valid_logloss: 0.06645 | valid_accuracy: 0.97427 |  0:01:19s\n",
            "epoch 87 | loss: 0.07563 | train_logloss: 0.07466 | train_accuracy: 0.97143 | valid_logloss: 0.07466 | valid_accuracy: 0.97143 |  0:01:20s\n",
            "epoch 88 | loss: 0.06945 | train_logloss: 0.0658  | train_accuracy: 0.97491 | valid_logloss: 0.0658  | valid_accuracy: 0.97491 |  0:01:21s\n",
            "epoch 89 | loss: 0.07678 | train_logloss: 0.06651 | train_accuracy: 0.97491 | valid_logloss: 0.06651 | valid_accuracy: 0.97491 |  0:01:22s\n",
            "epoch 90 | loss: 0.07322 | train_logloss: 0.06679 | train_accuracy: 0.9738  | valid_logloss: 0.06679 | valid_accuracy: 0.9738  |  0:01:23s\n",
            "epoch 91 | loss: 0.07121 | train_logloss: 0.06116 | train_accuracy: 0.97743 | valid_logloss: 0.06116 | valid_accuracy: 0.97743 |  0:01:23s\n",
            "epoch 92 | loss: 0.07435 | train_logloss: 0.07292 | train_accuracy: 0.97412 | valid_logloss: 0.07292 | valid_accuracy: 0.97412 |  0:01:24s\n",
            "epoch 93 | loss: 0.07412 | train_logloss: 0.06588 | train_accuracy: 0.97506 | valid_logloss: 0.06588 | valid_accuracy: 0.97506 |  0:01:25s\n",
            "epoch 94 | loss: 0.07337 | train_logloss: 0.06984 | train_accuracy: 0.97317 | valid_logloss: 0.06984 | valid_accuracy: 0.97317 |  0:01:26s\n",
            "epoch 95 | loss: 0.07071 | train_logloss: 0.06215 | train_accuracy: 0.97822 | valid_logloss: 0.06215 | valid_accuracy: 0.97822 |  0:01:27s\n",
            "epoch 96 | loss: 0.06944 | train_logloss: 0.07079 | train_accuracy: 0.97159 | valid_logloss: 0.07079 | valid_accuracy: 0.97159 |  0:01:28s\n",
            "epoch 97 | loss: 0.06824 | train_logloss: 0.0608  | train_accuracy: 0.97743 | valid_logloss: 0.0608  | valid_accuracy: 0.97743 |  0:01:29s\n",
            "epoch 98 | loss: 0.06454 | train_logloss: 0.06145 | train_accuracy: 0.97696 | valid_logloss: 0.06145 | valid_accuracy: 0.97696 |  0:01:30s\n",
            "epoch 99 | loss: 0.06694 | train_logloss: 0.06029 | train_accuracy: 0.97664 | valid_logloss: 0.06029 | valid_accuracy: 0.97664 |  0:01:31s\n",
            "epoch 100| loss: 0.06556 | train_logloss: 0.05932 | train_accuracy: 0.97775 | valid_logloss: 0.05932 | valid_accuracy: 0.97775 |  0:01:32s\n",
            "epoch 101| loss: 0.06675 | train_logloss: 0.05395 | train_accuracy: 0.97917 | valid_logloss: 0.05395 | valid_accuracy: 0.97917 |  0:01:33s\n",
            "epoch 102| loss: 0.062   | train_logloss: 0.05613 | train_accuracy: 0.97838 | valid_logloss: 0.05613 | valid_accuracy: 0.97838 |  0:01:33s\n",
            "epoch 103| loss: 0.06108 | train_logloss: 0.06239 | train_accuracy: 0.97664 | valid_logloss: 0.06239 | valid_accuracy: 0.97664 |  0:01:34s\n",
            "epoch 104| loss: 0.06351 | train_logloss: 0.05712 | train_accuracy: 0.97822 | valid_logloss: 0.05712 | valid_accuracy: 0.97822 |  0:01:35s\n",
            "epoch 105| loss: 0.06535 | train_logloss: 0.05548 | train_accuracy: 0.97996 | valid_logloss: 0.05548 | valid_accuracy: 0.97996 |  0:01:36s\n",
            "epoch 106| loss: 0.06049 | train_logloss: 0.0539  | train_accuracy: 0.97964 | valid_logloss: 0.0539  | valid_accuracy: 0.97964 |  0:01:37s\n",
            "epoch 107| loss: 0.06115 | train_logloss: 0.05366 | train_accuracy: 0.97996 | valid_logloss: 0.05366 | valid_accuracy: 0.97996 |  0:01:38s\n",
            "epoch 108| loss: 0.06148 | train_logloss: 0.05613 | train_accuracy: 0.97932 | valid_logloss: 0.05613 | valid_accuracy: 0.97932 |  0:01:39s\n",
            "epoch 109| loss: 0.06888 | train_logloss: 0.05847 | train_accuracy: 0.97743 | valid_logloss: 0.05847 | valid_accuracy: 0.97743 |  0:01:40s\n",
            "epoch 110| loss: 0.06565 | train_logloss: 0.05363 | train_accuracy: 0.97838 | valid_logloss: 0.05363 | valid_accuracy: 0.97838 |  0:01:41s\n",
            "epoch 111| loss: 0.06202 | train_logloss: 0.05983 | train_accuracy: 0.97822 | valid_logloss: 0.05983 | valid_accuracy: 0.97822 |  0:01:42s\n",
            "epoch 112| loss: 0.06617 | train_logloss: 0.05483 | train_accuracy: 0.9798  | valid_logloss: 0.05483 | valid_accuracy: 0.9798  |  0:01:43s\n",
            "epoch 113| loss: 0.06687 | train_logloss: 0.05393 | train_accuracy: 0.98027 | valid_logloss: 0.05393 | valid_accuracy: 0.98027 |  0:01:44s\n",
            "epoch 114| loss: 0.06132 | train_logloss: 0.05666 | train_accuracy: 0.97964 | valid_logloss: 0.05666 | valid_accuracy: 0.97964 |  0:01:44s\n",
            "epoch 115| loss: 0.06405 | train_logloss: 0.05682 | train_accuracy: 0.97838 | valid_logloss: 0.05682 | valid_accuracy: 0.97838 |  0:01:45s\n",
            "epoch 116| loss: 0.05999 | train_logloss: 0.05564 | train_accuracy: 0.97885 | valid_logloss: 0.05564 | valid_accuracy: 0.97885 |  0:01:46s\n",
            "epoch 117| loss: 0.05902 | train_logloss: 0.05213 | train_accuracy: 0.98232 | valid_logloss: 0.05213 | valid_accuracy: 0.98232 |  0:01:47s\n",
            "epoch 118| loss: 0.0591  | train_logloss: 0.05061 | train_accuracy: 0.98169 | valid_logloss: 0.05061 | valid_accuracy: 0.98169 |  0:01:48s\n",
            "epoch 119| loss: 0.05706 | train_logloss: 0.0598  | train_accuracy: 0.97838 | valid_logloss: 0.0598  | valid_accuracy: 0.97838 |  0:01:49s\n",
            "epoch 120| loss: 0.05705 | train_logloss: 0.05041 | train_accuracy: 0.98122 | valid_logloss: 0.05041 | valid_accuracy: 0.98122 |  0:01:50s\n",
            "epoch 121| loss: 0.05628 | train_logloss: 0.05353 | train_accuracy: 0.98138 | valid_logloss: 0.05353 | valid_accuracy: 0.98138 |  0:01:51s\n",
            "epoch 122| loss: 0.05919 | train_logloss: 0.05134 | train_accuracy: 0.98027 | valid_logloss: 0.05134 | valid_accuracy: 0.98027 |  0:01:52s\n",
            "epoch 123| loss: 0.06159 | train_logloss: 0.05345 | train_accuracy: 0.97996 | valid_logloss: 0.05345 | valid_accuracy: 0.97996 |  0:01:53s\n",
            "epoch 124| loss: 0.05985 | train_logloss: 0.05476 | train_accuracy: 0.97885 | valid_logloss: 0.05476 | valid_accuracy: 0.97885 |  0:01:54s\n",
            "epoch 125| loss: 0.06445 | train_logloss: 0.07173 | train_accuracy: 0.97475 | valid_logloss: 0.07173 | valid_accuracy: 0.97475 |  0:01:54s\n",
            "epoch 126| loss: 0.07223 | train_logloss: 0.07019 | train_accuracy: 0.97427 | valid_logloss: 0.07019 | valid_accuracy: 0.97427 |  0:01:55s\n",
            "epoch 127| loss: 0.08953 | train_logloss: 0.07839 | train_accuracy: 0.97238 | valid_logloss: 0.07839 | valid_accuracy: 0.97238 |  0:01:56s\n",
            "epoch 128| loss: 0.08989 | train_logloss: 0.07851 | train_accuracy: 0.97238 | valid_logloss: 0.07851 | valid_accuracy: 0.97238 |  0:01:57s\n",
            "epoch 129| loss: 0.084   | train_logloss: 0.07416 | train_accuracy: 0.97222 | valid_logloss: 0.07416 | valid_accuracy: 0.97222 |  0:01:58s\n",
            "epoch 130| loss: 0.07837 | train_logloss: 0.0743  | train_accuracy: 0.97017 | valid_logloss: 0.0743  | valid_accuracy: 0.97017 |  0:01:59s\n",
            "epoch 131| loss: 0.07875 | train_logloss: 0.06648 | train_accuracy: 0.97538 | valid_logloss: 0.06648 | valid_accuracy: 0.97538 |  0:02:00s\n",
            "epoch 132| loss: 0.07439 | train_logloss: 0.06207 | train_accuracy: 0.97633 | valid_logloss: 0.06207 | valid_accuracy: 0.97633 |  0:02:01s\n",
            "epoch 133| loss: 0.07138 | train_logloss: 0.06314 | train_accuracy: 0.97475 | valid_logloss: 0.06314 | valid_accuracy: 0.97475 |  0:02:02s\n",
            "epoch 134| loss: 0.07312 | train_logloss: 0.0642  | train_accuracy: 0.97664 | valid_logloss: 0.0642  | valid_accuracy: 0.97664 |  0:02:03s\n",
            "epoch 135| loss: 0.06962 | train_logloss: 0.06473 | train_accuracy: 0.97522 | valid_logloss: 0.06473 | valid_accuracy: 0.97522 |  0:02:04s\n",
            "epoch 136| loss: 0.07334 | train_logloss: 0.06793 | train_accuracy: 0.97491 | valid_logloss: 0.06793 | valid_accuracy: 0.97491 |  0:02:04s\n",
            "epoch 137| loss: 0.07652 | train_logloss: 0.06671 | train_accuracy: 0.97601 | valid_logloss: 0.06671 | valid_accuracy: 0.97601 |  0:02:05s\n",
            "epoch 138| loss: 0.07811 | train_logloss: 0.06983 | train_accuracy: 0.97443 | valid_logloss: 0.06983 | valid_accuracy: 0.97443 |  0:02:06s\n",
            "epoch 139| loss: 0.07689 | train_logloss: 0.07178 | train_accuracy: 0.97412 | valid_logloss: 0.07178 | valid_accuracy: 0.97412 |  0:02:07s\n",
            "epoch 140| loss: 0.07185 | train_logloss: 0.06665 | train_accuracy: 0.97569 | valid_logloss: 0.06665 | valid_accuracy: 0.97569 |  0:02:08s\n",
            "epoch 141| loss: 0.06866 | train_logloss: 0.05758 | train_accuracy: 0.97869 | valid_logloss: 0.05758 | valid_accuracy: 0.97869 |  0:02:09s\n",
            "epoch 142| loss: 0.06478 | train_logloss: 0.05915 | train_accuracy: 0.97885 | valid_logloss: 0.05915 | valid_accuracy: 0.97885 |  0:02:10s\n",
            "epoch 143| loss: 0.0635  | train_logloss: 0.05577 | train_accuracy: 0.97885 | valid_logloss: 0.05577 | valid_accuracy: 0.97885 |  0:02:11s\n",
            "epoch 144| loss: 0.06706 | train_logloss: 0.05761 | train_accuracy: 0.97838 | valid_logloss: 0.05761 | valid_accuracy: 0.97838 |  0:02:12s\n",
            "epoch 145| loss: 0.05925 | train_logloss: 0.06098 | train_accuracy: 0.97806 | valid_logloss: 0.06098 | valid_accuracy: 0.97806 |  0:02:13s\n",
            "epoch 146| loss: 0.0615  | train_logloss: 0.05198 | train_accuracy: 0.98153 | valid_logloss: 0.05198 | valid_accuracy: 0.98153 |  0:02:14s\n",
            "epoch 147| loss: 0.05632 | train_logloss: 0.05228 | train_accuracy: 0.98138 | valid_logloss: 0.05228 | valid_accuracy: 0.98138 |  0:02:14s\n",
            "epoch 148| loss: 0.05608 | train_logloss: 0.05512 | train_accuracy: 0.98074 | valid_logloss: 0.05512 | valid_accuracy: 0.98074 |  0:02:15s\n",
            "epoch 149| loss: 0.05909 | train_logloss: 0.05133 | train_accuracy: 0.97885 | valid_logloss: 0.05133 | valid_accuracy: 0.97885 |  0:02:16s\n",
            "epoch 150| loss: 0.05647 | train_logloss: 0.04979 | train_accuracy: 0.9798  | valid_logloss: 0.04979 | valid_accuracy: 0.9798  |  0:02:17s\n",
            "epoch 151| loss: 0.05669 | train_logloss: 0.04805 | train_accuracy: 0.98169 | valid_logloss: 0.04805 | valid_accuracy: 0.98169 |  0:02:18s\n",
            "epoch 152| loss: 0.05724 | train_logloss: 0.04871 | train_accuracy: 0.98169 | valid_logloss: 0.04871 | valid_accuracy: 0.98169 |  0:02:19s\n",
            "epoch 153| loss: 0.05312 | train_logloss: 0.06045 | train_accuracy: 0.97885 | valid_logloss: 0.06045 | valid_accuracy: 0.97885 |  0:02:20s\n",
            "epoch 154| loss: 0.05462 | train_logloss: 0.0491  | train_accuracy: 0.9809  | valid_logloss: 0.0491  | valid_accuracy: 0.9809  |  0:02:21s\n",
            "epoch 155| loss: 0.05813 | train_logloss: 0.04799 | train_accuracy: 0.98248 | valid_logloss: 0.04799 | valid_accuracy: 0.98248 |  0:02:22s\n",
            "epoch 156| loss: 0.05651 | train_logloss: 0.05093 | train_accuracy: 0.98138 | valid_logloss: 0.05093 | valid_accuracy: 0.98138 |  0:02:23s\n",
            "epoch 157| loss: 0.05825 | train_logloss: 0.05005 | train_accuracy: 0.98248 | valid_logloss: 0.05005 | valid_accuracy: 0.98248 |  0:02:23s\n",
            "epoch 158| loss: 0.05852 | train_logloss: 0.05268 | train_accuracy: 0.98059 | valid_logloss: 0.05268 | valid_accuracy: 0.98059 |  0:02:24s\n",
            "epoch 159| loss: 0.05447 | train_logloss: 0.05033 | train_accuracy: 0.98106 | valid_logloss: 0.05033 | valid_accuracy: 0.98106 |  0:02:25s\n",
            "epoch 160| loss: 0.05795 | train_logloss: 0.05141 | train_accuracy: 0.98122 | valid_logloss: 0.05141 | valid_accuracy: 0.98122 |  0:02:26s\n",
            "epoch 161| loss: 0.05259 | train_logloss: 0.04945 | train_accuracy: 0.98059 | valid_logloss: 0.04945 | valid_accuracy: 0.98059 |  0:02:27s\n",
            "epoch 162| loss: 0.05672 | train_logloss: 0.05311 | train_accuracy: 0.9798  | valid_logloss: 0.05311 | valid_accuracy: 0.9798  |  0:02:28s\n",
            "epoch 163| loss: 0.06077 | train_logloss: 0.04803 | train_accuracy: 0.98185 | valid_logloss: 0.04803 | valid_accuracy: 0.98185 |  0:02:29s\n",
            "epoch 164| loss: 0.05838 | train_logloss: 0.05253 | train_accuracy: 0.98074 | valid_logloss: 0.05253 | valid_accuracy: 0.98074 |  0:02:30s\n",
            "epoch 165| loss: 0.0557  | train_logloss: 0.05088 | train_accuracy: 0.9809  | valid_logloss: 0.05088 | valid_accuracy: 0.9809  |  0:02:31s\n",
            "epoch 166| loss: 0.06126 | train_logloss: 0.05146 | train_accuracy: 0.97932 | valid_logloss: 0.05146 | valid_accuracy: 0.97932 |  0:02:32s\n",
            "epoch 167| loss: 0.05224 | train_logloss: 0.0438  | train_accuracy: 0.98359 | valid_logloss: 0.0438  | valid_accuracy: 0.98359 |  0:02:33s\n",
            "epoch 168| loss: 0.05504 | train_logloss: 0.04854 | train_accuracy: 0.98138 | valid_logloss: 0.04854 | valid_accuracy: 0.98138 |  0:02:33s\n",
            "epoch 169| loss: 0.05275 | train_logloss: 0.04715 | train_accuracy: 0.98138 | valid_logloss: 0.04715 | valid_accuracy: 0.98138 |  0:02:34s\n",
            "epoch 170| loss: 0.04956 | train_logloss: 0.04252 | train_accuracy: 0.98343 | valid_logloss: 0.04252 | valid_accuracy: 0.98343 |  0:02:35s\n",
            "epoch 171| loss: 0.05182 | train_logloss: 0.04479 | train_accuracy: 0.98374 | valid_logloss: 0.04479 | valid_accuracy: 0.98374 |  0:02:36s\n",
            "epoch 172| loss: 0.05824 | train_logloss: 0.05108 | train_accuracy: 0.97996 | valid_logloss: 0.05108 | valid_accuracy: 0.97996 |  0:02:37s\n",
            "epoch 173| loss: 0.05199 | train_logloss: 0.04593 | train_accuracy: 0.98343 | valid_logloss: 0.04593 | valid_accuracy: 0.98343 |  0:02:38s\n",
            "epoch 174| loss: 0.05522 | train_logloss: 0.05163 | train_accuracy: 0.98059 | valid_logloss: 0.05163 | valid_accuracy: 0.98059 |  0:02:39s\n",
            "epoch 175| loss: 0.05596 | train_logloss: 0.04922 | train_accuracy: 0.98106 | valid_logloss: 0.04922 | valid_accuracy: 0.98106 |  0:02:40s\n",
            "epoch 176| loss: 0.05067 | train_logloss: 0.04265 | train_accuracy: 0.98485 | valid_logloss: 0.04265 | valid_accuracy: 0.98485 |  0:02:41s\n",
            "epoch 177| loss: 0.04795 | train_logloss: 0.04479 | train_accuracy: 0.98311 | valid_logloss: 0.04479 | valid_accuracy: 0.98311 |  0:02:42s\n",
            "epoch 178| loss: 0.05156 | train_logloss: 0.04969 | train_accuracy: 0.98074 | valid_logloss: 0.04969 | valid_accuracy: 0.98074 |  0:02:43s\n",
            "epoch 179| loss: 0.05083 | train_logloss: 0.0501  | train_accuracy: 0.98059 | valid_logloss: 0.0501  | valid_accuracy: 0.98059 |  0:02:43s\n",
            "epoch 180| loss: 0.05089 | train_logloss: 0.04316 | train_accuracy: 0.98359 | valid_logloss: 0.04316 | valid_accuracy: 0.98359 |  0:02:44s\n",
            "epoch 181| loss: 0.05223 | train_logloss: 0.04165 | train_accuracy: 0.9839  | valid_logloss: 0.04165 | valid_accuracy: 0.9839  |  0:02:45s\n",
            "epoch 182| loss: 0.05102 | train_logloss: 0.04662 | train_accuracy: 0.98343 | valid_logloss: 0.04662 | valid_accuracy: 0.98343 |  0:02:46s\n",
            "epoch 183| loss: 0.05613 | train_logloss: 0.04564 | train_accuracy: 0.98169 | valid_logloss: 0.04564 | valid_accuracy: 0.98169 |  0:02:47s\n",
            "epoch 184| loss: 0.04839 | train_logloss: 0.04462 | train_accuracy: 0.98438 | valid_logloss: 0.04462 | valid_accuracy: 0.98438 |  0:02:48s\n",
            "epoch 185| loss: 0.04756 | train_logloss: 0.04159 | train_accuracy: 0.98406 | valid_logloss: 0.04159 | valid_accuracy: 0.98406 |  0:02:49s\n",
            "epoch 186| loss: 0.04853 | train_logloss: 0.04614 | train_accuracy: 0.98185 | valid_logloss: 0.04614 | valid_accuracy: 0.98185 |  0:02:50s\n",
            "epoch 187| loss: 0.05393 | train_logloss: 0.04417 | train_accuracy: 0.98343 | valid_logloss: 0.04417 | valid_accuracy: 0.98343 |  0:02:51s\n",
            "epoch 188| loss: 0.04695 | train_logloss: 0.04754 | train_accuracy: 0.98232 | valid_logloss: 0.04754 | valid_accuracy: 0.98232 |  0:02:52s\n",
            "epoch 189| loss: 0.04696 | train_logloss: 0.04246 | train_accuracy: 0.98438 | valid_logloss: 0.04246 | valid_accuracy: 0.98438 |  0:02:53s\n",
            "epoch 190| loss: 0.04808 | train_logloss: 0.04068 | train_accuracy: 0.98422 | valid_logloss: 0.04068 | valid_accuracy: 0.98422 |  0:02:53s\n",
            "epoch 191| loss: 0.05328 | train_logloss: 0.04974 | train_accuracy: 0.98138 | valid_logloss: 0.04974 | valid_accuracy: 0.98138 |  0:02:54s\n",
            "epoch 192| loss: 0.05489 | train_logloss: 0.04664 | train_accuracy: 0.98327 | valid_logloss: 0.04664 | valid_accuracy: 0.98327 |  0:02:55s\n",
            "epoch 193| loss: 0.05788 | train_logloss: 0.04677 | train_accuracy: 0.98169 | valid_logloss: 0.04677 | valid_accuracy: 0.98169 |  0:02:56s\n",
            "epoch 194| loss: 0.05317 | train_logloss: 0.04792 | train_accuracy: 0.98138 | valid_logloss: 0.04792 | valid_accuracy: 0.98138 |  0:02:57s\n",
            "epoch 195| loss: 0.05818 | train_logloss: 0.04955 | train_accuracy: 0.98217 | valid_logloss: 0.04955 | valid_accuracy: 0.98217 |  0:02:58s\n",
            "epoch 196| loss: 0.05737 | train_logloss: 0.04315 | train_accuracy: 0.98532 | valid_logloss: 0.04315 | valid_accuracy: 0.98532 |  0:02:59s\n",
            "epoch 197| loss: 0.04954 | train_logloss: 0.04448 | train_accuracy: 0.98359 | valid_logloss: 0.04448 | valid_accuracy: 0.98359 |  0:03:00s\n",
            "epoch 198| loss: 0.04738 | train_logloss: 0.04114 | train_accuracy: 0.98548 | valid_logloss: 0.04114 | valid_accuracy: 0.98548 |  0:03:01s\n",
            "epoch 199| loss: 0.04773 | train_logloss: 0.03878 | train_accuracy: 0.98532 | valid_logloss: 0.03878 | valid_accuracy: 0.98532 |  0:03:02s\n",
            "epoch 200| loss: 0.04328 | train_logloss: 0.04064 | train_accuracy: 0.9869  | valid_logloss: 0.04064 | valid_accuracy: 0.9869  |  0:03:03s\n",
            "epoch 201| loss: 0.05273 | train_logloss: 0.04865 | train_accuracy: 0.98422 | valid_logloss: 0.04865 | valid_accuracy: 0.98422 |  0:03:04s\n",
            "epoch 202| loss: 0.04988 | train_logloss: 0.0411  | train_accuracy: 0.98453 | valid_logloss: 0.0411  | valid_accuracy: 0.98453 |  0:03:04s\n",
            "epoch 203| loss: 0.04776 | train_logloss: 0.04038 | train_accuracy: 0.9839  | valid_logloss: 0.04038 | valid_accuracy: 0.9839  |  0:03:05s\n",
            "epoch 204| loss: 0.04259 | train_logloss: 0.03713 | train_accuracy: 0.98595 | valid_logloss: 0.03713 | valid_accuracy: 0.98595 |  0:03:06s\n",
            "epoch 205| loss: 0.04515 | train_logloss: 0.04005 | train_accuracy: 0.98438 | valid_logloss: 0.04005 | valid_accuracy: 0.98438 |  0:03:07s\n",
            "epoch 206| loss: 0.04914 | train_logloss: 0.03922 | train_accuracy: 0.98706 | valid_logloss: 0.03922 | valid_accuracy: 0.98706 |  0:03:08s\n",
            "epoch 207| loss: 0.04157 | train_logloss: 0.04301 | train_accuracy: 0.98485 | valid_logloss: 0.04301 | valid_accuracy: 0.98485 |  0:03:09s\n",
            "epoch 208| loss: 0.04736 | train_logloss: 0.04222 | train_accuracy: 0.98374 | valid_logloss: 0.04222 | valid_accuracy: 0.98374 |  0:03:10s\n",
            "epoch 209| loss: 0.04531 | train_logloss: 0.04597 | train_accuracy: 0.98248 | valid_logloss: 0.04597 | valid_accuracy: 0.98248 |  0:03:11s\n",
            "epoch 210| loss: 0.04869 | train_logloss: 0.04137 | train_accuracy: 0.98438 | valid_logloss: 0.04137 | valid_accuracy: 0.98438 |  0:03:12s\n",
            "epoch 211| loss: 0.04785 | train_logloss: 0.03824 | train_accuracy: 0.98438 | valid_logloss: 0.03824 | valid_accuracy: 0.98438 |  0:03:13s\n",
            "epoch 212| loss: 0.04531 | train_logloss: 0.03726 | train_accuracy: 0.98516 | valid_logloss: 0.03726 | valid_accuracy: 0.98516 |  0:03:14s\n",
            "epoch 213| loss: 0.04677 | train_logloss: 0.03803 | train_accuracy: 0.98501 | valid_logloss: 0.03803 | valid_accuracy: 0.98501 |  0:03:14s\n",
            "epoch 214| loss: 0.04149 | train_logloss: 0.03897 | train_accuracy: 0.98643 | valid_logloss: 0.03897 | valid_accuracy: 0.98643 |  0:03:15s\n",
            "epoch 215| loss: 0.0481  | train_logloss: 0.03905 | train_accuracy: 0.98548 | valid_logloss: 0.03905 | valid_accuracy: 0.98548 |  0:03:16s\n",
            "epoch 216| loss: 0.04725 | train_logloss: 0.04206 | train_accuracy: 0.98438 | valid_logloss: 0.04206 | valid_accuracy: 0.98438 |  0:03:17s\n",
            "epoch 217| loss: 0.04458 | train_logloss: 0.03496 | train_accuracy: 0.98674 | valid_logloss: 0.03496 | valid_accuracy: 0.98674 |  0:03:18s\n",
            "epoch 218| loss: 0.03991 | train_logloss: 0.04037 | train_accuracy: 0.98516 | valid_logloss: 0.04037 | valid_accuracy: 0.98516 |  0:03:19s\n",
            "epoch 219| loss: 0.04349 | train_logloss: 0.03505 | train_accuracy: 0.98706 | valid_logloss: 0.03505 | valid_accuracy: 0.98706 |  0:03:20s\n",
            "epoch 220| loss: 0.04655 | train_logloss: 0.03869 | train_accuracy: 0.98611 | valid_logloss: 0.03869 | valid_accuracy: 0.98611 |  0:03:21s\n",
            "epoch 221| loss: 0.04522 | train_logloss: 0.04281 | train_accuracy: 0.98327 | valid_logloss: 0.04281 | valid_accuracy: 0.98327 |  0:03:22s\n",
            "epoch 222| loss: 0.04434 | train_logloss: 0.03854 | train_accuracy: 0.98564 | valid_logloss: 0.03854 | valid_accuracy: 0.98564 |  0:03:23s\n",
            "epoch 223| loss: 0.04172 | train_logloss: 0.03297 | train_accuracy: 0.98785 | valid_logloss: 0.03297 | valid_accuracy: 0.98785 |  0:03:23s\n",
            "epoch 224| loss: 0.04089 | train_logloss: 0.03605 | train_accuracy: 0.98801 | valid_logloss: 0.03605 | valid_accuracy: 0.98801 |  0:03:24s\n",
            "epoch 225| loss: 0.04036 | train_logloss: 0.03205 | train_accuracy: 0.98879 | valid_logloss: 0.03205 | valid_accuracy: 0.98879 |  0:03:25s\n",
            "epoch 226| loss: 0.04507 | train_logloss: 0.03733 | train_accuracy: 0.98548 | valid_logloss: 0.03733 | valid_accuracy: 0.98548 |  0:03:26s\n",
            "epoch 227| loss: 0.05025 | train_logloss: 0.04042 | train_accuracy: 0.98485 | valid_logloss: 0.04042 | valid_accuracy: 0.98485 |  0:03:27s\n",
            "epoch 228| loss: 0.04602 | train_logloss: 0.03894 | train_accuracy: 0.9858  | valid_logloss: 0.03894 | valid_accuracy: 0.9858  |  0:03:28s\n",
            "epoch 229| loss: 0.04863 | train_logloss: 0.03713 | train_accuracy: 0.98564 | valid_logloss: 0.03713 | valid_accuracy: 0.98564 |  0:03:29s\n",
            "epoch 230| loss: 0.04354 | train_logloss: 0.03443 | train_accuracy: 0.98816 | valid_logloss: 0.03443 | valid_accuracy: 0.98816 |  0:03:30s\n",
            "epoch 231| loss: 0.04133 | train_logloss: 0.03863 | train_accuracy: 0.98595 | valid_logloss: 0.03863 | valid_accuracy: 0.98595 |  0:03:31s\n",
            "epoch 232| loss: 0.0416  | train_logloss: 0.0321  | train_accuracy: 0.98753 | valid_logloss: 0.0321  | valid_accuracy: 0.98753 |  0:03:32s\n",
            "epoch 233| loss: 0.04212 | train_logloss: 0.03335 | train_accuracy: 0.98737 | valid_logloss: 0.03335 | valid_accuracy: 0.98737 |  0:03:33s\n",
            "epoch 234| loss: 0.04063 | train_logloss: 0.03253 | train_accuracy: 0.98864 | valid_logloss: 0.03253 | valid_accuracy: 0.98864 |  0:03:34s\n",
            "epoch 235| loss: 0.04124 | train_logloss: 0.03182 | train_accuracy: 0.98943 | valid_logloss: 0.03182 | valid_accuracy: 0.98943 |  0:03:34s\n",
            "epoch 236| loss: 0.04477 | train_logloss: 0.03576 | train_accuracy: 0.98658 | valid_logloss: 0.03576 | valid_accuracy: 0.98658 |  0:03:35s\n",
            "epoch 237| loss: 0.04142 | train_logloss: 0.03403 | train_accuracy: 0.98737 | valid_logloss: 0.03403 | valid_accuracy: 0.98737 |  0:03:36s\n",
            "epoch 238| loss: 0.04391 | train_logloss: 0.03368 | train_accuracy: 0.98785 | valid_logloss: 0.03368 | valid_accuracy: 0.98785 |  0:03:37s\n",
            "epoch 239| loss: 0.03803 | train_logloss: 0.03246 | train_accuracy: 0.9869  | valid_logloss: 0.03246 | valid_accuracy: 0.9869  |  0:03:38s\n",
            "epoch 240| loss: 0.04225 | train_logloss: 0.03841 | train_accuracy: 0.98595 | valid_logloss: 0.03841 | valid_accuracy: 0.98595 |  0:03:39s\n",
            "epoch 241| loss: 0.04881 | train_logloss: 0.04521 | train_accuracy: 0.98406 | valid_logloss: 0.04521 | valid_accuracy: 0.98406 |  0:03:40s\n",
            "epoch 242| loss: 0.04795 | train_logloss: 0.03644 | train_accuracy: 0.98627 | valid_logloss: 0.03644 | valid_accuracy: 0.98627 |  0:03:41s\n",
            "epoch 243| loss: 0.04485 | train_logloss: 0.03181 | train_accuracy: 0.98879 | valid_logloss: 0.03181 | valid_accuracy: 0.98879 |  0:03:42s\n",
            "epoch 244| loss: 0.0422  | train_logloss: 0.03807 | train_accuracy: 0.98516 | valid_logloss: 0.03807 | valid_accuracy: 0.98516 |  0:03:43s\n",
            "epoch 245| loss: 0.04265 | train_logloss: 0.03235 | train_accuracy: 0.98816 | valid_logloss: 0.03235 | valid_accuracy: 0.98816 |  0:03:43s\n",
            "epoch 246| loss: 0.0405  | train_logloss: 0.0339  | train_accuracy: 0.98674 | valid_logloss: 0.0339  | valid_accuracy: 0.98674 |  0:03:44s\n",
            "epoch 247| loss: 0.04095 | train_logloss: 0.03078 | train_accuracy: 0.98832 | valid_logloss: 0.03078 | valid_accuracy: 0.98832 |  0:03:45s\n",
            "epoch 248| loss: 0.03844 | train_logloss: 0.03593 | train_accuracy: 0.98564 | valid_logloss: 0.03593 | valid_accuracy: 0.98564 |  0:03:46s\n",
            "epoch 249| loss: 0.03918 | train_logloss: 0.03514 | train_accuracy: 0.98769 | valid_logloss: 0.03514 | valid_accuracy: 0.98769 |  0:03:47s\n",
            "epoch 250| loss: 0.03921 | train_logloss: 0.0351  | train_accuracy: 0.98722 | valid_logloss: 0.0351  | valid_accuracy: 0.98722 |  0:03:48s\n",
            "epoch 251| loss: 0.04083 | train_logloss: 0.03252 | train_accuracy: 0.98785 | valid_logloss: 0.03252 | valid_accuracy: 0.98785 |  0:03:49s\n",
            "epoch 252| loss: 0.03739 | train_logloss: 0.0305  | train_accuracy: 0.98785 | valid_logloss: 0.0305  | valid_accuracy: 0.98785 |  0:03:50s\n",
            "epoch 253| loss: 0.03759 | train_logloss: 0.03293 | train_accuracy: 0.98801 | valid_logloss: 0.03293 | valid_accuracy: 0.98801 |  0:03:51s\n",
            "epoch 254| loss: 0.03489 | train_logloss: 0.02964 | train_accuracy: 0.98943 | valid_logloss: 0.02964 | valid_accuracy: 0.98943 |  0:03:52s\n",
            "epoch 255| loss: 0.03695 | train_logloss: 0.03044 | train_accuracy: 0.98974 | valid_logloss: 0.03044 | valid_accuracy: 0.98974 |  0:03:53s\n",
            "epoch 256| loss: 0.0363  | train_logloss: 0.03167 | train_accuracy: 0.98801 | valid_logloss: 0.03167 | valid_accuracy: 0.98801 |  0:03:53s\n",
            "epoch 257| loss: 0.03489 | train_logloss: 0.03157 | train_accuracy: 0.98769 | valid_logloss: 0.03157 | valid_accuracy: 0.98769 |  0:03:54s\n",
            "epoch 258| loss: 0.03628 | train_logloss: 0.03248 | train_accuracy: 0.98848 | valid_logloss: 0.03248 | valid_accuracy: 0.98848 |  0:03:55s\n",
            "epoch 259| loss: 0.0346  | train_logloss: 0.02746 | train_accuracy: 0.98848 | valid_logloss: 0.02746 | valid_accuracy: 0.98848 |  0:03:56s\n",
            "epoch 260| loss: 0.03604 | train_logloss: 0.03726 | train_accuracy: 0.98658 | valid_logloss: 0.03726 | valid_accuracy: 0.98658 |  0:03:57s\n",
            "epoch 261| loss: 0.03472 | train_logloss: 0.03186 | train_accuracy: 0.98801 | valid_logloss: 0.03186 | valid_accuracy: 0.98801 |  0:03:58s\n",
            "epoch 262| loss: 0.03762 | train_logloss: 0.03147 | train_accuracy: 0.98864 | valid_logloss: 0.03147 | valid_accuracy: 0.98864 |  0:03:59s\n",
            "epoch 263| loss: 0.03519 | train_logloss: 0.02823 | train_accuracy: 0.98974 | valid_logloss: 0.02823 | valid_accuracy: 0.98974 |  0:04:00s\n",
            "epoch 264| loss: 0.03681 | train_logloss: 0.02838 | train_accuracy: 0.98927 | valid_logloss: 0.02838 | valid_accuracy: 0.98927 |  0:04:01s\n",
            "epoch 265| loss: 0.03727 | train_logloss: 0.029   | train_accuracy: 0.98943 | valid_logloss: 0.029   | valid_accuracy: 0.98943 |  0:04:02s\n",
            "epoch 266| loss: 0.03788 | train_logloss: 0.03745 | train_accuracy: 0.9858  | valid_logloss: 0.03745 | valid_accuracy: 0.9858  |  0:04:03s\n",
            "epoch 267| loss: 0.03469 | train_logloss: 0.02674 | train_accuracy: 0.98943 | valid_logloss: 0.02674 | valid_accuracy: 0.98943 |  0:04:03s\n",
            "epoch 268| loss: 0.03611 | train_logloss: 0.02715 | train_accuracy: 0.99021 | valid_logloss: 0.02715 | valid_accuracy: 0.99021 |  0:04:04s\n",
            "epoch 269| loss: 0.03534 | train_logloss: 0.02908 | train_accuracy: 0.98879 | valid_logloss: 0.02908 | valid_accuracy: 0.98879 |  0:04:05s\n",
            "epoch 270| loss: 0.03465 | train_logloss: 0.02901 | train_accuracy: 0.98879 | valid_logloss: 0.02901 | valid_accuracy: 0.98879 |  0:04:06s\n",
            "epoch 271| loss: 0.03424 | train_logloss: 0.02875 | train_accuracy: 0.99006 | valid_logloss: 0.02875 | valid_accuracy: 0.99006 |  0:04:07s\n",
            "epoch 272| loss: 0.03219 | train_logloss: 0.02524 | train_accuracy: 0.98943 | valid_logloss: 0.02524 | valid_accuracy: 0.98943 |  0:04:08s\n",
            "epoch 273| loss: 0.04044 | train_logloss: 0.03133 | train_accuracy: 0.98801 | valid_logloss: 0.03133 | valid_accuracy: 0.98801 |  0:04:09s\n",
            "epoch 274| loss: 0.03807 | train_logloss: 0.03772 | train_accuracy: 0.98548 | valid_logloss: 0.03772 | valid_accuracy: 0.98548 |  0:04:10s\n",
            "epoch 275| loss: 0.03803 | train_logloss: 0.03789 | train_accuracy: 0.9858  | valid_logloss: 0.03789 | valid_accuracy: 0.9858  |  0:04:11s\n",
            "epoch 276| loss: 0.03675 | train_logloss: 0.02782 | train_accuracy: 0.99085 | valid_logloss: 0.02782 | valid_accuracy: 0.99085 |  0:04:12s\n",
            "epoch 277| loss: 0.0355  | train_logloss: 0.02309 | train_accuracy: 0.99148 | valid_logloss: 0.02309 | valid_accuracy: 0.99148 |  0:04:13s\n",
            "epoch 278| loss: 0.03575 | train_logloss: 0.04026 | train_accuracy: 0.98532 | valid_logloss: 0.04026 | valid_accuracy: 0.98532 |  0:04:13s\n",
            "epoch 279| loss: 0.03847 | train_logloss: 0.03159 | train_accuracy: 0.98958 | valid_logloss: 0.03159 | valid_accuracy: 0.98958 |  0:04:14s\n",
            "epoch 280| loss: 0.03444 | train_logloss: 0.02699 | train_accuracy: 0.98911 | valid_logloss: 0.02699 | valid_accuracy: 0.98911 |  0:04:15s\n",
            "epoch 281| loss: 0.03116 | train_logloss: 0.02538 | train_accuracy: 0.99132 | valid_logloss: 0.02538 | valid_accuracy: 0.99132 |  0:04:16s\n",
            "epoch 282| loss: 0.03211 | train_logloss: 0.03737 | train_accuracy: 0.9869  | valid_logloss: 0.03737 | valid_accuracy: 0.9869  |  0:04:17s\n",
            "epoch 283| loss: 0.02726 | train_logloss: 0.02457 | train_accuracy: 0.99132 | valid_logloss: 0.02457 | valid_accuracy: 0.99132 |  0:04:18s\n",
            "epoch 284| loss: 0.02932 | train_logloss: 0.02339 | train_accuracy: 0.99195 | valid_logloss: 0.02339 | valid_accuracy: 0.99195 |  0:04:19s\n",
            "epoch 285| loss: 0.02979 | train_logloss: 0.02276 | train_accuracy: 0.99132 | valid_logloss: 0.02276 | valid_accuracy: 0.99132 |  0:04:20s\n",
            "epoch 286| loss: 0.03147 | train_logloss: 0.0328  | train_accuracy: 0.98769 | valid_logloss: 0.0328  | valid_accuracy: 0.98769 |  0:04:21s\n",
            "epoch 287| loss: 0.03318 | train_logloss: 0.02507 | train_accuracy: 0.99195 | valid_logloss: 0.02507 | valid_accuracy: 0.99195 |  0:04:22s\n",
            "epoch 288| loss: 0.03027 | train_logloss: 0.02348 | train_accuracy: 0.99195 | valid_logloss: 0.02348 | valid_accuracy: 0.99195 |  0:04:22s\n",
            "epoch 289| loss: 0.02782 | train_logloss: 0.02192 | train_accuracy: 0.99195 | valid_logloss: 0.02192 | valid_accuracy: 0.99195 |  0:04:23s\n",
            "epoch 290| loss: 0.03067 | train_logloss: 0.02399 | train_accuracy: 0.991   | valid_logloss: 0.02399 | valid_accuracy: 0.991   |  0:04:24s\n",
            "epoch 291| loss: 0.03368 | train_logloss: 0.05237 | train_accuracy: 0.98374 | valid_logloss: 0.05237 | valid_accuracy: 0.98374 |  0:04:25s\n",
            "epoch 292| loss: 0.04582 | train_logloss: 0.04677 | train_accuracy: 0.98343 | valid_logloss: 0.04677 | valid_accuracy: 0.98343 |  0:04:26s\n",
            "epoch 293| loss: 0.05098 | train_logloss: 0.0288  | train_accuracy: 0.99053 | valid_logloss: 0.0288  | valid_accuracy: 0.99053 |  0:04:27s\n",
            "epoch 294| loss: 0.03615 | train_logloss: 0.03    | train_accuracy: 0.98832 | valid_logloss: 0.03    | valid_accuracy: 0.98832 |  0:04:28s\n",
            "epoch 295| loss: 0.03771 | train_logloss: 0.03992 | train_accuracy: 0.98643 | valid_logloss: 0.03992 | valid_accuracy: 0.98643 |  0:04:29s\n",
            "epoch 296| loss: 0.03901 | train_logloss: 0.03094 | train_accuracy: 0.98927 | valid_logloss: 0.03094 | valid_accuracy: 0.98927 |  0:04:30s\n",
            "epoch 297| loss: 0.03389 | train_logloss: 0.02519 | train_accuracy: 0.99179 | valid_logloss: 0.02519 | valid_accuracy: 0.99179 |  0:04:31s\n",
            "epoch 298| loss: 0.03057 | train_logloss: 0.02406 | train_accuracy: 0.99242 | valid_logloss: 0.02406 | valid_accuracy: 0.99242 |  0:04:32s\n",
            "epoch 299| loss: 0.02885 | train_logloss: 0.02537 | train_accuracy: 0.99148 | valid_logloss: 0.02537 | valid_accuracy: 0.99148 |  0:04:33s\n",
            "Stop training because you reached max_epochs = 300 with best_epoch = 298 and best_valid_accuracy = 0.99242\n",
            "Best weights from best epoch are automatically used!\n",
            "273.32322549819946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZybdKiJKGeLH"
      },
      "source": [
        "X_test = X_test.astype('float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UULXz-Dv9fHa",
        "outputId": "06c48268-d6de-49a0-d726-9943e5919c05"
      },
      "source": [
        "pred = clf.predict(X_test)\n",
        "print('Classification Report: \\n', classification_report(y_test, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "clf_train_acc = accuracy_score(y_train, clf.predict(X_train.astype('float')))\n",
        "print('Training Score: ', clf_train_acc)\n",
        "clf_test_acc = accuracy_score(y_test, pred)\n",
        "print('Testing Score: ', clf_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.96      0.97       116\n",
            "           2       0.98      0.98      0.98       399\n",
            "           3       0.95      0.96      0.95       190\n",
            "\n",
            "    accuracy                           0.97       705\n",
            "   macro avg       0.97      0.96      0.97       705\n",
            "weighted avg       0.97      0.97      0.97       705\n",
            "\n",
            "Confusion Matrix: \n",
            " [[111   2   3]\n",
            " [  1 391   7]\n",
            " [  0   8 182]]\n",
            "Training Score:  0.9924242424242424\n",
            "Testing Score:  0.9702127659574468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-idwAGPax34_",
        "outputId": "65fb091c-9df5-4c29-8383-4ff9471a5826"
      },
      "source": [
        "pred = clf.predict(X2)\n",
        "print('Classification Report: \\n', classification_report(y2, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y2, pred))\n",
        "clf_train_acc = accuracy_score(y_train, clf.predict(X_train.astype('float')))\n",
        "print('Training Score: ', clf_train_acc)\n",
        "clf_test_acc = accuracy_score(y2, pred)\n",
        "print('Training Score: ', clf_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.99      0.99      1152\n",
            "           2       0.99      0.99      0.99      3989\n",
            "           3       0.98      0.99      0.99      1900\n",
            "\n",
            "    accuracy                           0.99      7041\n",
            "   macro avg       0.99      0.99      0.99      7041\n",
            "weighted avg       0.99      0.99      0.99      7041\n",
            "\n",
            "Confusion Matrix: \n",
            " [[1139    7    6]\n",
            " [   5 3954   30]\n",
            " [   0   21 1879]]\n",
            "Training Score:  0.9924242424242424\n",
            "Training Score:  0.9902002556455048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xapiiG8E8ST"
      },
      "source": [
        "lgb_fit_params = {\n",
        "    'early_stopping_rounds': 20,\n",
        "    'verbose': False\n",
        "}\n",
        "lgb_class_params = {\n",
        "    'min_child_samples':np.random.randint(1, 50, size = 5),\n",
        "    'colsample_bytree': np.random.uniform(0.1, 0.9, size = 5),\n",
        "    'num_leaves' :      np.random.randint(10, 100, size = 5),\n",
        "    'min_child_weight': np.random.uniform(0.001, 0.99, size = 5),\n",
        "}\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "LGBM = RandomizedSearchCV(LGBMClassifier(learning_rate=0.001, n_estimators=300), lgb_class_params, random_state=0, cv=kf)\n",
        "search = LGBM.fit(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF5fwDP0iRC2",
        "outputId": "995fccde-78bd-41a2-ad6c-95c08621de4e"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8669, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzrryKbk65A9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fac1fcd-34cb-42cd-8676-91a2a4e4b023"
      },
      "source": [
        "len(gb.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy6wg231azac",
        "outputId": "047fce4d-b94a-47f8-a93f-53275deae98a"
      },
      "source": [
        "np.where(gb.feature_importances_ == 366)[0].tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCeHKcO8bHGQ",
        "outputId": "f909dcf2-5979-4b70-b7a8-c5f094cf72bd"
      },
      "source": [
        "[columns_sdss[i] for i in np.where(gb.feature_importances_ == 930)[0].tolist()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sdssdr16_u-z_psf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfs6whYqbx2E"
      },
      "source": [
        "columns_not_ls = [i for i in columns_not_L if 'ls' not in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTOgfP3n2_p"
      },
      "source": [
        "columns_not_Err =  [i for i in columns if 'Err' not in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_BeXsfrosgZ"
      },
      "source": [
        "columns_not_Ivar = [i for i in columns_not_Err if 'Ivar' not in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yycpxwwpFul"
      },
      "source": [
        "columns_not_w = [i for i in columns_not_Ivar if 'w' not in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNtXF5YLpns0"
      },
      "source": [
        "columns_sdss = [i for i in columns_not_Ivar if 'sdss' in i] + ['LabelQ', 'LabelG', 'LabelS', 'Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5x0BfWgrkzm"
      },
      "source": [
        "columns_not_z = [i for i in columns_sdss if 'z' not in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdhkDK3qsOtH",
        "outputId": "5c01c26a-0a5f-4142-b323-ebbadc29a26b"
      },
      "source": [
        "columns_not_z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sdss_cModelFlux_u',\n",
              " 'sdss_cModelFlux_g',\n",
              " 'sdss_cModelFlux_r',\n",
              " 'sdss_cModelFlux_i',\n",
              " 'sdss_psfFlux_u',\n",
              " 'sdss_psfFlux_g',\n",
              " 'sdss_psfFlux_r',\n",
              " 'sdss_psfFlux_i',\n",
              " 'sdssdr16_u_psf',\n",
              " 'sdssdr16_g_psf',\n",
              " 'sdssdr16_r_psf',\n",
              " 'sdssdr16_i_psf',\n",
              " 'sdssdr16_u_cmodel',\n",
              " 'sdssdr16_g_cmodel',\n",
              " 'sdssdr16_r_cmodel',\n",
              " 'sdssdr16_i_cmodel',\n",
              " 'sdssdr16_u-g_psf',\n",
              " 'sdssdr16_u-r_psf',\n",
              " 'sdssdr16_u-i_psf',\n",
              " 'sdssdr16_u_psf-cmodel',\n",
              " 'sdssdr16_g-r_psf',\n",
              " 'sdssdr16_g-i_psf',\n",
              " 'sdssdr16_g_psf-cmodel',\n",
              " 'sdssdr16_r-i_psf',\n",
              " 'sdssdr16_r_psf-cmodel',\n",
              " 'sdssdr16_i_psf-cmodel',\n",
              " 'LabelQ',\n",
              " 'LabelG',\n",
              " 'LabelS',\n",
              " 'Label']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrws2b8ktWeD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}